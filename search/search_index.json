{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"basic-concepts/","text":"General concepts \u00b6 Application Programming Interface (API) \u00b6 An API is a well-defined interface that can be called from a remote process. There are many styles and protocols available to enable calls to APIs. The most popular at the moment is the REST API style that piggy-backs on top of the HTTP protocol that powers the Internet. Egeria makes extensive use of REST APIs. Further information Details of the different types of APIs provided by Egeria can be found in the developer guide . In addition, it is possible to automatically catalog details of the APIs that your organization uses: Cataloguing API Event \u00b6 An event is a message that describes a specific situation, or more typically a change in situation. It is sent on a topic to share its information with other servers. Further information Details of the different types of events used by Egeria OMRS topic events - for open metadata repository cohorts InTopic Events - for outgoing events to an Open Metadata Access Service ( OMAS ) OutTopic Events - for incoming events from an Open Metadata Access Service ( OMAS ) In addition, it is possible to automatically catalog details of the types of events that your organization uses: Cataloguing topics and event types for an event broker Event Broker \u00b6 An event broker is an infrastructure service that provides a publish-subscribe capability through topics . There are many different event broker implementations with greater or lesser reliability and performance. Egeria's default event broker is Apache Kafka . This is an open source event broker with a high level of performance, scalability and reliability. Many organizations establish a standard choice of their event broker service which is why Egeria uses connectors to connect to the event broker so that Apache Kafka can be swapped out for a different event broker implementation. As such, each topic is accessed through an open metadata topic connector . Further information Configuring the event broker for Egeria Cataloguing topics and event types for an event broker Topic \u00b6 A topic is a service provided by an event broker that offers a publish-subscribe capability for a specific type of event. Multiple servers can read and write events to a topic. Each topic maintains information about the events that a server has not read so that it can receive each event even if it restarts. Typically the events are processed in a first-in-first-out (FIFO) order, but that is not necessarily guaranteed since it depends on the type and configuration of the event broker. Further information Details of the different types of topics used by Egeria OMRSTopic - for open metadata repository cohorts InTopic - for sending events to an Open Metadata Access Service ( OMAS ) OutTopic - for receiving events from an Open Metadata Access Service ( OMAS ) In addition, it is possible to automatically catalog details of the event brokers that your organization uses: Cataloguing topics and event types for an event broker","title":"General Concepts"},{"location":"basic-concepts/#general-concepts","text":"","title":"General concepts"},{"location":"basic-concepts/#application-programming-interface-api","text":"An API is a well-defined interface that can be called from a remote process. There are many styles and protocols available to enable calls to APIs. The most popular at the moment is the REST API style that piggy-backs on top of the HTTP protocol that powers the Internet. Egeria makes extensive use of REST APIs. Further information Details of the different types of APIs provided by Egeria can be found in the developer guide . In addition, it is possible to automatically catalog details of the APIs that your organization uses: Cataloguing API","title":"Application Programming Interface (API)"},{"location":"basic-concepts/#event","text":"An event is a message that describes a specific situation, or more typically a change in situation. It is sent on a topic to share its information with other servers. Further information Details of the different types of events used by Egeria OMRS topic events - for open metadata repository cohorts InTopic Events - for outgoing events to an Open Metadata Access Service ( OMAS ) OutTopic Events - for incoming events from an Open Metadata Access Service ( OMAS ) In addition, it is possible to automatically catalog details of the types of events that your organization uses: Cataloguing topics and event types for an event broker","title":"Event"},{"location":"basic-concepts/#event-broker","text":"An event broker is an infrastructure service that provides a publish-subscribe capability through topics . There are many different event broker implementations with greater or lesser reliability and performance. Egeria's default event broker is Apache Kafka . This is an open source event broker with a high level of performance, scalability and reliability. Many organizations establish a standard choice of their event broker service which is why Egeria uses connectors to connect to the event broker so that Apache Kafka can be swapped out for a different event broker implementation. As such, each topic is accessed through an open metadata topic connector . Further information Configuring the event broker for Egeria Cataloguing topics and event types for an event broker","title":"Event Broker"},{"location":"basic-concepts/#topic","text":"A topic is a service provided by an event broker that offers a publish-subscribe capability for a specific type of event. Multiple servers can read and write events to a topic. Each topic maintains information about the events that a server has not read so that it can receive each event even if it restarts. Typically the events are processed in a first-in-first-out (FIFO) order, but that is not necessarily guaranteed since it depends on the type and configuration of the event broker. Further information Details of the different types of topics used by Egeria OMRSTopic - for open metadata repository cohorts InTopic - for sending events to an Open Metadata Access Service ( OMAS ) OutTopic - for receiving events from an Open Metadata Access Service ( OMAS ) In addition, it is possible to automatically catalog details of the event brokers that your organization uses: Cataloguing topics and event types for an event broker","title":"Topic"},{"location":"concepts/anchor/","text":"Anchor \u00b6 Anchors are Referenceable metadata entities that group other entities together. They act like containers. This means, for example, if the anchor entity is deleted then the entities anchored to this entity are also deleted. The value of establishing this grouping is to ensure that entities that have little meaning without their anchor entity are cleaned up properly and are not left to uselessly clutter the repository. Example: personal messages and profiles For example, if a personal message is attached to a personal profile then that personal profile is its anchor. If the personal profile is deleted then the personal message is deleted too. Anchored entities are also bound by the visibility and security restrictions of their anchor. Example: Assets For example, Asset visibility is controlled by governance zones . An Asset is only visible through a service if it is a member of that service's supportedZones . Similarly, authorization to perform specific operations on an Asset is granted by the Open Metadata Security Services . When a SchemaType is attached to an Asset, it is anchored to that Asset. Subsequent requests to read or update the SchemaType will result in visibility and authorization checks for the requesting user being made with respect to its Asset anchor. The anchor grouping is limited to particular types of metadata entities that are only meaningful in the context of their anchor entity. So not all metadata entities linked to an anchor are anchored to it. They have an independent existence and may be linked to many anchors, without obligation. Example: personal profiles and Assets For example, a personal profile may contain links to specific \"favorite\" Assets. When the personal profile is deleted, the assets are not effected - except that they lose their relationship to the personal profile. LatestChanges \u00b6 The LatestChange classification is attached to each anchor Referenceable. It is used to record the latest change to this anchor or any of the entities anchored to it. Example: SchemaElements and Comments So for example, if a hierarchy of SchemaElements, or a hierarchy of Comments (all Referenceables) were anchored to an Asset (also a Referenceable), then the LatestChange classification goes on the Asset and records changes to any of these entities. This includes changing property values, attaching or detaching entities through relationships as well as any changes to their classifications. Maintaining LatestChanges on Asset means that it is easier to monitor for changes affecting the Asset and any of its anchored entities. However, it also means that it must be easy to locate the Asset from any of the anchored entities when they change, even though they may not be directly connected. The Anchors classification makes it easier to find the anchor entity. It is attached to any entity anchored to a Referenceable. Example: SchemaElements and Comments In the example above, Anchors would be on all the SchemaElements and Comments. Anchors would contain the unique identifier ( GUID ) of the anchor Referenceable. There is also the unique identifier ( GUID ) of the SchemaType at the root of a schema structure and the unique identifier ( GUID ) of the Comment at root of a comment tree to make it easier to process these elements as a group. Following is an illustration of this example, with the addition of an Asset. The entities that have the Anchors classification are those that are anchored to the Asset. This includes entities such as Ratings, Likes and Attachments (from the Open Discovery Framework ( ODF ) . It is worthwhile maintaining the Anchors classification because reads of and updates to the anchored entities will happen many times, and it is rare that an anchored entity will change its anchor during its lifetime. If a GlossaryTerm , or InformalTag is attached to the Asset, they are not anchored to it. GlossaryTerms and InformalTags are independent entities. They are not anchored to the Asset and hence do not have an Anchors classification. Any change to these entities does not reflect in the LatestChange classification of the Asset. However, the act of attaching them to, or detaching them from the Asset is recorded in the Asset's LatestChange classification. Since the GlossaryTerm is also an anchor, when the GlossaryTerm and the Asset are linked together, this change is reflected in both of their LatestChange classifications because they are both Referenceable anchors. NoteLogs are Referenceables that can be attached to many other Referenceables. They can be set up either to be anchored with a single Referenceable or to be their own anchor to allow then to be attached to and detached from many Referenceables over its lifetime. Example: NoteLog and Referenceables For example, these are cases where the NoteLog is anchored to another Referenceable NoteLogs are used to support the personal blog linked off of the Personal Profile in Community Profile OMAS . Assets may have a NoteLog to record \"news\" for consumers such as planned maintenance and unexpected situations. Egeria uses the Anchors classification on a NoteLog to indicate that the NoteLog is tied to the Referenceable it is attached to. The presence of this classification would prevent it from being linked to another Referenceable. Following is an illustration of the additional objects connecting to an asset that do not have the Anchors classification because they are not anchored to the Asset. Also notice there are two NoteLogs attached to the asset, one with the Anchors classification and one without. The one with the Anchors classification is anchored to the the Asset. The one without the Anchors classification is independent of the Asset. Further information Generic Handlers provide support for the Anchors and LatestChange classifications.","title":"Anchor"},{"location":"concepts/anchor/#anchor","text":"Anchors are Referenceable metadata entities that group other entities together. They act like containers. This means, for example, if the anchor entity is deleted then the entities anchored to this entity are also deleted. The value of establishing this grouping is to ensure that entities that have little meaning without their anchor entity are cleaned up properly and are not left to uselessly clutter the repository. Example: personal messages and profiles For example, if a personal message is attached to a personal profile then that personal profile is its anchor. If the personal profile is deleted then the personal message is deleted too. Anchored entities are also bound by the visibility and security restrictions of their anchor. Example: Assets For example, Asset visibility is controlled by governance zones . An Asset is only visible through a service if it is a member of that service's supportedZones . Similarly, authorization to perform specific operations on an Asset is granted by the Open Metadata Security Services . When a SchemaType is attached to an Asset, it is anchored to that Asset. Subsequent requests to read or update the SchemaType will result in visibility and authorization checks for the requesting user being made with respect to its Asset anchor. The anchor grouping is limited to particular types of metadata entities that are only meaningful in the context of their anchor entity. So not all metadata entities linked to an anchor are anchored to it. They have an independent existence and may be linked to many anchors, without obligation. Example: personal profiles and Assets For example, a personal profile may contain links to specific \"favorite\" Assets. When the personal profile is deleted, the assets are not effected - except that they lose their relationship to the personal profile.","title":"Anchor"},{"location":"concepts/anchor/#latestchanges","text":"The LatestChange classification is attached to each anchor Referenceable. It is used to record the latest change to this anchor or any of the entities anchored to it. Example: SchemaElements and Comments So for example, if a hierarchy of SchemaElements, or a hierarchy of Comments (all Referenceables) were anchored to an Asset (also a Referenceable), then the LatestChange classification goes on the Asset and records changes to any of these entities. This includes changing property values, attaching or detaching entities through relationships as well as any changes to their classifications. Maintaining LatestChanges on Asset means that it is easier to monitor for changes affecting the Asset and any of its anchored entities. However, it also means that it must be easy to locate the Asset from any of the anchored entities when they change, even though they may not be directly connected. The Anchors classification makes it easier to find the anchor entity. It is attached to any entity anchored to a Referenceable. Example: SchemaElements and Comments In the example above, Anchors would be on all the SchemaElements and Comments. Anchors would contain the unique identifier ( GUID ) of the anchor Referenceable. There is also the unique identifier ( GUID ) of the SchemaType at the root of a schema structure and the unique identifier ( GUID ) of the Comment at root of a comment tree to make it easier to process these elements as a group. Following is an illustration of this example, with the addition of an Asset. The entities that have the Anchors classification are those that are anchored to the Asset. This includes entities such as Ratings, Likes and Attachments (from the Open Discovery Framework ( ODF ) . It is worthwhile maintaining the Anchors classification because reads of and updates to the anchored entities will happen many times, and it is rare that an anchored entity will change its anchor during its lifetime. If a GlossaryTerm , or InformalTag is attached to the Asset, they are not anchored to it. GlossaryTerms and InformalTags are independent entities. They are not anchored to the Asset and hence do not have an Anchors classification. Any change to these entities does not reflect in the LatestChange classification of the Asset. However, the act of attaching them to, or detaching them from the Asset is recorded in the Asset's LatestChange classification. Since the GlossaryTerm is also an anchor, when the GlossaryTerm and the Asset are linked together, this change is reflected in both of their LatestChange classifications because they are both Referenceable anchors. NoteLogs are Referenceables that can be attached to many other Referenceables. They can be set up either to be anchored with a single Referenceable or to be their own anchor to allow then to be attached to and detached from many Referenceables over its lifetime. Example: NoteLog and Referenceables For example, these are cases where the NoteLog is anchored to another Referenceable NoteLogs are used to support the personal blog linked off of the Personal Profile in Community Profile OMAS . Assets may have a NoteLog to record \"news\" for consumers such as planned maintenance and unexpected situations. Egeria uses the Anchors classification on a NoteLog to indicate that the NoteLog is tied to the Referenceable it is attached to. The presence of this classification would prevent it from being linked to another Referenceable. Following is an illustration of the additional objects connecting to an asset that do not have the Anchors classification because they are not anchored to the Asset. Also notice there are two NoteLogs attached to the asset, one with the Anchors classification and one without. The one with the Anchors classification is anchored to the the Asset. The one without the Anchors classification is independent of the Asset. Further information Generic Handlers provide support for the Anchors and LatestChange classifications.","title":"LatestChanges"},{"location":"concepts/archive-manager/","text":"Archive manager \u00b6 An open metadata archive provides pre-built definitions for types and metadata instances. OMRSArchiveManager manages the loading and unloading of open metadata archives for the local OMRS repository. It is invoked at server start up for a cohort member and whenever a new open metadata archive is loaded via a REST API. During server start up, it first calls the repository content manager to load the types into the local repository (if any) and to maintain the cache of know and active types in the server. It then calls the local repository instance event processor to load the instances. Related information \u00b6 A description of the utilities for building archives can be found in the open-metadata-archives modules. Details for configuring a metadata server to load archives can be found in the administration guide .","title":"Archive Manager"},{"location":"concepts/archive-manager/#archive-manager","text":"An open metadata archive provides pre-built definitions for types and metadata instances. OMRSArchiveManager manages the loading and unloading of open metadata archives for the local OMRS repository. It is invoked at server start up for a cohort member and whenever a new open metadata archive is loaded via a REST API. During server start up, it first calls the repository content manager to load the types into the local repository (if any) and to maintain the cache of know and active types in the server. It then calls the local repository instance event processor to load the instances.","title":"Archive manager"},{"location":"concepts/archive-manager/#related-information","text":"A description of the utilities for building archives can be found in the open-metadata-archives modules. Details for configuring a metadata server to load archives can be found in the administration guide .","title":"Related information"},{"location":"concepts/asset/","text":"Asset \u00b6 An asset is either a digital or physical object/property that provides value to the organization that owns it. Examples of an asset include: Data sources such as databases, files and data feeds. IT infrastructure and applications that automate many aspects of an organization's operation. APIs that provide access to the services offered by the organization. Analytical models and processes that differentiate an organization from its competitors or ensure it is operating legally and ethically. Buildings and other locations. Physical objects that have a unique identity (eg a serial number). Much governance is centered around an organization's assets since they represent tangible value. This involves maintaining information about each asset and managing events related to the asset in order to keep it protected and to get the maximum value from it. Egeria is particularly focused on providing the ability to maintain the information necessary for managing digital assets and the infrastructure that supports them. It also has a flexible model to allow the definition of asset to be expanded to include a broader range of physical assets. Open metadata types \u00b6 The information about an asset that is used to describe its characteristics and how it should be managed (that is, the asset's metadata) is stored in a sub-graph of open metadata instances (entities and relationships) with the Asset entity at the root. The asset entity contains a small amount of information that merely captures the existence of the real asset. Then other entities are linked to it to add more information. It is likely that this additional information is identified, captured and stored by different tools. The open metadata services gather this information together and distribute it to provide the most complete view of the asset's properties. More information on the types of attachments that can be added to an asset can be found here . Inheriting from asset is a hierarchy of increasingly-specialized definitions for different types of assets. Each definition adds more properties about the asset: Area 2 is where the asset hierarchy is built out. Accessing asset content through connectors \u00b6 Egeria provides an open framework for accessing the content of digital assets and the information about them. It is called the Open Connector Framework ( OCF ) and it provides specialized connectors (clients) for accessing specific types of assets and the information about them. The type of connector to use is specified in the connection entity that is linked to the asset. Model 0205 in the open metadata types shows how an asset is associated with a connection object. The connection object provides the properties necessary to create a connector to access the asset's contents. APIs and events for managing asset information (metadata) \u00b6 Egeria's Open Metadata Access Services ( OMAS ) provide the specialized services for managing assets. Each OMAS focuses on a particular part of the asset lifecycle or person/tool that is working with the assets. Some examples: OMAS Description Analytics Modeling OMAS enables business intelligence and data virtualization tools to maintain information about the data views and reporting assets they are maintaining. Asset Catalog OMAS provides a search service for locating assets. Asset Consumer OMAS provides a service for accessing the content of an asset, extracting additional information that is known about the asset and providing feedback about the asset. It is designed for tools that consume assets to support the work of their users. These users can provide feedback on the asset description and the resource that it describes. Asset Manager OMAS provides a service for exchanging metadata about assets and related information with a third party asset manager . This API supports the many-to-many correlation of identifiers used in the third party asset manager and the open metadata ecosystem. Asset Owner OMAS provides a service for the owner of an asset to classify and manage the asset, and understand how it is being used by the organization. Discovery Engine OMAS provides a service for adding annotations to an asset's information that has been determined by specific analysis of the asset's contents by a discovery service . Data Manager OMAS enables a data manager (such as a database or file system) to maintain information about the assets it stores. Governance Engine OMAS provides the metadata services for governance action services that verify, enhance and correct the properties of assets and their associated elements. IT Infrastructure OMAS provides a service for maintaining information about the IT assets and supporting infrastructure owned or used by an organization. Data Science OMAS provides a service for maintaining information about analytical models and related assets such as python notebooks. Sharing information about assets \u00b6 Egeria's Open Metadata Repository Services ( OMRS ) provides the ability to store and extract information about assets in a distributed collection of servers called an open metadata repository cohort . The cohort provides both peer-to-peer exchange of metadata via an event bus topic and federated queries between different members of the cohort. Egeria provides a metadata server , a metadata access point and a repository proxy server that are all able to join a cohort. The repository proxy supports the integration of third party servers (typically asset managers ) into the cohort. The mapping between the third party server's APIs and the open metadata APIs in this case is implemented in an repository connector . It is also possible to manage the exchange of asset metadata with other types of third party technologies using the Open Metadata Integration Services ( OMIS ) running in an integration daemon . Using this pattern is simpler to integrate but involves maintaining a copy of the third party technology's metadata in a metadata server that can then join one or more open metadata repository cohorts to share this metadata more broadly. The mapping between the third party technology's APIs and the open metadata APIs in this case is implemented in an integration connector .","title":"Asset"},{"location":"concepts/asset/#asset","text":"An asset is either a digital or physical object/property that provides value to the organization that owns it. Examples of an asset include: Data sources such as databases, files and data feeds. IT infrastructure and applications that automate many aspects of an organization's operation. APIs that provide access to the services offered by the organization. Analytical models and processes that differentiate an organization from its competitors or ensure it is operating legally and ethically. Buildings and other locations. Physical objects that have a unique identity (eg a serial number). Much governance is centered around an organization's assets since they represent tangible value. This involves maintaining information about each asset and managing events related to the asset in order to keep it protected and to get the maximum value from it. Egeria is particularly focused on providing the ability to maintain the information necessary for managing digital assets and the infrastructure that supports them. It also has a flexible model to allow the definition of asset to be expanded to include a broader range of physical assets.","title":"Asset"},{"location":"concepts/asset/#open-metadata-types","text":"The information about an asset that is used to describe its characteristics and how it should be managed (that is, the asset's metadata) is stored in a sub-graph of open metadata instances (entities and relationships) with the Asset entity at the root. The asset entity contains a small amount of information that merely captures the existence of the real asset. Then other entities are linked to it to add more information. It is likely that this additional information is identified, captured and stored by different tools. The open metadata services gather this information together and distribute it to provide the most complete view of the asset's properties. More information on the types of attachments that can be added to an asset can be found here . Inheriting from asset is a hierarchy of increasingly-specialized definitions for different types of assets. Each definition adds more properties about the asset: Area 2 is where the asset hierarchy is built out.","title":"Open metadata types"},{"location":"concepts/asset/#accessing-asset-content-through-connectors","text":"Egeria provides an open framework for accessing the content of digital assets and the information about them. It is called the Open Connector Framework ( OCF ) and it provides specialized connectors (clients) for accessing specific types of assets and the information about them. The type of connector to use is specified in the connection entity that is linked to the asset. Model 0205 in the open metadata types shows how an asset is associated with a connection object. The connection object provides the properties necessary to create a connector to access the asset's contents.","title":"Accessing asset content through connectors"},{"location":"concepts/asset/#apis-and-events-for-managing-asset-information-metadata","text":"Egeria's Open Metadata Access Services ( OMAS ) provide the specialized services for managing assets. Each OMAS focuses on a particular part of the asset lifecycle or person/tool that is working with the assets. Some examples: OMAS Description Analytics Modeling OMAS enables business intelligence and data virtualization tools to maintain information about the data views and reporting assets they are maintaining. Asset Catalog OMAS provides a search service for locating assets. Asset Consumer OMAS provides a service for accessing the content of an asset, extracting additional information that is known about the asset and providing feedback about the asset. It is designed for tools that consume assets to support the work of their users. These users can provide feedback on the asset description and the resource that it describes. Asset Manager OMAS provides a service for exchanging metadata about assets and related information with a third party asset manager . This API supports the many-to-many correlation of identifiers used in the third party asset manager and the open metadata ecosystem. Asset Owner OMAS provides a service for the owner of an asset to classify and manage the asset, and understand how it is being used by the organization. Discovery Engine OMAS provides a service for adding annotations to an asset's information that has been determined by specific analysis of the asset's contents by a discovery service . Data Manager OMAS enables a data manager (such as a database or file system) to maintain information about the assets it stores. Governance Engine OMAS provides the metadata services for governance action services that verify, enhance and correct the properties of assets and their associated elements. IT Infrastructure OMAS provides a service for maintaining information about the IT assets and supporting infrastructure owned or used by an organization. Data Science OMAS provides a service for maintaining information about analytical models and related assets such as python notebooks.","title":"APIs and events for managing asset information (metadata)"},{"location":"concepts/asset/#sharing-information-about-assets","text":"Egeria's Open Metadata Repository Services ( OMRS ) provides the ability to store and extract information about assets in a distributed collection of servers called an open metadata repository cohort . The cohort provides both peer-to-peer exchange of metadata via an event bus topic and federated queries between different members of the cohort. Egeria provides a metadata server , a metadata access point and a repository proxy server that are all able to join a cohort. The repository proxy supports the integration of third party servers (typically asset managers ) into the cohort. The mapping between the third party server's APIs and the open metadata APIs in this case is implemented in an repository connector . It is also possible to manage the exchange of asset metadata with other types of third party technologies using the Open Metadata Integration Services ( OMIS ) running in an integration daemon . Using this pattern is simpler to integrate but involves maintaining a copy of the third party technology's metadata in a metadata server that can then join one or more open metadata repository cohorts to share this metadata more broadly. The mapping between the third party technology's APIs and the open metadata APIs in this case is implemented in an integration connector .","title":"Sharing information about assets"},{"location":"concepts/audit-log/","text":"Audit Log \u00b6 The audit log provides detailed information relating to the activities within an OMAG Server . It builds on the Audit Log Framework to support multiple destinations for the audit log records written to the audit log by the server's subsystems. Details of the supported audit log store connectors and how to set them up are described in the various sub-sections of the configuring an OMAG Server portion of the administration guide.","title":"Audit Log"},{"location":"concepts/audit-log/#audit-log","text":"The audit log provides detailed information relating to the activities within an OMAG Server . It builds on the Audit Log Framework to support multiple destinations for the audit log records written to the audit log by the server's subsystems. Details of the supported audit log store connectors and how to set them up are described in the various sub-sections of the configuring an OMAG Server portion of the administration guide.","title":"Audit Log"},{"location":"concepts/configuration-document/","text":"Configuration Documents \u00b6 A configuration document provides the configuration details for a single OMAG Server . It defines which subsystems are activated in the server and which connector implementations it should use. Configuration document structure \u00b6 An OMAG Server's configuration document is structured into elements that each describe the configuration properties for each of its desired capabilities. The sections are as follows: Default values to use when creating other configuration elements. These values need to be set up first. Basic properties of any OMAG Server. Specific subsystem configurations that provide the key capabilities for different types of OMAG Servers. Audit trail that documents the changes that have been made to the configuration document. It is possible to retrieve the configuration document for a server using the following command: GET - retrieve configuration document for a server {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/configuration When the server is running, the following command returns the configuration document that was used to start it (since it may have changed in the configuration document store since the server was started): GET - retrieve configuration document used to start a server {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/instance/configuration Default values \u00b6 At the top of the configuration document are: Local server URL root , which defines the root of the network address for the OMAG Server Platform where the OMAG Server will run. Event bus config , which provides the configuration of the event bus (Apache Kafka or similar) where all the event topics that the server will use are located. Both of these elements provide default values for other configuration elements. If they are changed, their new values do not affect existing definitions in the configuration document. Basic properties \u00b6 The basic properties of the OMAG Server are used in logging, events handing and security. They help to document the purpose of the server (which helps with problem determination) and enable performance improvements by allowing the server to ignore activity or metadata that is not relevant to its operation. Find more information on configuring each through the sub-sections of the configuring an OMAG Server portion of the administration guide. Specific subsystem configurations \u00b6 Different types of servers can be configured with other specific subsystems, such as a local metadata repository, registering with a cohort, configuring access services or others. Since these vary depending on the type of OMAG Server, find more details in the sub-sections under the configuring an OMAG Server portion of the administration guide. Audit trail \u00b6 The audit trail allows you to keep track of changes to the configuration document. This is helpful to audit what any recent changes might have been - particularly if a working server suddenly stops working - the first question is always, \"what has changed recently?\" It also acts as a nice summary of how the server has been configured. Example of an audit trail { \"auditTrail\" : [ \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for local server's URL root to https://localhost:9444.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for maximum page size to 100.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for local server type name to Open Metadata Server.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for local server's owning organization's name to Coco Pharmaceuticals.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for local server's userId to cocoMDS1npa.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for local server's password to cocoMDS1passw0rd.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke added configuration for an Open Metadata Server Security Connector\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for default event bus.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for the local repository.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for the local repository.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke preserving local metadata collection id bfdfdc61-01bb-4564-9c29-6b81c0fb79f8.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for cohort cocoCohort.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:44:10 GMT 2020 garygeeke deployed configuration for server.\" ] } Storage \u00b6 By default, the configuration document is stored as a JSON in a file in the default directory for the OMAG Server Platform that creates them. These files may contain security certificates and passwords and so should be treated as sensitive. It is possible to change the storage location of configuration documents - or even the type of store. The configuration document's persistence is managed by the configuration document store connector . Configuration document store connector interface The admin-services-api module provides the interface definition for this connector. Its interface is simple -- consisting of save, retrieve and delete operations: /** * OMAGServerConfigStore provides the interface to the configuration for an OMAG Server. This is accessed * through a connector. */ public interface OMAGServerConfigStore { /** * Save the server configuration. * * @param configuration configuration properties to save */ void saveServerConfig ( OMAGServerConfig configuration ); /** * Retrieve the configuration saved from a previous run of the server. * * @return server configuration */ OMAGServerConfig retrieveServerConfig (); /** * Remove the server configuration. */ void removeServerConfig (); } The configuration document is represented by the OMAGServerConfig structure. The name of the server is stored in the localServerName property in OMAGServerConfig . Sample implementations \u00b6 The implementations of this connector provided by Egeria are found in the configuration-store-connectors module. There are two connectors: configuration-file-store-connector supports managing the open metadata configuration as a clear text JSON file. configuration-encrypted-file-store-connector supports managing the open metadata configuration as an encrypted JSON file. It is also possible to write your own implementation . Configuring the connector \u00b6 See configuring the configuration document store for the command to install a particular configuration document store connector into the OMAG Server Platform. Further information Open Connector Framework ( OCF ) defines open connectors and connections since many of the sections in the configuration document take connection objects for connectors. Configuring an OMAG Server provides more detail on the process of creating a configuration document for various types of OMAG Servers.","title":"Configuration Document"},{"location":"concepts/configuration-document/#configuration-documents","text":"A configuration document provides the configuration details for a single OMAG Server . It defines which subsystems are activated in the server and which connector implementations it should use.","title":"Configuration Documents"},{"location":"concepts/configuration-document/#configuration-document-structure","text":"An OMAG Server's configuration document is structured into elements that each describe the configuration properties for each of its desired capabilities. The sections are as follows: Default values to use when creating other configuration elements. These values need to be set up first. Basic properties of any OMAG Server. Specific subsystem configurations that provide the key capabilities for different types of OMAG Servers. Audit trail that documents the changes that have been made to the configuration document. It is possible to retrieve the configuration document for a server using the following command: GET - retrieve configuration document for a server {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/configuration When the server is running, the following command returns the configuration document that was used to start it (since it may have changed in the configuration document store since the server was started): GET - retrieve configuration document used to start a server {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/instance/configuration","title":"Configuration document structure"},{"location":"concepts/configuration-document/#default-values","text":"At the top of the configuration document are: Local server URL root , which defines the root of the network address for the OMAG Server Platform where the OMAG Server will run. Event bus config , which provides the configuration of the event bus (Apache Kafka or similar) where all the event topics that the server will use are located. Both of these elements provide default values for other configuration elements. If they are changed, their new values do not affect existing definitions in the configuration document.","title":"Default values"},{"location":"concepts/configuration-document/#basic-properties","text":"The basic properties of the OMAG Server are used in logging, events handing and security. They help to document the purpose of the server (which helps with problem determination) and enable performance improvements by allowing the server to ignore activity or metadata that is not relevant to its operation. Find more information on configuring each through the sub-sections of the configuring an OMAG Server portion of the administration guide.","title":"Basic properties"},{"location":"concepts/configuration-document/#specific-subsystem-configurations","text":"Different types of servers can be configured with other specific subsystems, such as a local metadata repository, registering with a cohort, configuring access services or others. Since these vary depending on the type of OMAG Server, find more details in the sub-sections under the configuring an OMAG Server portion of the administration guide.","title":"Specific subsystem configurations"},{"location":"concepts/configuration-document/#audit-trail","text":"The audit trail allows you to keep track of changes to the configuration document. This is helpful to audit what any recent changes might have been - particularly if a working server suddenly stops working - the first question is always, \"what has changed recently?\" It also acts as a nice summary of how the server has been configured. Example of an audit trail { \"auditTrail\" : [ \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for local server's URL root to https://localhost:9444.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for maximum page size to 100.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for local server type name to Open Metadata Server.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for local server's owning organization's name to Coco Pharmaceuticals.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for local server's userId to cocoMDS1npa.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for local server's password to cocoMDS1passw0rd.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke added configuration for an Open Metadata Server Security Connector\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for default event bus.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for the local repository.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for the local repository.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke preserving local metadata collection id bfdfdc61-01bb-4564-9c29-6b81c0fb79f8.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for cohort cocoCohort.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:12 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for access services.\", \"Thu Jan 30 22:37:13 GMT 2020 garygeeke updated configuration for enterprise repository services (used by access services).\", \"Thu Jan 30 22:44:10 GMT 2020 garygeeke deployed configuration for server.\" ] }","title":"Audit trail"},{"location":"concepts/configuration-document/#storage","text":"By default, the configuration document is stored as a JSON in a file in the default directory for the OMAG Server Platform that creates them. These files may contain security certificates and passwords and so should be treated as sensitive. It is possible to change the storage location of configuration documents - or even the type of store. The configuration document's persistence is managed by the configuration document store connector . Configuration document store connector interface The admin-services-api module provides the interface definition for this connector. Its interface is simple -- consisting of save, retrieve and delete operations: /** * OMAGServerConfigStore provides the interface to the configuration for an OMAG Server. This is accessed * through a connector. */ public interface OMAGServerConfigStore { /** * Save the server configuration. * * @param configuration configuration properties to save */ void saveServerConfig ( OMAGServerConfig configuration ); /** * Retrieve the configuration saved from a previous run of the server. * * @return server configuration */ OMAGServerConfig retrieveServerConfig (); /** * Remove the server configuration. */ void removeServerConfig (); } The configuration document is represented by the OMAGServerConfig structure. The name of the server is stored in the localServerName property in OMAGServerConfig .","title":"Storage"},{"location":"concepts/configuration-document/#sample-implementations","text":"The implementations of this connector provided by Egeria are found in the configuration-store-connectors module. There are two connectors: configuration-file-store-connector supports managing the open metadata configuration as a clear text JSON file. configuration-encrypted-file-store-connector supports managing the open metadata configuration as an encrypted JSON file. It is also possible to write your own implementation .","title":"Sample implementations"},{"location":"concepts/configuration-document/#configuring-the-connector","text":"See configuring the configuration document store for the command to install a particular configuration document store connector into the OMAG Server Platform. Further information Open Connector Framework ( OCF ) defines open connectors and connections since many of the sections in the configuration document take connection objects for connectors. Configuring an OMAG Server provides more detail on the process of creating a configuration document for various types of OMAG Servers.","title":"Configuring the connector"},{"location":"concepts/conformance-test-server/","text":"Conformance Test Server \u00b6 The conformance test server is an OMAG Server that hosts the Conformance Test Suite ( CTS ) .","title":"Conformance Test Server"},{"location":"concepts/conformance-test-server/#conformance-test-server","text":"The conformance test server is an OMAG Server that hosts the Conformance Test Suite ( CTS ) .","title":"Conformance Test Server"},{"location":"concepts/data-engine-proxy/","text":"Data engine proxy \u00b6 The data engine proxy is a governance server that can capture metadata about data movement processes (such as ETL jobs) from a data engine. This information results in new process assets being defined in open metadata linked to the data sources that it works with. This is valuable information for lineage. The data engine proxy is paired with the Data Engine OMAS . Its connector interfaces are defined in the data-engine-proxy-connector module. Further information Setting up the data engine proxy","title":"Data Engine Proxy"},{"location":"concepts/data-engine-proxy/#data-engine-proxy","text":"The data engine proxy is a governance server that can capture metadata about data movement processes (such as ETL jobs) from a data engine. This information results in new process assets being defined in open metadata linked to the data sources that it works with. This is valuable information for lineage. The data engine proxy is paired with the Data Engine OMAS . Its connector interfaces are defined in the data-engine-proxy-connector module. Further information Setting up the data engine proxy","title":"Data engine proxy"},{"location":"concepts/engine-host/","text":"Governance Engine Hosting Servers \u00b6 An engine host is an OMAG Server that hosts one or more governance engines. Governance engines provide collections of services used to support the governance of the digital landscape and the metadata that describes it. The services within the governance engines may access third party technology to perform their responsibilities or implement their behavior directly. The engine host uses a metadata server to store the definitions of the governance engines and the services within them. These definitions are retrieved through the Governance Engine OMAS . The Governance Engine OMAS also manages the definition of governance action processes that choreograph calls to the services in a governance engine in order to implement technical controls in the governance program. Typically, an engine host is deployed close to where the artifacts/resources/data are stored because it can generate a lot of network traffic when its services are running. The metadata interfaces needed by the governance engines are provided by the Open Metadata Engine Services ( OMES ) (or engine services for short). The engine services also run in the engine host OMAG Server. The engine services are: Asset Analysis - For running Open Discovery Services that analyse the content of an asset's real world counterpart in the digital landscape, generates annotations in an open discovery analysis report that is attached to the asset in the open metadata repositories. Governance Action - For running Governance Action Services . There are five types of governance action services: Watchdog governance service - Monitors changes in the open metadata repositories and initiates governance activity as a result. This is typically by creating a governance action , a governance action process or an incident report . One example of a watchdog governance service is to monitor for the addition of a new asset. Verification governance service - Runs checks on the metadata properties to ensure they are complete and correct. One example of a verification governance service is detection for metadata elements with the same qualified name, or an asset without an owner. Triage governance service - Runs triage tasks to determine how to manage an incident or situation. For example, it could initiate an external workflow, assign a task to a steward, wait for manual decision or initiate a remediation request. Remediation governance service - Makes updates to the open metadata or the digital landscape. An example of a remediation governance service could be to link or consolidate metadata elements with the same qualified name. Another remediation governance service may move assets between zones when a particular date is reached. Provisioning governance service - Invokes a provisioning service whenever a provisioning request is made. Typically, the provisioning service is an external service. It may also create lineage metadata to describe the work of the provisioning engine. An engine service is paired with a specific access service running in either a metadata access point or a metadata server . The specific access services are: Discovery Engine OMAS for Asset Analysis OMES . Governance Engine OMAS for Governance Action OMES . The name and URL root of the server where the access service is running is needed to configure an engine service. The metadata server used by the engine services does not need to be the same metadata server as the one used by the engine host server. This enables the management of metadata about the assets to be maintained close to the assets, and the definitions of the governance engines, services and processes to be maintained close to the governance team: The engine host services have a REST API to query the status of the governance engines running in the engine services. The engine services also have a REST API to query specific details of their governance engines. All these REST APIs may be called by a view server as part of the support for a user interface. Further information The capabilities of each of the engine services are described in the engine services . Details of how to create the definitions of the governance engines and governance services are described in the Governance Engine OMAS documentation.","title":"Engine Host"},{"location":"concepts/engine-host/#governance-engine-hosting-servers","text":"An engine host is an OMAG Server that hosts one or more governance engines. Governance engines provide collections of services used to support the governance of the digital landscape and the metadata that describes it. The services within the governance engines may access third party technology to perform their responsibilities or implement their behavior directly. The engine host uses a metadata server to store the definitions of the governance engines and the services within them. These definitions are retrieved through the Governance Engine OMAS . The Governance Engine OMAS also manages the definition of governance action processes that choreograph calls to the services in a governance engine in order to implement technical controls in the governance program. Typically, an engine host is deployed close to where the artifacts/resources/data are stored because it can generate a lot of network traffic when its services are running. The metadata interfaces needed by the governance engines are provided by the Open Metadata Engine Services ( OMES ) (or engine services for short). The engine services also run in the engine host OMAG Server. The engine services are: Asset Analysis - For running Open Discovery Services that analyse the content of an asset's real world counterpart in the digital landscape, generates annotations in an open discovery analysis report that is attached to the asset in the open metadata repositories. Governance Action - For running Governance Action Services . There are five types of governance action services: Watchdog governance service - Monitors changes in the open metadata repositories and initiates governance activity as a result. This is typically by creating a governance action , a governance action process or an incident report . One example of a watchdog governance service is to monitor for the addition of a new asset. Verification governance service - Runs checks on the metadata properties to ensure they are complete and correct. One example of a verification governance service is detection for metadata elements with the same qualified name, or an asset without an owner. Triage governance service - Runs triage tasks to determine how to manage an incident or situation. For example, it could initiate an external workflow, assign a task to a steward, wait for manual decision or initiate a remediation request. Remediation governance service - Makes updates to the open metadata or the digital landscape. An example of a remediation governance service could be to link or consolidate metadata elements with the same qualified name. Another remediation governance service may move assets between zones when a particular date is reached. Provisioning governance service - Invokes a provisioning service whenever a provisioning request is made. Typically, the provisioning service is an external service. It may also create lineage metadata to describe the work of the provisioning engine. An engine service is paired with a specific access service running in either a metadata access point or a metadata server . The specific access services are: Discovery Engine OMAS for Asset Analysis OMES . Governance Engine OMAS for Governance Action OMES . The name and URL root of the server where the access service is running is needed to configure an engine service. The metadata server used by the engine services does not need to be the same metadata server as the one used by the engine host server. This enables the management of metadata about the assets to be maintained close to the assets, and the definitions of the governance engines, services and processes to be maintained close to the governance team: The engine host services have a REST API to query the status of the governance engines running in the engine services. The engine services also have a REST API to query specific details of their governance engines. All these REST APIs may be called by a view server as part of the support for a user interface. Further information The capabilities of each of the engine services are described in the engine services . Details of how to create the definitions of the governance engines and governance services are described in the Governance Engine OMAS documentation.","title":"Governance Engine Hosting Servers"},{"location":"concepts/event-bus/","text":"Event Bus \u00b6 Egeria's event bus service is constructed from an event broker infrastructure service and a set of well-known topics . Collectively they provide the ability to reliably pass events between different OMAG Servers : To register with an open metadata repository cohort , exchange type definitions and share changes to metadata with other members of the cohort via the OMRS topic(s) . To exchange details of metadata changes through an Open Metadata Access Service ( OMAS ) 's InTopic and OutTopic . Each topic maintains a pointer to the last event that a server has read so that it receives each event that is added even if it restarts. There are different event broker implementations with greater or lesser reliability and performance. Many organizations establish a standard choice of their event broker service which is why Egeria uses connectors to implement its event bus. Egeria's default event broker is Apache Kafka . Each topic is accessed through an open metadata topic connector . Details of open metadata topic connectors are needed in multiple places in a server's configuration document . To simplify this configuration, the event bus config is added to the server's configuration document at the start of the configuration process . The event bus config establishes a set of defaults for the open metadata topic connectors. These defaults are used whenever open metadata topic connectors are configured. The subsystems using the event bus have a specialized connector that supports event exchange for a specific type of event. Since it is necessary to be able to swap the event broker implementation, these connectors embed an open metadata topic connector within their implementation. When the connection for one of these subsystem topic connectors is configured, the defaults from the event bus config are used to set up the nested open metadata topic connection. The resulting configuration for these nested connectors is as follows: The common configuration for the event bus is identified and configured using the event bus config. This configuration is encoded in a connection object for the generic open metadata topic connector. When the consuming component is configured, a connection object for its specialized topic connector is created, with the generic open metadata topic connector embedded inside. When the connector broker inside Egeria's runtime is called upon to create the specialized topic connector at server start up, it navigates the hierarchy of connection objects, creating the nested hierarchy of connectors as specified.","title":"Event Bus"},{"location":"concepts/event-bus/#event-bus","text":"Egeria's event bus service is constructed from an event broker infrastructure service and a set of well-known topics . Collectively they provide the ability to reliably pass events between different OMAG Servers : To register with an open metadata repository cohort , exchange type definitions and share changes to metadata with other members of the cohort via the OMRS topic(s) . To exchange details of metadata changes through an Open Metadata Access Service ( OMAS ) 's InTopic and OutTopic . Each topic maintains a pointer to the last event that a server has read so that it receives each event that is added even if it restarts. There are different event broker implementations with greater or lesser reliability and performance. Many organizations establish a standard choice of their event broker service which is why Egeria uses connectors to implement its event bus. Egeria's default event broker is Apache Kafka . Each topic is accessed through an open metadata topic connector . Details of open metadata topic connectors are needed in multiple places in a server's configuration document . To simplify this configuration, the event bus config is added to the server's configuration document at the start of the configuration process . The event bus config establishes a set of defaults for the open metadata topic connectors. These defaults are used whenever open metadata topic connectors are configured. The subsystems using the event bus have a specialized connector that supports event exchange for a specific type of event. Since it is necessary to be able to swap the event broker implementation, these connectors embed an open metadata topic connector within their implementation. When the connection for one of these subsystem topic connectors is configured, the defaults from the event bus config are used to set up the nested open metadata topic connection. The resulting configuration for these nested connectors is as follows: The common configuration for the event bus is identified and configured using the event bus config. This configuration is encoded in a connection object for the generic open metadata topic connector. When the consuming component is configured, a connection object for its specialized topic connector is created, with the generic open metadata topic connector embedded inside. When the connector broker inside Egeria's runtime is called upon to create the specialized topic connector at server start up, it navigates the hierarchy of connection objects, creating the nested hierarchy of connectors as specified.","title":"Event Bus"},{"location":"concepts/governance-server/","text":"Governance server \u00b6 Governance servers host specific integration or governance connectors for technology that does not integrate directly with open metadata. These are the different types: Engine hosts - host governance engines for active management of the open metadata ecosystem. Integration daemons - manage the exchange of metadata with third party technologies. Data engine proxy - captures information about processes and the data sources that they work with and catalogs them in open metadata. Open lineage server - accumulates lineage information to provide a comprehensive historical reporting service for lineage.","title":"Governance server"},{"location":"concepts/governance-server/#governance-server","text":"Governance servers host specific integration or governance connectors for technology that does not integrate directly with open metadata. These are the different types: Engine hosts - host governance engines for active management of the open metadata ecosystem. Integration daemons - manage the exchange of metadata with third party technologies. Data engine proxy - captures information about processes and the data sources that they work with and catalogs them in open metadata. Open lineage server - accumulates lineage information to provide a comprehensive historical reporting service for lineage.","title":"Governance server"},{"location":"concepts/governance-zone/","text":"Governance Zone \u00b6 A governance zone defines a list of assets that are grouped together for a specific purpose. A zone may represent assets that are consumed or managed in a particular way; or should only be visible to particular groups of users, or processed by particular types of engine. There may also be zones used to indicate that the asset is in a particular state. For example, Coco Pharmaceuticals use a quarantine zone for data that has arrived from an external partner. It is not visible to the researchers until it has been cataloged and verified. Then it is added to the zones that others can see. Zones are typically independent of one another, but they can be nested if desired. An asset can belong to all, one or many zones. The list of zones that an asset belongs to is configured in its zoneMembership property. If it is blank, it means the asset logically belongs to all zones. Otherwise, it belongs only to the zones that are listed. Add or remove it from a zone by updating the asset's zoneMembership property. All Open Metadata Access Services ( OMAS ) that retrieve assets, such as Asset Catalog , Asset Consumer and Asset Owner , use the supportedZones option that is configured for the service in their server's configuration document. This property defines the zones of assets that can be returned by this instance of the access service. In addition, access services that create assets use the defaultZones option to define the list of zones set up in any new asset they create. Finally, access services that are synchronizing assets between different third party technologies, such as the Data Manager OMAS , will also use the publishZones option to publish an asset to consumer zones once they are completely defined in the catalog. The meaning, purpose and governance requirements for assets within a specific zone are maintained through the Governance Program OMAS . It is also possible to associate security access control with a governance zone .","title":"Governance Zone"},{"location":"concepts/governance-zone/#governance-zone","text":"A governance zone defines a list of assets that are grouped together for a specific purpose. A zone may represent assets that are consumed or managed in a particular way; or should only be visible to particular groups of users, or processed by particular types of engine. There may also be zones used to indicate that the asset is in a particular state. For example, Coco Pharmaceuticals use a quarantine zone for data that has arrived from an external partner. It is not visible to the researchers until it has been cataloged and verified. Then it is added to the zones that others can see. Zones are typically independent of one another, but they can be nested if desired. An asset can belong to all, one or many zones. The list of zones that an asset belongs to is configured in its zoneMembership property. If it is blank, it means the asset logically belongs to all zones. Otherwise, it belongs only to the zones that are listed. Add or remove it from a zone by updating the asset's zoneMembership property. All Open Metadata Access Services ( OMAS ) that retrieve assets, such as Asset Catalog , Asset Consumer and Asset Owner , use the supportedZones option that is configured for the service in their server's configuration document. This property defines the zones of assets that can be returned by this instance of the access service. In addition, access services that create assets use the defaultZones option to define the list of zones set up in any new asset they create. Finally, access services that are synchronizing assets between different third party technologies, such as the Data Manager OMAS , will also use the publishZones option to publish an asset to consumer zones once they are completely defined in the catalog. The meaning, purpose and governance requirements for assets within a specific zone are maintained through the Governance Program OMAS . It is also possible to associate security access control with a governance zone .","title":"Governance Zone"},{"location":"concepts/integration-daemon/","text":"Integration Daemon \u00b6 An integration daemon is an OMAG Server that provides metadata exchange services between third party technology and the open metadata ecosystem. The integration daemon interacts with the open metadata ecosystem through Open Metadata Access Services ( OMAS ) running in a metadata access point or metadata server . Inside the integration daemon are one or more Open Metadata Integration Services ( OMIS ) that each focus on metadata exchange with a specific type of technology. They are paired with a specific Open Metadata Access Service ( OMAS ) running in the metadata access point / metadata server. To understand how an integration daemon works, it is necessary to look in a bit more detail at how technologies can be connected together to exchange metadata. Integration mechanisms \u00b6 Closed technology \u00b6 Closed technology describes technology that is only accessible through a user interface. Not considered Egeria does not provide any particular consideration for these technologies, given they provide no integration mechanisms. Active and passive open technology \u00b6 Passive open technology offers open APIs that can be called to configure and operate the technology, while active open technology provides active, ongoing exchange of information with another technology that covers its operation and specific situations it has detected. The integration daemon provides support for both active and passive open technologies: For passive open technology, an integration service will continuously poll the connector to allow it to repeatedly call the technology's API to determine if anything has changed and then pass any changes to the metadata access point / metadata server. The active open technology support is similar except that rather than polling for changes in the third party technology, the connector listens on the third party technology's event topic and translate the events it receives and passes the information onto the access service via calls to the integration service. The integration service also listens for events from its access service's Out Topic . If there is new metadata that is of interest to the third party technology, the access service publishes the information and it is picked up by the integration service and passed on to the connector. The connector may then push metadata to the third party technology. Thus, the integration services of the integration daemon enable metadata to flow both in and out of the open metadata ecosystem. Integrated technology \u00b6 Integrated technology describes technology that integrates with open metadata APIs to events \"out of the box.\" Where an Egeria conformance test exists, this technology has a conformance mark. An integrated technology is able to interact directly with a metadata access point or metadata server by calling the open metadata services or consuming them directly: Integration connectors \u00b6 The code that manages the specific APIs and formats of the third party technology is encapsulated in a special type of connector called an integration connector . The specific interface that the integration connector needs to implement is defined by the integration service . This interface enables the integration service to pass a context object to the connector before it is started. The context enables the connector to register a listener with the associated access service's Out Topic , or call its REST API, or to push events to the access service's In Topic . By default, the context uses the integration daemon's userId for requests to the access service which means that the metadata created by the integration connector will be credited to this user. If you want to use a different userId for metadata from each connector, the server's userId can be overridden in the connector's configuration.","title":"Integration Daemon"},{"location":"concepts/integration-daemon/#integration-daemon","text":"An integration daemon is an OMAG Server that provides metadata exchange services between third party technology and the open metadata ecosystem. The integration daemon interacts with the open metadata ecosystem through Open Metadata Access Services ( OMAS ) running in a metadata access point or metadata server . Inside the integration daemon are one or more Open Metadata Integration Services ( OMIS ) that each focus on metadata exchange with a specific type of technology. They are paired with a specific Open Metadata Access Service ( OMAS ) running in the metadata access point / metadata server. To understand how an integration daemon works, it is necessary to look in a bit more detail at how technologies can be connected together to exchange metadata.","title":"Integration Daemon"},{"location":"concepts/integration-daemon/#integration-mechanisms","text":"","title":"Integration mechanisms"},{"location":"concepts/integration-daemon/#closed-technology","text":"Closed technology describes technology that is only accessible through a user interface. Not considered Egeria does not provide any particular consideration for these technologies, given they provide no integration mechanisms.","title":"Closed technology"},{"location":"concepts/integration-daemon/#active-and-passive-open-technology","text":"Passive open technology offers open APIs that can be called to configure and operate the technology, while active open technology provides active, ongoing exchange of information with another technology that covers its operation and specific situations it has detected. The integration daemon provides support for both active and passive open technologies: For passive open technology, an integration service will continuously poll the connector to allow it to repeatedly call the technology's API to determine if anything has changed and then pass any changes to the metadata access point / metadata server. The active open technology support is similar except that rather than polling for changes in the third party technology, the connector listens on the third party technology's event topic and translate the events it receives and passes the information onto the access service via calls to the integration service. The integration service also listens for events from its access service's Out Topic . If there is new metadata that is of interest to the third party technology, the access service publishes the information and it is picked up by the integration service and passed on to the connector. The connector may then push metadata to the third party technology. Thus, the integration services of the integration daemon enable metadata to flow both in and out of the open metadata ecosystem.","title":"Active and passive open technology"},{"location":"concepts/integration-daemon/#integrated-technology","text":"Integrated technology describes technology that integrates with open metadata APIs to events \"out of the box.\" Where an Egeria conformance test exists, this technology has a conformance mark. An integrated technology is able to interact directly with a metadata access point or metadata server by calling the open metadata services or consuming them directly:","title":"Integrated technology"},{"location":"concepts/integration-daemon/#integration-connectors","text":"The code that manages the specific APIs and formats of the third party technology is encapsulated in a special type of connector called an integration connector . The specific interface that the integration connector needs to implement is defined by the integration service . This interface enables the integration service to pass a context object to the connector before it is started. The context enables the connector to register a listener with the associated access service's Out Topic , or call its REST API, or to push events to the access service's In Topic . By default, the context uses the integration daemon's userId for requests to the access service which means that the metadata created by the integration connector will be credited to this user. If you want to use a different userId for metadata from each connector, the server's userId can be overridden in the connector's configuration.","title":"Integration connectors"},{"location":"concepts/metadata-access-point/","text":"Metadata Access Point \u00b6 A metadata access point is an OMAG Server that can be a member of a cohort and supports the access services . This means it provides specialist metadata APIs to user interfaces and governance servers that embrace metadata from all connected open metadata repository cohorts. The basic metadata access point has no metadata repository and metadata is retrieved and stored from remote repositories via the cohort . It can be upgraded to a metadata server by adding a metadata repository which will enable it to store metadata locally.","title":"Metadata Access Point"},{"location":"concepts/metadata-access-point/#metadata-access-point","text":"A metadata access point is an OMAG Server that can be a member of a cohort and supports the access services . This means it provides specialist metadata APIs to user interfaces and governance servers that embrace metadata from all connected open metadata repository cohorts. The basic metadata access point has no metadata repository and metadata is retrieved and stored from remote repositories via the cohort . It can be upgraded to a metadata server by adding a metadata repository which will enable it to store metadata locally.","title":"Metadata Access Point"},{"location":"concepts/metadata-server/","text":"Metadata Server \u00b6 A metadata server is an OMAG Server that hosts a metadata repository with native support for open metadata types and instances. It is able to be a member of a cohort and so can exchange metadata with other members of the cohort. The metadata server typically has the access services configured and so can act as a metadata access point . It is important to have at least one metadata server in each cohort in order to support all types of metadata.","title":"Metadata Server"},{"location":"concepts/metadata-server/#metadata-server","text":"A metadata server is an OMAG Server that hosts a metadata repository with native support for open metadata types and instances. It is able to be a member of a cohort and so can exchange metadata with other members of the cohort. The metadata server typically has the access services configured and so can act as a metadata access point . It is important to have at least one metadata server in each cohort in order to support all types of metadata.","title":"Metadata Server"},{"location":"concepts/omag-server-platform/","text":"OMAG Server Platform \u00b6 The OMAG Server Platform provides a runtime process and platform for four broad groups of services: Server origin service - used to determine the type and level of the OMAG Server Platform. Platform services - used to determine the servers and their services running on the platform. Administration services - used to configure and manage the OMAG Servers running inside the OMAG Server Platform. Open Metadata and Governance ( OMAG ) services - used to work with metadata and govern the assets of an organization. The OMAG services are configured and activated in OMAG Servers using the administration services . The configuration operations of the admin services create configuration documents , one for each OMAG Server. Inside a configuration document is the definition of which OMAG services to activate in the server. These include the repository services (any type of server), the access services (for metadata access points and metadata servers), governance services (for governance servers) and view services (for view servers). Once a configuration document is defined, the OMAG Server can be started and stopped multiple times by the admin services server instance operations. The OMAG Server Platform also supports some platform services to query details of the servers running on the platform. The OMAG Server Platform can also host multiple OMAG Servers at any one time: The choices are as follows: A - Each OMAG Server has its own dedicated OMAG Server Platform - useful when only one server is needed in a deployment environment, or there is a desire to keep each server isolated in its own stack. B - Multiple OMAG Servers are hosted on the same OMAG Server Platform. The OMAG Server Platform routes inbound requests to the right server based on the server name specified in the request URL. The servers may all be of the same type (multi-tenant operation) or be a set of collaborating servers of different types consolidated onto the same platform. C - Multiple copies of same server instance running on different platforms to provide high availability and distribution of workload (horizontal scalability). Each OMAG Server is isolated within the server platform and so the OMAG Server platform can be used to support multi-tenant operation for a cloud service, or host a variety of different OMAG Servers needed at a particular location. Startup sequence \u00b6 When the OMAG Server Platform first starts up, a limited set of functionality is available: The server origin service is operational at this point. It can be used by operational scripts to determine if the OMAG Server Platform is still running. The administration services are active at this point, while the open metadata and governance services will return an error if called since there are no OMAG Servers running. The configuration services are used to create configuration documents . Each configuration document describes the open metadata and governance services that should be activated in an OMAG Server. In the following diagram, the configuration services create three configuration documents: one for the cdoMetadataRepository OMAG Server one for the stewardshipServer OMAG Server one for the dataLakeDiscoveryEngine OMAG Server The admin guide provides detailed instructions on creating configuration documents for various configurations. Once a configuration document for an OMAG Server is used by the operational services initialize the requested services in the OMAG Server. The OMAG Server can be started in any OMAG Server Platform: it does not have to be the same OMAG Server Platform that created the configuration document. For example, the following diagram shows an OMAG Server platform with the cdoMetadataRepository local OMAG Server running. Once the OMAG Server has initialized successfully, the open metadata and governance services can route requests to it. An OMAG Server Platform can run multiple OMAG Servers at one time: Further reading Configuring the OMAG Server Platform Installing the OMAG Server Platform Tutorial Running the OMAG Server Platform Tutorial","title":"OMAG Server Platform"},{"location":"concepts/omag-server-platform/#omag-server-platform","text":"The OMAG Server Platform provides a runtime process and platform for four broad groups of services: Server origin service - used to determine the type and level of the OMAG Server Platform. Platform services - used to determine the servers and their services running on the platform. Administration services - used to configure and manage the OMAG Servers running inside the OMAG Server Platform. Open Metadata and Governance ( OMAG ) services - used to work with metadata and govern the assets of an organization. The OMAG services are configured and activated in OMAG Servers using the administration services . The configuration operations of the admin services create configuration documents , one for each OMAG Server. Inside a configuration document is the definition of which OMAG services to activate in the server. These include the repository services (any type of server), the access services (for metadata access points and metadata servers), governance services (for governance servers) and view services (for view servers). Once a configuration document is defined, the OMAG Server can be started and stopped multiple times by the admin services server instance operations. The OMAG Server Platform also supports some platform services to query details of the servers running on the platform. The OMAG Server Platform can also host multiple OMAG Servers at any one time: The choices are as follows: A - Each OMAG Server has its own dedicated OMAG Server Platform - useful when only one server is needed in a deployment environment, or there is a desire to keep each server isolated in its own stack. B - Multiple OMAG Servers are hosted on the same OMAG Server Platform. The OMAG Server Platform routes inbound requests to the right server based on the server name specified in the request URL. The servers may all be of the same type (multi-tenant operation) or be a set of collaborating servers of different types consolidated onto the same platform. C - Multiple copies of same server instance running on different platforms to provide high availability and distribution of workload (horizontal scalability). Each OMAG Server is isolated within the server platform and so the OMAG Server platform can be used to support multi-tenant operation for a cloud service, or host a variety of different OMAG Servers needed at a particular location.","title":"OMAG Server Platform"},{"location":"concepts/omag-server-platform/#startup-sequence","text":"When the OMAG Server Platform first starts up, a limited set of functionality is available: The server origin service is operational at this point. It can be used by operational scripts to determine if the OMAG Server Platform is still running. The administration services are active at this point, while the open metadata and governance services will return an error if called since there are no OMAG Servers running. The configuration services are used to create configuration documents . Each configuration document describes the open metadata and governance services that should be activated in an OMAG Server. In the following diagram, the configuration services create three configuration documents: one for the cdoMetadataRepository OMAG Server one for the stewardshipServer OMAG Server one for the dataLakeDiscoveryEngine OMAG Server The admin guide provides detailed instructions on creating configuration documents for various configurations. Once a configuration document for an OMAG Server is used by the operational services initialize the requested services in the OMAG Server. The OMAG Server can be started in any OMAG Server Platform: it does not have to be the same OMAG Server Platform that created the configuration document. For example, the following diagram shows an OMAG Server platform with the cdoMetadataRepository local OMAG Server running. Once the OMAG Server has initialized successfully, the open metadata and governance services can route requests to it. An OMAG Server Platform can run multiple OMAG Servers at one time: Further reading Configuring the OMAG Server Platform Installing the OMAG Server Platform Tutorial Running the OMAG Server Platform Tutorial","title":"Startup sequence"},{"location":"concepts/omag-server/","text":"Open Metadata and Governance ( OMAG ) Server \u00b6 An OMAG Server is a software server that runs inside the OMAG Server Platform . It is therefore sometimes referred to as a \"logical\" server rather than a physical server that runs in its own process. It supports the integration of one or more technologies by hosting connectors that interact with that technology, or providing specialist APIs or event topics (both in and out). Because of the wide variety of technologies deployed in organizations today, each with very different capabilities and needs, the integration and exchange of metadata needs to be organized. This organization is managed through the Egeria frameworks and services supported by the OMAG Servers. There are different types of OMAG Server, each supporting specific technologies. The OMAG Server ensures this type of technology is integrated appropriately for its needs. The capabilities that are activated in an OMAG Server are defined in its configuration document . When the server is started, the operational services of the OMAG Server Platform reads the configuration document and activates the OMAG Server with the requested services. Platform URL root \u00b6 An OMAG Server's platform URL root is the network address of the OMAG Server Platform where the OMAG Server is going to run. This is often the host name of the computer or container where the platform runs plus the port number allocated to the OMAG Server Platform. Its value is needed when creating clients or configuring services that will call the OMAG Server because it provides the root of the URL used to call the server's open metadata and governance REST calls, which have the following format in their URLs: {{platformURLRoot}}/servers/{{serverName}}/<operation-name-and-parameters> The platform URL root is the content of the URL prior to /servers/ . The default value an OMAG Server Platform is https://localhost:9443 . Server name \u00b6 Most APIs in Egeria require both a platform URL root and a server name. The server name is the name of an OMAG Server where the desired service is running. Types of OMAG Server \u00b6 Detailed explanation of diagram The way to understand the diagram is that the arrows should be read as IS A . For example, the repository proxy IS A cohort member and the cohort member IS A OMAG Server . This means that everything documented about a particular type of server is also true for all server types that point to it through the IS A arrow, all the way down the hierarchy. Object-oriented software engineers would know of this type of relationship as behavior inheritance. Cohort member - able to exchange metadata through an open metadata repository cohort Metadata server - supports a metadata repository that can natively store open metadata types as well as specialized metadata APIs for different types of tools (these APIs are called access services ). Metadata access point - supports the access services like the metadata server but does not have a repository. All metadata it serves up and stores belongs to the metadata repositories in other members of the cohort. Repository proxy - acts as an open metadata translator for a proprietary metadata repository. It supports open metadata API calls and translates them to the proprietary APIs of the repository. It also translates events from the proprietary repository into open metadata events that flow over the cohort. Conformance test server - validates that a member of the cohort is conforming with the open metadata protocols. This server is typically only seen in development and test cohorts rather than production. View server - manages specialist services for user interfaces. Governance server - supports the use of metadata in the broader IT landscape. Engine host - provides a runtime for a specific type of governance engine . Integration daemon - manages the synchronization with third party technology that can not call the access services directly. Data engine proxy - supports the capture of metadata from a data engine. This includes details of the processing of data that it is doing which is valuable when piecing together lineage. Open lineage server - Manages the collation of lineage information am maintains it in a format for reporting. This includes the state of the lineage at different points in time. Interconnectivity \u00b6 The different types of OMAG Servers connect together as illustrated below. There is an inner ring of cohort members communicating via the cohort. Each cohort member is sharing the metadata they receive with the governance servers and view servers that connect to it. The governance servers connect out to external tools, engines and platforms. Further information The configuration for an OMAG Server is defined in a configuration document . This configuration document is stored by a configuration document store connector . Configuring an OMAG Server Start and stop an OMAG Server","title":"OMAG Server"},{"location":"concepts/omag-server/#open-metadata-and-governance-omag-server","text":"An OMAG Server is a software server that runs inside the OMAG Server Platform . It is therefore sometimes referred to as a \"logical\" server rather than a physical server that runs in its own process. It supports the integration of one or more technologies by hosting connectors that interact with that technology, or providing specialist APIs or event topics (both in and out). Because of the wide variety of technologies deployed in organizations today, each with very different capabilities and needs, the integration and exchange of metadata needs to be organized. This organization is managed through the Egeria frameworks and services supported by the OMAG Servers. There are different types of OMAG Server, each supporting specific technologies. The OMAG Server ensures this type of technology is integrated appropriately for its needs. The capabilities that are activated in an OMAG Server are defined in its configuration document . When the server is started, the operational services of the OMAG Server Platform reads the configuration document and activates the OMAG Server with the requested services.","title":"Open Metadata and Governance (OMAG) Server"},{"location":"concepts/omag-server/#platform-url-root","text":"An OMAG Server's platform URL root is the network address of the OMAG Server Platform where the OMAG Server is going to run. This is often the host name of the computer or container where the platform runs plus the port number allocated to the OMAG Server Platform. Its value is needed when creating clients or configuring services that will call the OMAG Server because it provides the root of the URL used to call the server's open metadata and governance REST calls, which have the following format in their URLs: {{platformURLRoot}}/servers/{{serverName}}/<operation-name-and-parameters> The platform URL root is the content of the URL prior to /servers/ . The default value an OMAG Server Platform is https://localhost:9443 .","title":"Platform URL root"},{"location":"concepts/omag-server/#server-name","text":"Most APIs in Egeria require both a platform URL root and a server name. The server name is the name of an OMAG Server where the desired service is running.","title":"Server name"},{"location":"concepts/omag-server/#types-of-omag-server","text":"Detailed explanation of diagram The way to understand the diagram is that the arrows should be read as IS A . For example, the repository proxy IS A cohort member and the cohort member IS A OMAG Server . This means that everything documented about a particular type of server is also true for all server types that point to it through the IS A arrow, all the way down the hierarchy. Object-oriented software engineers would know of this type of relationship as behavior inheritance. Cohort member - able to exchange metadata through an open metadata repository cohort Metadata server - supports a metadata repository that can natively store open metadata types as well as specialized metadata APIs for different types of tools (these APIs are called access services ). Metadata access point - supports the access services like the metadata server but does not have a repository. All metadata it serves up and stores belongs to the metadata repositories in other members of the cohort. Repository proxy - acts as an open metadata translator for a proprietary metadata repository. It supports open metadata API calls and translates them to the proprietary APIs of the repository. It also translates events from the proprietary repository into open metadata events that flow over the cohort. Conformance test server - validates that a member of the cohort is conforming with the open metadata protocols. This server is typically only seen in development and test cohorts rather than production. View server - manages specialist services for user interfaces. Governance server - supports the use of metadata in the broader IT landscape. Engine host - provides a runtime for a specific type of governance engine . Integration daemon - manages the synchronization with third party technology that can not call the access services directly. Data engine proxy - supports the capture of metadata from a data engine. This includes details of the processing of data that it is doing which is valuable when piecing together lineage. Open lineage server - Manages the collation of lineage information am maintains it in a format for reporting. This includes the state of the lineage at different points in time.","title":"Types of OMAG Server"},{"location":"concepts/omag-server/#interconnectivity","text":"The different types of OMAG Servers connect together as illustrated below. There is an inner ring of cohort members communicating via the cohort. Each cohort member is sharing the metadata they receive with the governance servers and view servers that connect to it. The governance servers connect out to external tools, engines and platforms. Further information The configuration for an OMAG Server is defined in a configuration document . This configuration document is stored by a configuration document store connector . Configuring an OMAG Server Start and stop an OMAG Server","title":"Interconnectivity"},{"location":"concepts/omag-subsystem/","text":"Open Metadata and Governance ( OMAG ) Subsystems \u00b6 A subsystem is a collection of components within a software server that supports one or more related services. Subsystems can be organized in a hierarchy where course-grained subsystems can be decomposed into more fine-grained subsystems. The OMAG Server is a flexible software server whose subsystems can be activated (or not) through the presence (or absence) of the subsystem's configuration properties in the OMAG Server's configuration document . The potential subsystems within an OMAG Server are as follows: Open Metadata Repository Services ( OMRS ) for supporting access to metadata stored in metadata repositories and the exchange of metadata between repositories via an open metadata repository cohort . The repository services are further divided into OMRS subsystems that can be activated independently. Integration daemon services for running integration connectors that exchange metadata with third party technologies. Connected Asset Services for supporting the ConnectedAsset interface of a connector. Dynamically registered services provide specialist APIs for particular technologies and user roles. Each of these services runs in their own subsystem independent of the other registered services. The implementation may come from Egeria or a third party. The links are to Egeria provided dynamic services. Open Metadata Access Services ( OMAS ) for supporting domain-specific services for metadata access and governance. Access services run in the metadata server and metadata access point server. Open Metadata Engine Services ( OMES ) for supporting specialized governance engines that drive governance activity in the open metadata ecosystem. The engine services run in the engine host server. Open Metadata Integration Services ( OMIS ) for supporting specific types of integration connectors . The integration services run in the integration daemon server. Open Metadata View Services ( OMVS ) for supporting REST services for a User Interface (UI). The view services run in a view server .","title":"OMAG Subsystem"},{"location":"concepts/omag-subsystem/#open-metadata-and-governance-omag-subsystems","text":"A subsystem is a collection of components within a software server that supports one or more related services. Subsystems can be organized in a hierarchy where course-grained subsystems can be decomposed into more fine-grained subsystems. The OMAG Server is a flexible software server whose subsystems can be activated (or not) through the presence (or absence) of the subsystem's configuration properties in the OMAG Server's configuration document . The potential subsystems within an OMAG Server are as follows: Open Metadata Repository Services ( OMRS ) for supporting access to metadata stored in metadata repositories and the exchange of metadata between repositories via an open metadata repository cohort . The repository services are further divided into OMRS subsystems that can be activated independently. Integration daemon services for running integration connectors that exchange metadata with third party technologies. Connected Asset Services for supporting the ConnectedAsset interface of a connector. Dynamically registered services provide specialist APIs for particular technologies and user roles. Each of these services runs in their own subsystem independent of the other registered services. The implementation may come from Egeria or a third party. The links are to Egeria provided dynamic services. Open Metadata Access Services ( OMAS ) for supporting domain-specific services for metadata access and governance. Access services run in the metadata server and metadata access point server. Open Metadata Engine Services ( OMES ) for supporting specialized governance engines that drive governance activity in the open metadata ecosystem. The engine services run in the engine host server. Open Metadata Integration Services ( OMIS ) for supporting specific types of integration connectors . The integration services run in the integration daemon server. Open Metadata View Services ( OMVS ) for supporting REST services for a User Interface (UI). The view services run in a view server .","title":"Open Metadata and Governance (OMAG) Subsystems"},{"location":"concepts/open-lineage-server/","text":"Open lineage server \u00b6 The open lineage server is a governance server that manages a historical warehouse of lineage information. Its behavior and configuration is described here .","title":"Open Lineage Server"},{"location":"concepts/open-lineage-server/#open-lineage-server","text":"The open lineage server is a governance server that manages a historical warehouse of lineage information. Its behavior and configuration is described here .","title":"Open lineage server"},{"location":"concepts/open-metadata-archive/","text":"Open Metadata Archive \u00b6 An open metadata archive is a document containing open metadata type definitions and instances . The open metadata archive has two types: A content pack containing reusable definitions that are generally useful. They may come from the Egeria community or third parties. A metadata export containing an export of metadata from a repository. They are used to transfer metadata between repositories that are not connected to the same cohort . Structure \u00b6 The logical structure of an open metadata archive is as follows: Instances are linked together as follows: Entities are stored as EntityDetail structures. Relationships are stored as Relationship structures and link to their entities through the embedded EntityProxy structure. The entities will include their classifications; however, for classifications that are attached to entities that are not included in the archive, they are stored in an ClassificationEntityExtension structure. Typically, open metadata archives are encoded in JSON format and stored in a file; however, both the format and storage method can be changed by changing the open metadata archive connector . Example of the header from the Cloud Information Model archive { \"class\" : \"OpenMetadataArchive\" , \"archiveProperties\" : { \"class\" : \"OpenMetadataArchiveProperties\" , \"archiveGUID\" : \"9dc75637-92a7-4926-b47b-a3d407546f89\" , \"archiveName\" : \"Cloud Information Model (CIM) glossary and concept model\" , \"archiveDescription\" : \"Data types for commerce focused cloud applications.\" , \"archiveType\" : \"CONTENT_PACK\" , \"originatorName\" : \"The Cloud Information Model\" , \"originatorLicense\" : \"Apache 2.0\" , \"creationDate\" : 1570383385107 , \"dependsOnArchives\" :[ \"bce3b0a0-662a-4f87-b8dc-844078a11a6e\" ] }, \"archiveTypeStore\" :{}, \"archiveInstanceStore\" :{} } Processing \u00b6 Open metadata archives are introduced into the server through the admin services either: provided as part of the contents of the server's configuration document, or through the operational command that added the archive directly into the running server's repository. The archive is passed to the repository services' operational services, which in turn passes it on to the archive manager . Type information is passed to the repository content manager . Both the types and instances are passed to the local repository (if there is one). The archive loads in the following order: Attribute Type Definitions ( AttributeTypeDef s) from the type store, through verifyAttributeTypeDef() and then addAttributeTypeDef() : PrimitiveDefs CollectionDefs EnumDefs New Type Definitions ( TypeDef s) from the type store, through verifyTypeDef() and addTypeDef() : EntityDefs RelationshipDefs ClassificationDefs Updates to types ( TypeDefPatch es) Instances, as reference copies: Entities Relationships Classifications Cohort propagation If the server is connected to the cohort, the new content is sent as notifications to the rest of the cohort. Further information More information about open metadata archives can be found in the open-metadata-archives module. In addition, these articles may be of interest: Configuring an open metadata archive in an OMAG Server Adding an open metadata archive to a running OMAG Server","title":"Open Metadata Archive"},{"location":"concepts/open-metadata-archive/#open-metadata-archive","text":"An open metadata archive is a document containing open metadata type definitions and instances . The open metadata archive has two types: A content pack containing reusable definitions that are generally useful. They may come from the Egeria community or third parties. A metadata export containing an export of metadata from a repository. They are used to transfer metadata between repositories that are not connected to the same cohort .","title":"Open Metadata Archive"},{"location":"concepts/open-metadata-archive/#structure","text":"The logical structure of an open metadata archive is as follows: Instances are linked together as follows: Entities are stored as EntityDetail structures. Relationships are stored as Relationship structures and link to their entities through the embedded EntityProxy structure. The entities will include their classifications; however, for classifications that are attached to entities that are not included in the archive, they are stored in an ClassificationEntityExtension structure. Typically, open metadata archives are encoded in JSON format and stored in a file; however, both the format and storage method can be changed by changing the open metadata archive connector . Example of the header from the Cloud Information Model archive { \"class\" : \"OpenMetadataArchive\" , \"archiveProperties\" : { \"class\" : \"OpenMetadataArchiveProperties\" , \"archiveGUID\" : \"9dc75637-92a7-4926-b47b-a3d407546f89\" , \"archiveName\" : \"Cloud Information Model (CIM) glossary and concept model\" , \"archiveDescription\" : \"Data types for commerce focused cloud applications.\" , \"archiveType\" : \"CONTENT_PACK\" , \"originatorName\" : \"The Cloud Information Model\" , \"originatorLicense\" : \"Apache 2.0\" , \"creationDate\" : 1570383385107 , \"dependsOnArchives\" :[ \"bce3b0a0-662a-4f87-b8dc-844078a11a6e\" ] }, \"archiveTypeStore\" :{}, \"archiveInstanceStore\" :{} }","title":"Structure"},{"location":"concepts/open-metadata-archive/#processing","text":"Open metadata archives are introduced into the server through the admin services either: provided as part of the contents of the server's configuration document, or through the operational command that added the archive directly into the running server's repository. The archive is passed to the repository services' operational services, which in turn passes it on to the archive manager . Type information is passed to the repository content manager . Both the types and instances are passed to the local repository (if there is one). The archive loads in the following order: Attribute Type Definitions ( AttributeTypeDef s) from the type store, through verifyAttributeTypeDef() and then addAttributeTypeDef() : PrimitiveDefs CollectionDefs EnumDefs New Type Definitions ( TypeDef s) from the type store, through verifyTypeDef() and addTypeDef() : EntityDefs RelationshipDefs ClassificationDefs Updates to types ( TypeDefPatch es) Instances, as reference copies: Entities Relationships Classifications Cohort propagation If the server is connected to the cohort, the new content is sent as notifications to the rest of the cohort. Further information More information about open metadata archives can be found in the open-metadata-archives module. In addition, these articles may be of interest: Configuring an open metadata archive in an OMAG Server Adding an open metadata archive to a running OMAG Server","title":"Processing"},{"location":"concepts/presentation-server/","text":"Presentation server \u00b6 The presentation server hosts the JavaScript applications that provide an interactive browser-based user interface for Egeria. The JavaScript applications call REST API services running in a view server to retrieve information and perform operations relating to open metadata. The presentation server supports multi-tenant operation. Each presentation server tenant is designed to support an organization. These may be independent organizations or divisions/departments within an organization. The tenant is configured with the appropriate view server to use, which in turn routes requests to its governance servers and metadata servers . Therefore, each tenant sees a different collection of metadata and operates in isolation to the other tenants. Further information The setup and user guide for the presentation server is held in a separate repository.","title":"Presentation server"},{"location":"concepts/presentation-server/#presentation-server","text":"The presentation server hosts the JavaScript applications that provide an interactive browser-based user interface for Egeria. The JavaScript applications call REST API services running in a view server to retrieve information and perform operations relating to open metadata. The presentation server supports multi-tenant operation. Each presentation server tenant is designed to support an organization. These may be independent organizations or divisions/departments within an organization. The tenant is configured with the appropriate view server to use, which in turn routes requests to its governance servers and metadata servers . Therefore, each tenant sees a different collection of metadata and operates in isolation to the other tenants. Further information The setup and user guide for the presentation server is held in a separate repository.","title":"Presentation server"},{"location":"concepts/referenceable/","text":"Referenceable \u00b6 A referenceable is the description of a resource that is significant enough to need its own unique name so that it can be referred to accurately in different contexts (typically outside of open metadata). The unique name is stored in the qualified name ( qualifiedName ) property and the open metadata servers validate that this value is unique within its type. It is possible that two different metadata elements with the same qualified name could be created in different metadata servers. This may be because both servers have loaded metadata about the same object. Alternatively, there is a name clash as two servers have used the same unique name for two different objects.","title":"Referenceable"},{"location":"concepts/referenceable/#referenceable","text":"A referenceable is the description of a resource that is significant enough to need its own unique name so that it can be referred to accurately in different contexts (typically outside of open metadata). The unique name is stored in the qualified name ( qualifiedName ) property and the open metadata servers validate that this value is unique within its type. It is possible that two different metadata elements with the same qualified name could be created in different metadata servers. This may be because both servers have loaded metadata about the same object. Alternatively, there is a name clash as two servers have used the same unique name for two different objects.","title":"Referenceable"},{"location":"concepts/repository-content-manager/","text":"Repository content manager \u00b6 The repository content manager provides the function of the TypeDef manager plus the implementation of many repository helper and repository validator methods.","title":"Repository content manager"},{"location":"concepts/repository-content-manager/#repository-content-manager","text":"The repository content manager provides the function of the TypeDef manager plus the implementation of many repository helper and repository validator methods.","title":"Repository content manager"},{"location":"concepts/repository-helper/","text":"Repository helper \u00b6 The repository helper provides methods to simplify the actions of creating and manipulating the repository services property objects.","title":"Repository helper"},{"location":"concepts/repository-helper/#repository-helper","text":"The repository helper provides methods to simplify the actions of creating and manipulating the repository services property objects.","title":"Repository helper"},{"location":"concepts/repository-proxy/","text":"Repository Proxy \u00b6 A repository proxy is an OMAG Server that has been configured to act as a proxy to a third party metadata repository. It is used by a technology that is integrating using the adapter integration pattern . There are two repository proxy implementations included with Egeria Repository proxy for Apache Atlas Repository proxy for IBM Information Governance Catalog ( IGC )","title":"Repository Proxy"},{"location":"concepts/repository-proxy/#repository-proxy","text":"A repository proxy is an OMAG Server that has been configured to act as a proxy to a third party metadata repository. It is used by a technology that is integrating using the adapter integration pattern . There are two repository proxy implementations included with Egeria Repository proxy for Apache Atlas Repository proxy for IBM Information Governance Catalog ( IGC )","title":"Repository Proxy"},{"location":"concepts/repository-validator/","text":"Repository validator \u00b6 The repository validator provides common validation functions for use when working with repository services property objects.","title":"Repository validator"},{"location":"concepts/repository-validator/#repository-validator","text":"The repository validator provides common validation functions for use when working with repository services property objects.","title":"Repository validator"},{"location":"concepts/typedef-manager/","text":"TypeDef manager \u00b6 The TypeDef manager is the component that caches and manages the validation of TypeDefs. The TypeDef manager is implemented by the repository content manager","title":"Typedef manager"},{"location":"concepts/typedef-manager/#typedef-manager","text":"The TypeDef manager is the component that caches and manages the validation of TypeDefs. The TypeDef manager is implemented by the repository content manager","title":"TypeDef manager"},{"location":"concepts/view-server/","text":"View Server \u00b6 A view server is an OMAG Server that hosts the REST API services to support a UI. These REST API services are called Open Metadata View Services ( OMVS ) and they are designed to support specific display paradigms needed by different types of UIs. The view services are typically called by the presentation server , but may also be used by third parties. The presentation server hosts the JavaScript that controls the browser display.","title":"View Server"},{"location":"concepts/view-server/#view-server","text":"A view server is an OMAG Server that hosts the REST API services to support a UI. These REST API services are called Open Metadata View Services ( OMVS ) and they are designed to support specific display paradigms needed by different types of UIs. The view services are typically called by the presentation server , but may also be used by third parties. The presentation server hosts the JavaScript that controls the browser display.","title":"View Server"},{"location":"connectors/","text":"Connectors Overview \u00b6 A connector is a Java class that supports the standard Open Connector Framework ( OCF ) APIs. It is a client to a third party technology: Egeria calls the connector, and the connector translates these calls into requests to the third party technology. Some connectors are also able to listen for notifications from the third party technology: when a notification is received, the connector converts its content into a call to Egeria to distribute the information to the open metadata ecosystem. Egeria has a growing collection of connectors to third party technologies. These connectors help to accelerate the rollout of your open metadata ecosystem since they can be used to automate the extraction and distribution of metadata to the third party technologies. Connectors enable Egeria to operate in many environments and with many types of third party technologies, just by managing the configuration of the OMAG Servers . This allows new implementations to be provided and plugged-in over time, without changing the core code of Egeria itself. Connector implementations supplied by the Egeria community can be broadly organized into three categories: Runtime connectors \u00b6 Runtime connectors support Egeria's runtimes: Type Description Event bus connectors Cohort registry store connectors Configuration store connectors Audit log destination connector connectors Audit log store connectors support the reading and writing of audit log records to specific destinations on behalf of the audit log Open metadata archive store connectors REST client connectors Cohort member remote repository connectors OMRS topic connectors manage the exchange of OMRS events with the OMRS topic(s) by calling the open metadata topic connector Open metadata archive store connectors support the reading and writing of open metadata archives on behalf of the archive manager Open metadata topic connectors manage the calls to the event bus to support specific topic connectors such as the OMRS topic connector described above Cohort registry store connector support the reading and writing of the cohort registry store to specific destinations on behalf of the cohort registry Exchange connectors \u00b6 Exchange connectors support the exchange and maintenance of metadata: Type Description Integration connectors manage the metadata exchange to a third party technology through an integration service Repository connectors integrate metadata repositories through the repository services : either with native support for the open metadata types and instances or that map to a third party metadata repository API, in which case they are often paired with an event mapper connector Event mapper connectors inform a cohort of changes to metadata homed in a third party metadata repository that occurred through the third party technology's own mechanisms Discovery services analyze the content of resources in the digital landscape and create annotations that are attached to the resource's asset metadata element in the open metadata repositories in the form of an open discovery report Governance action services perform monitoring of metadata changes, validation of metadata, triage of issues, assessment and/or remediation activities on request Data connectors \u00b6 Data connectors provide access to digital resources and their metadata that is stored in the open metadata ecosystem.","title":"Connectors Overview"},{"location":"connectors/#connectors-overview","text":"A connector is a Java class that supports the standard Open Connector Framework ( OCF ) APIs. It is a client to a third party technology: Egeria calls the connector, and the connector translates these calls into requests to the third party technology. Some connectors are also able to listen for notifications from the third party technology: when a notification is received, the connector converts its content into a call to Egeria to distribute the information to the open metadata ecosystem. Egeria has a growing collection of connectors to third party technologies. These connectors help to accelerate the rollout of your open metadata ecosystem since they can be used to automate the extraction and distribution of metadata to the third party technologies. Connectors enable Egeria to operate in many environments and with many types of third party technologies, just by managing the configuration of the OMAG Servers . This allows new implementations to be provided and plugged-in over time, without changing the core code of Egeria itself. Connector implementations supplied by the Egeria community can be broadly organized into three categories:","title":"Connectors Overview"},{"location":"connectors/#runtime-connectors","text":"Runtime connectors support Egeria's runtimes: Type Description Event bus connectors Cohort registry store connectors Configuration store connectors Audit log destination connector connectors Audit log store connectors support the reading and writing of audit log records to specific destinations on behalf of the audit log Open metadata archive store connectors REST client connectors Cohort member remote repository connectors OMRS topic connectors manage the exchange of OMRS events with the OMRS topic(s) by calling the open metadata topic connector Open metadata archive store connectors support the reading and writing of open metadata archives on behalf of the archive manager Open metadata topic connectors manage the calls to the event bus to support specific topic connectors such as the OMRS topic connector described above Cohort registry store connector support the reading and writing of the cohort registry store to specific destinations on behalf of the cohort registry","title":"Runtime connectors"},{"location":"connectors/#exchange-connectors","text":"Exchange connectors support the exchange and maintenance of metadata: Type Description Integration connectors manage the metadata exchange to a third party technology through an integration service Repository connectors integrate metadata repositories through the repository services : either with native support for the open metadata types and instances or that map to a third party metadata repository API, in which case they are often paired with an event mapper connector Event mapper connectors inform a cohort of changes to metadata homed in a third party metadata repository that occurred through the third party technology's own mechanisms Discovery services analyze the content of resources in the digital landscape and create annotations that are attached to the resource's asset metadata element in the open metadata repositories in the form of an open discovery report Governance action services perform monitoring of metadata changes, validation of metadata, triage of issues, assessment and/or remediation activities on request","title":"Exchange connectors"},{"location":"connectors/#data-connectors","text":"Data connectors provide access to digital resources and their metadata that is stored in the open metadata ecosystem.","title":"Data connectors"},{"location":"connectors/audit-log-store-connector/","text":"Audit Log Store Connectors \u00b6 The audit log store connectors provide a common interface to an audit log destination. It is used by the OMRS 's audit log . The audit log supports multiple instances of the audit log store and will pass audit log records to each configured instance of the audit log store connectors. Implementations of this type of connector are located in the adapters/open-connectors/repository-services-connectors/audit-log-connectors module.","title":"Audit Log Store Connectors"},{"location":"connectors/audit-log-store-connector/#audit-log-store-connectors","text":"The audit log store connectors provide a common interface to an audit log destination. It is used by the OMRS 's audit log . The audit log supports multiple instances of the audit log store and will pass audit log records to each configured instance of the audit log store connectors. Implementations of this type of connector are located in the adapters/open-connectors/repository-services-connectors/audit-log-connectors module.","title":"Audit Log Store Connectors"},{"location":"connectors/cohort-registry-store-connector/","text":"Cohort Registry Store Connectors \u00b6 The cohort registry store stores information about the repositories registered in the open metadata repository cohort . Each server in the open metadata repository cohort has a cohort registry component to manage its registration with the cohort and maintain the contents of its local cohort registry store. The following diagram shows a cohort with two members: notice that each server has its own cohort registry store - there is no central store. The logical structure within the cohort registry store is as follows: There is one local registration record describing the information sent to the other members of the cohort and a list of remote registration records received from the other members of the cohort. An implementations of this type of connector is located in the adapters/open-connectors/repository-services-connectors/cohort-registry-store-connectors module.","title":"Cohort Registry Store Connector"},{"location":"connectors/cohort-registry-store-connector/#cohort-registry-store-connectors","text":"The cohort registry store stores information about the repositories registered in the open metadata repository cohort . Each server in the open metadata repository cohort has a cohort registry component to manage its registration with the cohort and maintain the contents of its local cohort registry store. The following diagram shows a cohort with two members: notice that each server has its own cohort registry store - there is no central store. The logical structure within the cohort registry store is as follows: There is one local registration record describing the information sent to the other members of the cohort and a list of remote registration records received from the other members of the cohort. An implementations of this type of connector is located in the adapters/open-connectors/repository-services-connectors/cohort-registry-store-connectors module.","title":"Cohort Registry Store Connectors"},{"location":"connectors/event-mapper-connector/","text":"Event mapper connector \u00b6 The event mapper connector provides a common API for specific implementations of OMRS event mappers to implement. Event mappers are needed in repository proxy servers if the third party technology that it is integrating into the open metadata repository cohort also has its own mechanisms for maintaining metadata. The event mapper's role is to notify the cohort of any changes to the metadata mastered in the third party repository that has occurred through the third party technology's own mechanisms. Since each event mapper is tied to a third party technology, core Egeria does not supply any implementations of this connector. There are, however, the following implementations available: Event mapper for Apache Atlas (Experimental) event mapper for IBM Information Governance Catalog","title":"Event mapper connector"},{"location":"connectors/event-mapper-connector/#event-mapper-connector","text":"The event mapper connector provides a common API for specific implementations of OMRS event mappers to implement. Event mappers are needed in repository proxy servers if the third party technology that it is integrating into the open metadata repository cohort also has its own mechanisms for maintaining metadata. The event mapper's role is to notify the cohort of any changes to the metadata mastered in the third party repository that has occurred through the third party technology's own mechanisms. Since each event mapper is tied to a third party technology, core Egeria does not supply any implementations of this connector. There are, however, the following implementations available: Event mapper for Apache Atlas (Experimental) event mapper for IBM Information Governance Catalog","title":"Event mapper connector"},{"location":"connectors/omrs-topic-connector/","text":"OMRS Topic Connector \u00b6 The OMRS topic connectors provide a common interface to interact with an instance of the OMRS event topic(s) .","title":"OMRS Topic Connectors"},{"location":"connectors/omrs-topic-connector/#omrs-topic-connector","text":"The OMRS topic connectors provide a common interface to interact with an instance of the OMRS event topic(s) .","title":"OMRS Topic Connector"},{"location":"connectors/open-metadata-archive-store-connector/","text":"Open Metadata Archive Store Connectors \u00b6 The open archive store connectors provide a common interface for managing stores of open metadata archives . Its interface is defined in OpenMetadataArchiveStore The open metadata archive structure is defined by OpenMetadataArchive :material-github . There are 3 sections: Section Description archiveProperties provides details of the source and contents of the archive archiveTypeStore a list of new AttributeTypeDefs, new TypeDefs and patches to existing TypeDefs archiveInstanceStore a list of new metadata instances (Entities, Relationships and Classifications) An implementations of this type of connector is located in the adapters/open-connectors/repository-services-connectors/open-metadata-archive-connectors module.","title":"Open Metadata Archive Store Connectors"},{"location":"connectors/open-metadata-archive-store-connector/#open-metadata-archive-store-connectors","text":"The open archive store connectors provide a common interface for managing stores of open metadata archives . Its interface is defined in OpenMetadataArchiveStore The open metadata archive structure is defined by OpenMetadataArchive :material-github . There are 3 sections: Section Description archiveProperties provides details of the source and contents of the archive archiveTypeStore a list of new AttributeTypeDefs, new TypeDefs and patches to existing TypeDefs archiveInstanceStore a list of new metadata instances (Entities, Relationships and Classifications) An implementations of this type of connector is located in the adapters/open-connectors/repository-services-connectors/open-metadata-archive-connectors module.","title":"Open Metadata Archive Store Connectors"},{"location":"connectors/open-metadata-topic-connector/","text":"Open Metadata Topic Connector \u00b6 The open metadata topic connector provides a topic interface to a generic string event. It is the interface implemented by specific event buses . Topic connectors for specific types of events (such as the OMRS Topic Connector ) are configured with an instance of an open metadata topic connector embedded inside it. The open metadata topic connector means that only one connector need be implemented for each type of event bus - rather than one for each type of topic that Egeria supports. Implementations of this type of connector are located in the adapters/open-connectors/repository-services-connectors/audit-log-connectors module.","title":"Open Metadata Topic Connectors"},{"location":"connectors/open-metadata-topic-connector/#open-metadata-topic-connector","text":"The open metadata topic connector provides a topic interface to a generic string event. It is the interface implemented by specific event buses . Topic connectors for specific types of events (such as the OMRS Topic Connector ) are configured with an instance of an open metadata topic connector embedded inside it. The open metadata topic connector means that only one connector need be implemented for each type of event bus - rather than one for each type of topic that Egeria supports. Implementations of this type of connector are located in the adapters/open-connectors/repository-services-connectors/audit-log-connectors module.","title":"Open Metadata Topic Connector"},{"location":"connectors/discovery/","text":"Open Discovery Services \u00b6 Open discovery services are connectors analyze the content of resources in the digital landscape and create annotations that are attached to the resource's Asset metadata element in the open metadata repositories in the form of an open discovery report. The definition of the connector interfaces for discovery services is defined in the open-discovery-services module. Connector Description CSV Discovery Service extracts the column names from the first line of the file, counts up the number of records in the file and extracts its last modified time Sequential Discovery Pipeline runs nested discovery services in a sequence ( more information on discovery pipelines )","title":"Open Discovery Services"},{"location":"connectors/discovery/#open-discovery-services","text":"Open discovery services are connectors analyze the content of resources in the digital landscape and create annotations that are attached to the resource's Asset metadata element in the open metadata repositories in the form of an open discovery report. The definition of the connector interfaces for discovery services is defined in the open-discovery-services module. Connector Description CSV Discovery Service extracts the column names from the first line of the file, counts up the number of records in the file and extracts its last modified time Sequential Discovery Pipeline runs nested discovery services in a sequence ( more information on discovery pipelines )","title":"Open Discovery Services"},{"location":"connectors/exchange/","text":"Exchange connectors \u00b6 These connectors help to accelerate the rollout of your open metadata ecosystem since they can be used to automate the extraction and distribution of metadata to the third party technologies. Integration connectors \u00b6 The integration connectors support the exchange of metadata with third party technologies. This exchange may be inbound, outbound, synchronous, polling or event-driven. Files \u00b6 The files integration connectors run in the Files Integrator Open Metadata Integration Service ( OMIS ) hosted in the integration daemon . Connector Description Data files monitor maintains a DataFile asset for each file in the directory (or any subdirectory) Data folder monitor maintains a DataFolder asset for the directory Databases \u00b6 The database integration connectors run in the Database Integrator Open Metadata Integration Service ( OMIS ) hosted in the integration daemon . Connector Description PostgreSQL database connector automatically maintains the open metadata instances for the databases hosted on a PostgreSQL server Security enforcement engines \u00b6 The security integration connectors run in the Security Integrator Open Metadata Integration Service ( OMIS ) hosted in the integration daemon . Repository connectors \u00b6 The repository connectors implement the OMRSMetadataCollection interface to allow metadata to be communicated and exchanged according to Egeria's protocols and type definitions . Pluggable repositories \u00b6 These connectors allow different back-ends to act as a native open metadata repositories in a metadata server . Connector Description JanusGraph OMRS Repository Connector provides a native repository for a metadata server using JanusGraph as the backend XTDB OMRS Repository Connector provides a native repository for a metadata server that supports historical queries, using XTDB as the backend In-memory OMRS Repository Connector provides a simple native repository implementation that \"stores\" metadata in HashMaps within the JVM ; it is used for testing, or for environments where metadata maintained in other repositories needs to be cached locally for performance/scalability reasons Read-only OMRS Repository Connector provides a native repository implementation that does not support the interfaces for create, update, delete; however, it does support the search interfaces and is able to cache metadata -- this means it can be loaded with open metadata archives to provide standard metadata definitions Repository adapters \u00b6 These connectors act as an adapter to integrate third party metadata repositories (catalogs) into an open metadata repository cohort . Connector Description Apache Atlas OMRS Repository Connector implements read-only connectivity to the Apache Atlas metadata repository IBM Information Governance Catalog ( IGC ) OMRS Repository Connector implements read-only connectivity to the metadata repository within the IBM InfoSphere Information Server suite SAS Viya OMRS Repository Connector implements metadata exchange to the metadata repository within the SAS Viya Platform Open discovery services \u00b6 Open discovery services are connectors that analyze the content of resources in the digital landscape and create annotations that are attached to the resource's Asset metadata element in the open metadata repositories in the form of an open discovery report. The interfaces used by a discovery service are defined in the Open Discovery Framework ( ODF ) along with a guide on how to write a discovery service. Connector Description CSV Discovery Service extracts the column names from the first line of the file, counts up the number of records in the file and extracts its last modified time Sequential Discovery Pipeline runs nested discovery services in a sequence ( more information on discovery pipelines ) The definition of the connector interfaces for discovery services is defined in the open-discovery-services module. Governance action services \u00b6 Governance action services are connectors that perform monitoring of metadata changes, validation of metadata, triage of issues, assessment and/or remediation activities on request. They run in the Governance Action Open Metadata Engine Service ( OMES ) hosted by the Engine Host OMAG Server . Connector Description Generic Element Watchdog Governance Action Service listens for changing metadata elements and initiates governance action processes when certain events occur Generic Folder Watchdog Governance Action Service listens for changing assets linked to a DataFolder element and initiates governance action processes when specific events occur Move/Copy File Provisioning Governance Action Service moves or copies files from one location to another and maintains the lineage of the action Origin Seeker Remediation Governance Action Service walks backwards through the lineage mappings to discover the origin of the data The definition of the connector interfaces for governance action services is defined in the governance-action-framework module.","title":"Index"},{"location":"connectors/exchange/#exchange-connectors","text":"These connectors help to accelerate the rollout of your open metadata ecosystem since they can be used to automate the extraction and distribution of metadata to the third party technologies.","title":"Exchange connectors"},{"location":"connectors/exchange/#integration-connectors","text":"The integration connectors support the exchange of metadata with third party technologies. This exchange may be inbound, outbound, synchronous, polling or event-driven.","title":"Integration connectors"},{"location":"connectors/exchange/#files","text":"The files integration connectors run in the Files Integrator Open Metadata Integration Service ( OMIS ) hosted in the integration daemon . Connector Description Data files monitor maintains a DataFile asset for each file in the directory (or any subdirectory) Data folder monitor maintains a DataFolder asset for the directory","title":"Files"},{"location":"connectors/exchange/#databases","text":"The database integration connectors run in the Database Integrator Open Metadata Integration Service ( OMIS ) hosted in the integration daemon . Connector Description PostgreSQL database connector automatically maintains the open metadata instances for the databases hosted on a PostgreSQL server","title":"Databases"},{"location":"connectors/exchange/#security-enforcement-engines","text":"The security integration connectors run in the Security Integrator Open Metadata Integration Service ( OMIS ) hosted in the integration daemon .","title":"Security enforcement engines"},{"location":"connectors/exchange/#repository-connectors","text":"The repository connectors implement the OMRSMetadataCollection interface to allow metadata to be communicated and exchanged according to Egeria's protocols and type definitions .","title":"Repository connectors"},{"location":"connectors/exchange/#pluggable-repositories","text":"These connectors allow different back-ends to act as a native open metadata repositories in a metadata server . Connector Description JanusGraph OMRS Repository Connector provides a native repository for a metadata server using JanusGraph as the backend XTDB OMRS Repository Connector provides a native repository for a metadata server that supports historical queries, using XTDB as the backend In-memory OMRS Repository Connector provides a simple native repository implementation that \"stores\" metadata in HashMaps within the JVM ; it is used for testing, or for environments where metadata maintained in other repositories needs to be cached locally for performance/scalability reasons Read-only OMRS Repository Connector provides a native repository implementation that does not support the interfaces for create, update, delete; however, it does support the search interfaces and is able to cache metadata -- this means it can be loaded with open metadata archives to provide standard metadata definitions","title":"Pluggable repositories"},{"location":"connectors/exchange/#repository-adapters","text":"These connectors act as an adapter to integrate third party metadata repositories (catalogs) into an open metadata repository cohort . Connector Description Apache Atlas OMRS Repository Connector implements read-only connectivity to the Apache Atlas metadata repository IBM Information Governance Catalog ( IGC ) OMRS Repository Connector implements read-only connectivity to the metadata repository within the IBM InfoSphere Information Server suite SAS Viya OMRS Repository Connector implements metadata exchange to the metadata repository within the SAS Viya Platform","title":"Repository adapters"},{"location":"connectors/exchange/#open-discovery-services","text":"Open discovery services are connectors that analyze the content of resources in the digital landscape and create annotations that are attached to the resource's Asset metadata element in the open metadata repositories in the form of an open discovery report. The interfaces used by a discovery service are defined in the Open Discovery Framework ( ODF ) along with a guide on how to write a discovery service. Connector Description CSV Discovery Service extracts the column names from the first line of the file, counts up the number of records in the file and extracts its last modified time Sequential Discovery Pipeline runs nested discovery services in a sequence ( more information on discovery pipelines ) The definition of the connector interfaces for discovery services is defined in the open-discovery-services module.","title":"Open discovery services"},{"location":"connectors/exchange/#governance-action-services","text":"Governance action services are connectors that perform monitoring of metadata changes, validation of metadata, triage of issues, assessment and/or remediation activities on request. They run in the Governance Action Open Metadata Engine Service ( OMES ) hosted by the Engine Host OMAG Server . Connector Description Generic Element Watchdog Governance Action Service listens for changing metadata elements and initiates governance action processes when certain events occur Generic Folder Watchdog Governance Action Service listens for changing assets linked to a DataFolder element and initiates governance action processes when specific events occur Move/Copy File Provisioning Governance Action Service moves or copies files from one location to another and maintains the lineage of the action Origin Seeker Remediation Governance Action Service walks backwards through the lineage mappings to discover the origin of the data The definition of the connector interfaces for governance action services is defined in the governance-action-framework module.","title":"Governance action services"},{"location":"connectors/governance-action/","text":"Governance Action Services \u00b6 Governance action services are connectors that perform monitoring of metadata changes, validation of metadata, triage of issues, assessment and/or remediation activities on request. They run in the Governance Action OMES hosted by the Engine Host OMAG Server . The definition of the connector interfaces for governance action services is defined in the governance-action-framework module.","title":"Governance Action Services"},{"location":"connectors/governance-action/#governance-action-services","text":"Governance action services are connectors that perform monitoring of metadata changes, validation of metadata, triage of issues, assessment and/or remediation activities on request. They run in the Governance Action OMES hosted by the Engine Host OMAG Server . The definition of the connector interfaces for governance action services is defined in the governance-action-framework module.","title":"Governance Action Services"},{"location":"connectors/governance-action/generic-element-watchdog-governance-action-service/","text":"Generic Element Watchdog Governance Action Service \u00b6 Connector details A watchdog governance action service , hosted by the Governance Action OMES , running on an engine host . Source: GenericElementWatchdogGovernanceActionProvider Connector archive: governance-action-connectors.jar The generic element watchdog governance action service detects changes to requested elements and initiates a governance action process when they occur. It has two modes of operation: listening for a single event and then terminating when it occurs, or continuously listening for multiple events. It is possible to listen for: specific types of elements specific instances specific types of events Configuration \u00b6 This connector uses the Governance Action OMES running in the engine host . The following properties can be set up in its connection's configuration properties and overridden by the request parameters: The interestingTypeName property takes the name of an element type. If set, it determines which types of elements are to be monitored. This monitoring includes all subtypes of this interesting type. If interestingTypeName is not set the default value is OpenMetadataRoot - effectively all elements with an open metadata type. The instanceToMonitor property takes the unique identifier of a metadata element. If set, this service will only consider events for this instance. If it is not set then all elements of the interesting type are monitored unless there are one or more action targets that are labelled with instanceToMonitor when this service starts. If the action targets are set up then these are the instances that are monitored. The rest of the properties are the governance action processes to call for specific types of events. The property is set to the qualified name of the process to run if the type of event occurs on the metadata instance(s) being monitored. If the property is not set, the type of event it refers to is ignored. Property Description newElementProcessName listen for new or refreshed elements updatedElementProcessName listen for changes to the properties in the element deletedElementProcessName listen for elements that have been deleted classifiedElementProcessName listen for elements that have had a new classification attached reclassifiedElementProcessName listen for elements that have had the properties in one of their classifications changed declassifiedElementProcessName listen for elements that have had a classification removed newRelationshipProcessName listen for new relationships linking these elements to other elements updatedRelationshipProcessName listen for changes to the properties of relationships that are attached to these elements deletedRelationshipProcessName listen for the removal of relationships attached to these elements Connection configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.connectors.governanceactions.watchdog.GenericElementWatchdogGovernanceActionProvider\" }, \"configurationProperties\" : { \"interestingTypeName\" : \"{{typeName}}\" , \"instanceToMonitor\" : \"{{guid}}\" , \"newElementProcessName\" : \"{{processQualifiedName}}\" , \"updatedElementProcessName\" : \"{{processQualifiedName}}\" , \"deletedElementProcessName\" : \"{{processQualifiedName}}\" , \"classifiedElementProcessName\" : \"{{processQualifiedName}}\" , \"reclassifiedElementProcessName\" : \"{{processQualifiedName}}\" , \"declassifiedElementProcessName\" : \"{{processQualifiedName}}\" , \"newRelationshipProcessName\" : \"{{processQualifiedName}}\" , \"updatedRelationshipProcessName\" : \"{{processQualifiedName}}\" , \"deletedRelationshipProcessName\" : \"{{processQualifiedName}}\" } } } This is its connection definition to use when creating the definition of the governance action service using the Governance Engine OMAS . Remove the configuration properties that are not required. Replace {{typeName}} , {{guid}} and {{processQualifiedName}} as required. Governance action settings \u00b6 When this governance action service is called through a GovernanceAction it supports the following options: Request types and parameters \u00b6 There are two request types that control its modes of operation: process-single-event to request it monitors for a single specific event and then completes. process-multiple-events to request it continuously monitors for events until it fails. If the engine host server where it is running is restarted, this governance action service is also restarted. Any of the configuration properties can be overridden by request parameters of the same name. Action targets \u00b6 The instanceToMonitor property can be supplied as a name action target. Using action targets allows the instance to be dynamically controlled and for multiple instances to be monitored. Completion status and guards \u00b6 This service will only complete and produce a guard if it encounters an unrecoverable error or it is set up to listen for a single event and that event occurs. On completion, this governance action service uses: CompletionStatus.ACTIONED with guard monitoring-complete - requested single event occurred, or CompletionStatus.FAILED with guard monitoring-failed - monitor not configured correctly or failed Further information This connector is configured in the governDL01 engine host server as part of the automated curation asset management hands-on lab .","title":"Generic Element Watchdog"},{"location":"connectors/governance-action/generic-element-watchdog-governance-action-service/#generic-element-watchdog-governance-action-service","text":"Connector details A watchdog governance action service , hosted by the Governance Action OMES , running on an engine host . Source: GenericElementWatchdogGovernanceActionProvider Connector archive: governance-action-connectors.jar The generic element watchdog governance action service detects changes to requested elements and initiates a governance action process when they occur. It has two modes of operation: listening for a single event and then terminating when it occurs, or continuously listening for multiple events. It is possible to listen for: specific types of elements specific instances specific types of events","title":"Generic Element Watchdog Governance Action Service"},{"location":"connectors/governance-action/generic-element-watchdog-governance-action-service/#configuration","text":"This connector uses the Governance Action OMES running in the engine host . The following properties can be set up in its connection's configuration properties and overridden by the request parameters: The interestingTypeName property takes the name of an element type. If set, it determines which types of elements are to be monitored. This monitoring includes all subtypes of this interesting type. If interestingTypeName is not set the default value is OpenMetadataRoot - effectively all elements with an open metadata type. The instanceToMonitor property takes the unique identifier of a metadata element. If set, this service will only consider events for this instance. If it is not set then all elements of the interesting type are monitored unless there are one or more action targets that are labelled with instanceToMonitor when this service starts. If the action targets are set up then these are the instances that are monitored. The rest of the properties are the governance action processes to call for specific types of events. The property is set to the qualified name of the process to run if the type of event occurs on the metadata instance(s) being monitored. If the property is not set, the type of event it refers to is ignored. Property Description newElementProcessName listen for new or refreshed elements updatedElementProcessName listen for changes to the properties in the element deletedElementProcessName listen for elements that have been deleted classifiedElementProcessName listen for elements that have had a new classification attached reclassifiedElementProcessName listen for elements that have had the properties in one of their classifications changed declassifiedElementProcessName listen for elements that have had a classification removed newRelationshipProcessName listen for new relationships linking these elements to other elements updatedRelationshipProcessName listen for changes to the properties of relationships that are attached to these elements deletedRelationshipProcessName listen for the removal of relationships attached to these elements Connection configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.connectors.governanceactions.watchdog.GenericElementWatchdogGovernanceActionProvider\" }, \"configurationProperties\" : { \"interestingTypeName\" : \"{{typeName}}\" , \"instanceToMonitor\" : \"{{guid}}\" , \"newElementProcessName\" : \"{{processQualifiedName}}\" , \"updatedElementProcessName\" : \"{{processQualifiedName}}\" , \"deletedElementProcessName\" : \"{{processQualifiedName}}\" , \"classifiedElementProcessName\" : \"{{processQualifiedName}}\" , \"reclassifiedElementProcessName\" : \"{{processQualifiedName}}\" , \"declassifiedElementProcessName\" : \"{{processQualifiedName}}\" , \"newRelationshipProcessName\" : \"{{processQualifiedName}}\" , \"updatedRelationshipProcessName\" : \"{{processQualifiedName}}\" , \"deletedRelationshipProcessName\" : \"{{processQualifiedName}}\" } } } This is its connection definition to use when creating the definition of the governance action service using the Governance Engine OMAS . Remove the configuration properties that are not required. Replace {{typeName}} , {{guid}} and {{processQualifiedName}} as required.","title":"Configuration"},{"location":"connectors/governance-action/generic-element-watchdog-governance-action-service/#governance-action-settings","text":"When this governance action service is called through a GovernanceAction it supports the following options:","title":"Governance action settings"},{"location":"connectors/governance-action/generic-element-watchdog-governance-action-service/#request-types-and-parameters","text":"There are two request types that control its modes of operation: process-single-event to request it monitors for a single specific event and then completes. process-multiple-events to request it continuously monitors for events until it fails. If the engine host server where it is running is restarted, this governance action service is also restarted. Any of the configuration properties can be overridden by request parameters of the same name.","title":"Request types and parameters"},{"location":"connectors/governance-action/generic-element-watchdog-governance-action-service/#action-targets","text":"The instanceToMonitor property can be supplied as a name action target. Using action targets allows the instance to be dynamically controlled and for multiple instances to be monitored.","title":"Action targets"},{"location":"connectors/governance-action/generic-element-watchdog-governance-action-service/#completion-status-and-guards","text":"This service will only complete and produce a guard if it encounters an unrecoverable error or it is set up to listen for a single event and that event occurs. On completion, this governance action service uses: CompletionStatus.ACTIONED with guard monitoring-complete - requested single event occurred, or CompletionStatus.FAILED with guard monitoring-failed - monitor not configured correctly or failed Further information This connector is configured in the governDL01 engine host server as part of the automated curation asset management hands-on lab .","title":"Completion status and guards"},{"location":"connectors/governance-action/generic-folder-watchdog-governance-action-service/","text":"Generic Folder Watchdog Governance Action Service \u00b6 Connector details A watchdog governance action service , hosted by the Governance Action OMES , running on an engine host . Source: GenericFolderWatchdogGovernanceActionProvider Connector archive: governance-action-connectors.jar The generic folder watchdog governance action service detects changes to the assets in a specific folder and initiates a governance action process when they occur. It has two modes of operation: listening for a single event and then terminating when it occurs, or continuously listening for multiple events. It is possible to listen for: assets directly in the folder - and optionally assets in any nested folder specific types of events Configuration \u00b6 This connector uses the Governance Action OMES running in the engine host . The following properties that can be set up in its connection's configuration properties and overridden by the request parameters. The interestingTypeName property takes the name of a DataFile type. If set, it determines which types of assets are to be monitored. This monitoring includes all subtypes of this interesting type. If interestingTypeName is not set the default value is DataFile - effectively all files with an open metadata type. The rest of the properties are the governance action processes to call for specific types of events. The property is set to the qualified name of the process to run if the type of event occurs on the metadata instance(s) being monitored. If the property is not set, the type of event it refers to is ignored. Property Description newElementProcessName listen for new or refreshed elements updatedElementProcessName listen for changes to the properties in the element deletedElementProcessName listen for elements that have been deleted classifiedElementProcessName listen for elements that have had a new classification attached reclassifiedElementProcessName listen for elements that have had the properties in one of their classifications changed declassifiedElementProcessName listen for elements that have had a classification removed Connection configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.connectors.governanceactions.watchdog.GenericFolderWatchdogGovernanceActionProvider\" }, \"configurationProperties\" : { \"interestingTypeName\" : \"{{typeName}}\" , \"newElementProcessName\" : \"{{processQualifiedName}}\" , \"updatedElementProcessName\" : \"{{processQualifiedName}}\" , \"deletedElementProcessName\" : \"{{processQualifiedName}}\" , \"classifiedElementProcessName\" : \"{{processQualifiedName}}\" , \"reclassifiedElementProcessName\" : \"{{processQualifiedName}}\" , \"declassifiedElementProcessName\" : \"{{processQualifiedName}}\" } } } This is its connection definition to use when creating the definition of the governance action service using the Governance Engine OMAS . Remove the configuration properties that are not required. Replace {{typeName}} , {{guid}} and {{processQualifiedName}} as required. Governance action settings \u00b6 When this governance action service is called through a GovernanceAction it supports the following options: Request types and parameters \u00b6 There are two request types that control its modes of operation: member-of-folder to request it monitors for file assets that are directly in the named folder. nested-in-folder to request it monitors for file assets that are either directly in the named folder or any sub-folder. Any of the configuration properties can be overridden by request parameters of the same name. Action targets \u00b6 The folderTarget property can be supplied as a named action target. Using action targets allows the instance to be dynamically controlled and for multiple instances to be monitored. Completion status and guards \u00b6 This service will only complete and produce a guard if it encounters an unrecoverable error. In which case, this governance action service uses: CompletionStatus.FAILED with guard monitoring-failed It is also possible to shutdown the governance action service by setting CompletionStatus.ACTIONED with guard monitoring-completed in the governance action. Further information This connector is configured in the governDL01 engine host server as part of the automated curation asset management hands-on lab .","title":"Generic Folder Watchdog"},{"location":"connectors/governance-action/generic-folder-watchdog-governance-action-service/#generic-folder-watchdog-governance-action-service","text":"Connector details A watchdog governance action service , hosted by the Governance Action OMES , running on an engine host . Source: GenericFolderWatchdogGovernanceActionProvider Connector archive: governance-action-connectors.jar The generic folder watchdog governance action service detects changes to the assets in a specific folder and initiates a governance action process when they occur. It has two modes of operation: listening for a single event and then terminating when it occurs, or continuously listening for multiple events. It is possible to listen for: assets directly in the folder - and optionally assets in any nested folder specific types of events","title":"Generic Folder Watchdog Governance Action Service"},{"location":"connectors/governance-action/generic-folder-watchdog-governance-action-service/#configuration","text":"This connector uses the Governance Action OMES running in the engine host . The following properties that can be set up in its connection's configuration properties and overridden by the request parameters. The interestingTypeName property takes the name of a DataFile type. If set, it determines which types of assets are to be monitored. This monitoring includes all subtypes of this interesting type. If interestingTypeName is not set the default value is DataFile - effectively all files with an open metadata type. The rest of the properties are the governance action processes to call for specific types of events. The property is set to the qualified name of the process to run if the type of event occurs on the metadata instance(s) being monitored. If the property is not set, the type of event it refers to is ignored. Property Description newElementProcessName listen for new or refreshed elements updatedElementProcessName listen for changes to the properties in the element deletedElementProcessName listen for elements that have been deleted classifiedElementProcessName listen for elements that have had a new classification attached reclassifiedElementProcessName listen for elements that have had the properties in one of their classifications changed declassifiedElementProcessName listen for elements that have had a classification removed Connection configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.connectors.governanceactions.watchdog.GenericFolderWatchdogGovernanceActionProvider\" }, \"configurationProperties\" : { \"interestingTypeName\" : \"{{typeName}}\" , \"newElementProcessName\" : \"{{processQualifiedName}}\" , \"updatedElementProcessName\" : \"{{processQualifiedName}}\" , \"deletedElementProcessName\" : \"{{processQualifiedName}}\" , \"classifiedElementProcessName\" : \"{{processQualifiedName}}\" , \"reclassifiedElementProcessName\" : \"{{processQualifiedName}}\" , \"declassifiedElementProcessName\" : \"{{processQualifiedName}}\" } } } This is its connection definition to use when creating the definition of the governance action service using the Governance Engine OMAS . Remove the configuration properties that are not required. Replace {{typeName}} , {{guid}} and {{processQualifiedName}} as required.","title":"Configuration"},{"location":"connectors/governance-action/generic-folder-watchdog-governance-action-service/#governance-action-settings","text":"When this governance action service is called through a GovernanceAction it supports the following options:","title":"Governance action settings"},{"location":"connectors/governance-action/generic-folder-watchdog-governance-action-service/#request-types-and-parameters","text":"There are two request types that control its modes of operation: member-of-folder to request it monitors for file assets that are directly in the named folder. nested-in-folder to request it monitors for file assets that are either directly in the named folder or any sub-folder. Any of the configuration properties can be overridden by request parameters of the same name.","title":"Request types and parameters"},{"location":"connectors/governance-action/generic-folder-watchdog-governance-action-service/#action-targets","text":"The folderTarget property can be supplied as a named action target. Using action targets allows the instance to be dynamically controlled and for multiple instances to be monitored.","title":"Action targets"},{"location":"connectors/governance-action/generic-folder-watchdog-governance-action-service/#completion-status-and-guards","text":"This service will only complete and produce a guard if it encounters an unrecoverable error. In which case, this governance action service uses: CompletionStatus.FAILED with guard monitoring-failed It is also possible to shutdown the governance action service by setting CompletionStatus.ACTIONED with guard monitoring-completed in the governance action. Further information This connector is configured in the governDL01 engine host server as part of the automated curation asset management hands-on lab .","title":"Completion status and guards"},{"location":"connectors/integration/","text":"Integration Connectors \u00b6 An integration connector is a pluggable component that manages the metadata exchange to a third party technology. It is hosted in an integration service which is, in turn, running in an integration daemon . The integration connectors can: Listen on a blocking call for the third party technology to send a notification. Register with an external notification service that sends notifications on its own thread. Register a listener with the OMAS client to act on notifications from the OMAS 's Out Topic . Poll the third party technology each time that the refresh() method is called. Interface \u00b6 The interface that all integration connectors must implement is defined by IntegrationConnectorBase : initialize is a standard method for all connectors that is called by the connector broker when the connector is created. The connector is passed the connection object from the configuration and a unique identifier for this instance of the connector. setAuditLog provides a logging destination (see Audit Log Framework ( ALF ) ). setConnectorName provides the name of the connector for logging. setContext sets up the integration-specific context. This provides an interface to the services of the OMAS that the integration service is paired with. Although the interfaces vary from integration service to integration service, they typically offer the following types of method call for each type of metadata it supports: The ability to register a listener to receive events from the OMAS 's Out Topic , or send events to the OMAS 's In Topic . The ability to create and update metadata instances. For assets, the ability to change an asset's visibility by changing its zone membership using the publish and withdraw methods. The ability to delete metadata. Various retrieval methods to help when comparing the metadata in the open metadata repositories with the metadata in the third party technology. start indicates that the connector is completely configured and can begin processing. This call can be used to register with non-blocking services. For example, it can register a listener with the OMAS Out Topic with the context. engage is used when the connector is configured to need to issue blocking calls to wait for new metadata. It is called from its own thread. It is recommended that the engage() method returns when each blocking call completes. The integration daemon will pause a second and then call engage() again. This pattern enables the calling thread to detect the shutdown of the integration daemon server. refresh requests that the connector does a comparison of the metadata in the third party technology and open metadata repositories. Refresh is called: when the integration connector first starts and then at intervals defined in the connector's configuration as well as any external REST API calls to explicitly refresh the connector. disconnect is called when the server is shutting down. The connector should free up any resources that it holds since it is not needed any more. Further information Open Connector Framework ( OCF ) that defines the behavior of all connectors. Configuring an integration daemon to understand how to set up an integration connector. Developer guide for more information on writing connectors.","title":"Integration Connectors"},{"location":"connectors/integration/#integration-connectors","text":"An integration connector is a pluggable component that manages the metadata exchange to a third party technology. It is hosted in an integration service which is, in turn, running in an integration daemon . The integration connectors can: Listen on a blocking call for the third party technology to send a notification. Register with an external notification service that sends notifications on its own thread. Register a listener with the OMAS client to act on notifications from the OMAS 's Out Topic . Poll the third party technology each time that the refresh() method is called.","title":"Integration Connectors"},{"location":"connectors/integration/#interface","text":"The interface that all integration connectors must implement is defined by IntegrationConnectorBase : initialize is a standard method for all connectors that is called by the connector broker when the connector is created. The connector is passed the connection object from the configuration and a unique identifier for this instance of the connector. setAuditLog provides a logging destination (see Audit Log Framework ( ALF ) ). setConnectorName provides the name of the connector for logging. setContext sets up the integration-specific context. This provides an interface to the services of the OMAS that the integration service is paired with. Although the interfaces vary from integration service to integration service, they typically offer the following types of method call for each type of metadata it supports: The ability to register a listener to receive events from the OMAS 's Out Topic , or send events to the OMAS 's In Topic . The ability to create and update metadata instances. For assets, the ability to change an asset's visibility by changing its zone membership using the publish and withdraw methods. The ability to delete metadata. Various retrieval methods to help when comparing the metadata in the open metadata repositories with the metadata in the third party technology. start indicates that the connector is completely configured and can begin processing. This call can be used to register with non-blocking services. For example, it can register a listener with the OMAS Out Topic with the context. engage is used when the connector is configured to need to issue blocking calls to wait for new metadata. It is called from its own thread. It is recommended that the engage() method returns when each blocking call completes. The integration daemon will pause a second and then call engage() again. This pattern enables the calling thread to detect the shutdown of the integration daemon server. refresh requests that the connector does a comparison of the metadata in the third party technology and open metadata repositories. Refresh is called: when the integration connector first starts and then at intervals defined in the connector's configuration as well as any external REST API calls to explicitly refresh the connector. disconnect is called when the server is shutting down. The connector should free up any resources that it holds since it is not needed any more. Further information Open Connector Framework ( OCF ) that defines the behavior of all connectors. Configuring an integration daemon to understand how to set up an integration connector. Developer guide for more information on writing connectors.","title":"Interface"},{"location":"connectors/integration/data-files-monitor-integration-connector/","text":"Data Files Monitor Integration Connector \u00b6 Connector details An integration connector , hosted by the Files Integrator OMIS , running on an integration daemon . Source: files-integration-connectors Connector archive: files-integration-connectors.jar The data files monitor integration connector monitors changes in a file directory (folder) and updates the open metadata repository/repositories to reflect the changes to both the files and folders underneath it. Specifically: A DataFile asset is created and then maintained for each file in the folder (or any sub-folder). When a new file is created, a new DataFile asset is created. If a file is modified, the lastModified property of the corresponding DataFile asset is updated. When a file is deleted, its corresponding DataFile asset is either: Archived: this means the asset is no longer returned on standard asset catalog searches, but it is still visible in lineage graphs . This is the default behavior. Deleted: this means that all metadata associated with the data file is removed. Only use this option if lineage is not important for these file. A FileFolder metadata asset for the monitored folder is created when the first file is catalogued, if it does not already exist. Configuration \u00b6 This connector uses the Files Integrator OMIS running in the integration daemon . Following is its connection definition to use on the administration commands that configure the Files Integrator OMIS : Connection configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.connectors.integration.basicfiles.DataFilesMonitorIntegrationProvider\" }, \"endpoint\" : { \"class\" : \"Endpoint\" , \"address\" : \"{{folderName}}\" }, \"configurationProperties\" : { \"templateQualifiedName\" : \"{{templateQualifiedName}}\" , \"allowCatalogDelete\" : \"\" } } } Replace {{folderName}} with the path name of the folder where the files will be located. The configurationProperties are optional and are used to override the connector's default behavior: If templateQualifiedName is present in the configuration properties then {{templateQualifiedName}} must be set to the qualified name of a DataFile metadata element that should be used as a template for the catalog entry for new files discovered by this connector. The base properties, schema, connection, classifications and any other attachments connected to the template are copied to the new metadata element for the file. (See templated cataloging for more information on the use of templates.) If allowCatalogDelete is present in the configuration properties then the metadata element for a file is deleted when the file is deleted. If this property is not in the configuration properties, then the metadata element is archived (by adding the Memento classification to its entry). The archived element is no longer returned in standard catalog queries, but it is still visible in lineage . Do not set allowCatalogDelete if lineage of these files is important. Further information This connector is configured in the exchangeDL01 integration daemon server in the open metadata labs","title":"Data Files Monitor"},{"location":"connectors/integration/data-files-monitor-integration-connector/#data-files-monitor-integration-connector","text":"Connector details An integration connector , hosted by the Files Integrator OMIS , running on an integration daemon . Source: files-integration-connectors Connector archive: files-integration-connectors.jar The data files monitor integration connector monitors changes in a file directory (folder) and updates the open metadata repository/repositories to reflect the changes to both the files and folders underneath it. Specifically: A DataFile asset is created and then maintained for each file in the folder (or any sub-folder). When a new file is created, a new DataFile asset is created. If a file is modified, the lastModified property of the corresponding DataFile asset is updated. When a file is deleted, its corresponding DataFile asset is either: Archived: this means the asset is no longer returned on standard asset catalog searches, but it is still visible in lineage graphs . This is the default behavior. Deleted: this means that all metadata associated with the data file is removed. Only use this option if lineage is not important for these file. A FileFolder metadata asset for the monitored folder is created when the first file is catalogued, if it does not already exist.","title":"Data Files Monitor Integration Connector"},{"location":"connectors/integration/data-files-monitor-integration-connector/#configuration","text":"This connector uses the Files Integrator OMIS running in the integration daemon . Following is its connection definition to use on the administration commands that configure the Files Integrator OMIS : Connection configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.connectors.integration.basicfiles.DataFilesMonitorIntegrationProvider\" }, \"endpoint\" : { \"class\" : \"Endpoint\" , \"address\" : \"{{folderName}}\" }, \"configurationProperties\" : { \"templateQualifiedName\" : \"{{templateQualifiedName}}\" , \"allowCatalogDelete\" : \"\" } } } Replace {{folderName}} with the path name of the folder where the files will be located. The configurationProperties are optional and are used to override the connector's default behavior: If templateQualifiedName is present in the configuration properties then {{templateQualifiedName}} must be set to the qualified name of a DataFile metadata element that should be used as a template for the catalog entry for new files discovered by this connector. The base properties, schema, connection, classifications and any other attachments connected to the template are copied to the new metadata element for the file. (See templated cataloging for more information on the use of templates.) If allowCatalogDelete is present in the configuration properties then the metadata element for a file is deleted when the file is deleted. If this property is not in the configuration properties, then the metadata element is archived (by adding the Memento classification to its entry). The archived element is no longer returned in standard catalog queries, but it is still visible in lineage . Do not set allowCatalogDelete if lineage of these files is important. Further information This connector is configured in the exchangeDL01 integration daemon server in the open metadata labs","title":"Configuration"},{"location":"connectors/integration/data-folder-monitor-integration-connector/","text":"Data Folder Monitor Integration Connector \u00b6 Connector details An integration connector , hosted by the Files Integrator OMIS , running on an integration daemon . Source: files-integration-connectors Connector archive: files-integration-connectors.jar The data folder monitor integration connector monitor changes in a file directory (folder) and maintains a DataFolder asset for the folder. The files and directories underneath it are assumed to be elements/records in the DataFolder asset and so each time there is a change to the files and directories under the monitored directory, it results in an update to the lastModified property of the corresponding DataFolder asset. Assumes the DataFolder asset already exists This connector assumes that the DataFolder asset is already defined. If it cannot retrieve the DataFolder asset, it ignores file changes. Configuration \u00b6 This connector uses the Files Integrator OMIS running in the integration daemon . Following is its connection definition to use on the administration commands that configure the Files Integrator OMIS : Connection configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.connectors.integration.basicfiles.DataFolderMonitorIntegrationProvider\" }, \"endpoint\" : { \"class\" : \"Endpoint\" , \"address\" : \"{{folderName}}\" } } } Replace {{folderName}} with the path name of the folder to monitor. Further information This connector is configured in the exchangeDL01 integration daemon server in the open metadata labs","title":"Data Folder Monitor"},{"location":"connectors/integration/data-folder-monitor-integration-connector/#data-folder-monitor-integration-connector","text":"Connector details An integration connector , hosted by the Files Integrator OMIS , running on an integration daemon . Source: files-integration-connectors Connector archive: files-integration-connectors.jar The data folder monitor integration connector monitor changes in a file directory (folder) and maintains a DataFolder asset for the folder. The files and directories underneath it are assumed to be elements/records in the DataFolder asset and so each time there is a change to the files and directories under the monitored directory, it results in an update to the lastModified property of the corresponding DataFolder asset. Assumes the DataFolder asset already exists This connector assumes that the DataFolder asset is already defined. If it cannot retrieve the DataFolder asset, it ignores file changes.","title":"Data Folder Monitor Integration Connector"},{"location":"connectors/integration/data-folder-monitor-integration-connector/#configuration","text":"This connector uses the Files Integrator OMIS running in the integration daemon . Following is its connection definition to use on the administration commands that configure the Files Integrator OMIS : Connection configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.connectors.integration.basicfiles.DataFolderMonitorIntegrationProvider\" }, \"endpoint\" : { \"class\" : \"Endpoint\" , \"address\" : \"{{folderName}}\" } } } Replace {{folderName}} with the path name of the folder to monitor. Further information This connector is configured in the exchangeDL01 integration daemon server in the open metadata labs","title":"Configuration"},{"location":"connectors/repository/","text":"OMRS Repository Connector \u00b6 Repository Connectors make use of the Egeria meta-model to represent and communicate metadata. The OMRS Repository Connector API defines a call interface to create, search, query, update and delete metadata stored in a metadata repository. The implementation of a specific OMRS connector determines which type(s) of metadata repository it is able to access. The OMRS has three repository connector implementations that form part of the core open metadata capability for a cohort member : Enterprise Repository Connector - This connector can issue calls to multiple OMRS connectors and aggregate the results as if the metadata was stored in a single repository. This is how metadata queries are federated across open metadata repositories. Since all implementations of OMRS repository connectors have the same API, the Enterprise Repository Connector is able to work with a heterogeneous collection of repositories. Local OMRS Repository Connector - This connector wraps a \"real\" repository connector (see below) and manages events and validation for this connector. OMRS REST Repository Connector - This is the connector used by the Enterprise Repository Connector to make a direct call to another open metadata repository through the OMRS REST API . These are the \"real\" OMRS Repository Connector implementations that provide open metadata access to specific types of metadata repositories. Apache Atlas Repository Connector - This is the connector that runs in an Egeria repository proxy server, pulling in the key parts of Egeria it needs to support the open metadata standards. It calls directly to Apache Atlas's REST API interface for the metadata repository. IGC OMRS Repository Connector - This is the connector for retrieving metadata from IBM's Information Governance Catalog (aka IGC ). This connector translates the calls to its OMRS Connector API to IGC 's REST API and then translates the results of these calls to appropriate responses on its API. This connector also runs in a repository proxy server. In-memory OMRS Repository Connector - This connector provides a simple in-memory repository for testing/demos or small-scale environments where metadata is being managed remotely and cached locally. It has native support for the open metadata types an instances and so runs in a metadata server . Graph OMRS Repository Connector - This connector is provides a high functioning open metadata repository implementation built on JanusGraph. It also has native support for the open metadata types an instances and so runs in a metadata server .","title":"Repository Connectors"},{"location":"connectors/repository/#omrs-repository-connector","text":"Repository Connectors make use of the Egeria meta-model to represent and communicate metadata. The OMRS Repository Connector API defines a call interface to create, search, query, update and delete metadata stored in a metadata repository. The implementation of a specific OMRS connector determines which type(s) of metadata repository it is able to access. The OMRS has three repository connector implementations that form part of the core open metadata capability for a cohort member : Enterprise Repository Connector - This connector can issue calls to multiple OMRS connectors and aggregate the results as if the metadata was stored in a single repository. This is how metadata queries are federated across open metadata repositories. Since all implementations of OMRS repository connectors have the same API, the Enterprise Repository Connector is able to work with a heterogeneous collection of repositories. Local OMRS Repository Connector - This connector wraps a \"real\" repository connector (see below) and manages events and validation for this connector. OMRS REST Repository Connector - This is the connector used by the Enterprise Repository Connector to make a direct call to another open metadata repository through the OMRS REST API . These are the \"real\" OMRS Repository Connector implementations that provide open metadata access to specific types of metadata repositories. Apache Atlas Repository Connector - This is the connector that runs in an Egeria repository proxy server, pulling in the key parts of Egeria it needs to support the open metadata standards. It calls directly to Apache Atlas's REST API interface for the metadata repository. IGC OMRS Repository Connector - This is the connector for retrieving metadata from IBM's Information Governance Catalog (aka IGC ). This connector translates the calls to its OMRS Connector API to IGC 's REST API and then translates the results of these calls to appropriate responses on its API. This connector also runs in a repository proxy server. In-memory OMRS Repository Connector - This connector provides a simple in-memory repository for testing/demos or small-scale environments where metadata is being managed remotely and cached locally. It has native support for the open metadata types an instances and so runs in a metadata server . Graph OMRS Repository Connector - This connector is provides a high functioning open metadata repository implementation built on JanusGraph. It also has native support for the open metadata types an instances and so runs in a metadata server .","title":"OMRS Repository Connector"},{"location":"connectors/repository/xtdb/","text":"XTDB OMRS Repository Connector \u00b6 Fully conformant with all Egeria profiles Last tested on release 3.1 of Egeria, release 3.1 of connector using release 1.18.1 of XTDB . Profile Result Metadata sharing CONFORMANT_FULL_SUPPORT Reference copies CONFORMANT_FULL_SUPPORT Metadata maintenance CONFORMANT_FULL_SUPPORT Dynamic types UNKNOWN_STATUS Graph queries CONFORMANT_FULL_SUPPORT Historical search CONFORMANT_FULL_SUPPORT Entity proxies CONFORMANT_FULL_SUPPORT Soft-delete and restore CONFORMANT_FULL_SUPPORT Undo an update CONFORMANT_FULL_SUPPORT Reidentify instance CONFORMANT_FULL_SUPPORT Retype instance CONFORMANT_FULL_SUPPORT Rehome instance CONFORMANT_FULL_SUPPORT Entity search CONFORMANT_FULL_SUPPORT Relationship search CONFORMANT_FULL_SUPPORT Entity advanced search CONFORMANT_FULL_SUPPORT Relationship advanced search CONFORMANT_FULL_SUPPORT Additional notes The entity search tests could fail a particular long-running query pattern unless Lucene is configured: typically where a query by value or attribute is done without providing any restriction on the type of instances against which the query should run. Configure the connector with Lucene to avoid these timeouts. The Dynamic types profile currently does not have any tests defined, so will be UNKNOWN_STATUS for all repositories and connectors. Latest release Latest snapshot Navigate to the latest snapshot directory, and within that find the latest connector archive with the name: egeria-connector-xtdb-{version}-jar-with-dependencies.jar Source The connector is hosted in its own repository at odpi/egeria-connector-xtdb , where the source code can be cloned and the connector built from source. The XTDB OMRS repository connector enables the use of XTDB (formerly known as \"Crux\") and its own pluggable architecture to support a variety of underlying storage back-ends including S3, RocksDB, Apache Kafka, LMDB, JDBC and more. XTDB supports temporal graph queries to provide native support for storing historical information and answering temporal queries. The connector is also capable of running as a highly-available service. In addition, currently this is the highest-performance open source persistent repository for Egeria across all operations: read, write, update, search and purge. How it works \u00b6 The XTDB OMRS Repository Connector is a repository connector , hosted by the plugin repository proxy , running on a metadata server . XTDB itself is started as an embedded process within the connector. It can be configured to use any of the various pluggable persistence layers supported by XTDB itself, and communication between the Java code of the connector and XTDB itself (which is implemented in Clojure) occurs directly via the XTDB Java API (not via REST). The repository connector (and metadata collection) methods of the repository connector interface simply communicate with XTDB via XTDB's Java API to read and write information to the underlying XTDB node. XTDB itself handles write transactions and persistence guarantees via its APIs, ensuring that all data is at least recorded into the transaction log and document store prior to any write method returning. Synchronous by default, but configurable for asynchronous operation By default, the repository connector further awaits confirmation that any write has been indexed (and is therefore available for read operations) prior to returning. However, it is also possible to configure the connector in an \"ingest-optimized\" mode that allows the indexing to occur asynchronously, and can therefore improve the speed of write operations significantly. Configuration \u00b6 The following options are used to configure this connector, as part of the configure the local repository step when configuring a metadata server . Pluggable persistence \u00b6 There are many options for configuring XTDB itself. A list of overall persistence modules and deeper configuration options for each can be found through XTDB's own documentation . To enable persistence, there are two options: send in the JSON document configuration outlined in XTDB's own documentation directly to the xtdbConfig key of the configurationProperties property of Egeria's connector configuration send in a string to the xtdbConfigEDN key of the configurationProperties of Egeria's connector configuration, which gives the EDN form of configuration outlined in XTDB's own documentation Both approaches are valid and should be equally functional, but occasionally a bug may crop up that makes one or the other more or less feasible for a particular configuration. Example persistence using JSON configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.egeria.connectors.juxt.xtdb.repositoryconnector.XtdbOMRSRepositoryConnectorProvider\" }, \"configurationProperties\" : { \"xtdbConfig\" : { \"xtdb.lucene/lucene-store\" : { \"db-dir\" : \"data/servers/xtdb/lucene\" }, \"xtdb/index-store\" : { \"kv-store\" : { \"xtdb/module\" : \"xtdb.rocksdb/->kv-store\" , \"db-dir\" : \"data/servers/xtdb/rdb-index\" } }, \"xtdb/document-store\" : { \"kv-store\" : { \"xtdb/module\" : \"xtdb.rocksdb/->kv-store\" , \"db-dir\" : \"data/servers/xtdb/rdb-docs\" } }, \"xtdb/tx-log\" : { \"kv-store\" : { \"xtdb/module\" : \"xtdb.rocksdb/->kv-store\" , \"db-dir\" : \"data/servers/xtdb/rdb-tx\" } } } } } Some of the Lucene configuration will be automatically injected When using the JSON-based configuration, some additional entries will be automatically injected to the Lucene configuration by Egeria: specifically the indexer and analyzer entries used to configure the Lucene index optimally for the OMRS -level search interfaces that Egeria defines. If you have defined your own analyzer or indexer in the configuration, these will be overridden by the connector's injection process -- in other words, any custom configuration you attempt for analyzer or indexer will be ignored. It is highly recommended to include the Lucene entry like that above as it offers significant performance improvements for any text-based queries. The remainder of the configuration in this example defines RocksDB to act as the persistence layer for XTDB's index and document stores, as well as its transaction log. Example persistence using EDN configuration 1 2 3 4 5 6 7 8 9 10 { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.egeria.connectors.juxt.xtdb.repositoryconnector.XtdbOMRSRepositoryConnectorProvider\" }, \"configurationProperties\" : { \"xtdbConfigEDN\" : \"{:xtdb/index-store {:kv-store {:xtdb/module xtdb.rocksdb/->kv-store :db-dir \\\"data/servers/xtdb/rdb-index\\\"}} :xtdb/document-store {:xtdb/module xtdb.jdbc/->document-store :connection-pool {:dialect {:xtdb/module xtdb.jdbc.psql/->dialect} :db-spec {:jdbcUrl \\\"jdbc:postgresql://myserver.com:5432/mydb?user=myuser&password=mypassword\\\"}}} :xtdb/tx-log {:kv-store {:xtdb/module xtdb.rocksdb/->kv-store :db-dir \\\"data/servers/xtdb/rdb-tx\\\"}} :xtdb.lucene/lucene-store {:db-dir \\\"data/servers/xtdb/lucene\\\" :indexer {:xtdb/module xtdb.lucene.egeria/->egeria-indexer} :analyzer {:xtdb/module xtdb.lucene.egeria/->ci-analyzer}}}\" } } The Lucene configuration will NOT be automatically injected Unlike the JSON-based configuration, when using the EDN-based configuration the necessary Egeria components of the Lucene configuration will not be automatically injected. Therefore, make sure that your EDN configuration string includes in the Lucene configuration the following keys and settings in addition to the :db-dir : { :xtdb.lucene/lucene-store { :db-dir \"data/servers/xtdb/lucene\" :indexer { :xtdb/module xtdb.lucene.egeria/->egeria-indexer } :analyzer { :xtdb/module xtdb.lucene.egeria/->ci-analyzer }} These configure the Lucene index optimally for the OMRS -level search interfaces that Egeria defines. The remainder of the configuration in this example defines RocksDB to act as the persistence layer for XTDB's index and document stores, as well as its transaction log. You may need to download additional dependencies In general the dependent libraries for most persistence (other than JDBC) is included in the connector .jar file itself. For JDBC, you will need to download the appropriate driver for your specific data store and make this .jar file available in the same directory as the connector. For example, when using PostgreSQL you will need org.postgresql:postgresql . You can generally determine the additional dependencies you will need by looking at the project.clj file of the relevant XTDB module -- specifically its :dependencies section. For example, sticking with JDBC, here is the project.clj : 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 :dependencies [[ org.clojure/clojure \"1.10.3\" ] [ org.clojure/tools.logging \"1.1.0\" ] [ com.xtdb/xtdb-core ] [ pro.juxt.clojars-mirrors.com.github.seancorfield/next.jdbc \"1.2.674\" ] [ org.clojure/java.data \"1.0.86\" ] [ com.zaxxer/HikariCP \"3.4.5\" ] [ pro.juxt.clojars-mirrors.com.taoensso/nippy \"3.1.1\" ] ;; Sample driver dependencies [ org.postgresql/postgresql \"42.2.18\" :scope \"provided\" ] [ com.oracle.ojdbc/ojdbc8 \"19.3.0.0\" :scope \"provided\" ] [ com.h2database/h2 \"1.4.200\" :scope \"provided\" ] [ org.xerial/sqlite-jdbc \"3.28.0\" :scope \"provided\" ] [ mysql/mysql-connector-java \"8.0.23\" :scope \"provided\" ] [ com.microsoft.sqlserver/mssql-jdbc \"8.2.2.jre8\" :scope \"provided\" ]] Connector options \u00b6 There are currently two configuration options for the connector itself: Option Description luceneRegexes Controls whether the connector will interpret unquoted regexes as Lucene-compatible (true) or not (false): in the latter case ensuring that we fallback to full Java regex checking (which will be significantly slower). syncIndex Controls whether the connector will wait for the XTDB indexes to be updated before returning from write operations (true) or only that they are only guaranteed to be persisted (false). Example configuration showing the default settings { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.egeria.connectors.juxt.xtdb.repositoryconnector.XtdbOMRSRepositoryConnectorProvider\" }, \"configurationProperties\" : { \"xtdbConfig\" : { }, \"luceneRegexes\" : true , \"syncIndex\" : true } } Be careful with syncIndex set to false The syncIndex parameter should only be set to false for mass ingestion where you make your own guarantees that the same metadata instances are only created or updated once by that ingestion process. Afterwards, the configuration of the connector for that repository should have the syncIndex setting changed back to true for business-as-usual operation. When set to false, all write operations are guaranteed but are not done atomically relative to other read operations. In particular, this means that certain operations that rely on first retrieving some metadata instance's state, changing it, and persisting that change must be done only once (up to you to guarantee when syncIndex is false) or in synchronous mode ( syncIndex set to true). Otherwise, there is every possibility that the operation will retrieve a stale version of the metadata instance, update it, and persist that -- ultimately clobbering whatever asynchronous update may have been made in-between. This applies to essentially all write operations: classifyEntity , updateEntityClassification , declassifyEntity , save...ReferenceCopy , delete... , purge... , purge...ReferenceCopy , restore... , update...Status , update...Properties , undo...Update , reIdentify... , reType... , and reHome... High availability \u00b6 A sample Helm chart is provided for configuring the XTDB connector for high availability . This chart starts up a number of different elements and configures them in a specific sequence. Requires a pre-existing JDBC database The chart relies on a pre-existing JDBC database somewhere to use as the document store, and the moment assumes this will be of a PostgreSQL variety (that's the only driver it downloads). A quick setup would be to use Enterprise DB's k8s operator to quickly start up a PostgreSQL cluster in your kubernetes cluster, which is what the following diagrams illustrate 1 . Startup \u00b6 When it is first deployed, the Helm chart starts a number of pods and services: for Egeria (purple), Kafka (red), execution of the Performance Test Suite and a pod used for configuration. (As mentioned above, it assumes a pre-existing JDBC database: a vanilla PostgreSQL cluster (grey) deployed and managed independently by EnterpriseDB's k8s operator.) Each XTDB pod runs its own separate OMAG Server Platform , in its own JVM , and a script in the init-and-report pod will wait until all three pods' OMAG Server Platforms are running before proceeding to any of the following steps. (The headless service allows each pod to be directly addressed, without load-balancing, to do such a check.) Configure \u00b6 The next script creates a singular configuration document via the pts pod, and deploys this common configuration to each of the pods (again using the headless service to directly address each one individually): each will have a separate xtdb server configured with the same XTDB connector (same metadata collection ID ). When the /instance is called against each pod to start the connector, each will create a local index and instance of the IXtdb interface: all pointing to the same golden stores (in this example, Kafka and EDB) where all persistence for XTDB is handled. All servers will refer to the singular xtdb load-balancing service as their root URL. Run \u00b6 Now when we start the Performance Test Suite, all traffic to the technology under test is routed via this xtdb load-balancing service: which will round-robin each request it receives to the underlying pods running the XTDB plugin repository connector. Kafka has a similar service, which handles load-balancing across its own pods for all write operations. The underlying JDBC cluster may have a similar load-balancing service again (e.g. if the data store uses sharding), but also may not. In this example the edb-rw service layer is instead an abstraction of the primary data store ( edb-1 ): all writes will go to this primary data store, while the others act as secondary / standby servers to which EnterpriseDB is automatically handling data replication from the primary. If the primary pod fails, EnterpriseDB can re-point the edb-rw service layer to one of these existing secondary stores (which is automatically promoted to primary by EnterpriseDB). Outages \u00b6 Should there be any outage (in the example above, an Egeria pod, a Kafka pod, and an EnterpriseDB pod all going down) the Kubernetes services will simply stop routing traffic to those pods and the overall service will continue uninterrupted. Depending on how the underlying services are managed, they may also be able to self-heal: Kafka is deployed as a StatefulSet in kubernetes, so if any pod fails kubernetes will automatically attempt to start another in its place to keep the total number of replicas defined by the StatefulSet running at all times. EnterpriseDB in our example was deployed through an operator: this operator self-heals any individual pod failure to e.g. start another standby server pointing at the same PersistentVolumeClaim as the failed pod (to pick up the data that was already replicated), switch the primary server to one of the standby servers if the primary server fails, and so on. Limitations \u00b6 There are a number of limitations to be aware of with the high availability configuration: Must use a non-embedded XTDB back-end Write operations will only be consistent when using a non-embedded XTDB back-end: e.g. Kafka, S3, or JDBC. Read operations are eventually consistent Since the indexes are local to each pod, read operations will be eventually consistent: the specific pod to which a query is routed may not yet have updated its embedded index with the results of the very latest write operations from some other pod. (Note in particular that this has a knock-on impact to our test suites, which currently assume immediate consistency: expect various scenarios to fail if you decide to run them against an eventually-consistent HA configuration.) Cannot yet be dynamically scaled Currently configuration of Egeria requires making a number of REST API calls, which limits how dynamic we can be in adding or removing pods to an already-running cluster (in particular: we cannot rely on a readiness probe to indicate pod readiness to process actual work, but only its readiness to be configured ). We hope to address this soon by allowing configuration and startup to be done without relying on REST calls, at which point we should be able to also support dynamically adding and removing pods from the cluster. For other databases, modify the JDBC_DRIVER_URL value in the configmap.yaml of the chart to point to the location of the appropriate driver, and replace the use of the bin/bootstrapConfig.sh script in the init-and-report.yaml template with an inline script in that template (to specify the appropriate XTDB configuration and JDBC dialect to use for the document store ). \u21a9","title":"XTDB OMRS Repository Connector"},{"location":"connectors/repository/xtdb/#xtdb-omrs-repository-connector","text":"Fully conformant with all Egeria profiles Last tested on release 3.1 of Egeria, release 3.1 of connector using release 1.18.1 of XTDB . Profile Result Metadata sharing CONFORMANT_FULL_SUPPORT Reference copies CONFORMANT_FULL_SUPPORT Metadata maintenance CONFORMANT_FULL_SUPPORT Dynamic types UNKNOWN_STATUS Graph queries CONFORMANT_FULL_SUPPORT Historical search CONFORMANT_FULL_SUPPORT Entity proxies CONFORMANT_FULL_SUPPORT Soft-delete and restore CONFORMANT_FULL_SUPPORT Undo an update CONFORMANT_FULL_SUPPORT Reidentify instance CONFORMANT_FULL_SUPPORT Retype instance CONFORMANT_FULL_SUPPORT Rehome instance CONFORMANT_FULL_SUPPORT Entity search CONFORMANT_FULL_SUPPORT Relationship search CONFORMANT_FULL_SUPPORT Entity advanced search CONFORMANT_FULL_SUPPORT Relationship advanced search CONFORMANT_FULL_SUPPORT Additional notes The entity search tests could fail a particular long-running query pattern unless Lucene is configured: typically where a query by value or attribute is done without providing any restriction on the type of instances against which the query should run. Configure the connector with Lucene to avoid these timeouts. The Dynamic types profile currently does not have any tests defined, so will be UNKNOWN_STATUS for all repositories and connectors. Latest release Latest snapshot Navigate to the latest snapshot directory, and within that find the latest connector archive with the name: egeria-connector-xtdb-{version}-jar-with-dependencies.jar Source The connector is hosted in its own repository at odpi/egeria-connector-xtdb , where the source code can be cloned and the connector built from source. The XTDB OMRS repository connector enables the use of XTDB (formerly known as \"Crux\") and its own pluggable architecture to support a variety of underlying storage back-ends including S3, RocksDB, Apache Kafka, LMDB, JDBC and more. XTDB supports temporal graph queries to provide native support for storing historical information and answering temporal queries. The connector is also capable of running as a highly-available service. In addition, currently this is the highest-performance open source persistent repository for Egeria across all operations: read, write, update, search and purge.","title":"XTDB OMRS Repository Connector"},{"location":"connectors/repository/xtdb/#how-it-works","text":"The XTDB OMRS Repository Connector is a repository connector , hosted by the plugin repository proxy , running on a metadata server . XTDB itself is started as an embedded process within the connector. It can be configured to use any of the various pluggable persistence layers supported by XTDB itself, and communication between the Java code of the connector and XTDB itself (which is implemented in Clojure) occurs directly via the XTDB Java API (not via REST). The repository connector (and metadata collection) methods of the repository connector interface simply communicate with XTDB via XTDB's Java API to read and write information to the underlying XTDB node. XTDB itself handles write transactions and persistence guarantees via its APIs, ensuring that all data is at least recorded into the transaction log and document store prior to any write method returning. Synchronous by default, but configurable for asynchronous operation By default, the repository connector further awaits confirmation that any write has been indexed (and is therefore available for read operations) prior to returning. However, it is also possible to configure the connector in an \"ingest-optimized\" mode that allows the indexing to occur asynchronously, and can therefore improve the speed of write operations significantly.","title":"How it works"},{"location":"connectors/repository/xtdb/#configuration","text":"The following options are used to configure this connector, as part of the configure the local repository step when configuring a metadata server .","title":"Configuration"},{"location":"connectors/repository/xtdb/#pluggable-persistence","text":"There are many options for configuring XTDB itself. A list of overall persistence modules and deeper configuration options for each can be found through XTDB's own documentation . To enable persistence, there are two options: send in the JSON document configuration outlined in XTDB's own documentation directly to the xtdbConfig key of the configurationProperties property of Egeria's connector configuration send in a string to the xtdbConfigEDN key of the configurationProperties of Egeria's connector configuration, which gives the EDN form of configuration outlined in XTDB's own documentation Both approaches are valid and should be equally functional, but occasionally a bug may crop up that makes one or the other more or less feasible for a particular configuration. Example persistence using JSON configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.egeria.connectors.juxt.xtdb.repositoryconnector.XtdbOMRSRepositoryConnectorProvider\" }, \"configurationProperties\" : { \"xtdbConfig\" : { \"xtdb.lucene/lucene-store\" : { \"db-dir\" : \"data/servers/xtdb/lucene\" }, \"xtdb/index-store\" : { \"kv-store\" : { \"xtdb/module\" : \"xtdb.rocksdb/->kv-store\" , \"db-dir\" : \"data/servers/xtdb/rdb-index\" } }, \"xtdb/document-store\" : { \"kv-store\" : { \"xtdb/module\" : \"xtdb.rocksdb/->kv-store\" , \"db-dir\" : \"data/servers/xtdb/rdb-docs\" } }, \"xtdb/tx-log\" : { \"kv-store\" : { \"xtdb/module\" : \"xtdb.rocksdb/->kv-store\" , \"db-dir\" : \"data/servers/xtdb/rdb-tx\" } } } } } Some of the Lucene configuration will be automatically injected When using the JSON-based configuration, some additional entries will be automatically injected to the Lucene configuration by Egeria: specifically the indexer and analyzer entries used to configure the Lucene index optimally for the OMRS -level search interfaces that Egeria defines. If you have defined your own analyzer or indexer in the configuration, these will be overridden by the connector's injection process -- in other words, any custom configuration you attempt for analyzer or indexer will be ignored. It is highly recommended to include the Lucene entry like that above as it offers significant performance improvements for any text-based queries. The remainder of the configuration in this example defines RocksDB to act as the persistence layer for XTDB's index and document stores, as well as its transaction log. Example persistence using EDN configuration 1 2 3 4 5 6 7 8 9 10 { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.egeria.connectors.juxt.xtdb.repositoryconnector.XtdbOMRSRepositoryConnectorProvider\" }, \"configurationProperties\" : { \"xtdbConfigEDN\" : \"{:xtdb/index-store {:kv-store {:xtdb/module xtdb.rocksdb/->kv-store :db-dir \\\"data/servers/xtdb/rdb-index\\\"}} :xtdb/document-store {:xtdb/module xtdb.jdbc/->document-store :connection-pool {:dialect {:xtdb/module xtdb.jdbc.psql/->dialect} :db-spec {:jdbcUrl \\\"jdbc:postgresql://myserver.com:5432/mydb?user=myuser&password=mypassword\\\"}}} :xtdb/tx-log {:kv-store {:xtdb/module xtdb.rocksdb/->kv-store :db-dir \\\"data/servers/xtdb/rdb-tx\\\"}} :xtdb.lucene/lucene-store {:db-dir \\\"data/servers/xtdb/lucene\\\" :indexer {:xtdb/module xtdb.lucene.egeria/->egeria-indexer} :analyzer {:xtdb/module xtdb.lucene.egeria/->ci-analyzer}}}\" } } The Lucene configuration will NOT be automatically injected Unlike the JSON-based configuration, when using the EDN-based configuration the necessary Egeria components of the Lucene configuration will not be automatically injected. Therefore, make sure that your EDN configuration string includes in the Lucene configuration the following keys and settings in addition to the :db-dir : { :xtdb.lucene/lucene-store { :db-dir \"data/servers/xtdb/lucene\" :indexer { :xtdb/module xtdb.lucene.egeria/->egeria-indexer } :analyzer { :xtdb/module xtdb.lucene.egeria/->ci-analyzer }} These configure the Lucene index optimally for the OMRS -level search interfaces that Egeria defines. The remainder of the configuration in this example defines RocksDB to act as the persistence layer for XTDB's index and document stores, as well as its transaction log. You may need to download additional dependencies In general the dependent libraries for most persistence (other than JDBC) is included in the connector .jar file itself. For JDBC, you will need to download the appropriate driver for your specific data store and make this .jar file available in the same directory as the connector. For example, when using PostgreSQL you will need org.postgresql:postgresql . You can generally determine the additional dependencies you will need by looking at the project.clj file of the relevant XTDB module -- specifically its :dependencies section. For example, sticking with JDBC, here is the project.clj : 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 :dependencies [[ org.clojure/clojure \"1.10.3\" ] [ org.clojure/tools.logging \"1.1.0\" ] [ com.xtdb/xtdb-core ] [ pro.juxt.clojars-mirrors.com.github.seancorfield/next.jdbc \"1.2.674\" ] [ org.clojure/java.data \"1.0.86\" ] [ com.zaxxer/HikariCP \"3.4.5\" ] [ pro.juxt.clojars-mirrors.com.taoensso/nippy \"3.1.1\" ] ;; Sample driver dependencies [ org.postgresql/postgresql \"42.2.18\" :scope \"provided\" ] [ com.oracle.ojdbc/ojdbc8 \"19.3.0.0\" :scope \"provided\" ] [ com.h2database/h2 \"1.4.200\" :scope \"provided\" ] [ org.xerial/sqlite-jdbc \"3.28.0\" :scope \"provided\" ] [ mysql/mysql-connector-java \"8.0.23\" :scope \"provided\" ] [ com.microsoft.sqlserver/mssql-jdbc \"8.2.2.jre8\" :scope \"provided\" ]]","title":"Pluggable persistence"},{"location":"connectors/repository/xtdb/#connector-options","text":"There are currently two configuration options for the connector itself: Option Description luceneRegexes Controls whether the connector will interpret unquoted regexes as Lucene-compatible (true) or not (false): in the latter case ensuring that we fallback to full Java regex checking (which will be significantly slower). syncIndex Controls whether the connector will wait for the XTDB indexes to be updated before returning from write operations (true) or only that they are only guaranteed to be persisted (false). Example configuration showing the default settings { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.egeria.connectors.juxt.xtdb.repositoryconnector.XtdbOMRSRepositoryConnectorProvider\" }, \"configurationProperties\" : { \"xtdbConfig\" : { }, \"luceneRegexes\" : true , \"syncIndex\" : true } } Be careful with syncIndex set to false The syncIndex parameter should only be set to false for mass ingestion where you make your own guarantees that the same metadata instances are only created or updated once by that ingestion process. Afterwards, the configuration of the connector for that repository should have the syncIndex setting changed back to true for business-as-usual operation. When set to false, all write operations are guaranteed but are not done atomically relative to other read operations. In particular, this means that certain operations that rely on first retrieving some metadata instance's state, changing it, and persisting that change must be done only once (up to you to guarantee when syncIndex is false) or in synchronous mode ( syncIndex set to true). Otherwise, there is every possibility that the operation will retrieve a stale version of the metadata instance, update it, and persist that -- ultimately clobbering whatever asynchronous update may have been made in-between. This applies to essentially all write operations: classifyEntity , updateEntityClassification , declassifyEntity , save...ReferenceCopy , delete... , purge... , purge...ReferenceCopy , restore... , update...Status , update...Properties , undo...Update , reIdentify... , reType... , and reHome...","title":"Connector options"},{"location":"connectors/repository/xtdb/#high-availability","text":"A sample Helm chart is provided for configuring the XTDB connector for high availability . This chart starts up a number of different elements and configures them in a specific sequence. Requires a pre-existing JDBC database The chart relies on a pre-existing JDBC database somewhere to use as the document store, and the moment assumes this will be of a PostgreSQL variety (that's the only driver it downloads). A quick setup would be to use Enterprise DB's k8s operator to quickly start up a PostgreSQL cluster in your kubernetes cluster, which is what the following diagrams illustrate 1 .","title":"High availability"},{"location":"connectors/repository/xtdb/#startup","text":"When it is first deployed, the Helm chart starts a number of pods and services: for Egeria (purple), Kafka (red), execution of the Performance Test Suite and a pod used for configuration. (As mentioned above, it assumes a pre-existing JDBC database: a vanilla PostgreSQL cluster (grey) deployed and managed independently by EnterpriseDB's k8s operator.) Each XTDB pod runs its own separate OMAG Server Platform , in its own JVM , and a script in the init-and-report pod will wait until all three pods' OMAG Server Platforms are running before proceeding to any of the following steps. (The headless service allows each pod to be directly addressed, without load-balancing, to do such a check.)","title":"Startup"},{"location":"connectors/repository/xtdb/#configure","text":"The next script creates a singular configuration document via the pts pod, and deploys this common configuration to each of the pods (again using the headless service to directly address each one individually): each will have a separate xtdb server configured with the same XTDB connector (same metadata collection ID ). When the /instance is called against each pod to start the connector, each will create a local index and instance of the IXtdb interface: all pointing to the same golden stores (in this example, Kafka and EDB) where all persistence for XTDB is handled. All servers will refer to the singular xtdb load-balancing service as their root URL.","title":"Configure"},{"location":"connectors/repository/xtdb/#run","text":"Now when we start the Performance Test Suite, all traffic to the technology under test is routed via this xtdb load-balancing service: which will round-robin each request it receives to the underlying pods running the XTDB plugin repository connector. Kafka has a similar service, which handles load-balancing across its own pods for all write operations. The underlying JDBC cluster may have a similar load-balancing service again (e.g. if the data store uses sharding), but also may not. In this example the edb-rw service layer is instead an abstraction of the primary data store ( edb-1 ): all writes will go to this primary data store, while the others act as secondary / standby servers to which EnterpriseDB is automatically handling data replication from the primary. If the primary pod fails, EnterpriseDB can re-point the edb-rw service layer to one of these existing secondary stores (which is automatically promoted to primary by EnterpriseDB).","title":"Run"},{"location":"connectors/repository/xtdb/#outages","text":"Should there be any outage (in the example above, an Egeria pod, a Kafka pod, and an EnterpriseDB pod all going down) the Kubernetes services will simply stop routing traffic to those pods and the overall service will continue uninterrupted. Depending on how the underlying services are managed, they may also be able to self-heal: Kafka is deployed as a StatefulSet in kubernetes, so if any pod fails kubernetes will automatically attempt to start another in its place to keep the total number of replicas defined by the StatefulSet running at all times. EnterpriseDB in our example was deployed through an operator: this operator self-heals any individual pod failure to e.g. start another standby server pointing at the same PersistentVolumeClaim as the failed pod (to pick up the data that was already replicated), switch the primary server to one of the standby servers if the primary server fails, and so on.","title":"Outages"},{"location":"connectors/repository/xtdb/#limitations","text":"There are a number of limitations to be aware of with the high availability configuration: Must use a non-embedded XTDB back-end Write operations will only be consistent when using a non-embedded XTDB back-end: e.g. Kafka, S3, or JDBC. Read operations are eventually consistent Since the indexes are local to each pod, read operations will be eventually consistent: the specific pod to which a query is routed may not yet have updated its embedded index with the results of the very latest write operations from some other pod. (Note in particular that this has a knock-on impact to our test suites, which currently assume immediate consistency: expect various scenarios to fail if you decide to run them against an eventually-consistent HA configuration.) Cannot yet be dynamically scaled Currently configuration of Egeria requires making a number of REST API calls, which limits how dynamic we can be in adding or removing pods to an already-running cluster (in particular: we cannot rely on a readiness probe to indicate pod readiness to process actual work, but only its readiness to be configured ). We hope to address this soon by allowing configuration and startup to be done without relying on REST calls, at which point we should be able to also support dynamically adding and removing pods from the cluster. For other databases, modify the JDBC_DRIVER_URL value in the configmap.yaml of the chart to point to the location of the appropriate driver, and replace the use of the bin/bootstrapConfig.sh script in the init-and-report.yaml template with an inline script in that template (to specify the appropriate XTDB configuration and JDBC dialect to use for the document store ). \u21a9","title":"Limitations"},{"location":"connectors/repository/xtdb/performance/","text":"XTDB Connector Performance \u00b6 Following are details on XTDB's performance at the latest release of the connector (v3.1, using XTDB v1.18.1). All raw metrics and elements used to produce the results are described further under reproducibility below, but the historical details themselves are no longer included below in the interest of being concise. Details on the performance metrics The median of all results for that method across all executions for a given set of volume parameters is given (all times in milliseconds) to give an idea of the \"typical\" result, while limiting potential skew from significant outliers. A more detailed set of statistics is best reviewed through the Jupyter Notebook provided in each results directory, where you can review: the full distributions of execution times (including the outliers) detailed individual outlier results (e.g. the top-10 slowest response times per method) volumes in place during the tests (how many entities, how many relationships, etc) The volume parameters that were used for each test are specified using the convention i-s , where i is the value for the instancesPerType parameter to the PTS and s is the value for maxSearchResults . For example, 5-2 means 5 instances will be created for every open metadata type and 2 will be the maximum number of results per page for methods that include paging. All tests are run from 5-2 through 20-10 to give a sense of the performance impact of doubling the number of instances and search results. Above this, the graph queries are no longer included: they become exponentially more complex as the volumes grow, and while they will still return results, the depth of their testing in the PTS means that they can contribute many hours (or even days) to the overall suite execution -- they are therefore left out to be able to more quickly produce results for the other methods at progressively higher volumes. The page size is left at a maximum of 10 for subsequent tests so that it is only the volume of instances in total that are doubling each time, rather than also the number of detailed results. Instance counts range from a few thousand (at 5-2 ) up to nearly one hundred thousand (at 80-10 ). In the graphical comparisons, a point plot is used to show the typical execution time of each method at the different volumes / by repository. Each point on the plot represents the median execution time for that method, at a given volume of metadata. (For the repository comparison plots, pts = XTDB and janus = JanusGraph.) The horizontal lines that appear around each point are confidence intervals calculated by a bootstrapping process: in simple terms, the larger the horizontal line, the more variability there is for that particular method's execution time (a singular median value is insufficient to represent such variability on its own). XTDB at varying volumes \u00b6 Summary The retrieval and write operations remain very consistent, with almost no variability, throughout the growth in volume. The search operations, however, begin to clearly degrade at the highest volumes tested. Further investigation into other optimized settings for the search operations for these larger volumes is likely warranted as the next step to continue to improve performance. Profile Method 05-02 (4,850) 10-05 (9,700) 20-10 (19,400) 40-10 (38,800) 80-10 (77,600) Entity creation addEntity 55.0 48.0 46.0 48.0 44.0 ... saveEntityReferenceCopy 52.0 45.0 43.0 46.0 42.0 Entity search findEntities 54.0 65.0 88.0 215.0 413.0 ... findEntitiesByProperty 31.0 36.0 44.0 100.0 151.0 ... findEntitiesByPropertyValue 58.0 71.0 102.0 170.0 331.0 Relationship creation addRelationship 48.0 47.0 45.0 47.0 44.0 ... saveRelationshipReferenceCopy 52.0 49.0 47.0 49.0 46.0 Relationship search findRelationships 28.0 30.0 33.0 39.0 39.0 ... findRelationshipsByProperty 29.0 35.0 43.0 105.0 166.0 ... findRelationshipsByPropertyValue 47.0 59.0 78.0 159.0 362.0 Entity classification classifyEntity 78.0 73.5 72.0 75.0 73.0 ... saveClassificationReferenceCopy 70.0 59.0 63.0 64.0 62.0 Classification search findEntitiesByClassification 37.0 44.0 60.0 97.0 113.0 Entity update reTypeEntity 44.0 41.0 40.0 46.0 41.0 ... updateEntityProperties 58.0 53.0 53.0 49.0 49.0 Relationship update updateRelationshipProperties 63.0 56.0 56.0 56.0 54.0 Classification update updateEntityClassification 96.0 92.5 87.0 90.0 88.0 Entity undo undoEntityUpdate 49.0 53.0 50.0 48.0 44.0 Relationship undo undoRelationshipUpdate 56.0 55.0 54.0 53.0 52.0 Entity retrieval getEntityDetail 17.0 16.0 16.0 16.0 16.0 ... getEntitySummary 17.0 16.0 16.0 16.0 16.0 ... isEntityKnown 17.0 16.0 16.0 16.0 16.0 Entity history retrieval getEntityDetail 20.0 19.0 19.0 19.0 19.0 ... getEntityDetailHistory 22.0 21.0 21.0 21.0 21.0 Relationship retrieval getRelationship 18.0 17.0 17.0 18.0 17.0 ... isRelationshipKnown 18.0 17.0 17.0 18.0 17.0 Relationship history retrieval getRelationship 21.0 21.0 21.0 21.0 21.0 ... getRelationshipHistory 23.0 22.0 22.0 22.0 22.0 Entity history search findEntities 60.5 89.0 140.0 549.5 1574.0 ... findEntitiesByProperty 31.0 34.0 40.0 55.0 73.0 ... findEntitiesByPropertyValue 54.0 75.0 129.5 295.5 676.0 Relationship history search findRelationships 28.0 34.0 43.0 49.0 49.0 ... findRelationshipsByProperty 33.0 41.0 55.0 63.0 65.0 ... findRelationshipsByPropertyValue 55.0 84.0 143.0 228.0 516.0 Graph queries getEntityNeighborhood 27.0 26.0 24.0 -- -- ... getLinkingEntities 21.0 25.0 26.0 -- -- ... getRelatedEntities 563.0 1057.0 1873.0 -- -- ... getRelationshipsForEntity 26.0 27.0 25.0 -- -- Graph history queries getEntityNeighborhood 26.0 25.0 24.0 -- -- ... getLinkingEntities 21.0 25.0 26.0 -- -- ... getRelatedEntities 559.5 1057.5 1873.0 -- -- ... getRelationshipsForEntity 25.0 25.0 24.0 -- -- Entity re-home reHomeEntity 46.0 45.0 44.0 51.0 45.0 Relationship re-home reHomeRelationship 43.0 45.0 41.0 46.0 44.0 Entity declassify declassifyEntity 66.0 63.0 63.0 68.5 64.0 ... purgeClassificationReferenceCopy 58.0 55.5 55.5 62.0 56.0 Entity re-identify reIdentifyEntity 55.0 51.0 49.0 64.0 56.0 Relationship re-identify reIdentifyRelationship 44.0 47.0 40.0 49.0 44.0 Relationship delete deleteRelationship 40.0 40.0 39.0 47.0 41.0 Entity delete deleteEntity 45.0 45.0 43.0 55.0 48.0 Entity restore restoreEntity 39.0 39.0 37.0 45.0 40.0 Relationship restore restoreRelationship 37.0 39.0 36.0 43.0 36.0 Relationship purge purgeRelationship 32.0 32.0 30.0 39.0 33.0 ... purgeRelationshipReferenceCopy 23.0 24.0 22.0 27.0 24.0 Entity purge purgeEntity 40.0 40.0 40.0 52.0 45.0 ... purgeEntityReferenceCopy 24.0 24.0 24.0 29.0 26.0 XTDB vs JanusGraph \u00b6 Summary In almost all cases, the XTDB repository is significantly faster than JanusGraph: at most volumes completing all methods in less than 100ms and with very little variability. For JanusGraph, on the other hand, there is significant variability (in particular for methods like findEntitiesByClassification ), and there are numerous examples of the median execution time taking more than multiple seconds. Even at 8 times the volume of metadata the XTDB connector still outperforms the JanusGraph connector in almost every method (the only exceptions being a few of the find methods, where the performance is approximately even at 2-4 times the volume). Graph queries were disabled for JanusGraph The graph queries were disabled for JanusGraph in order to have results in a timely manner: it would take more than a month to produce results for these queries for the JanusGraph connector. The XTDB results can be difficult to see in detail due to the skew from the Janus results, so it may be easier to look at this more granular comparison that drops the higher scales of Janus for readability of the XTDB results: Profile Method 05-02 (XTDB) 05-02 (Janus) 10-05 (XTDB) 10-05 (Janus) 20-10 (XTDB) 20-10 (Janus) 40-10 (XTDB) 40-10 (Janus) 80-10 (XTDB) 80-10 (Janus) Entity creation addEntity 55.0 440.0 48.0 450.0 46.0 483.0 48.0 481.0 44.0 DNF ... saveEntityReferenceCopy 52.0 435.0 45.0 451.0 43.0 481.0 46.0 479.0 42.0 DNF Entity search findEntities 54.0 260.0 65.0 454.0 88.0 973.5 215.0 2104.0 413.0 DNF ... findEntitiesByProperty 31.0 40.0 36.0 50.0 44.0 79.0 100.0 115.0 151.0 DNF ... findEntitiesByPropertyValue 58.0 85.0 71.0 94.0 102.0 123.0 170.0 165.0 331.0 DNF Relationship creation addRelationship 48.0 160.0 47.0 162.0 45.0 162.5 47.0 156.0 44.0 DNF ... saveRelationshipReferenceCopy 52.0 460.0 49.0 456.0 47.0 480.0 49.0 464.0 46.0 DNF Relationship search findRelationships 28.0 45.0 30.0 60.0 33.0 99.0 39.0 146.0 39.0 DNF ... findRelationshipsByProperty 29.0 48.0 35.0 63.0 43.0 109.0 105.0 171.0 166.0 DNF ... findRelationshipsByPropertyValue 47.0 71.0 59.0 86.0 78.0 120.0 159.0 181.0 362.0 DNF Entity classification classifyEntity 78.0 886.0 73.5 921.5 72.0 1031.0 75.0 961.0 73.0 DNF ... saveClassificationReferenceCopy 70.0 738.0 59.0 845.5 63.0 969.5 64.0 895.5 62.0 DNF Classification search findEntitiesByClassification 37.0 606.0 44.0 1029.0 60.0 2195.5 97.0 3696.0 113.0 DNF Entity update reTypeEntity 44.0 390.0 41.0 374.5 40.0 453.0 46.0 381.0 41.0 DNF ... updateEntityProperties 58.0 698.0 53.0 720.5 53.0 804.0 49.0 776.5 49.0 DNF Relationship update updateRelationshipProperties 63.0 456.5 56.0 467.0 56.0 499.0 56.0 456.0 54.0 DNF Classification update updateEntityClassification 96.0 1178.5 92.5 1259.5 87.0 1410.0 90.0 1262.0 88.0 DNF Entity undo undoEntityUpdate 49.0 -- 53.0 -- 50.0 -- 48.0 -- 44.0 -- Relationship undo undoRelationshipUpdate 56.0 -- 55.0 -- 54.0 -- 53.0 -- 52.0 -- Entity retrieval getEntityDetail 17.0 18.0 16.0 18.0 16.0 16.0 16.0 16.0 16.0 DNF ... getEntitySummary 17.0 17.0 16.0 17.0 16.0 15.0 16.0 15.0 16.0 DNF ... isEntityKnown 17.0 19.0 16.0 18.0 16.0 16.0 16.0 16.0 16.0 DNF Entity history retrieval getEntityDetail 20.0 -- 19.0 -- 19.0 -- 19.0 -- 19.0 -- ... getEntityDetailHistory 22.0 -- 21.0 -- 21.0 -- 21.0 -- 21.0 -- Relationship retrieval getRelationship 18.0 23.0 17.0 20.0 17.0 19.0 18.0 18.0 17.0 DNF ... isRelationshipKnown 18.0 23.0 17.0 20.0 17.0 19.0 18.0 18.0 17.0 DNF Relationship history retrieval getRelationship 21.0 -- 21.0 -- 21.0 -- 21.0 -- 21.0 -- ... getRelationshipHistory 23.0 -- 22.0 -- 22.0 -- 22.0 -- 22.0 -- Entity history search findEntities 60.5 -- 89.0 -- 140.0 -- 549.5 -- 1574.0 -- ... findEntitiesByProperty 31.0 -- 34.0 -- 40.0 -- 55.0 -- 73.0 -- ... findEntitiesByPropertyValue 54.0 -- 75.0 -- 129.5 -- 295.5 -- 676.0 -- Relationship history search findRelationships 28.0 -- 34.0 -- 43.0 -- 49.0 -- 49.0 -- ... findRelationshipsByProperty 33.0 -- 41.0 -- 55.0 -- 63.0 -- 65.0 -- ... findRelationshipsByPropertyValue 55.0 -- 84.0 -- 143.0 -- 228.0 -- 516.0 -- Graph queries getEntityNeighborhood 27.0 -- 26.0 -- 24.0 -- -- -- -- -- ... getLinkingEntities 21.0 -- 25.0 -- 26.0 -- -- -- -- -- ... getRelatedEntities 563.0 -- 1057.0 -- 1873.0 -- -- -- -- -- ... getRelationshipsForEntity 26.0 -- 27.0 -- 25.0 -- -- -- -- -- Graph history queries getEntityNeighborhood 26.0 -- 25.0 -- 24.0 -- -- -- -- -- ... getLinkingEntities 21.0 -- 25.0 -- 26.0 -- -- -- -- -- ... getRelatedEntities 559.5 -- 1057.5 -- 1873.0 -- -- -- -- -- ... getRelationshipsForEntity 25.0 -- 25.0 -- 24.0 -- -- -- -- -- Entity re-home reHomeEntity 46.0 759.0 45.0 739.0 44.0 909.0 51.0 775.0 45.0 DNF Relationship re-home reHomeRelationship 43.0 405.5 45.0 394.0 41.0 453.0 46.0 394.0 44.0 DNF Entity declassify declassifyEntity 66.0 1302.0 63.0 1374.5 63.0 1629.0 68.5 1420.0 64.0 DNF ... purgeClassificationReferenceCopy 58.0 -- 55.5 -- 55.5 -- 62.0 -- 56.0 -- Entity re-identify reIdentifyEntity 55.0 1745.0 51.0 1735.0 49.0 2310.5 64.0 1864.0 56.0 DNF Relationship re-identify reIdentifyRelationship 44.0 855.0 47.0 823.5 40.0 950.5 49.0 885.0 44.0 DNF Relationship delete deleteRelationship 40.0 398.0 40.0 407.0 39.0 466.0 47.0 434.0 41.0 DNF Entity delete deleteEntity 45.0 785.0 45.0 824.0 43.0 1054.0 55.0 886.0 48.0 DNF Entity restore restoreEntity 39.0 809.5 39.0 871.0 37.0 1091.0 45.0 874.0 40.0 DNF Relationship restore restoreRelationship 37.0 395.0 39.0 401.0 36.0 517.0 43.0 443.0 36.0 DNF Relationship purge purgeRelationship 32.0 146.0 32.0 194.0 30.0 210.0 39.0 202.0 33.0 DNF ... purgeRelationshipReferenceCopy 23.0 118.0 24.0 116.0 22.0 126.0 27.0 117.0 24.0 DNF Entity purge purgeEntity 40.0 271.0 40.0 381.0 40.0 433.5 52.0 416.0 45.0 DNF ... purgeEntityReferenceCopy 24.0 271.0 24.0 259.0 24.0 277.0 29.0 253.0 26.0 DNF Reproducibility \u00b6 Re-running the tests \u00b6 Two Helm charts are provided, that were used to automate the execution of these suites against the XTDB repository connector: The Helm chart used to execute the CTS suite The Helm chart used to execute the PTS suite These use a default configuration for the XTDB repository where Lucene is used as a text index and RocksDB is used for all persistence: index store, document store and transaction log. No additional tuning of any parameters (XTDB or RocksDB) is applied: they use all of their default settings. Data points \u00b6 The cts/results directory in the code repository for the connector contains results of running the suites against the XTDB connector. For each test suite execution, you will find the following details: openmetadata_cts_summary.json - a summary of the results of each profile Description of the k8s environment deployment - details of the deployed components used for the test configmap.yaml - details of the variables used within the components of the test The OMAG Server configurations: omag.server.[crux|xtdb].config - the configuration of the XTDB connector (proxy) omag.server.cts.config - the configuration of the test workbench The cohort registrations: cohort.coco.[crux|xtdb].local - the local XTDB connector (proxy) cohort registration information cohort.coco.[crux|xtdb].remote - the cohort members considered remote from the XTDB connector (proxy)'s perspective cohort.coco.cts.local - the local test Workbench cohort registration cohort.coco.cts.remote - the cohort members considered remote from the test Workbench's perspective Detailed results: pd.tar.gz - an archive containing the full detailed results of every profile tested tcd.tar.gz - an archive containing the full detailed results of every test case executed Jupyter Notebooks used to analyze the results: analyze-performance-results.ipynb - details about the environment, instance counts, and distribution of elapsed times per method, also illustrating how the results can be analyzed more deeply calculate-medians.ipynb - used to calculate the medians displayed in the table further below (to run either of these notebooks, you will need to first extract the pd.tar.gz file to have the JSON results files for analysis)","title":"XTDB Connector Performance"},{"location":"connectors/repository/xtdb/performance/#xtdb-connector-performance","text":"Following are details on XTDB's performance at the latest release of the connector (v3.1, using XTDB v1.18.1). All raw metrics and elements used to produce the results are described further under reproducibility below, but the historical details themselves are no longer included below in the interest of being concise. Details on the performance metrics The median of all results for that method across all executions for a given set of volume parameters is given (all times in milliseconds) to give an idea of the \"typical\" result, while limiting potential skew from significant outliers. A more detailed set of statistics is best reviewed through the Jupyter Notebook provided in each results directory, where you can review: the full distributions of execution times (including the outliers) detailed individual outlier results (e.g. the top-10 slowest response times per method) volumes in place during the tests (how many entities, how many relationships, etc) The volume parameters that were used for each test are specified using the convention i-s , where i is the value for the instancesPerType parameter to the PTS and s is the value for maxSearchResults . For example, 5-2 means 5 instances will be created for every open metadata type and 2 will be the maximum number of results per page for methods that include paging. All tests are run from 5-2 through 20-10 to give a sense of the performance impact of doubling the number of instances and search results. Above this, the graph queries are no longer included: they become exponentially more complex as the volumes grow, and while they will still return results, the depth of their testing in the PTS means that they can contribute many hours (or even days) to the overall suite execution -- they are therefore left out to be able to more quickly produce results for the other methods at progressively higher volumes. The page size is left at a maximum of 10 for subsequent tests so that it is only the volume of instances in total that are doubling each time, rather than also the number of detailed results. Instance counts range from a few thousand (at 5-2 ) up to nearly one hundred thousand (at 80-10 ). In the graphical comparisons, a point plot is used to show the typical execution time of each method at the different volumes / by repository. Each point on the plot represents the median execution time for that method, at a given volume of metadata. (For the repository comparison plots, pts = XTDB and janus = JanusGraph.) The horizontal lines that appear around each point are confidence intervals calculated by a bootstrapping process: in simple terms, the larger the horizontal line, the more variability there is for that particular method's execution time (a singular median value is insufficient to represent such variability on its own).","title":"XTDB Connector Performance"},{"location":"connectors/repository/xtdb/performance/#xtdb-at-varying-volumes","text":"Summary The retrieval and write operations remain very consistent, with almost no variability, throughout the growth in volume. The search operations, however, begin to clearly degrade at the highest volumes tested. Further investigation into other optimized settings for the search operations for these larger volumes is likely warranted as the next step to continue to improve performance. Profile Method 05-02 (4,850) 10-05 (9,700) 20-10 (19,400) 40-10 (38,800) 80-10 (77,600) Entity creation addEntity 55.0 48.0 46.0 48.0 44.0 ... saveEntityReferenceCopy 52.0 45.0 43.0 46.0 42.0 Entity search findEntities 54.0 65.0 88.0 215.0 413.0 ... findEntitiesByProperty 31.0 36.0 44.0 100.0 151.0 ... findEntitiesByPropertyValue 58.0 71.0 102.0 170.0 331.0 Relationship creation addRelationship 48.0 47.0 45.0 47.0 44.0 ... saveRelationshipReferenceCopy 52.0 49.0 47.0 49.0 46.0 Relationship search findRelationships 28.0 30.0 33.0 39.0 39.0 ... findRelationshipsByProperty 29.0 35.0 43.0 105.0 166.0 ... findRelationshipsByPropertyValue 47.0 59.0 78.0 159.0 362.0 Entity classification classifyEntity 78.0 73.5 72.0 75.0 73.0 ... saveClassificationReferenceCopy 70.0 59.0 63.0 64.0 62.0 Classification search findEntitiesByClassification 37.0 44.0 60.0 97.0 113.0 Entity update reTypeEntity 44.0 41.0 40.0 46.0 41.0 ... updateEntityProperties 58.0 53.0 53.0 49.0 49.0 Relationship update updateRelationshipProperties 63.0 56.0 56.0 56.0 54.0 Classification update updateEntityClassification 96.0 92.5 87.0 90.0 88.0 Entity undo undoEntityUpdate 49.0 53.0 50.0 48.0 44.0 Relationship undo undoRelationshipUpdate 56.0 55.0 54.0 53.0 52.0 Entity retrieval getEntityDetail 17.0 16.0 16.0 16.0 16.0 ... getEntitySummary 17.0 16.0 16.0 16.0 16.0 ... isEntityKnown 17.0 16.0 16.0 16.0 16.0 Entity history retrieval getEntityDetail 20.0 19.0 19.0 19.0 19.0 ... getEntityDetailHistory 22.0 21.0 21.0 21.0 21.0 Relationship retrieval getRelationship 18.0 17.0 17.0 18.0 17.0 ... isRelationshipKnown 18.0 17.0 17.0 18.0 17.0 Relationship history retrieval getRelationship 21.0 21.0 21.0 21.0 21.0 ... getRelationshipHistory 23.0 22.0 22.0 22.0 22.0 Entity history search findEntities 60.5 89.0 140.0 549.5 1574.0 ... findEntitiesByProperty 31.0 34.0 40.0 55.0 73.0 ... findEntitiesByPropertyValue 54.0 75.0 129.5 295.5 676.0 Relationship history search findRelationships 28.0 34.0 43.0 49.0 49.0 ... findRelationshipsByProperty 33.0 41.0 55.0 63.0 65.0 ... findRelationshipsByPropertyValue 55.0 84.0 143.0 228.0 516.0 Graph queries getEntityNeighborhood 27.0 26.0 24.0 -- -- ... getLinkingEntities 21.0 25.0 26.0 -- -- ... getRelatedEntities 563.0 1057.0 1873.0 -- -- ... getRelationshipsForEntity 26.0 27.0 25.0 -- -- Graph history queries getEntityNeighborhood 26.0 25.0 24.0 -- -- ... getLinkingEntities 21.0 25.0 26.0 -- -- ... getRelatedEntities 559.5 1057.5 1873.0 -- -- ... getRelationshipsForEntity 25.0 25.0 24.0 -- -- Entity re-home reHomeEntity 46.0 45.0 44.0 51.0 45.0 Relationship re-home reHomeRelationship 43.0 45.0 41.0 46.0 44.0 Entity declassify declassifyEntity 66.0 63.0 63.0 68.5 64.0 ... purgeClassificationReferenceCopy 58.0 55.5 55.5 62.0 56.0 Entity re-identify reIdentifyEntity 55.0 51.0 49.0 64.0 56.0 Relationship re-identify reIdentifyRelationship 44.0 47.0 40.0 49.0 44.0 Relationship delete deleteRelationship 40.0 40.0 39.0 47.0 41.0 Entity delete deleteEntity 45.0 45.0 43.0 55.0 48.0 Entity restore restoreEntity 39.0 39.0 37.0 45.0 40.0 Relationship restore restoreRelationship 37.0 39.0 36.0 43.0 36.0 Relationship purge purgeRelationship 32.0 32.0 30.0 39.0 33.0 ... purgeRelationshipReferenceCopy 23.0 24.0 22.0 27.0 24.0 Entity purge purgeEntity 40.0 40.0 40.0 52.0 45.0 ... purgeEntityReferenceCopy 24.0 24.0 24.0 29.0 26.0","title":"XTDB at varying volumes"},{"location":"connectors/repository/xtdb/performance/#xtdb-vs-janusgraph","text":"Summary In almost all cases, the XTDB repository is significantly faster than JanusGraph: at most volumes completing all methods in less than 100ms and with very little variability. For JanusGraph, on the other hand, there is significant variability (in particular for methods like findEntitiesByClassification ), and there are numerous examples of the median execution time taking more than multiple seconds. Even at 8 times the volume of metadata the XTDB connector still outperforms the JanusGraph connector in almost every method (the only exceptions being a few of the find methods, where the performance is approximately even at 2-4 times the volume). Graph queries were disabled for JanusGraph The graph queries were disabled for JanusGraph in order to have results in a timely manner: it would take more than a month to produce results for these queries for the JanusGraph connector. The XTDB results can be difficult to see in detail due to the skew from the Janus results, so it may be easier to look at this more granular comparison that drops the higher scales of Janus for readability of the XTDB results: Profile Method 05-02 (XTDB) 05-02 (Janus) 10-05 (XTDB) 10-05 (Janus) 20-10 (XTDB) 20-10 (Janus) 40-10 (XTDB) 40-10 (Janus) 80-10 (XTDB) 80-10 (Janus) Entity creation addEntity 55.0 440.0 48.0 450.0 46.0 483.0 48.0 481.0 44.0 DNF ... saveEntityReferenceCopy 52.0 435.0 45.0 451.0 43.0 481.0 46.0 479.0 42.0 DNF Entity search findEntities 54.0 260.0 65.0 454.0 88.0 973.5 215.0 2104.0 413.0 DNF ... findEntitiesByProperty 31.0 40.0 36.0 50.0 44.0 79.0 100.0 115.0 151.0 DNF ... findEntitiesByPropertyValue 58.0 85.0 71.0 94.0 102.0 123.0 170.0 165.0 331.0 DNF Relationship creation addRelationship 48.0 160.0 47.0 162.0 45.0 162.5 47.0 156.0 44.0 DNF ... saveRelationshipReferenceCopy 52.0 460.0 49.0 456.0 47.0 480.0 49.0 464.0 46.0 DNF Relationship search findRelationships 28.0 45.0 30.0 60.0 33.0 99.0 39.0 146.0 39.0 DNF ... findRelationshipsByProperty 29.0 48.0 35.0 63.0 43.0 109.0 105.0 171.0 166.0 DNF ... findRelationshipsByPropertyValue 47.0 71.0 59.0 86.0 78.0 120.0 159.0 181.0 362.0 DNF Entity classification classifyEntity 78.0 886.0 73.5 921.5 72.0 1031.0 75.0 961.0 73.0 DNF ... saveClassificationReferenceCopy 70.0 738.0 59.0 845.5 63.0 969.5 64.0 895.5 62.0 DNF Classification search findEntitiesByClassification 37.0 606.0 44.0 1029.0 60.0 2195.5 97.0 3696.0 113.0 DNF Entity update reTypeEntity 44.0 390.0 41.0 374.5 40.0 453.0 46.0 381.0 41.0 DNF ... updateEntityProperties 58.0 698.0 53.0 720.5 53.0 804.0 49.0 776.5 49.0 DNF Relationship update updateRelationshipProperties 63.0 456.5 56.0 467.0 56.0 499.0 56.0 456.0 54.0 DNF Classification update updateEntityClassification 96.0 1178.5 92.5 1259.5 87.0 1410.0 90.0 1262.0 88.0 DNF Entity undo undoEntityUpdate 49.0 -- 53.0 -- 50.0 -- 48.0 -- 44.0 -- Relationship undo undoRelationshipUpdate 56.0 -- 55.0 -- 54.0 -- 53.0 -- 52.0 -- Entity retrieval getEntityDetail 17.0 18.0 16.0 18.0 16.0 16.0 16.0 16.0 16.0 DNF ... getEntitySummary 17.0 17.0 16.0 17.0 16.0 15.0 16.0 15.0 16.0 DNF ... isEntityKnown 17.0 19.0 16.0 18.0 16.0 16.0 16.0 16.0 16.0 DNF Entity history retrieval getEntityDetail 20.0 -- 19.0 -- 19.0 -- 19.0 -- 19.0 -- ... getEntityDetailHistory 22.0 -- 21.0 -- 21.0 -- 21.0 -- 21.0 -- Relationship retrieval getRelationship 18.0 23.0 17.0 20.0 17.0 19.0 18.0 18.0 17.0 DNF ... isRelationshipKnown 18.0 23.0 17.0 20.0 17.0 19.0 18.0 18.0 17.0 DNF Relationship history retrieval getRelationship 21.0 -- 21.0 -- 21.0 -- 21.0 -- 21.0 -- ... getRelationshipHistory 23.0 -- 22.0 -- 22.0 -- 22.0 -- 22.0 -- Entity history search findEntities 60.5 -- 89.0 -- 140.0 -- 549.5 -- 1574.0 -- ... findEntitiesByProperty 31.0 -- 34.0 -- 40.0 -- 55.0 -- 73.0 -- ... findEntitiesByPropertyValue 54.0 -- 75.0 -- 129.5 -- 295.5 -- 676.0 -- Relationship history search findRelationships 28.0 -- 34.0 -- 43.0 -- 49.0 -- 49.0 -- ... findRelationshipsByProperty 33.0 -- 41.0 -- 55.0 -- 63.0 -- 65.0 -- ... findRelationshipsByPropertyValue 55.0 -- 84.0 -- 143.0 -- 228.0 -- 516.0 -- Graph queries getEntityNeighborhood 27.0 -- 26.0 -- 24.0 -- -- -- -- -- ... getLinkingEntities 21.0 -- 25.0 -- 26.0 -- -- -- -- -- ... getRelatedEntities 563.0 -- 1057.0 -- 1873.0 -- -- -- -- -- ... getRelationshipsForEntity 26.0 -- 27.0 -- 25.0 -- -- -- -- -- Graph history queries getEntityNeighborhood 26.0 -- 25.0 -- 24.0 -- -- -- -- -- ... getLinkingEntities 21.0 -- 25.0 -- 26.0 -- -- -- -- -- ... getRelatedEntities 559.5 -- 1057.5 -- 1873.0 -- -- -- -- -- ... getRelationshipsForEntity 25.0 -- 25.0 -- 24.0 -- -- -- -- -- Entity re-home reHomeEntity 46.0 759.0 45.0 739.0 44.0 909.0 51.0 775.0 45.0 DNF Relationship re-home reHomeRelationship 43.0 405.5 45.0 394.0 41.0 453.0 46.0 394.0 44.0 DNF Entity declassify declassifyEntity 66.0 1302.0 63.0 1374.5 63.0 1629.0 68.5 1420.0 64.0 DNF ... purgeClassificationReferenceCopy 58.0 -- 55.5 -- 55.5 -- 62.0 -- 56.0 -- Entity re-identify reIdentifyEntity 55.0 1745.0 51.0 1735.0 49.0 2310.5 64.0 1864.0 56.0 DNF Relationship re-identify reIdentifyRelationship 44.0 855.0 47.0 823.5 40.0 950.5 49.0 885.0 44.0 DNF Relationship delete deleteRelationship 40.0 398.0 40.0 407.0 39.0 466.0 47.0 434.0 41.0 DNF Entity delete deleteEntity 45.0 785.0 45.0 824.0 43.0 1054.0 55.0 886.0 48.0 DNF Entity restore restoreEntity 39.0 809.5 39.0 871.0 37.0 1091.0 45.0 874.0 40.0 DNF Relationship restore restoreRelationship 37.0 395.0 39.0 401.0 36.0 517.0 43.0 443.0 36.0 DNF Relationship purge purgeRelationship 32.0 146.0 32.0 194.0 30.0 210.0 39.0 202.0 33.0 DNF ... purgeRelationshipReferenceCopy 23.0 118.0 24.0 116.0 22.0 126.0 27.0 117.0 24.0 DNF Entity purge purgeEntity 40.0 271.0 40.0 381.0 40.0 433.5 52.0 416.0 45.0 DNF ... purgeEntityReferenceCopy 24.0 271.0 24.0 259.0 24.0 277.0 29.0 253.0 26.0 DNF","title":"XTDB vs JanusGraph"},{"location":"connectors/repository/xtdb/performance/#reproducibility","text":"","title":"Reproducibility"},{"location":"connectors/repository/xtdb/performance/#re-running-the-tests","text":"Two Helm charts are provided, that were used to automate the execution of these suites against the XTDB repository connector: The Helm chart used to execute the CTS suite The Helm chart used to execute the PTS suite These use a default configuration for the XTDB repository where Lucene is used as a text index and RocksDB is used for all persistence: index store, document store and transaction log. No additional tuning of any parameters (XTDB or RocksDB) is applied: they use all of their default settings.","title":"Re-running the tests"},{"location":"connectors/repository/xtdb/performance/#data-points","text":"The cts/results directory in the code repository for the connector contains results of running the suites against the XTDB connector. For each test suite execution, you will find the following details: openmetadata_cts_summary.json - a summary of the results of each profile Description of the k8s environment deployment - details of the deployed components used for the test configmap.yaml - details of the variables used within the components of the test The OMAG Server configurations: omag.server.[crux|xtdb].config - the configuration of the XTDB connector (proxy) omag.server.cts.config - the configuration of the test workbench The cohort registrations: cohort.coco.[crux|xtdb].local - the local XTDB connector (proxy) cohort registration information cohort.coco.[crux|xtdb].remote - the cohort members considered remote from the XTDB connector (proxy)'s perspective cohort.coco.cts.local - the local test Workbench cohort registration cohort.coco.cts.remote - the cohort members considered remote from the test Workbench's perspective Detailed results: pd.tar.gz - an archive containing the full detailed results of every profile tested tcd.tar.gz - an archive containing the full detailed results of every test case executed Jupyter Notebooks used to analyze the results: analyze-performance-results.ipynb - details about the environment, instance counts, and distribution of elapsed times per method, also illustrating how the results can be analyzed more deeply calculate-medians.ipynb - used to calculate the medians displayed in the table further below (to run either of these notebooks, you will need to first extract the pd.tar.gz file to have the JSON results files for analysis)","title":"Data points"},{"location":"connectors/repository/xtdb/upgrade/","text":"Upgrading the XTDB Connector \u00b6 The connector embeds its own persistence layer (storage) for metadata in XTDB. While we try to keep this underlying storage unchanged as much as possible to ease moving from one version of the connector to another, this is not always possible. It may therefore be necessary to occasionally migrate any pre-existing metadata stored in the embedded XTDB repository in order to make use of the latest features and performance benefits of a new version of the connector. Persistence layer version must be compatible with connector version To ensure the integrity of the metadata, the connector will validate that the version of the persistence matches the version the connector expects before even attempting to run -- if this validation fails, you will see an OMRS-XTDB-REPOSITORY-500-003 error in the audit log to indicate that you must first migrate the metadata before running this version of the connector. In other words: if migration is needed, the newer version of the connector will not allow you to run against an older set of metadata without first running the migration. Your only options will be to continue to use an older version of the connector (with which your pre-existing metadata is compatible), or to run this offline migration of your repository and then run the newer version of the connector. The migration itself runs outside the connector (while the connector is offline), in order to maximize the throughput of the in-place upgrade of the repository. The time it takes to run the migration naturally depends on a number of factors, such as the amount of pre-existing metadata that must be migrated and the specific changes needed by the upgrade. As a very approximate metric, we would expect the in-place upgrade to be capable of migrating 60-100 metadata instances (entities or relationships) per second. So 10 000 instances should take approximately 2 minutes. 1. Obtain migrator \u00b6 Start by downloading the XTDB repository migrator: Latest release Latest snapshot The migrator is egeria-connector-xtdb-migrator-{version}-jar-with-dependencies.jar 2. Configure repository \u00b6 Before running the migrator, define the configuration of your repository. The configuration must be defined in a JSON file, following XTDB's JSON configuration format . Take XTDB configuration from Egeria connector configuration The simplest way to ensure this matches the configuration used by your connector is to copy the xtdbConfig property from the request you POST to Egeria to configure your connector. Simply be certain to replace any relative paths with absolute paths to data to ensure the migrator can find the metadata. Example connector configuration in Egeria 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.egeria.connectors.juxt.xtdb.repositoryconnector.XtdbOMRSRepositoryConnectorProvider\" }, \"configurationProperties\" : { \"xtdbConfig\" : { \"xtdb/index-store\" : { \"kv-store\" : { \"xtdb/module\" : \"xtdb.rocksdb/->kv-store\" , \"db-dir\" : \"data/servers/xtdb/rdb-index\" } }, \"xtdb/document-store\" : { \"kv-store\" : { \"xtdb/module\" : \"xtdb.rocksdb/->kv-store\" , \"db-dir\" : \"data/servers/xtdb/rdb-docs\" } }, \"xtdb/tx-log\" : { \"kv-store\" : { \"xtdb/module\" : \"xtdb.rocksdb/->kv-store\" , \"db-dir\" : \"data/servers/xtdb/rdb-tx\" } }, \"xtdb.lucene/lucene-store\" : { \"db-dir\" : \"data/servers/xtdb/lucene\" } } } } 3. Make a backup \u00b6 As the migration runs in-place, it is always a good practice to take a backup of your repository first. Different configurations will require different approaches to backup. If your configuration makes use of pluggable components that have their own backup utilities (Kafka, JDBC, etc) then you should follow the best practices for backing up those components themselves: e.g. from Apache Kafka or the database vendor. For pluggable components that persist their data directly to the filesystem (RocksDB, Lucene, LMDB, etc), you may be able to simply backup the files directly. Ensure connector is not running before taking backup In all cases, you should ensure that your connector is shutdown (not running), and that any data is appropriately quiesced prior to running the backup. Example filesystem backup For example, running this command when the persistence is using the example configuration above: tar cvf backup.tar .../data/servers/xtdb will backup all the files used by RocksDB and Lucene: a .../data/servers/xtdb a .../data/servers/xtdb/rdb-index a .../data/servers/xtdb/lucene a .../data/servers/xtdb/config a .../data/servers/xtdb/cohorts a .../data/servers/xtdb/rdb-tx a .../data/servers/xtdb/rdb-docs ... 4. Run the in-place upgrade \u00b6 Run the following command to execute the in-place upgrade: Execute the in-place upgrade java -jar egeria-connector-xtdb-migrator-*-jar-with-dependencies.jar config.json Where config.json is the file containing your repository's configuration. If you run the migrator and no migration is actually needed, the output will indicate that your repository is already at the necessary version to work with this version of the connector, and no migration is needed. You can simply proceed with running the connector. Example output when no migration is needed Starting a XTDB node using configuration: config.json SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\". SLF4J: Defaulting to no-operation (NOP) logger implementation SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. ... checking if migration is needed (xtdb.api.java.JXtdbNode@25c4bb06) This node is already at the latest version of the persistence layer -- no migration needed. If a migration is necessary, it will immediately begin to run. The output will indicate the existing version of the persistence layer, and the version that is expected by this version of the connector. If you are making a jump across several migrations, you may see this last line appear multiple times with different existing versions listed each time: the in-place upgrade sequentially applies the necessary migrations step-by-step. Example output when migration is required Starting a XTDB node using configuration: config.json SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\". SLF4J: Defaulting to no-operation (NOP) logger implementation SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. ... checking if migration is needed (xtdb.api.java.JXtdbNode@25c4bb06) The node is at version -1, while latest is 2 -- migrating... 5. Start the connector \u00b6 Once the previous (in-place upgrade) command completes, you should now be able to start your connector. See the instructions on configuring the connector for more details. Change log \u00b6 There should be no need to actually understand these details, as the connector (and migration) will handle them for you. However, for the interested reader, the following changes were made to the storage layer in the specified release: 2.9 \u00b6 InstanceAuditHeaderMapping no longer separates the type GUID and supertype GUIDs, but places all such information into a single vector (for improved search performance) RelationshipMapping no longer has separate properties for each entity proxy, but stores them as a vector: this retains their ordering, but allows relationships to be more efficiently searched from either related entity","title":"Upgrading the XTDB Connector"},{"location":"connectors/repository/xtdb/upgrade/#upgrading-the-xtdb-connector","text":"The connector embeds its own persistence layer (storage) for metadata in XTDB. While we try to keep this underlying storage unchanged as much as possible to ease moving from one version of the connector to another, this is not always possible. It may therefore be necessary to occasionally migrate any pre-existing metadata stored in the embedded XTDB repository in order to make use of the latest features and performance benefits of a new version of the connector. Persistence layer version must be compatible with connector version To ensure the integrity of the metadata, the connector will validate that the version of the persistence matches the version the connector expects before even attempting to run -- if this validation fails, you will see an OMRS-XTDB-REPOSITORY-500-003 error in the audit log to indicate that you must first migrate the metadata before running this version of the connector. In other words: if migration is needed, the newer version of the connector will not allow you to run against an older set of metadata without first running the migration. Your only options will be to continue to use an older version of the connector (with which your pre-existing metadata is compatible), or to run this offline migration of your repository and then run the newer version of the connector. The migration itself runs outside the connector (while the connector is offline), in order to maximize the throughput of the in-place upgrade of the repository. The time it takes to run the migration naturally depends on a number of factors, such as the amount of pre-existing metadata that must be migrated and the specific changes needed by the upgrade. As a very approximate metric, we would expect the in-place upgrade to be capable of migrating 60-100 metadata instances (entities or relationships) per second. So 10 000 instances should take approximately 2 minutes.","title":"Upgrading the XTDB Connector"},{"location":"connectors/repository/xtdb/upgrade/#1-obtain-migrator","text":"Start by downloading the XTDB repository migrator: Latest release Latest snapshot The migrator is egeria-connector-xtdb-migrator-{version}-jar-with-dependencies.jar","title":"1. Obtain migrator"},{"location":"connectors/repository/xtdb/upgrade/#2-configure-repository","text":"Before running the migrator, define the configuration of your repository. The configuration must be defined in a JSON file, following XTDB's JSON configuration format . Take XTDB configuration from Egeria connector configuration The simplest way to ensure this matches the configuration used by your connector is to copy the xtdbConfig property from the request you POST to Egeria to configure your connector. Simply be certain to replace any relative paths with absolute paths to data to ensure the migrator can find the metadata. Example connector configuration in Egeria 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.egeria.connectors.juxt.xtdb.repositoryconnector.XtdbOMRSRepositoryConnectorProvider\" }, \"configurationProperties\" : { \"xtdbConfig\" : { \"xtdb/index-store\" : { \"kv-store\" : { \"xtdb/module\" : \"xtdb.rocksdb/->kv-store\" , \"db-dir\" : \"data/servers/xtdb/rdb-index\" } }, \"xtdb/document-store\" : { \"kv-store\" : { \"xtdb/module\" : \"xtdb.rocksdb/->kv-store\" , \"db-dir\" : \"data/servers/xtdb/rdb-docs\" } }, \"xtdb/tx-log\" : { \"kv-store\" : { \"xtdb/module\" : \"xtdb.rocksdb/->kv-store\" , \"db-dir\" : \"data/servers/xtdb/rdb-tx\" } }, \"xtdb.lucene/lucene-store\" : { \"db-dir\" : \"data/servers/xtdb/lucene\" } } } }","title":"2. Configure repository"},{"location":"connectors/repository/xtdb/upgrade/#3-make-a-backup","text":"As the migration runs in-place, it is always a good practice to take a backup of your repository first. Different configurations will require different approaches to backup. If your configuration makes use of pluggable components that have their own backup utilities (Kafka, JDBC, etc) then you should follow the best practices for backing up those components themselves: e.g. from Apache Kafka or the database vendor. For pluggable components that persist their data directly to the filesystem (RocksDB, Lucene, LMDB, etc), you may be able to simply backup the files directly. Ensure connector is not running before taking backup In all cases, you should ensure that your connector is shutdown (not running), and that any data is appropriately quiesced prior to running the backup. Example filesystem backup For example, running this command when the persistence is using the example configuration above: tar cvf backup.tar .../data/servers/xtdb will backup all the files used by RocksDB and Lucene: a .../data/servers/xtdb a .../data/servers/xtdb/rdb-index a .../data/servers/xtdb/lucene a .../data/servers/xtdb/config a .../data/servers/xtdb/cohorts a .../data/servers/xtdb/rdb-tx a .../data/servers/xtdb/rdb-docs ...","title":"3. Make a backup"},{"location":"connectors/repository/xtdb/upgrade/#4-run-the-in-place-upgrade","text":"Run the following command to execute the in-place upgrade: Execute the in-place upgrade java -jar egeria-connector-xtdb-migrator-*-jar-with-dependencies.jar config.json Where config.json is the file containing your repository's configuration. If you run the migrator and no migration is actually needed, the output will indicate that your repository is already at the necessary version to work with this version of the connector, and no migration is needed. You can simply proceed with running the connector. Example output when no migration is needed Starting a XTDB node using configuration: config.json SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\". SLF4J: Defaulting to no-operation (NOP) logger implementation SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. ... checking if migration is needed (xtdb.api.java.JXtdbNode@25c4bb06) This node is already at the latest version of the persistence layer -- no migration needed. If a migration is necessary, it will immediately begin to run. The output will indicate the existing version of the persistence layer, and the version that is expected by this version of the connector. If you are making a jump across several migrations, you may see this last line appear multiple times with different existing versions listed each time: the in-place upgrade sequentially applies the necessary migrations step-by-step. Example output when migration is required Starting a XTDB node using configuration: config.json SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\". SLF4J: Defaulting to no-operation (NOP) logger implementation SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. ... checking if migration is needed (xtdb.api.java.JXtdbNode@25c4bb06) The node is at version -1, while latest is 2 -- migrating...","title":"4. Run the in-place upgrade"},{"location":"connectors/repository/xtdb/upgrade/#5-start-the-connector","text":"Once the previous (in-place upgrade) command completes, you should now be able to start your connector. See the instructions on configuring the connector for more details.","title":"5. Start the connector"},{"location":"connectors/repository/xtdb/upgrade/#change-log","text":"There should be no need to actually understand these details, as the connector (and migration) will handle them for you. However, for the interested reader, the following changes were made to the storage layer in the specified release:","title":"Change log"},{"location":"connectors/repository/xtdb/upgrade/#29","text":"InstanceAuditHeaderMapping no longer separates the type GUID and supertype GUIDs, but places all such information into a single vector (for improved search performance) RelationshipMapping no longer has separate properties for each entity proxy, but stores them as a vector: this retains their ordering, but allows relationships to be more efficiently searched from either related entity","title":"2.9"},{"location":"education/planned-webinars/","text":"Planned Egeria Webinars \u00b6 For the webinars being planned see page https://wiki.lfaidata.foundation/display/EG/Egeria+Webinar+program Please check the calendar has the latest confirmed Webinar information Return to Git Repository Home Page","title":"Index"},{"location":"education/planned-webinars/#planned-egeria-webinars","text":"For the webinars being planned see page https://wiki.lfaidata.foundation/display/EG/Egeria+Webinar+program Please check the calendar has the latest confirmed Webinar information Return to Git Repository Home Page","title":"Planned Egeria Webinars"},{"location":"education/previous-webinars/","text":"Previous Egeria Webinars \u00b6 Webinars on open metadata and governance are run on a regular basis, there was a break during the pandemic. Each webinar focuses on a specific audience or issue. The material for our webinars is stored below along with links to the recordings. Value of Egeria - October 2021 Visualising a metadata eco system - September 2021 Building a governed data lake with Egeria - June 2020 Building a Data Catalog with Egeria - April 2020 Egeria virtual metadata show - March 2020 Three days of topics relating to Egeria from ING, IBM and SAS. New Approaches to Managing Access to Sensitive Data - December 2019 Managing data privacy - July 2018 Metadata standards - April 2018 Free your metadata - October 2017 Return to Git Repository Home Page","title":"Index"},{"location":"education/previous-webinars/#previous-egeria-webinars","text":"Webinars on open metadata and governance are run on a regular basis, there was a break during the pandemic. Each webinar focuses on a specific audience or issue. The material for our webinars is stored below along with links to the recordings. Value of Egeria - October 2021 Visualising a metadata eco system - September 2021 Building a governed data lake with Egeria - June 2020 Building a Data Catalog with Egeria - April 2020 Egeria virtual metadata show - March 2020 Three days of topics relating to Egeria from ING, IBM and SAS. New Approaches to Managing Access to Sensitive Data - December 2019 Managing data privacy - July 2018 Metadata standards - April 2018 Free your metadata - October 2017 Return to Git Repository Home Page","title":"Previous Egeria Webinars"},{"location":"education/previous-webinars/april-2018/","text":"Open Metadata Standards \u00b6 Date: 26th April 2018 This presentation provides a description of the open metadata standards associated with exchanging metadata across a cohort. It is still accurate except for the chart that describes Apache Atlas as the reference implementation for open metadata. The reference implementation has moved to the ODPi Egeria project . Presentations Day 1 Day 2 Day 3 YouTube Video: Link to video Return to Webinar list","title":"Index"},{"location":"education/previous-webinars/april-2018/#open-metadata-standards","text":"Date: 26th April 2018 This presentation provides a description of the open metadata standards associated with exchanging metadata across a cohort. It is still accurate except for the chart that describes Apache Atlas as the reference implementation for open metadata. The reference implementation has moved to the ODPi Egeria project . Presentations Day 1 Day 2 Day 3 YouTube Video: Link to video Return to Webinar list","title":"Open Metadata Standards"},{"location":"education/previous-webinars/april-2020/","text":"Webinar - Building a data catalog with Egeria \u00b6 Date: 28th April 2020 The Building a data catalog with Egeria webinar covers the different APIs and services that support the creation and maintenance of a catalog of data assets. This catalog helps an organization find and use data and related assets effectively. Presentation YouTube Video: Link to video \u00b6 Return to Webinar list","title":"Index"},{"location":"education/previous-webinars/april-2020/#webinar-building-a-data-catalog-with-egeria","text":"Date: 28th April 2020 The Building a data catalog with Egeria webinar covers the different APIs and services that support the creation and maintenance of a catalog of data assets. This catalog helps an organization find and use data and related assets effectively. Presentation YouTube Video:","title":"Webinar - Building a data catalog with Egeria"},{"location":"education/previous-webinars/april-2020/#link-to-video","text":"Return to Webinar list","title":"Link to video"},{"location":"education/previous-webinars/december-2019/","text":"Webinar - New Approaches to Managing Access to Sensitive Data \u00b6 Date: 6th December 2010 What happens when you need your data scientist to repeatedly work with your most valuable and sensitive data? How do you prevent them from seeing more than they need whilst ensuring that they have a productive and enabling work environment. In this webinar we look at 3 different approaches to managing secure access to data sets that include the most personal and sensitive data. In each approach we use increasing automated means to create selective access to an employee data set that includes correlated personal, performance and financial information. The technology involved is all open source and includes ODPi Egeria and Palisade. Together they will change the way you think about access control. Presentation YouTube Video: Link to video Return to Webinar list","title":"Index"},{"location":"education/previous-webinars/december-2019/#webinar-new-approaches-to-managing-access-to-sensitive-data","text":"Date: 6th December 2010 What happens when you need your data scientist to repeatedly work with your most valuable and sensitive data? How do you prevent them from seeing more than they need whilst ensuring that they have a productive and enabling work environment. In this webinar we look at 3 different approaches to managing secure access to data sets that include the most personal and sensitive data. In each approach we use increasing automated means to create selective access to an employee data set that includes correlated personal, performance and financial information. The technology involved is all open source and includes ODPi Egeria and Palisade. Together they will change the way you think about access control. Presentation YouTube Video: Link to video Return to Webinar list","title":"Webinar - New Approaches to Managing Access to Sensitive Data"},{"location":"education/previous-webinars/july-2018/","text":"Managing Privacy \u00b6 Date: 12th July 2018 There is increasing public awareness about the issues of data privacy. Individuals are concerned with how data about themselves is used in digital services, such as websites, and mobile apps. In this webinar we will be discussing how an organization that offers digital services can manage data about individuals so that their privacy is respected and the organization is compliant with new regulations on data privacy such as the EU General Data Protection Regulation (GDPR). You will learn: The life cycle of a digital service as it is developed, sold, enhanced and used. This life cycle breaks the work into six stages. Each stage describes the roles and the activities involved to ensure data privacy. The types of artifacts that need to be collected about a digital service and the methods used to develop it. How these artifacts link together in an open metadata repository (data catalog). Finally we will demonstrate the ODPi's data privacy pack that provides documentation and open metadata definitions for a privacy program. The ODPi's data privacy pack is the first open source initiative for data privacy. It is being developed by a team of cross-industry, multi-vendor data privacy experts. Join the call to find out more and maybe join the team to build your skills and share your expertise. Presentation YouTube Video: Link to video \u00b6 Return to Webinar list","title":"Index"},{"location":"education/previous-webinars/july-2018/#managing-privacy","text":"Date: 12th July 2018 There is increasing public awareness about the issues of data privacy. Individuals are concerned with how data about themselves is used in digital services, such as websites, and mobile apps. In this webinar we will be discussing how an organization that offers digital services can manage data about individuals so that their privacy is respected and the organization is compliant with new regulations on data privacy such as the EU General Data Protection Regulation (GDPR). You will learn: The life cycle of a digital service as it is developed, sold, enhanced and used. This life cycle breaks the work into six stages. Each stage describes the roles and the activities involved to ensure data privacy. The types of artifacts that need to be collected about a digital service and the methods used to develop it. How these artifacts link together in an open metadata repository (data catalog). Finally we will demonstrate the ODPi's data privacy pack that provides documentation and open metadata definitions for a privacy program. The ODPi's data privacy pack is the first open source initiative for data privacy. It is being developed by a team of cross-industry, multi-vendor data privacy experts. Join the call to find out more and maybe join the team to build your skills and share your expertise. Presentation YouTube Video:","title":"Managing Privacy"},{"location":"education/previous-webinars/july-2018/#link-to-video","text":"Return to Webinar list","title":"Link to video"},{"location":"education/previous-webinars/june-2020/","text":"Webinar - Building a governed data lake with Egeria \u00b6 Date: 2nd June 2020 Data lakes provide a flexible environment for managing many types of data at scale. In this presentation, we will cover how open metadata can increase the effectiveness and protection of the data lake and avoid it running into a mighty data swamp. Presentation YouTube Video: Link to video Return to Webinar list","title":"Index"},{"location":"education/previous-webinars/june-2020/#webinar-building-a-governed-data-lake-with-egeria","text":"Date: 2nd June 2020 Data lakes provide a flexible environment for managing many types of data at scale. In this presentation, we will cover how open metadata can increase the effectiveness and protection of the data lake and avoid it running into a mighty data swamp. Presentation YouTube Video: Link to video Return to Webinar list","title":"Webinar - Building a governed data lake with Egeria"},{"location":"education/previous-webinars/march-2020/","text":"Egeria Virtual Metadata Show \u00b6 Date: 24th \u2013 26th March Find out how metadata can enable enterprise-wide knowledge and understanding for all data assets in an organization. Get the latest information on the Egeria Open Metadata exchange standard and capabilities, that makes it possible for any tool or application repository to share metadata in real-time. Day 1 Why Egeria is an open source project \u2013 John Mertic (Linux Foundation) The Egeria Journey so far \u2013 Maryna Strelchuk (ING Bank) Design Lineage - ING Bank Perspective Q&A Day 2 Metadata essentials for the data driven organisation - Mandy Chessell (IBM) Everything Metadata - Chris Replogle (SAS) How to adopt the Egeria Framework Q&A Day 3 Lightening Talks 3pm Supporting ethical AI Industry Models and Business Glossaries Egeria and Cloud Pak\u2019s Egeria driving Enterprise API's Q&A why is Egeria an open source project Day 1 presentation Day 2 presentation Day 3 presentation YouTube Day 1 Video: Link to video YouTube Day 2 Video: Link to video YouTube Day 3 Video: Link to video Return to Webinar list","title":"Index"},{"location":"education/previous-webinars/march-2020/#egeria-virtual-metadata-show","text":"Date: 24th \u2013 26th March Find out how metadata can enable enterprise-wide knowledge and understanding for all data assets in an organization. Get the latest information on the Egeria Open Metadata exchange standard and capabilities, that makes it possible for any tool or application repository to share metadata in real-time. Day 1 Why Egeria is an open source project \u2013 John Mertic (Linux Foundation) The Egeria Journey so far \u2013 Maryna Strelchuk (ING Bank) Design Lineage - ING Bank Perspective Q&A Day 2 Metadata essentials for the data driven organisation - Mandy Chessell (IBM) Everything Metadata - Chris Replogle (SAS) How to adopt the Egeria Framework Q&A Day 3 Lightening Talks 3pm Supporting ethical AI Industry Models and Business Glossaries Egeria and Cloud Pak\u2019s Egeria driving Enterprise API's Q&A why is Egeria an open source project Day 1 presentation Day 2 presentation Day 3 presentation YouTube Day 1 Video: Link to video YouTube Day 2 Video: Link to video YouTube Day 3 Video: Link to video Return to Webinar list","title":"Egeria Virtual Metadata Show"},{"location":"education/previous-webinars/october-2017/","text":"Webinar - Free your metadata \u00b6 A definition of metadata and why it is essential to your organization. Presentation YouTube Video: Link to video Return to Webinar list","title":"Index"},{"location":"education/previous-webinars/october-2017/#webinar-free-your-metadata","text":"A definition of metadata and why it is essential to your organization. Presentation YouTube Video: Link to video Return to Webinar list","title":"Webinar - Free your metadata"},{"location":"education/previous-webinars/october-2021/","text":"The Value of Egeria \u00b6 Date: 4th October 2021 Presenter: Mandy Chessell This session is for people wanting to understand the value of Egeria in enabling data centric, metadata driven integration. The session will start with the core Egeria constructs, including entities, explaining the principles behind why they are as they are. The session will go through the layers and aspects of the Egeria architecture, at each stage talking about the applicability to solving real world problems. By the end of the session you should have awareness of the parts of Egeria at a high level; why they have been implemented that way and the value that each piece brings. Link to event advertising TBA Link to recording on YouTube TBA Presentation TBA Return to Webinar list","title":"Index"},{"location":"education/previous-webinars/october-2021/#the-value-of-egeria","text":"Date: 4th October 2021 Presenter: Mandy Chessell This session is for people wanting to understand the value of Egeria in enabling data centric, metadata driven integration. The session will start with the core Egeria constructs, including entities, explaining the principles behind why they are as they are. The session will go through the layers and aspects of the Egeria architecture, at each stage talking about the applicability to solving real world problems. By the end of the session you should have awareness of the parts of Egeria at a high level; why they have been implemented that way and the value that each piece brings. Link to event advertising TBA Link to recording on YouTube TBA Presentation TBA Return to Webinar list","title":"The Value of Egeria"},{"location":"education/previous-webinars/september-2021/","text":"Visualising a Metadata Ecosystem \u00b6 Date: 13th September 2021 Presenter: David Radley The session is for people looking to understand the metadata across their ecosystem in terms of the Egeria open types and instances using visualisations in the Egeria React User Interface. Understanding the types is important knowledge when developing connectors and new APIs like OMAS\u2019s. This session will also show how metadata instances can be explored at a low level. This will be contrasted with an exploration of semantic data that is based on the Subject Area Open Metadata Access Service (OMAS). Link to event advertising Presentation YouTube Video: Link to video Return to Webinar list","title":"Index"},{"location":"education/previous-webinars/september-2021/#visualising-a-metadata-ecosystem","text":"Date: 13th September 2021 Presenter: David Radley The session is for people looking to understand the metadata across their ecosystem in terms of the Egeria open types and instances using visualisations in the Egeria React User Interface. Understanding the types is important knowledge when developing connectors and new APIs like OMAS\u2019s. This session will also show how metadata instances can be explored at a low level. This will be contrasted with an exploration of semantic data that is based on the Subject Area Open Metadata Access Service (OMAS). Link to event advertising Presentation YouTube Video: Link to video Return to Webinar list","title":"Visualising a Metadata Ecosystem"},{"location":"frameworks/alf/","text":"Released This function is complete and can be used. The interfaces will be supported until the function is removed from the project via the deprecation process. There will be ongoing extensions to this function, but it will be done to ensure backward compatibility as far as possible. If there is a need to break backward compatibility, this will be discussed and reviewed in the community, with a documented timeline. Audit Log Framework ( ALF ) \u00b6 The audit log framework ( ALF ) provides interface definitions and classes to enable connectors to support natural language enabled diagnostics such as exception messages and audit log messages. The audit log framework provides the ability to route audit log messages to multiple destinations where they can be stored or processed automatically. This second option is particularly important in today's world of continuous operations. When processing activity wishes to log a message to the audit log, it selects a message definition from a message set, optionally passing in the values to fill out the placeholders in the message template. The message definition is passed to the audit log where it calls the message formatter, builds a log record and passes it on to the audit destination. The audit log destination can be extended to allow routing to different destinations for review and processing. Usage \u00b6 The Open Metadata Repository Services ( OMRS ) provide an extension to the audit log destination that supports audit log store connectors. This means that an OMAG Server can be configured to route audit log messages to different destinations. Details of the supported audit log store connectors and how to set them up are described in configuring the audit log .","title":"Audit Logs (ALF)"},{"location":"frameworks/alf/#audit-log-framework-alf","text":"The audit log framework ( ALF ) provides interface definitions and classes to enable connectors to support natural language enabled diagnostics such as exception messages and audit log messages. The audit log framework provides the ability to route audit log messages to multiple destinations where they can be stored or processed automatically. This second option is particularly important in today's world of continuous operations. When processing activity wishes to log a message to the audit log, it selects a message definition from a message set, optionally passing in the values to fill out the placeholders in the message template. The message definition is passed to the audit log where it calls the message formatter, builds a log record and passes it on to the audit destination. The audit log destination can be extended to allow routing to different destinations for review and processing.","title":"Audit Log Framework (ALF)"},{"location":"frameworks/alf/#usage","text":"The Open Metadata Repository Services ( OMRS ) provide an extension to the audit log destination that supports audit log store connectors. This means that an OMAG Server can be configured to route audit log messages to different destinations. Details of the supported audit log store connectors and how to set them up are described in configuring the audit log .","title":"Usage"},{"location":"frameworks/gaf/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Governance Action Framework ( GAF ) \u00b6 The governance action framework ( GAF ) provides the interfaces and base implementations for components (called governance action services ) that take action to: detect, report and eventually correct a situation that is harmful to the data or the organization in some way or to enhance the metadata to improve its use. The governance action framework can be used for three purposes: Provide the complete implementation and orchestration of a governance process. Provide coordination between processes run by specialized governance systems. For example, coordinating a DevOps pipeline with a data movement and quality process and security incident management. Provide contextual metadata plus an audit trail of actions managed by an external governance process. Governance action \u00b6 A governance action describes a specific governance activity that needs to be performed on one or more metadata elements, or their counterparts in the digital landscape. A governance action is represented as a metadata entity in the open metadata repositories and linked to: The source (cause) of the governance action. The target elements that need to be acted upon. The governance engine that will run the governance service that implements the desired behavior. The GovernanceAction metadata entity is used to coordinate the desired activity in the governance engine, record its current state and act as a record of the activity for future audits. Governance actions can be created through the Governance Engine OMAS . Some governance services (for example, the watchdog governance action service ) can create governance actions when they run. Governance services produce output strings called guards that indicate specific conditions or outcomes. These guards can be used to trigger new governance actions. Triggered governance actions are linked to their predecessor so it possible to trace through the governance actions that ran. The governance action process defines the flow of governance actions. It uses governance action types to build up a template of possible governance actions linked via the guards. When the process runs, its linked governance action types control the triggering of new governance actions. If the start date of the governance action is in the future, the engine host services running in the same engine host as the nominated governance engine will schedule the governance service to run soon after the requested start date. If the start date is left blank, the requested governance service is run as soon as possible. Governance action process \u00b6 A governance action process defines a prescribed sequence of governance actions . Its definition consists of a linked set of governance action types . Each governance action type describes which governance action service to run from which governance action engine along with the request type and request parameters to pass. The linkage between the governance action types shows the guards that must be true to initiate the next governance action in the flow. The 0461 governance action engines model shows how the request type links the governance action engine to the governance action service via the SupportedGovernanceActionService relationship. The 0470 incident reporting model shows the structure of the incident report. It is a Referenceable so it can support comments and have governance actions linked to it. Further information Governance action processes are defined using the Governance Engine OMAS . The Open Metadata Engine Services ( OMES ) provide the mechanisms that support the different types of governance action engines . These engines run the governance action services that execute the governance actions defined by the governance action process. Governance action type \u00b6 A governance action type is a template for a governance action . A set of linked governance action types form the definition of a governance action process. Governance action types are defined through the Governance Engine OMAS and this OMAS also coordinates the creation of a governance action from the governance action type as part of its execution of the governance action process. The governance action type is defined in the 0462 governance action type model. Guard \u00b6 Guards are labels that are created by governance action services and are used by the Governance Engine OMAS to determine which governance action service to run next. Incident report \u00b6 An incident report describes a situation that is out of line with the governance definitions (such as policies and rules). It provides a focus point to coordinate efforts to resolve the situation. As the incident is handled, details of the cause, affected resources and actions taken are attached to the incident report to create a complete record of the incident for future analysis. Incident reports are typically created by governance watchdog services . The 0470 incident reporting model shows the structure of the incident report. It is a Referenceable so it can support comments and linked classifications and tags. Governance action services \u00b6 A governance action service is a specialized connector that performs monitoring of metadata changes, validation of metadata, triage of issues, assessment and/or remediation activities on request. There are five types of governance action services, each of which supports a specialist governance activity (see subsections). These are often used in conjunction with the open discovery services from the Open Discovery Framework ( ODF ) . Collectively they are called the governance services and they can be linked together into governance action processes . Some governance action services invoke functions in external engines that are working with data and related assets. The GAF offers embeddable functions and APIs to simplify the implementation of governance action services, and their integration into the broader digital landscape, whilst being resilient and with good performance. Watchdog governance service \u00b6 The watchdog governance service monitors changes in the metadata and initiates one of the following as a result: governance action governance action process incident report One example of a watchdog governance service is to monitor for new assets . Another example is to monitor the addition of open discovery reports and take action on their content. Verification governance service \u00b6 Verification governance services test the properties of specific open metadata elements to ensure they are set up correctly and do not indicate a situation where governance activity is required. The results returned from the verification governance service can be used to trigger other governance services as part of a governance action process . The verification services may also publish guards to report on any errors it finds. For example, it may check that a new asset has an owner, is set up with zones and includes a connection and a schema. Triage governance service \u00b6 Triage governance services run triage rules to determine how to manage a situation. This could be to initiate an external workflow, wait for manual decision or initiate a remediation request through either an external workflow or by creating a ToDo for a specific person. Remediation governance service \u00b6 The remediation governance services perform updates to metadata. Examples of remediation services are duplicate linking and consolidation. Provisioning governance service \u00b6 A provisioning governance service invokes a provisioning service whenever a provisioning request is made. Typically, the provisioning service is an external service. It may also create lineage metadata to describe the work of the provisioning engine. Implementing governance action services \u00b6 Governance action services are open connectors that support the interfaces defined by the GAF . They may produce audit log records and exceptions, and they may make changes to metadata through the Open Metadata Access Services ( OMAS ) . A governance action service is passed a context as it is started. This provides access to the request type and associated parameters (name-value pairs) used to invoke the governance action service, along with a client to access open metadata through the Governance Engine OMAS . This context is then specialized for each type of governance action service. Details of the specific context for each service can be found in the links above to the various governance action service types. Configuring governance action services \u00b6 A collection of related governance action services are grouped into governance action engines for deployment. The governance action engine maps governance action request types to the governance action service that should be invoked along with. These definitions are created through the Governance Engine OMAS and are stored in the open metadata repositories. Governance action engines are hosted in an Open Metadata Engine Service ( OMES ) running on one or more engine hosts . The Open Metadata Types used to define the governance action engines are located in 0461 governance action engines . Running governance action services \u00b6 Governance action engines are hosted by the Governance Action OMES . The engine services run in dedicated OMAG Server called the engine host . You can find instructions for configuring the engine services in the engine host in the administration guide. The Governance Engine OMAS provides the services for: setting up the definitions of a governance action engine. configuring governance action processes . managing governance actions and incident reports . Governance pack \u00b6 A governance pack is a collection of pre-defined governance engines and services definitions plus governance service implementations. A team can use the governance pack to distribute the governance engine function to different metadata ecosystems.","title":"Governance Actions (GAF)"},{"location":"frameworks/gaf/#governance-action-framework-gaf","text":"The governance action framework ( GAF ) provides the interfaces and base implementations for components (called governance action services ) that take action to: detect, report and eventually correct a situation that is harmful to the data or the organization in some way or to enhance the metadata to improve its use. The governance action framework can be used for three purposes: Provide the complete implementation and orchestration of a governance process. Provide coordination between processes run by specialized governance systems. For example, coordinating a DevOps pipeline with a data movement and quality process and security incident management. Provide contextual metadata plus an audit trail of actions managed by an external governance process.","title":"Governance Action Framework (GAF)"},{"location":"frameworks/gaf/#governance-action","text":"A governance action describes a specific governance activity that needs to be performed on one or more metadata elements, or their counterparts in the digital landscape. A governance action is represented as a metadata entity in the open metadata repositories and linked to: The source (cause) of the governance action. The target elements that need to be acted upon. The governance engine that will run the governance service that implements the desired behavior. The GovernanceAction metadata entity is used to coordinate the desired activity in the governance engine, record its current state and act as a record of the activity for future audits. Governance actions can be created through the Governance Engine OMAS . Some governance services (for example, the watchdog governance action service ) can create governance actions when they run. Governance services produce output strings called guards that indicate specific conditions or outcomes. These guards can be used to trigger new governance actions. Triggered governance actions are linked to their predecessor so it possible to trace through the governance actions that ran. The governance action process defines the flow of governance actions. It uses governance action types to build up a template of possible governance actions linked via the guards. When the process runs, its linked governance action types control the triggering of new governance actions. If the start date of the governance action is in the future, the engine host services running in the same engine host as the nominated governance engine will schedule the governance service to run soon after the requested start date. If the start date is left blank, the requested governance service is run as soon as possible.","title":"Governance action"},{"location":"frameworks/gaf/#governance-action-process","text":"A governance action process defines a prescribed sequence of governance actions . Its definition consists of a linked set of governance action types . Each governance action type describes which governance action service to run from which governance action engine along with the request type and request parameters to pass. The linkage between the governance action types shows the guards that must be true to initiate the next governance action in the flow. The 0461 governance action engines model shows how the request type links the governance action engine to the governance action service via the SupportedGovernanceActionService relationship. The 0470 incident reporting model shows the structure of the incident report. It is a Referenceable so it can support comments and have governance actions linked to it. Further information Governance action processes are defined using the Governance Engine OMAS . The Open Metadata Engine Services ( OMES ) provide the mechanisms that support the different types of governance action engines . These engines run the governance action services that execute the governance actions defined by the governance action process.","title":"Governance action process"},{"location":"frameworks/gaf/#governance-action-type","text":"A governance action type is a template for a governance action . A set of linked governance action types form the definition of a governance action process. Governance action types are defined through the Governance Engine OMAS and this OMAS also coordinates the creation of a governance action from the governance action type as part of its execution of the governance action process. The governance action type is defined in the 0462 governance action type model.","title":"Governance action type"},{"location":"frameworks/gaf/#guard","text":"Guards are labels that are created by governance action services and are used by the Governance Engine OMAS to determine which governance action service to run next.","title":"Guard"},{"location":"frameworks/gaf/#incident-report","text":"An incident report describes a situation that is out of line with the governance definitions (such as policies and rules). It provides a focus point to coordinate efforts to resolve the situation. As the incident is handled, details of the cause, affected resources and actions taken are attached to the incident report to create a complete record of the incident for future analysis. Incident reports are typically created by governance watchdog services . The 0470 incident reporting model shows the structure of the incident report. It is a Referenceable so it can support comments and linked classifications and tags.","title":"Incident report"},{"location":"frameworks/gaf/#governance-action-services","text":"A governance action service is a specialized connector that performs monitoring of metadata changes, validation of metadata, triage of issues, assessment and/or remediation activities on request. There are five types of governance action services, each of which supports a specialist governance activity (see subsections). These are often used in conjunction with the open discovery services from the Open Discovery Framework ( ODF ) . Collectively they are called the governance services and they can be linked together into governance action processes . Some governance action services invoke functions in external engines that are working with data and related assets. The GAF offers embeddable functions and APIs to simplify the implementation of governance action services, and their integration into the broader digital landscape, whilst being resilient and with good performance.","title":"Governance action services"},{"location":"frameworks/gaf/#watchdog-governance-service","text":"The watchdog governance service monitors changes in the metadata and initiates one of the following as a result: governance action governance action process incident report One example of a watchdog governance service is to monitor for new assets . Another example is to monitor the addition of open discovery reports and take action on their content.","title":"Watchdog governance service"},{"location":"frameworks/gaf/#verification-governance-service","text":"Verification governance services test the properties of specific open metadata elements to ensure they are set up correctly and do not indicate a situation where governance activity is required. The results returned from the verification governance service can be used to trigger other governance services as part of a governance action process . The verification services may also publish guards to report on any errors it finds. For example, it may check that a new asset has an owner, is set up with zones and includes a connection and a schema.","title":"Verification governance service"},{"location":"frameworks/gaf/#triage-governance-service","text":"Triage governance services run triage rules to determine how to manage a situation. This could be to initiate an external workflow, wait for manual decision or initiate a remediation request through either an external workflow or by creating a ToDo for a specific person.","title":"Triage governance service"},{"location":"frameworks/gaf/#remediation-governance-service","text":"The remediation governance services perform updates to metadata. Examples of remediation services are duplicate linking and consolidation.","title":"Remediation governance service"},{"location":"frameworks/gaf/#provisioning-governance-service","text":"A provisioning governance service invokes a provisioning service whenever a provisioning request is made. Typically, the provisioning service is an external service. It may also create lineage metadata to describe the work of the provisioning engine.","title":"Provisioning governance service"},{"location":"frameworks/gaf/#implementing-governance-action-services","text":"Governance action services are open connectors that support the interfaces defined by the GAF . They may produce audit log records and exceptions, and they may make changes to metadata through the Open Metadata Access Services ( OMAS ) . A governance action service is passed a context as it is started. This provides access to the request type and associated parameters (name-value pairs) used to invoke the governance action service, along with a client to access open metadata through the Governance Engine OMAS . This context is then specialized for each type of governance action service. Details of the specific context for each service can be found in the links above to the various governance action service types.","title":"Implementing governance action services"},{"location":"frameworks/gaf/#configuring-governance-action-services","text":"A collection of related governance action services are grouped into governance action engines for deployment. The governance action engine maps governance action request types to the governance action service that should be invoked along with. These definitions are created through the Governance Engine OMAS and are stored in the open metadata repositories. Governance action engines are hosted in an Open Metadata Engine Service ( OMES ) running on one or more engine hosts . The Open Metadata Types used to define the governance action engines are located in 0461 governance action engines .","title":"Configuring governance action services"},{"location":"frameworks/gaf/#running-governance-action-services","text":"Governance action engines are hosted by the Governance Action OMES . The engine services run in dedicated OMAG Server called the engine host . You can find instructions for configuring the engine services in the engine host in the administration guide. The Governance Engine OMAS provides the services for: setting up the definitions of a governance action engine. configuring governance action processes . managing governance actions and incident reports .","title":"Running governance action services"},{"location":"frameworks/gaf/#governance-pack","text":"A governance pack is a collection of pre-defined governance engines and services definitions plus governance service implementations. A team can use the governance pack to distribute the governance engine function to different metadata ecosystems.","title":"Governance pack"},{"location":"frameworks/ocf/","text":"Released This function is complete and can be used. The interfaces will be supported until the function is removed from the project via the deprecation process. There will be ongoing extensions to this function, but it will be done to ensure backward compatibility as far as possible. If there is a need to break backward compatibility, this will be discussed and reviewed in the community, with a documented timeline. Open Connector Framework ( OCF ) \u00b6 The open connector framework ( OCF ), as the name suggests, is an open framework for supporting connectors . Connector provide client-side access to remote digital assets such as data sets, APIs and software components. OCF connectors also provide access to metadata about the asset and may call the Governance Action Framework ( GAF ) to log audit messages and execute appropriate governance actions related to the use of these assets in real-time. Benefits \u00b6 Applications and tools benefit from using OCF connectors because: Network and security parameters for accessing the data resources are managed in the metadata repository as part of a named connection. The application need only supply the identifier of the connection and provided they have the appropriate security credentials then a connector is returned to them for use. There is no need to hard-code user ids and passwords in the application code - nor manage key stores for this sensitive information since the metadata repository handles this. If the location of the data changes, then the named connection configuration is changed in the metadata repository and the application will be connected to the new location the next time they request a connector. The OCF connector provides two sets of APIs. The first set provides access to the asset contents and the second set provides access to the properties of the asset stored in the open metadata repositories. This provides applications and tools with a simple mechanism to make use of metadata as they process about the asset. It is particularly useful for data science tools where these properties can help guide the end user in the use of the asset. OCF connectors are not limited to representing assets as they are physically implemented. An OCF connector can represent a simplified logical (virtual) data resource that is designed for the needs of a specific application or tool. This type of connector delegates the requests it receives to one or more physical data resources. Organizations benefit from advocating the use of OCF connectors for their systems because the OCF connectors provide a consistent approach to governance enforcement and audit logging. This is particularly important in data-rich environments where individuals are able to combine data from different assets creating new, potentially sensitive insight. The common approach to auditing, and the linkage between the data accessed and the metadata that describes its characteristics help to detect and prevent such actions. Design rationale \u00b6 The following factors influenced the design of the OCF . There are many existing connectors and connector frameworks in the industry today. It is important that these existing connectors can be incorporated into the OCF . Thus, the OCF includes placeholders for adapters to external connector providers and connectors. Application developers will only adopt a connector framework if it is easy to use. Thus, the connector interfaces allow for the use of native data APIs to minimize the effort an application developer has to take in order to use the OCF connectors. Governance enforcement is a complex topic, typically managed externally to the application development team. As a result, a separate framework called the governance action framework ( GAF ) manages governance enforcement. The role of the OCF is to bridge from the asset access requests to the GAF where necessary. Access to the all properties known about an asset should be available to the consumers of the asset.Therefore, the OCF provides a standard interface for accessing these properties. Different providers of these properties can plug into the OCF . Egeria provides an implementation of this interface to supply asset properties stored in open metadata repositories in the OCF metadata management common service . Terminology \u00b6 There are a number of key components within the OCF : Connector \u00b6 A connector is a Java client object that provides applications with access to a data source or service (known as an asset ) along with its related metadata and governance functions. An OCF connector provides four APIs: API Description Connector lifecycle Manages the lifecycle state of the connector and includes initialize() , start() and disconnect() . Metadata store initialization If the connector is created by a metadata service then it adds a client to the metadata server called ConnectedAssetProperties to the connector between initialize() and start() . The ConnectedAssetProperties client can be retrieved from the connector instance and used to retrieve metadata about the asset that is stored in the metadata server. Specific initialization for the type of connector Some types of connectors need additional initialization. These methods are called by the component creating the connector before the start() method is called. Asset content This API is crafted to provide the most natural interface to the asset's contents. Therefore, the asset content API is typically different for each type of connector. OCF connectors are not limited to representing assets as they are physically implemented. An OCF connector can represent a simplified logical (virtual) asset, such as a data set, that is designed for the needs of a specific application or tool. This type of connector delegates the requests it receives to one or more physical data resources. It is called a virtual connector . Further information See the developer guide for information on writing connectors. Connection \u00b6 The connection provides the set of properties needed to create and initialize an instance of a connector . A connection contains properties about the specific use of the connector, such as user Id and password, or parameters that control the scope or resources that should be made available to the connector. It links to an optional: Connector type that describes the type of the connector that needs to be created in order to access the asset. Endpoint that describes the server endpoint where the asset is accessed from. Connector types and endpoints can be reused in multiple connections. Connections are typically managed in a metadata repository, but they can also be manually populated. Connection implementations \u00b6 The OCF offers two implementations of the connection: Connection is a bean implementation of the connection used in REST API requests and events. It allows properties to be set up and retrieved. ConnectionProperties is a read-only wrapper for the connection properties that is used in client interfaces that do not allow the properties to be updated. Connection properties \u00b6 The properties for a connection are defined in model 0201 and include: Property Description guid GUID for the connection. url URL of the connection definition in the metadata repository. qualifiedName The official (unique) name for the connection. This is often defined by the IT systems management organization and should be used (when available) on audit logs and error messages. (Sourced from the qualifiedName attribute in Referenceable - model 0010 ) displayName A consumable name for the connection. Often a shortened form of the qualifiedName for use on user interfaces and messages. The displayName should be only be used for audit logs and error messages if the qualifiedName is not set. type Information about the TypeDef for the connection. description A full description of the connection covering details of the assets it connects to along with usage and version information. additionalProperties Any additional properties associated with the connection. configurationProperties Properties for configuring the connector. securedProperties Protected properties for secure log on by connector to back end server. These are protected properties that can only be retrieved by privileged connector code. userId Name or URI of connecting user. encryptedPassword Password for the userId - needs decrypting by connector before use. clearPassword Password for userId - ready to use. connectorType Properties that describe the connector type for the connector. endpoint Properties that describe the server endpoint where the connector will retrieve the assets. Using Connections from open metadata repositories \u00b6 Each connection stored in a metadata repository has a unique identifier. An application can request a connector instance through selected Egeria OMAS interfaces, such as the Asset Consumer OMAS , with just the unique identifier or name of a connection. The OMAS retrieves the connection object from the open metadata repositories and passes it to the connector broker factory object. The connector broker (and underlying connector provider ) uses the information from the connection object to create an instance of the connector. The advantage of retrieving the connection information from a metadata repository is that the connection properties do not need to be hard-coded in the consuming applications and the metadata associated with the linked asset can be retrieved via the connector's connected asset properties interface. Connections can be created in the open metadata repositories through the following interfaces: Asset Owner OMAS Asset Manager OMAS Data Manager OMAS Database Integrator OMIS Files Integrator OMIS Governance Action OMES Configuring connections \u00b6 The administration guide describes how to configure Egeria's OMAG Server Platforms and servers. Both the platform and the servers use connectors for access to the external resources to support their basic operation and to coordinate metadata and governance with third party technologies. This means that the configuration includes connection definitions for these connectors. All of these interfaces have Java clients that enable you to set up the connection using the OCF connection bean. However, if you want to use the REST API directly, then you need to specify the connection in JSON. Example connection definition in JSON Egeria's JSON structures map one-to-ene with the properties in the equivalent Java beans and also include a class property that gives the name of the class that it maps to. So a simple connection object would look something like this in JSON: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"...fully qualified class name...\" }, \"endpoint\" : { \"class\" : \"Endpoint\" , \"address\" : \"... network address of resource ...\" } } } Connector broker \u00b6 The connector broker is a generic factory class for all open connectors. Given a valid connection object, the connector broker is able to create a new instance of a connector . This means the caller does not need to know the implementation details of the connector - just its interface. It is implemented in the ConnectorBroker class, and is used as follows: Example usage of a connector broker 1 2 3 4 5 6 7 import org.odpi.openmetadata.frameworks.connectors.Connector ; import org.odpi.openmetadata.frameworks.connectors.ConnectorBroker ; // ... ConnectorBroker connectorBroker = new ConnectorBroker (); Connector connector = connectorBroker . getConnector ( connection ); When the connector instance is requested, the connector broker uses the connector type properties from the supplied connection to identify the appropriate connector provider . The connector broker delegates the connector instance request to the connector provider and returns the result to its caller. The connector broker is used in the client code of the Open Metadata Access Services ( OMAS ) that provide connector instances to their consumers, for example: Asset Consumer OMAS Asset Owner OMAS Connector type \u00b6 The connector type is a set of properties that defines the supported capabilities and the identity of the connector provider for a connector . Its properties are: Property Description guid GUID for the connector type. url External link address for the connector type properties in the metadata repository. qualifiedName The official (unique) name for the connector type. This is often defined by the IT systems management organization and should be used (when available) on audit logs and error messages. displayName A consumable name for the connector type. Often a shortened form of the qualifiedName for use on user interfaces and messages. The displayName should be only be used for audit logs and error messages if the qualifiedName is not set. description A full description of the connector type covering details of the assets it connects to along with usage and versioning information. connectorProviderClassName The connector provider is the factory for a particular type of connector . This property defines the class name for the connector provider that the connector broker should use to request new connector instances. recognizedAdditionalProperties These are the connection 's additional properties recognized by the connector implementation. recognizedConfigurationProperties These are the connection 's configuration properties recognized by the connector implementation. recognizedSecuredProperties These are the connection 's secured properties recognized by the connector implementation. additionalProperties Any additional properties that the connector provider needs to know in order to create connector instances. The connector type is linked to the connection objects that request this type of connector. Further information The open metadata type for a connector type is defined in model 0201 . The open connector archives module provides an open metadata archive that contains connector types for connectors supported by Egeria. Connector provider \u00b6 A connector provider is the factory for a particular type of connector . It is typically called from the connector broker , although it may be called directly. Each connector provider implements the ConnectorProvider interface. It has two types of methods: Return the connector type object that is added to a connection object used to hold the properties needed to create an instance of the connector . Return a new instance of the connector based on the properties in a connection object. The connection object that has all the properties needed to create and configure the instance of the connector. The ConnectorProviderBase class provides much of the implementation for a connector provider. Example implementation of the connector provider for a simple connector If you have a simple connector implementation then your connector provider follows the following template. It assumes the connector is for the XXXStore and is called XXXStoreConnector . With this base implementation, a specific connector provider implementation need only implement a constructor to configure the base class's function with details of itself and the Java class of the connector it needs. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 /** * XXXStoreProvider is the OCF connector provider for the XXX store connector. */ public class XXXStoreProvider extends ConnectorProviderBase { static final String connectorTypeGUID = \"Add unique GUID here\" ; static final String connectorTypeName = \"XXX Store Connector\" ; static final String connectorTypeDescription = \"Connector supports ... add details here\" ; /** * Constructor used to initialize the ConnectorProviderBase with the Java class name of the specific * store implementation. */ public BasicFileStoreProvider () { Class <?> connectorClass = XXXStoreConnector . class ; super . setConnectorClassName ( connectorClass . getName ()); ConnectorType connectorType = new ConnectorType (); connectorType . setType ( ConnectorType . getConnectorTypeType ()); connectorType . setGUID ( connectorTypeGUID ); connectorType . setQualifiedName ( connectorTypeName ); connectorType . setDisplayName ( connectorTypeName ); connectorType . setDescription ( connectorTypeDescription ); connectorType . setConnectorProviderClassName ( this . getClass (). getName ()); super . connectorTypeBean = connectorType ; } } Actual implementation of the connector provider for the basic file connector 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 /* SPDX-License-Identifier: Apache-2.0 */ /* Copyright Contributors to the Egeria project. */ package org.odpi.openmetadata.adapters.connectors.datastore.basicfile ; import org.odpi.openmetadata.frameworks.connectors.ConnectorProviderBase ; import org.odpi.openmetadata.frameworks.connectors.properties.beans.ConnectorType ; /** * BasicFileStoreProvider is the OCF connector provider for the basic file store connector. */ public class BasicFileStoreProvider extends ConnectorProviderBase { static final String connectorTypeGUID = \"ba213761-f5f5-4cf5-a95f-6150aef09e0b\" ; static final String connectorTypeName = \"Basic File Store Connector\" ; static final String connectorTypeDescription = \"Connector supports reading of Files.\" ; /** * Constructor used to initialize the ConnectorProviderBase with the Java class name of the specific * store implementation. */ public BasicFileStoreProvider () { Class <?> connectorClass = BasicFileStoreConnector . class ; super . setConnectorClassName ( connectorClass . getName ()); ConnectorType connectorType = new ConnectorType (); connectorType . setType ( ConnectorType . getConnectorTypeType ()); connectorType . setGUID ( connectorTypeGUID ); connectorType . setQualifiedName ( connectorTypeName ); connectorType . setDisplayName ( connectorTypeName ); connectorType . setDescription ( connectorTypeDescription ); connectorType . setConnectorProviderClassName ( this . getClass (). getName ()); super . connectorTypeBean = connectorType ; } } Connected asset properties \u00b6 Connected asset properties are the properties known about an asset accessed through a connector, hosted by a metadata server . These properties are presented at three levels: Asset summary \u00b6 AssetSummary holds asset properties that are used for displaying details of an asset in summary lists or hover text: Property Description type metadata type information for the asset guid GUID for the asset url external link for the asset qualifiedName The official (unique) name for the asset. This is often defined by the IT systems management organization and should be used (when available) on audit logs and error messages. (Sourced from the qualifiedName attribute in Referenceable - model 0010 ) displayName A consumable name for the asset. Often a shortened form of the asset's qualifiedName for use on user interfaces and messages. The asset's displayName should be only be used for audit logs and error messages if the qualifiedName is not set. (Sourced from displayName attribute within Asset - model 0010 )) shortDescription Short description about the asset. (Sourced from assetSummary within ConnectionsToAsset - model 0205 ) description Full description of the asset. (Sourced from description attribute within Asset - model 0010 ) owner Name of the person or organization that owns the asset. (Sourced from the AssetOwnership classification - model 0445 ) zoneMembership List of governance zones assigned to the asset. (Sourced from the AssetZoneMembership classification - model 0445 ) classifications Full list of the classifications assigned to the asset along with their properties. Asset detail \u00b6 AssetDetail extends AssetSummary to provide all the properties directly related to this asset: Property Description ExternalIdentifiers List of identifiers for this asset that are used in other systems. RelatedMediaReferences List of links to external media (images, audio, video) about this asset. NoteLogs List of NoteLogs for this asset, often providing more detail on how to use the asset and its current status. ExternalReferences List of links to additional information about this asset. Connections List of connections defined to access this asset. Licenses List of licenses associated with the asset. Certifications List of certifications that have been awarded to this asset. Asset universe \u00b6 AssetUniverse extends AssetDetail , and adds information about the common open metadata entities related to this asset: Property Description meanings Glossary term(s) assigned to this asset. schema Details of the schema type associated with the asset. feedback Details of the likes, reviews and comments, that are connected to the asset. knownLocations Details of the known locations of the asset. lineage Details of the lineage for the asset. relatedAssets Details of the assets linked to this asset. Implementation details \u00b6 The connector broker does not have access to a metadata repository because the OCF is metadata repository neutral. When it creates a connector, the connected asset properties are null. Egeria Open Metadata Access Services ( OMAS ) such as Asset Consumer OMAS , Asset Owner OMAS and Discovery Engine OMAS , include the connector broker in their clients and support APIs for managing connections and creating connectors. Connectors created by the Egeria access services will include the connected asset properties object configured to retrieve metadata from the same open metadata repository where the OMAS is running. The connected asset properties are retrieved from the open metadata repositories by OCF Metadata Management common services . It will use the same user id that was used to create the connector. Endpoint \u00b6 The endpoint is a set of properties that defines the network address and how to connect to it for a resource deployed in the digital landscape. Its properties are: Property Description guid GUID for the endpoint. url External link address for the endpoint properties in the metadata repository. This URL can be stored as a property in another entity to create an explicit link to this endpoint. qualifiedName The official (unique) name for the endpoint. This is often defined by the IT systems management organization and should be used (when available) on audit logs and error messages. displayName A consumable name for the endpoint. Often a shortened form of the qualifiedName for use on user interfaces and messages. The displayName should be only be used for audit logs and error messages if the qualifiedName is not set. description A description for the endpoint. address The location of the asset. For network connected resources, this is typically the URL and port number (if needed) for the server where the asset is located (or at least accessible by the connector). For file-based resources, this is typically the name of the file. protocol The communication protocol that the connection should use to connect to the server. encryptionMethod Describes the encryption method to use (if any). This is an open value allowing information needed by the connector user to retrieve all of the information they need to work with the endpoint. additionalProperties Any additional properties that the connector need to know in order to access the asset . Types \u00b6 Open metadata repositories are able to store information needed to use OCF connectors. Details of the types involved are as follows: Model 0040 defines the structure of an Endpoint . Model 0201 defines the structures for Connection s and ConnectorType s. Model 0205 defines the linkage between the connection and the connected asset. Further information The OCF Metadata Management common services supports the retrieval of connection and connected asset properties from the open metadata repository/repositories. The Asset Consumer OMAS embeds the OCF to provide client-side support for connectors. The Open Metadata Repository Services ( OMRS ) make extensive use of OCF connectors for accessing open metadata repository servers and other resources. These connectors are collectively called the OMRS connectors . Many of the Open Metadata Governance Servers make use of OCF connectors to loosely-couple integration with a variety of underlying technologies. The developer guide provides more information on writing connectors for Egeria. The connector catalog lists the pre-built connectors supplied by Egeria.","title":"Open Connectors (OCF)"},{"location":"frameworks/ocf/#open-connector-framework-ocf","text":"The open connector framework ( OCF ), as the name suggests, is an open framework for supporting connectors . Connector provide client-side access to remote digital assets such as data sets, APIs and software components. OCF connectors also provide access to metadata about the asset and may call the Governance Action Framework ( GAF ) to log audit messages and execute appropriate governance actions related to the use of these assets in real-time.","title":"Open Connector Framework (OCF)"},{"location":"frameworks/ocf/#benefits","text":"Applications and tools benefit from using OCF connectors because: Network and security parameters for accessing the data resources are managed in the metadata repository as part of a named connection. The application need only supply the identifier of the connection and provided they have the appropriate security credentials then a connector is returned to them for use. There is no need to hard-code user ids and passwords in the application code - nor manage key stores for this sensitive information since the metadata repository handles this. If the location of the data changes, then the named connection configuration is changed in the metadata repository and the application will be connected to the new location the next time they request a connector. The OCF connector provides two sets of APIs. The first set provides access to the asset contents and the second set provides access to the properties of the asset stored in the open metadata repositories. This provides applications and tools with a simple mechanism to make use of metadata as they process about the asset. It is particularly useful for data science tools where these properties can help guide the end user in the use of the asset. OCF connectors are not limited to representing assets as they are physically implemented. An OCF connector can represent a simplified logical (virtual) data resource that is designed for the needs of a specific application or tool. This type of connector delegates the requests it receives to one or more physical data resources. Organizations benefit from advocating the use of OCF connectors for their systems because the OCF connectors provide a consistent approach to governance enforcement and audit logging. This is particularly important in data-rich environments where individuals are able to combine data from different assets creating new, potentially sensitive insight. The common approach to auditing, and the linkage between the data accessed and the metadata that describes its characteristics help to detect and prevent such actions.","title":"Benefits"},{"location":"frameworks/ocf/#design-rationale","text":"The following factors influenced the design of the OCF . There are many existing connectors and connector frameworks in the industry today. It is important that these existing connectors can be incorporated into the OCF . Thus, the OCF includes placeholders for adapters to external connector providers and connectors. Application developers will only adopt a connector framework if it is easy to use. Thus, the connector interfaces allow for the use of native data APIs to minimize the effort an application developer has to take in order to use the OCF connectors. Governance enforcement is a complex topic, typically managed externally to the application development team. As a result, a separate framework called the governance action framework ( GAF ) manages governance enforcement. The role of the OCF is to bridge from the asset access requests to the GAF where necessary. Access to the all properties known about an asset should be available to the consumers of the asset.Therefore, the OCF provides a standard interface for accessing these properties. Different providers of these properties can plug into the OCF . Egeria provides an implementation of this interface to supply asset properties stored in open metadata repositories in the OCF metadata management common service .","title":"Design rationale"},{"location":"frameworks/ocf/#terminology","text":"There are a number of key components within the OCF :","title":"Terminology"},{"location":"frameworks/ocf/#connector","text":"A connector is a Java client object that provides applications with access to a data source or service (known as an asset ) along with its related metadata and governance functions. An OCF connector provides four APIs: API Description Connector lifecycle Manages the lifecycle state of the connector and includes initialize() , start() and disconnect() . Metadata store initialization If the connector is created by a metadata service then it adds a client to the metadata server called ConnectedAssetProperties to the connector between initialize() and start() . The ConnectedAssetProperties client can be retrieved from the connector instance and used to retrieve metadata about the asset that is stored in the metadata server. Specific initialization for the type of connector Some types of connectors need additional initialization. These methods are called by the component creating the connector before the start() method is called. Asset content This API is crafted to provide the most natural interface to the asset's contents. Therefore, the asset content API is typically different for each type of connector. OCF connectors are not limited to representing assets as they are physically implemented. An OCF connector can represent a simplified logical (virtual) asset, such as a data set, that is designed for the needs of a specific application or tool. This type of connector delegates the requests it receives to one or more physical data resources. It is called a virtual connector . Further information See the developer guide for information on writing connectors.","title":"Connector"},{"location":"frameworks/ocf/#connection","text":"The connection provides the set of properties needed to create and initialize an instance of a connector . A connection contains properties about the specific use of the connector, such as user Id and password, or parameters that control the scope or resources that should be made available to the connector. It links to an optional: Connector type that describes the type of the connector that needs to be created in order to access the asset. Endpoint that describes the server endpoint where the asset is accessed from. Connector types and endpoints can be reused in multiple connections. Connections are typically managed in a metadata repository, but they can also be manually populated.","title":"Connection"},{"location":"frameworks/ocf/#connection-implementations","text":"The OCF offers two implementations of the connection: Connection is a bean implementation of the connection used in REST API requests and events. It allows properties to be set up and retrieved. ConnectionProperties is a read-only wrapper for the connection properties that is used in client interfaces that do not allow the properties to be updated.","title":"Connection implementations"},{"location":"frameworks/ocf/#connection-properties","text":"The properties for a connection are defined in model 0201 and include: Property Description guid GUID for the connection. url URL of the connection definition in the metadata repository. qualifiedName The official (unique) name for the connection. This is often defined by the IT systems management organization and should be used (when available) on audit logs and error messages. (Sourced from the qualifiedName attribute in Referenceable - model 0010 ) displayName A consumable name for the connection. Often a shortened form of the qualifiedName for use on user interfaces and messages. The displayName should be only be used for audit logs and error messages if the qualifiedName is not set. type Information about the TypeDef for the connection. description A full description of the connection covering details of the assets it connects to along with usage and version information. additionalProperties Any additional properties associated with the connection. configurationProperties Properties for configuring the connector. securedProperties Protected properties for secure log on by connector to back end server. These are protected properties that can only be retrieved by privileged connector code. userId Name or URI of connecting user. encryptedPassword Password for the userId - needs decrypting by connector before use. clearPassword Password for userId - ready to use. connectorType Properties that describe the connector type for the connector. endpoint Properties that describe the server endpoint where the connector will retrieve the assets.","title":"Connection properties"},{"location":"frameworks/ocf/#using-connections-from-open-metadata-repositories","text":"Each connection stored in a metadata repository has a unique identifier. An application can request a connector instance through selected Egeria OMAS interfaces, such as the Asset Consumer OMAS , with just the unique identifier or name of a connection. The OMAS retrieves the connection object from the open metadata repositories and passes it to the connector broker factory object. The connector broker (and underlying connector provider ) uses the information from the connection object to create an instance of the connector. The advantage of retrieving the connection information from a metadata repository is that the connection properties do not need to be hard-coded in the consuming applications and the metadata associated with the linked asset can be retrieved via the connector's connected asset properties interface. Connections can be created in the open metadata repositories through the following interfaces: Asset Owner OMAS Asset Manager OMAS Data Manager OMAS Database Integrator OMIS Files Integrator OMIS Governance Action OMES","title":"Using Connections from open metadata repositories"},{"location":"frameworks/ocf/#configuring-connections","text":"The administration guide describes how to configure Egeria's OMAG Server Platforms and servers. Both the platform and the servers use connectors for access to the external resources to support their basic operation and to coordinate metadata and governance with third party technologies. This means that the configuration includes connection definitions for these connectors. All of these interfaces have Java clients that enable you to set up the connection using the OCF connection bean. However, if you want to use the REST API directly, then you need to specify the connection in JSON. Example connection definition in JSON Egeria's JSON structures map one-to-ene with the properties in the equivalent Java beans and also include a class property that gives the name of the class that it maps to. So a simple connection object would look something like this in JSON: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"...fully qualified class name...\" }, \"endpoint\" : { \"class\" : \"Endpoint\" , \"address\" : \"... network address of resource ...\" } } }","title":"Configuring connections"},{"location":"frameworks/ocf/#connector-broker","text":"The connector broker is a generic factory class for all open connectors. Given a valid connection object, the connector broker is able to create a new instance of a connector . This means the caller does not need to know the implementation details of the connector - just its interface. It is implemented in the ConnectorBroker class, and is used as follows: Example usage of a connector broker 1 2 3 4 5 6 7 import org.odpi.openmetadata.frameworks.connectors.Connector ; import org.odpi.openmetadata.frameworks.connectors.ConnectorBroker ; // ... ConnectorBroker connectorBroker = new ConnectorBroker (); Connector connector = connectorBroker . getConnector ( connection ); When the connector instance is requested, the connector broker uses the connector type properties from the supplied connection to identify the appropriate connector provider . The connector broker delegates the connector instance request to the connector provider and returns the result to its caller. The connector broker is used in the client code of the Open Metadata Access Services ( OMAS ) that provide connector instances to their consumers, for example: Asset Consumer OMAS Asset Owner OMAS","title":"Connector broker"},{"location":"frameworks/ocf/#connector-type","text":"The connector type is a set of properties that defines the supported capabilities and the identity of the connector provider for a connector . Its properties are: Property Description guid GUID for the connector type. url External link address for the connector type properties in the metadata repository. qualifiedName The official (unique) name for the connector type. This is often defined by the IT systems management organization and should be used (when available) on audit logs and error messages. displayName A consumable name for the connector type. Often a shortened form of the qualifiedName for use on user interfaces and messages. The displayName should be only be used for audit logs and error messages if the qualifiedName is not set. description A full description of the connector type covering details of the assets it connects to along with usage and versioning information. connectorProviderClassName The connector provider is the factory for a particular type of connector . This property defines the class name for the connector provider that the connector broker should use to request new connector instances. recognizedAdditionalProperties These are the connection 's additional properties recognized by the connector implementation. recognizedConfigurationProperties These are the connection 's configuration properties recognized by the connector implementation. recognizedSecuredProperties These are the connection 's secured properties recognized by the connector implementation. additionalProperties Any additional properties that the connector provider needs to know in order to create connector instances. The connector type is linked to the connection objects that request this type of connector. Further information The open metadata type for a connector type is defined in model 0201 . The open connector archives module provides an open metadata archive that contains connector types for connectors supported by Egeria.","title":"Connector type"},{"location":"frameworks/ocf/#connector-provider","text":"A connector provider is the factory for a particular type of connector . It is typically called from the connector broker , although it may be called directly. Each connector provider implements the ConnectorProvider interface. It has two types of methods: Return the connector type object that is added to a connection object used to hold the properties needed to create an instance of the connector . Return a new instance of the connector based on the properties in a connection object. The connection object that has all the properties needed to create and configure the instance of the connector. The ConnectorProviderBase class provides much of the implementation for a connector provider. Example implementation of the connector provider for a simple connector If you have a simple connector implementation then your connector provider follows the following template. It assumes the connector is for the XXXStore and is called XXXStoreConnector . With this base implementation, a specific connector provider implementation need only implement a constructor to configure the base class's function with details of itself and the Java class of the connector it needs. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 /** * XXXStoreProvider is the OCF connector provider for the XXX store connector. */ public class XXXStoreProvider extends ConnectorProviderBase { static final String connectorTypeGUID = \"Add unique GUID here\" ; static final String connectorTypeName = \"XXX Store Connector\" ; static final String connectorTypeDescription = \"Connector supports ... add details here\" ; /** * Constructor used to initialize the ConnectorProviderBase with the Java class name of the specific * store implementation. */ public BasicFileStoreProvider () { Class <?> connectorClass = XXXStoreConnector . class ; super . setConnectorClassName ( connectorClass . getName ()); ConnectorType connectorType = new ConnectorType (); connectorType . setType ( ConnectorType . getConnectorTypeType ()); connectorType . setGUID ( connectorTypeGUID ); connectorType . setQualifiedName ( connectorTypeName ); connectorType . setDisplayName ( connectorTypeName ); connectorType . setDescription ( connectorTypeDescription ); connectorType . setConnectorProviderClassName ( this . getClass (). getName ()); super . connectorTypeBean = connectorType ; } } Actual implementation of the connector provider for the basic file connector 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 /* SPDX-License-Identifier: Apache-2.0 */ /* Copyright Contributors to the Egeria project. */ package org.odpi.openmetadata.adapters.connectors.datastore.basicfile ; import org.odpi.openmetadata.frameworks.connectors.ConnectorProviderBase ; import org.odpi.openmetadata.frameworks.connectors.properties.beans.ConnectorType ; /** * BasicFileStoreProvider is the OCF connector provider for the basic file store connector. */ public class BasicFileStoreProvider extends ConnectorProviderBase { static final String connectorTypeGUID = \"ba213761-f5f5-4cf5-a95f-6150aef09e0b\" ; static final String connectorTypeName = \"Basic File Store Connector\" ; static final String connectorTypeDescription = \"Connector supports reading of Files.\" ; /** * Constructor used to initialize the ConnectorProviderBase with the Java class name of the specific * store implementation. */ public BasicFileStoreProvider () { Class <?> connectorClass = BasicFileStoreConnector . class ; super . setConnectorClassName ( connectorClass . getName ()); ConnectorType connectorType = new ConnectorType (); connectorType . setType ( ConnectorType . getConnectorTypeType ()); connectorType . setGUID ( connectorTypeGUID ); connectorType . setQualifiedName ( connectorTypeName ); connectorType . setDisplayName ( connectorTypeName ); connectorType . setDescription ( connectorTypeDescription ); connectorType . setConnectorProviderClassName ( this . getClass (). getName ()); super . connectorTypeBean = connectorType ; } }","title":"Connector provider"},{"location":"frameworks/ocf/#connected-asset-properties","text":"Connected asset properties are the properties known about an asset accessed through a connector, hosted by a metadata server . These properties are presented at three levels:","title":"Connected asset properties"},{"location":"frameworks/ocf/#asset-summary","text":"AssetSummary holds asset properties that are used for displaying details of an asset in summary lists or hover text: Property Description type metadata type information for the asset guid GUID for the asset url external link for the asset qualifiedName The official (unique) name for the asset. This is often defined by the IT systems management organization and should be used (when available) on audit logs and error messages. (Sourced from the qualifiedName attribute in Referenceable - model 0010 ) displayName A consumable name for the asset. Often a shortened form of the asset's qualifiedName for use on user interfaces and messages. The asset's displayName should be only be used for audit logs and error messages if the qualifiedName is not set. (Sourced from displayName attribute within Asset - model 0010 )) shortDescription Short description about the asset. (Sourced from assetSummary within ConnectionsToAsset - model 0205 ) description Full description of the asset. (Sourced from description attribute within Asset - model 0010 ) owner Name of the person or organization that owns the asset. (Sourced from the AssetOwnership classification - model 0445 ) zoneMembership List of governance zones assigned to the asset. (Sourced from the AssetZoneMembership classification - model 0445 ) classifications Full list of the classifications assigned to the asset along with their properties.","title":"Asset summary"},{"location":"frameworks/ocf/#asset-detail","text":"AssetDetail extends AssetSummary to provide all the properties directly related to this asset: Property Description ExternalIdentifiers List of identifiers for this asset that are used in other systems. RelatedMediaReferences List of links to external media (images, audio, video) about this asset. NoteLogs List of NoteLogs for this asset, often providing more detail on how to use the asset and its current status. ExternalReferences List of links to additional information about this asset. Connections List of connections defined to access this asset. Licenses List of licenses associated with the asset. Certifications List of certifications that have been awarded to this asset.","title":"Asset detail"},{"location":"frameworks/ocf/#asset-universe","text":"AssetUniverse extends AssetDetail , and adds information about the common open metadata entities related to this asset: Property Description meanings Glossary term(s) assigned to this asset. schema Details of the schema type associated with the asset. feedback Details of the likes, reviews and comments, that are connected to the asset. knownLocations Details of the known locations of the asset. lineage Details of the lineage for the asset. relatedAssets Details of the assets linked to this asset.","title":"Asset universe"},{"location":"frameworks/ocf/#implementation-details","text":"The connector broker does not have access to a metadata repository because the OCF is metadata repository neutral. When it creates a connector, the connected asset properties are null. Egeria Open Metadata Access Services ( OMAS ) such as Asset Consumer OMAS , Asset Owner OMAS and Discovery Engine OMAS , include the connector broker in their clients and support APIs for managing connections and creating connectors. Connectors created by the Egeria access services will include the connected asset properties object configured to retrieve metadata from the same open metadata repository where the OMAS is running. The connected asset properties are retrieved from the open metadata repositories by OCF Metadata Management common services . It will use the same user id that was used to create the connector.","title":"Implementation details"},{"location":"frameworks/ocf/#endpoint","text":"The endpoint is a set of properties that defines the network address and how to connect to it for a resource deployed in the digital landscape. Its properties are: Property Description guid GUID for the endpoint. url External link address for the endpoint properties in the metadata repository. This URL can be stored as a property in another entity to create an explicit link to this endpoint. qualifiedName The official (unique) name for the endpoint. This is often defined by the IT systems management organization and should be used (when available) on audit logs and error messages. displayName A consumable name for the endpoint. Often a shortened form of the qualifiedName for use on user interfaces and messages. The displayName should be only be used for audit logs and error messages if the qualifiedName is not set. description A description for the endpoint. address The location of the asset. For network connected resources, this is typically the URL and port number (if needed) for the server where the asset is located (or at least accessible by the connector). For file-based resources, this is typically the name of the file. protocol The communication protocol that the connection should use to connect to the server. encryptionMethod Describes the encryption method to use (if any). This is an open value allowing information needed by the connector user to retrieve all of the information they need to work with the endpoint. additionalProperties Any additional properties that the connector need to know in order to access the asset .","title":"Endpoint"},{"location":"frameworks/ocf/#types","text":"Open metadata repositories are able to store information needed to use OCF connectors. Details of the types involved are as follows: Model 0040 defines the structure of an Endpoint . Model 0201 defines the structures for Connection s and ConnectorType s. Model 0205 defines the linkage between the connection and the connected asset. Further information The OCF Metadata Management common services supports the retrieval of connection and connected asset properties from the open metadata repository/repositories. The Asset Consumer OMAS embeds the OCF to provide client-side support for connectors. The Open Metadata Repository Services ( OMRS ) make extensive use of OCF connectors for accessing open metadata repository servers and other resources. These connectors are collectively called the OMRS connectors . Many of the Open Metadata Governance Servers make use of OCF connectors to loosely-couple integration with a variety of underlying technologies. The developer guide provides more information on writing connectors for Egeria. The connector catalog lists the pre-built connectors supplied by Egeria.","title":"Types"},{"location":"frameworks/odf/","text":"Technical preview Technical preview function is in a state that it can be tried. The development is complete, there is documentation and there are samples, tutorials and hands-on labs as appropriate. The community is looking for feedback on the function before releasing it. This feedback may result in changes to the external interfaces. Open Discovery Framework ( ODF ) \u00b6 The open discovery framework ( ODF ) enables metadata discovery tools to integrate with open metadata repositories by defining the interfaces for metadata discovery components (called discovery services ) to: Access metadata discovery configuration. Search for assets in the metadata repository. Extract all the metadata known about a specific asset. Record the results of the analysis in the open metadata repository and attach it to the asset's metadata for later processing. Discovery service \u00b6 A discovery service provides specific analysis of the metadata and contents of an asset on request. It is implemented as a specialized connector . A discovery service is initialized with a connector to the asset it is to analyze and details of the results of other discovery services that have run before it if it is part of a discovery pipeline . The result is one or more sets of related properties that the discovery service has discovered about the asset, its metadata, structure and/or content. These are stored in a set of discovery annotations linked off of a discovery analysis report . The discovery analysis report is linked off of the asset definition in the open metadata repository. Discovery services run in a discovery engine that is hosted in a discovery server . Discovery context \u00b6 A discovery context provides the discovery service with access to information about the discovery request along with the open metadata repository interfaces. The discovery context provides parameters used by a discovery service to locate and analyze an asset and then record the results. Discovery request type \u00b6 The discovery request type , as the name suggests, is the name of the type of discovery that a discovery engine should run. It is a string value and is defined in the discovery configuration server . Each discovery request type is associated with a discovery service. When a discovery request is made the discovery engine, it looks up the discovery request type and runs the associated discovery service. Implementation in Egeria \u00b6 Egeria's discovery configuration server support is implemented by the Discovery Engine OMAS . It has a client called DiscoveryConfigurationClient that implements the ODF 's DiscoveryConfigurationServer interface. It also supports event notifications through the Discovery Engine OMAS 's out topic . Discovery pipeline \u00b6 A discovery pipeline is a specialized implementation of a discovery service that runs a set of discovery services against a single asset. The implementation of the discovery pipeline determines the order that these discovery services are run. The aim of the discovery pipeline is to enable a detailed picture of the properties of an asset to be built up by the discovery services it calls. Each discovery service is able to access the results of the discovery services that have run before it. Discovery annotation \u00b6 A discovery annotation describes one or more related properties about an asset that has been discovered by a discovery service . Some discovery annotations refer to an entire asset and others refer to a data field within an asset. The annotations that describe a single data field are called data field annotations . Annotation type Description Classification annotation Captures a recommendation of which classifications to attach to this asset. It can be made at the asset or data field level. Data class annotation Captures a recommendation of which data class this data field closely represents. Data profile annotation Capture the characteristics of the data values stored in a specific data field in a data source. Data profile log annotation Capture the names of the log files where profile characteristics of the data values stored in a specific data field. This is used when the profile results are too large to store in open metadata. Data source measurement annotation Collect arbitrary properties about a data source. Data source physical status annotation Documents the physical characteristics of a data source asset. Relationship advice annotation Document a recommended relationship that should be established with the asset. Quality annotation Document calculated quality scores on different dimensions. Schema analysis annotation Document the structure of the data (schema) inside the asset. Semantic annotation Documents suggested meanings for this data based on the values and name of the field. Suspect duplicate annotation Identifies other asset definitions that seem to point to the same physical asset. The open metadata types for a discovery annotations are described in area 6 of the model. The main entity type is called Annotation . It is extended by DataFieldAnnotation to distinguish annotations that refer, primarily to a data field. Other more specialist annotations extend these two basic annotation types. Discovery analysis report \u00b6 The discovery analysis report lists the discovery annotations that were created during the execution of a discovery service . The discovery analysis report is created in the open metadata repository by the discovery engine when it creates the discovery service instance. The discovery service can retrieve information about the discovery analysis report through the discovery analysis report store client. Discovery analysis report store \u00b6 The discovery analysis report store is a client to an open metadata server that enables a discovery service to query the properties of its discovery analysis report and update the analysis step that is currently executing. The discovery analysis report store is accessed from the discovery annotation store . The discovery analysis report store also enables a long-running discovery service (typically a discovery pipeline ) to record its current analysis step. Discovery annotation store \u00b6 The discovery annotation store provides a discovery-service with a client to write discovery annotations to an open metadata repository. These annotations describe the results of the analysis performed on an asset by the discovery service. The annotations are linked to a discovery analysis report that is in turn linked off of the analysed asset. The discovery service is passed the discovery annotation store via the discovery context . Discovery engine \u00b6 A discovery engine is the execution environment for discovery services . The discovery engine configuration defines a set of discovery services. Its definition is stored in an open metadata repository and maintained through the Discovery Engine OMAS . Discovery engines are hosted in discovery servers . Egeria's implementation of the discovery engine is provided by the Asset Analysis OMES . Discovery server \u00b6 The discovery server is the server environment that hosts one or more discovery engines . Discovery servers are deployed close to the physical assets they are analysing. They connect to the Discovery Engine OMAS running in a metadata server to provide metadata about assets and to store the results of the discovery service's analysis. Many discovery servers can use the same metadata server. In Egeria, the discovery server is implemented by the Asset Analysis OMES running in the engine host OMAG Server. Discovery configuration server \u00b6 The discovery configuration server is the server responsible for holding and managing the configuration needed by the discovery servers and the discovery engines within them. This configuration consists of defining which discovery request types are supported and which discovery services they map to. Discovery asset catalog store \u00b6 The discovery asset catalog store provides a search interface that enables a discovery service to locate assets that are described in the open metadata repository. The discovery service is passed the discovery asset catalog store via the discovery context . Framework implementation \u00b6 Egeria provides a full implementation of the ODF . It provides a discovery server as well as an implementation of the metadata server APIs by the Discovery Engine OMAS . There are also implementations of discovery services in the discovery-service-connectors module.","title":"Open Discovery (ODF)"},{"location":"frameworks/odf/#open-discovery-framework-odf","text":"The open discovery framework ( ODF ) enables metadata discovery tools to integrate with open metadata repositories by defining the interfaces for metadata discovery components (called discovery services ) to: Access metadata discovery configuration. Search for assets in the metadata repository. Extract all the metadata known about a specific asset. Record the results of the analysis in the open metadata repository and attach it to the asset's metadata for later processing.","title":"Open Discovery Framework (ODF)"},{"location":"frameworks/odf/#discovery-service","text":"A discovery service provides specific analysis of the metadata and contents of an asset on request. It is implemented as a specialized connector . A discovery service is initialized with a connector to the asset it is to analyze and details of the results of other discovery services that have run before it if it is part of a discovery pipeline . The result is one or more sets of related properties that the discovery service has discovered about the asset, its metadata, structure and/or content. These are stored in a set of discovery annotations linked off of a discovery analysis report . The discovery analysis report is linked off of the asset definition in the open metadata repository. Discovery services run in a discovery engine that is hosted in a discovery server .","title":"Discovery service"},{"location":"frameworks/odf/#discovery-context","text":"A discovery context provides the discovery service with access to information about the discovery request along with the open metadata repository interfaces. The discovery context provides parameters used by a discovery service to locate and analyze an asset and then record the results.","title":"Discovery context"},{"location":"frameworks/odf/#discovery-request-type","text":"The discovery request type , as the name suggests, is the name of the type of discovery that a discovery engine should run. It is a string value and is defined in the discovery configuration server . Each discovery request type is associated with a discovery service. When a discovery request is made the discovery engine, it looks up the discovery request type and runs the associated discovery service.","title":"Discovery request type"},{"location":"frameworks/odf/#implementation-in-egeria","text":"Egeria's discovery configuration server support is implemented by the Discovery Engine OMAS . It has a client called DiscoveryConfigurationClient that implements the ODF 's DiscoveryConfigurationServer interface. It also supports event notifications through the Discovery Engine OMAS 's out topic .","title":"Implementation in Egeria"},{"location":"frameworks/odf/#discovery-pipeline","text":"A discovery pipeline is a specialized implementation of a discovery service that runs a set of discovery services against a single asset. The implementation of the discovery pipeline determines the order that these discovery services are run. The aim of the discovery pipeline is to enable a detailed picture of the properties of an asset to be built up by the discovery services it calls. Each discovery service is able to access the results of the discovery services that have run before it.","title":"Discovery pipeline"},{"location":"frameworks/odf/#discovery-annotation","text":"A discovery annotation describes one or more related properties about an asset that has been discovered by a discovery service . Some discovery annotations refer to an entire asset and others refer to a data field within an asset. The annotations that describe a single data field are called data field annotations . Annotation type Description Classification annotation Captures a recommendation of which classifications to attach to this asset. It can be made at the asset or data field level. Data class annotation Captures a recommendation of which data class this data field closely represents. Data profile annotation Capture the characteristics of the data values stored in a specific data field in a data source. Data profile log annotation Capture the names of the log files where profile characteristics of the data values stored in a specific data field. This is used when the profile results are too large to store in open metadata. Data source measurement annotation Collect arbitrary properties about a data source. Data source physical status annotation Documents the physical characteristics of a data source asset. Relationship advice annotation Document a recommended relationship that should be established with the asset. Quality annotation Document calculated quality scores on different dimensions. Schema analysis annotation Document the structure of the data (schema) inside the asset. Semantic annotation Documents suggested meanings for this data based on the values and name of the field. Suspect duplicate annotation Identifies other asset definitions that seem to point to the same physical asset. The open metadata types for a discovery annotations are described in area 6 of the model. The main entity type is called Annotation . It is extended by DataFieldAnnotation to distinguish annotations that refer, primarily to a data field. Other more specialist annotations extend these two basic annotation types.","title":"Discovery annotation"},{"location":"frameworks/odf/#discovery-analysis-report","text":"The discovery analysis report lists the discovery annotations that were created during the execution of a discovery service . The discovery analysis report is created in the open metadata repository by the discovery engine when it creates the discovery service instance. The discovery service can retrieve information about the discovery analysis report through the discovery analysis report store client.","title":"Discovery analysis report"},{"location":"frameworks/odf/#discovery-analysis-report-store","text":"The discovery analysis report store is a client to an open metadata server that enables a discovery service to query the properties of its discovery analysis report and update the analysis step that is currently executing. The discovery analysis report store is accessed from the discovery annotation store . The discovery analysis report store also enables a long-running discovery service (typically a discovery pipeline ) to record its current analysis step.","title":"Discovery analysis report store"},{"location":"frameworks/odf/#discovery-annotation-store","text":"The discovery annotation store provides a discovery-service with a client to write discovery annotations to an open metadata repository. These annotations describe the results of the analysis performed on an asset by the discovery service. The annotations are linked to a discovery analysis report that is in turn linked off of the analysed asset. The discovery service is passed the discovery annotation store via the discovery context .","title":"Discovery annotation store"},{"location":"frameworks/odf/#discovery-engine","text":"A discovery engine is the execution environment for discovery services . The discovery engine configuration defines a set of discovery services. Its definition is stored in an open metadata repository and maintained through the Discovery Engine OMAS . Discovery engines are hosted in discovery servers . Egeria's implementation of the discovery engine is provided by the Asset Analysis OMES .","title":"Discovery engine"},{"location":"frameworks/odf/#discovery-server","text":"The discovery server is the server environment that hosts one or more discovery engines . Discovery servers are deployed close to the physical assets they are analysing. They connect to the Discovery Engine OMAS running in a metadata server to provide metadata about assets and to store the results of the discovery service's analysis. Many discovery servers can use the same metadata server. In Egeria, the discovery server is implemented by the Asset Analysis OMES running in the engine host OMAG Server.","title":"Discovery server"},{"location":"frameworks/odf/#discovery-configuration-server","text":"The discovery configuration server is the server responsible for holding and managing the configuration needed by the discovery servers and the discovery engines within them. This configuration consists of defining which discovery request types are supported and which discovery services they map to.","title":"Discovery configuration server"},{"location":"frameworks/odf/#discovery-asset-catalog-store","text":"The discovery asset catalog store provides a search interface that enables a discovery service to locate assets that are described in the open metadata repository. The discovery service is passed the discovery asset catalog store via the discovery context .","title":"Discovery asset catalog store"},{"location":"frameworks/odf/#framework-implementation","text":"Egeria provides a full implementation of the ODF . It provides a discovery server as well as an implementation of the metadata server APIs by the Discovery Engine OMAS . There are also implementations of discovery services in the discovery-service-connectors module.","title":"Framework implementation"},{"location":"getting-started/dojo/introduction/","text":"Dojo Introduction \u00b6 The Egeria \"dojo\" is an intensive course to help you learn about Egeria. It is designed as a 3-day effort, although, since it is self-study you can dip in and out of it as time permits. The objectives of the three day are as follows: Day 1 : Learning about setting up and running Egeria on you own machine. Day 2 : Learning how to make a contribution to Egeria. Day 3 : Learning how to become either an advocate or a maintainer. The sessions are color-coded like ski runs: Beginner session Intermediate session Advanced session Expert session As you progress through the dojo, the colors of the sessions show how advanced your knowledge is becoming. The schedule also includes estimated times needed to complete each session. Do take breaks whenever needed! Overview Day 1 summary \u00b6 Day 1 After completing day 1 of the Egeria dojo you should feel comfortable with setting up and running the Egeria technology. It includes sessions on the prerequisite technology that Egeria uses, how to configure Egeria, how to start and stop various capabilities and well as diagnosing any problems you may come across. Egeria introduction (30 mins) Egeria project introduction (30 mins) Running Egeria on your machine, step-by-step (5 hrs) Platform setup and configuration (90 mins) Running metadata servers (2 hrs) Running metadata ecosystems (90 mins) Participating in the community (30 mins) Day 2 summary \u00b6 Day 2 Day 2 of the Egeria dojo is all about making changes to the Egeria project. This may be to add code, documentation or samples. You will have an opportunity to add a new file to the Egeria project and take it all the way through the process to update Egeria's git repository. Open source philosophy (30 mins) Tools for contributors (90 mins) Making a contribution, step-by-step (90 mins) Types of contribution (60 mins) Becoming a contributor (30 mins) Day 3 summary \u00b6 Day 3 Day 3 prepares you to become an Egeria professional - either as an advocate of the technology or a maintainer. It goes much deeper into the philosophy, design and processes of the project. Becoming an advocate (90 mins) Becoming a maintainer (90 mins) Egeria architecture and philosophy (the \"deep stuff\") (90 mins) Egeria social (90 mins)","title":"Dojo Introduction"},{"location":"getting-started/dojo/introduction/#dojo-introduction","text":"The Egeria \"dojo\" is an intensive course to help you learn about Egeria. It is designed as a 3-day effort, although, since it is self-study you can dip in and out of it as time permits. The objectives of the three day are as follows: Day 1 : Learning about setting up and running Egeria on you own machine. Day 2 : Learning how to make a contribution to Egeria. Day 3 : Learning how to become either an advocate or a maintainer. The sessions are color-coded like ski runs: Beginner session Intermediate session Advanced session Expert session As you progress through the dojo, the colors of the sessions show how advanced your knowledge is becoming. The schedule also includes estimated times needed to complete each session. Do take breaks whenever needed! Overview","title":"Dojo Introduction"},{"location":"getting-started/dojo/introduction/#day-1-summary","text":"Day 1 After completing day 1 of the Egeria dojo you should feel comfortable with setting up and running the Egeria technology. It includes sessions on the prerequisite technology that Egeria uses, how to configure Egeria, how to start and stop various capabilities and well as diagnosing any problems you may come across. Egeria introduction (30 mins) Egeria project introduction (30 mins) Running Egeria on your machine, step-by-step (5 hrs) Platform setup and configuration (90 mins) Running metadata servers (2 hrs) Running metadata ecosystems (90 mins) Participating in the community (30 mins)","title":"Day 1 summary"},{"location":"getting-started/dojo/introduction/#day-2-summary","text":"Day 2 Day 2 of the Egeria dojo is all about making changes to the Egeria project. This may be to add code, documentation or samples. You will have an opportunity to add a new file to the Egeria project and take it all the way through the process to update Egeria's git repository. Open source philosophy (30 mins) Tools for contributors (90 mins) Making a contribution, step-by-step (90 mins) Types of contribution (60 mins) Becoming a contributor (30 mins)","title":"Day 2 summary"},{"location":"getting-started/dojo/introduction/#day-3-summary","text":"Day 3 Day 3 prepares you to become an Egeria professional - either as an advocate of the technology or a maintainer. It goes much deeper into the philosophy, design and processes of the project. Becoming an advocate (90 mins) Becoming a maintainer (90 mins) Egeria architecture and philosophy (the \"deep stuff\") (90 mins) Egeria social (90 mins)","title":"Day 3 summary"},{"location":"getting-started/dojo/1/","text":"Dojo Day 1 \u00b6 Egeria introduction \u00b6 In this session, you will learn about the function and value of Egeria along with the key concepts and use cases it supports. Egeria introduction Test yourself \u00b6 Which of the following are part of the Open Metadata Manifesto? Metadata needs to be centralized so it can be managed. Maintenance of metadata must be automated. The availability of metadata management must become ubiquitous. Metadata access must become open and remotely accessible. Name 3 tools that could connect to Egeria. Name a metadata standard. Egeria project introduction \u00b6 In this session, you will learn about the contents of the Egeria project. It will also describe the software to download in preparation for the next session. Egeria project introduction GitHub repositories \u00b6 GitHub is a public service for managing files - particularly files associated with a software project. Many open source projects use GitHub and Egeria is no exception. All content for the Egeria project is stored in git repositories. For example, these web pages you are reading as part of the dojo are managed in Egeria's documentation git repository . Other resources \u00b6 The Egeria community love to collaborate on the work they do. Git and GitHub is an excellent way to exchange and manage files. In addition, the community runs public calls that anyone can join, as well as a number of Slack channels. Details of the different ways the community operates is described in our community guide . Test yourself \u00b6 Name three of the git repositories owned by the Egeria project, and describe what they do. Describe why the Egeria project is called Egeria Login to Slack and post a message to the #egeria-dojo-live channel A simple \"I'm working my way through the dojo!\" will do Running Egeria \u00b6 In this session, you will learn about the Open Metadata and Governance (OMAG) Server Platform that hosts many of the services provided by Egeria. Running Egeria Prerequisites \u00b6 Before we get started there are some steps to prepare your machine. Ensure Docker Desktop and Postman are installed and running For this session you will need both of these tools installed and running on your machine: Docker Desktop Postman Follow the links below to find out a little bit about these technologies and ensure the software is installed. Docker Desktop Postman Once these technologies are installed, work through the tutorials - starting with Docker to get the OMAG Server Platform running and then Postman to get ready to work with the platform and the servers running on top if it. Docker Tutorial Postman Tutorial At this point you should have Postman installed with the collections loaded, and Egeria's OMAG Server Platform running as a docker container. Test yourself \u00b6 What is the message from the OMAG Server Platform that says it is ready to process requests? How do you find out the version of Egeria running in an OMAG Server Platform? What is the url to view the Swagger UI page for the OMAG Server Platform? Configuring the OMAG Server Platform \u00b6 In this session, you will learn how to set up the OMAG Server Platform. Configuring the OMAG Server Platform Server origin \u00b6 In the previous session you downloaded an application called Postman and loaded collections of pre-defined requests. This tool makes it easy to issue REST API requests to the OMAG Server Platform. Check that it is working by locating the Get Server Origin request in the Egeria-platform-services collection. When you click on that request in the left-hand list, a new tab opens and you can click on send to issue the request. You should see the same response as when you issues the platform origin request from Swagger earlier. Below is this response in Postman. If this does not work, then there is something wrong either in Postman or your platform. Check the URL string that was used in the request (shown in orange in the middle of the screen.) The screen shot below shows the error message when the egeria environment is not set. This can be fixed by setting it in the top right-hand dropdown. If the Egeria environment is not listed then you need to load the environment ( Postman tutorial ). If the baseURL variable is set to a different value to the server platform then Postman can not connect. In the screen capture below, you can see the baseURL is set to the default of https://localhost:9443 when it should be https://localhost:9443 because the platform is running in docker. Finally, if the OMAG Server Platform is not running the even though everything is set up correctly in Postman, it has nothing to connect to. Restart the platform ( Docker tutorial ). In last part of this session you will learn how to set up the OMAG Server Platform so that it is secure and determine the services and servers that are associated with the platform. Configuration document store \u00b6 Platform security \u00b6 Registered services \u00b6 Known / active servers \u00b6 Review the description of the OMAG Server Platform configuration: Configuring the OMAG Server Platform The link below takes you to a task description in the Egeria Administration User Guide. The user guide describes the REST API call(s) needed to complete the task. You can choose to type the request into postman, or use the requests already defined in the Egeria-admin-services-platform-configuration Postman collection. Add the Coco Pharmaceuticals platform security connector to the platform Try running the platform origin command again - it should fail with a security error. Change the user variable in the Egeria environment from me to garygeeke and rerun the request. It will work again because garygeeke is the user id of the Coco Pharmaceuticals IT infrastructure lead and has permission to run the platform commands. Finally, use the Egeria-admin-services-platform-configuration Postman collection to experiment with the different registered services and and known and active server requests. These are useful to know as we move to configure servers on the platform. This is the end of the session on the OMAG Server Platform.","title":"Index"},{"location":"getting-started/dojo/1/#dojo-day-1","text":"","title":"Dojo Day 1"},{"location":"getting-started/dojo/1/#egeria-introduction","text":"In this session, you will learn about the function and value of Egeria along with the key concepts and use cases it supports. Egeria introduction","title":"Egeria introduction"},{"location":"getting-started/dojo/1/#test-yourself","text":"Which of the following are part of the Open Metadata Manifesto? Metadata needs to be centralized so it can be managed. Maintenance of metadata must be automated. The availability of metadata management must become ubiquitous. Metadata access must become open and remotely accessible. Name 3 tools that could connect to Egeria. Name a metadata standard.","title":"Test yourself"},{"location":"getting-started/dojo/1/#egeria-project-introduction","text":"In this session, you will learn about the contents of the Egeria project. It will also describe the software to download in preparation for the next session. Egeria project introduction","title":"Egeria project introduction"},{"location":"getting-started/dojo/1/#github-repositories","text":"GitHub is a public service for managing files - particularly files associated with a software project. Many open source projects use GitHub and Egeria is no exception. All content for the Egeria project is stored in git repositories. For example, these web pages you are reading as part of the dojo are managed in Egeria's documentation git repository .","title":"GitHub repositories"},{"location":"getting-started/dojo/1/#other-resources","text":"The Egeria community love to collaborate on the work they do. Git and GitHub is an excellent way to exchange and manage files. In addition, the community runs public calls that anyone can join, as well as a number of Slack channels. Details of the different ways the community operates is described in our community guide .","title":"Other resources"},{"location":"getting-started/dojo/1/#test-yourself_1","text":"Name three of the git repositories owned by the Egeria project, and describe what they do. Describe why the Egeria project is called Egeria Login to Slack and post a message to the #egeria-dojo-live channel A simple \"I'm working my way through the dojo!\" will do","title":"Test yourself"},{"location":"getting-started/dojo/1/#running-egeria","text":"In this session, you will learn about the Open Metadata and Governance (OMAG) Server Platform that hosts many of the services provided by Egeria. Running Egeria","title":"Running Egeria"},{"location":"getting-started/dojo/1/#prerequisites","text":"Before we get started there are some steps to prepare your machine. Ensure Docker Desktop and Postman are installed and running For this session you will need both of these tools installed and running on your machine: Docker Desktop Postman Follow the links below to find out a little bit about these technologies and ensure the software is installed. Docker Desktop Postman Once these technologies are installed, work through the tutorials - starting with Docker to get the OMAG Server Platform running and then Postman to get ready to work with the platform and the servers running on top if it. Docker Tutorial Postman Tutorial At this point you should have Postman installed with the collections loaded, and Egeria's OMAG Server Platform running as a docker container.","title":"Prerequisites"},{"location":"getting-started/dojo/1/#test-yourself_2","text":"What is the message from the OMAG Server Platform that says it is ready to process requests? How do you find out the version of Egeria running in an OMAG Server Platform? What is the url to view the Swagger UI page for the OMAG Server Platform?","title":"Test yourself"},{"location":"getting-started/dojo/1/#configuring-the-omag-server-platform","text":"In this session, you will learn how to set up the OMAG Server Platform. Configuring the OMAG Server Platform","title":"Configuring the OMAG Server Platform"},{"location":"getting-started/dojo/1/#server-origin","text":"In the previous session you downloaded an application called Postman and loaded collections of pre-defined requests. This tool makes it easy to issue REST API requests to the OMAG Server Platform. Check that it is working by locating the Get Server Origin request in the Egeria-platform-services collection. When you click on that request in the left-hand list, a new tab opens and you can click on send to issue the request. You should see the same response as when you issues the platform origin request from Swagger earlier. Below is this response in Postman. If this does not work, then there is something wrong either in Postman or your platform. Check the URL string that was used in the request (shown in orange in the middle of the screen.) The screen shot below shows the error message when the egeria environment is not set. This can be fixed by setting it in the top right-hand dropdown. If the Egeria environment is not listed then you need to load the environment ( Postman tutorial ). If the baseURL variable is set to a different value to the server platform then Postman can not connect. In the screen capture below, you can see the baseURL is set to the default of https://localhost:9443 when it should be https://localhost:9443 because the platform is running in docker. Finally, if the OMAG Server Platform is not running the even though everything is set up correctly in Postman, it has nothing to connect to. Restart the platform ( Docker tutorial ). In last part of this session you will learn how to set up the OMAG Server Platform so that it is secure and determine the services and servers that are associated with the platform.","title":"Server origin"},{"location":"getting-started/dojo/1/#configuration-document-store","text":"","title":"Configuration document store"},{"location":"getting-started/dojo/1/#platform-security","text":"","title":"Platform security"},{"location":"getting-started/dojo/1/#registered-services","text":"","title":"Registered services"},{"location":"getting-started/dojo/1/#known-active-servers","text":"Review the description of the OMAG Server Platform configuration: Configuring the OMAG Server Platform The link below takes you to a task description in the Egeria Administration User Guide. The user guide describes the REST API call(s) needed to complete the task. You can choose to type the request into postman, or use the requests already defined in the Egeria-admin-services-platform-configuration Postman collection. Add the Coco Pharmaceuticals platform security connector to the platform Try running the platform origin command again - it should fail with a security error. Change the user variable in the Egeria environment from me to garygeeke and rerun the request. It will work again because garygeeke is the user id of the Coco Pharmaceuticals IT infrastructure lead and has permission to run the platform commands. Finally, use the Egeria-admin-services-platform-configuration Postman collection to experiment with the different registered services and and known and active server requests. These are useful to know as we move to configure servers on the platform. This is the end of the session on the OMAG Server Platform.","title":"Known / active servers"},{"location":"getting-started/dojo/1/introduction/","text":"Egeria introduction \u00b6 In this session, you will learn about the function and value of Egeria along with the key concepts and use cases it supports. Egeria introduction Test yourself \u00b6 Which of the following are part of the Open Metadata Manifesto? Metadata needs to be centralized so it can be managed. Maintenance of metadata must be automated. The availability of metadata management must become ubiquitous. Metadata access must become open and remotely accessible. Name 3 tools that could connect to Egeria. Name a metadata standard.","title":"Egeria Introduction"},{"location":"getting-started/dojo/1/introduction/#egeria-introduction","text":"In this session, you will learn about the function and value of Egeria along with the key concepts and use cases it supports. Egeria introduction","title":"Egeria introduction"},{"location":"getting-started/dojo/1/introduction/#test-yourself","text":"Which of the following are part of the Open Metadata Manifesto? Metadata needs to be centralized so it can be managed. Maintenance of metadata must be automated. The availability of metadata management must become ubiquitous. Metadata access must become open and remotely accessible. Name 3 tools that could connect to Egeria. Name a metadata standard.","title":"Test yourself"},{"location":"guides/community/","text":"Community Guide \u00b6 This project welcomes contributors from any organization or background, provided they are willing to follow the simple processes outlined below, as well as adhere to the Code of Conduct . Joining the community \u00b6 Live discussions \u00b6 We have a variety of weekly and bi-weekly meetings to which all are welcome: Call Purpose Developers call Discussion on code development (not minuted) Community call Demos, meetups and other activities going on in the community ( agenda and minutes ) TSC call Strategy, planning and decision-making with the Technical Steering Committee (TSC) Asynchronous dialog \u00b6 Egeria uses the LF AI & Data Slack community to provide an ongoing dialogue between members. This creates a recorded discussion of design decisions and discussions that complement the project meetings. Follow the link above and register with the LF AI & Data Slack service using your email address. Once signed in you can see all the active Slack channels. The main Slack channel for the Egeria project is called #egeria-discussion Additional channels are added from time to time as new workgroups and discussion topics are established. For Egeria these channel names will begin with #egeria . You can also view Slack channels from other LF AI & Data projects. More information about Slack? Slack is an instant messaging tool that allows multiple conversations to occur amongst the community members at any one time. Each conversation is called a channel . Channels can be set up for a specific event or have a long-term existence. Mailing list \u00b6 The community tends to use the mailing list only for automated meeting reminders. We are best contacted through Slack (above). Other websites and resources \u00b6 Previous Webinars - previous virtual events covering topics of interest. Planned Webinars - planned virtual events covering topics of interest. Workshops - face-to-face workshops promoting discussion and education on Egeria Presentations - presentations given at conferences and private gatherings Git repositories \u00b6 The Egeria project uses GitHub to maintain its content across a number of repositories. All of these repositories are publicly visible; however, if you want to contribute new content you need to create a GitHub account. This can be created from the top of the GitHub home page. If you are not familiar with Git and GitHub, there is additional education in the developer-resources/tools pages. Steps to contribute content to the project \u00b6 Egeria uses GitHub's fork and pull model to create a contribution. This process is described in detail in the Egeria dojo education . Each change should have a GitHub issue explaining why the change is being made. The new or updated content should follow the Egeria developer guidelines . There are additional resources for contributors under developer-resources . Review the developer guidelines to understand the requirements associated with new content for Egeria. Every contribution is signed to say that the contributor has the rights to make the contribution and agrees with the Developer Certificate of Origin (DCO) Creating a Linux Foundation account \u00b6 The Linux Foundation provide build and distribution facilities. You need an account to access some of the reports from the build. This is the link to create a Linux Foundation account . Note the username and password you selected.","title":"Community Guide"},{"location":"guides/community/#community-guide","text":"This project welcomes contributors from any organization or background, provided they are willing to follow the simple processes outlined below, as well as adhere to the Code of Conduct .","title":"Community Guide"},{"location":"guides/community/#joining-the-community","text":"","title":"Joining the community"},{"location":"guides/community/#live-discussions","text":"We have a variety of weekly and bi-weekly meetings to which all are welcome: Call Purpose Developers call Discussion on code development (not minuted) Community call Demos, meetups and other activities going on in the community ( agenda and minutes ) TSC call Strategy, planning and decision-making with the Technical Steering Committee (TSC)","title":"Live discussions"},{"location":"guides/community/#asynchronous-dialog","text":"Egeria uses the LF AI & Data Slack community to provide an ongoing dialogue between members. This creates a recorded discussion of design decisions and discussions that complement the project meetings. Follow the link above and register with the LF AI & Data Slack service using your email address. Once signed in you can see all the active Slack channels. The main Slack channel for the Egeria project is called #egeria-discussion Additional channels are added from time to time as new workgroups and discussion topics are established. For Egeria these channel names will begin with #egeria . You can also view Slack channels from other LF AI & Data projects. More information about Slack? Slack is an instant messaging tool that allows multiple conversations to occur amongst the community members at any one time. Each conversation is called a channel . Channels can be set up for a specific event or have a long-term existence.","title":"Asynchronous dialog"},{"location":"guides/community/#mailing-list","text":"The community tends to use the mailing list only for automated meeting reminders. We are best contacted through Slack (above).","title":"Mailing list"},{"location":"guides/community/#other-websites-and-resources","text":"Previous Webinars - previous virtual events covering topics of interest. Planned Webinars - planned virtual events covering topics of interest. Workshops - face-to-face workshops promoting discussion and education on Egeria Presentations - presentations given at conferences and private gatherings","title":"Other websites and resources"},{"location":"guides/community/#git-repositories","text":"The Egeria project uses GitHub to maintain its content across a number of repositories. All of these repositories are publicly visible; however, if you want to contribute new content you need to create a GitHub account. This can be created from the top of the GitHub home page. If you are not familiar with Git and GitHub, there is additional education in the developer-resources/tools pages.","title":"Git repositories"},{"location":"guides/community/#steps-to-contribute-content-to-the-project","text":"Egeria uses GitHub's fork and pull model to create a contribution. This process is described in detail in the Egeria dojo education . Each change should have a GitHub issue explaining why the change is being made. The new or updated content should follow the Egeria developer guidelines . There are additional resources for contributors under developer-resources . Review the developer guidelines to understand the requirements associated with new content for Egeria. Every contribution is signed to say that the contributor has the rights to make the contribution and agrees with the Developer Certificate of Origin (DCO)","title":"Steps to contribute content to the project"},{"location":"guides/community/#creating-a-linux-foundation-account","text":"The Linux Foundation provide build and distribution facilities. You need an account to access some of the reports from the build. This is the link to create a Linux Foundation account . Note the username and password you selected.","title":"Creating a Linux Foundation account"},{"location":"guides/project-operations/","text":"Project Operations \u00b6 The Egeria project provides content (standards, data, code and documentation) that is intended for wide consumption across many types of organizations: from those that rely on data in their operation, to organizations that have products or technology designed to help manage data and its related processing. A project of this scope requires input from a wide range of subject matter experts with different backgrounds and allegiances. As such, we need a set of principles, roles and operating practices to ensure the results of our contributions are useful, have high quality and are widely consumable. General principles \u00b6 The principles set the tone of the operation of Egeria: The activities of the project ensure open collaboration. Through this open collaboration we aim to build a community of people who are interested in the success of the project. The scope of the content is determined by the individuals who are actively contributing. The resulting content is licensed under the Apache 2.0 license. An individual's privileges and position is awarded through their contribution and engagement. These principles should be respected as the procedures used to manage the Egeria project are evolved and matured. Community members \u00b6 Anyone can become a member of the Egeria community by signing up to the Egeria mailing list, joining the Slack community, attending the project online meetings or contributing content to one of more of the GitHub repositories. The community guide describes how to connect to these channels. All participants in the Egeria community are bound by the project's Code of Conduct . As a member you are able to attend our meetings, just to listen, or to play an active part in the discussion. The online meetings are recorded to allow community members to catch up if they are not able to attend the live meeting. When you attend the community meetings specifically, your name will be recorded in the meeting minutes along with any remarks or suggestions you make. The agenda and minutes of our community meetings are publicly available on the Egeria wiki . A member may make contributions to the Egeria content by submitting a GitHub pull request on the appropriate Git repository. This will be reviewed and processed by the Egeria maintainers . The process for making a contribution is described in the Egeria Dojo education. Each contribution is signed by the contributor to confirm they agree to our Developer Certificate of Origin (DCO) . Community members can progress to be Egeria contributors and then Egeria maintainers . Contributors \u00b6 Egeria contributors are members who have actively taken additional steps to promote and foster the success of Egeria and its acceptance/adoption across the IT community. The activities that contributors engage in might include: Provide best practices for information governance, lineage, metadata management and other related disciplines during active discussions and/or development of material. Actively participate in meetings and discussions Promote the goals of Egeria and the benefits of open metadata to the IT community (deliver presentations, informal talks, assist at trade shows, independent blogs, etc.) Assist in the recruitment of new members. Contribute where appropriate to documentation and code reviews, specification development, demonstration assets and other artifacts that help move Egeria forward. How to become a contributor Being recognized as an Egeria contributor is done by nomination of an Egeria maintainer with a majority vote of Egeria maintainers to confirm. Once confirmed, you will receive an Egeria Contributor badge to add to your social profiles and/or website, and can publicly refer to yourself as an Egeria contributor. Egeria's contributors are recognized in the contributors list Maintainers \u00b6 Maintainers are members of the Egeria community that have permission to change the Egeria content. This may be content that they have created themselves, or has been provided by another member. Maintainers also have responsibility for helping other project members with their contributions. This includes: Monitoring email aliases. Monitoring Slack (delayed response is perfectly acceptable). Triage GitHub issues and perform pull request reviews for other maintainers and the community. Make sure that ongoing git pull requests are moving forward at the right pace or closing them. In general continue to be willing to spend at least 25% of one's time working on the project (approximately 1.25 business days per week). How to become a maintainer New maintainers are voted onto the maintainers list by the existing maintainers . A person wishing to become a maintainer sends a note to the existing mailing list at egeria-technical-discuss@lists.lfaidata.foundation, listing their Egeria contributions to date and requesting to be made a maintainer. The maintainers vote and if a majority agree then the requester is added to the maintainers list and given write access to the Egeria Git repositories . Once confirmed, you will receive an an Egeria Maintainer badge to add to your social profiles and/or website, and can publicly refer to yourself as an Egeria maintainer. Losing maintainer status If a maintainer is no longer interested or cannot perform the maintainer duties listed above, they should volunteer to be moved to emeritus status. In extreme cases this can also occur by a vote of the maintainers per the voting process below. Emeritus maintainers can rejoin the maintainer list through a vote of the existing maintainers. Leadership \u00b6 The leadership of Egeria is granted through a vote of the Egeria maintainers. Egeria is currently led by Mandy Chessell. Project meetings \u00b6 Some meetings are face-to-face, but most are conference calls. Attendance at meetings is open to all. Conference calls can be joined without an explicit invitation. However, due to physical security requirements at some venues we use, it is necessary to ensure you are added to the invitee list of any face-to-face meetings that you wish to attend and complete the necessary formalities for the venue. For example, the face-to-face meeting may be at a conference that requires you to register for the conference to attend, or a meeting may be at an organization's offices that are required to maintain a list of everyone on site. See details in the community guide Refer to the community guide for further details on the specific meetings that are planned, conference call links and dial-in numbers, as well as other communications channels like Slack and email. Releases \u00b6 The Egeria team aim to create an official release of the open metadata and governance capability every month. This release will be available to include in products and other technology through Maven's Central Repository , or through a download from the GitHub site. Details of the releases are maintained in the release notes . In between official releases, the latest build is also available to developers in GitHub. The process for creating a release is described in the developer guide Conflict resolution and voting \u00b6 In general, we prefer that technical issues and maintainer membership are amicably worked out between the persons involved. If a dispute cannot be decided independently, the maintainers can be called in to decide an issue. If the maintainers themselves cannot decide an issue, the issue will be resolved by voting. The voting process is a simple majority in which each maintainer receives one vote.","title":"Project Operations"},{"location":"guides/project-operations/#project-operations","text":"The Egeria project provides content (standards, data, code and documentation) that is intended for wide consumption across many types of organizations: from those that rely on data in their operation, to organizations that have products or technology designed to help manage data and its related processing. A project of this scope requires input from a wide range of subject matter experts with different backgrounds and allegiances. As such, we need a set of principles, roles and operating practices to ensure the results of our contributions are useful, have high quality and are widely consumable.","title":"Project Operations"},{"location":"guides/project-operations/#general-principles","text":"The principles set the tone of the operation of Egeria: The activities of the project ensure open collaboration. Through this open collaboration we aim to build a community of people who are interested in the success of the project. The scope of the content is determined by the individuals who are actively contributing. The resulting content is licensed under the Apache 2.0 license. An individual's privileges and position is awarded through their contribution and engagement. These principles should be respected as the procedures used to manage the Egeria project are evolved and matured.","title":"General principles"},{"location":"guides/project-operations/#community-members","text":"Anyone can become a member of the Egeria community by signing up to the Egeria mailing list, joining the Slack community, attending the project online meetings or contributing content to one of more of the GitHub repositories. The community guide describes how to connect to these channels. All participants in the Egeria community are bound by the project's Code of Conduct . As a member you are able to attend our meetings, just to listen, or to play an active part in the discussion. The online meetings are recorded to allow community members to catch up if they are not able to attend the live meeting. When you attend the community meetings specifically, your name will be recorded in the meeting minutes along with any remarks or suggestions you make. The agenda and minutes of our community meetings are publicly available on the Egeria wiki . A member may make contributions to the Egeria content by submitting a GitHub pull request on the appropriate Git repository. This will be reviewed and processed by the Egeria maintainers . The process for making a contribution is described in the Egeria Dojo education. Each contribution is signed by the contributor to confirm they agree to our Developer Certificate of Origin (DCO) . Community members can progress to be Egeria contributors and then Egeria maintainers .","title":"Community members"},{"location":"guides/project-operations/#contributors","text":"Egeria contributors are members who have actively taken additional steps to promote and foster the success of Egeria and its acceptance/adoption across the IT community. The activities that contributors engage in might include: Provide best practices for information governance, lineage, metadata management and other related disciplines during active discussions and/or development of material. Actively participate in meetings and discussions Promote the goals of Egeria and the benefits of open metadata to the IT community (deliver presentations, informal talks, assist at trade shows, independent blogs, etc.) Assist in the recruitment of new members. Contribute where appropriate to documentation and code reviews, specification development, demonstration assets and other artifacts that help move Egeria forward. How to become a contributor Being recognized as an Egeria contributor is done by nomination of an Egeria maintainer with a majority vote of Egeria maintainers to confirm. Once confirmed, you will receive an Egeria Contributor badge to add to your social profiles and/or website, and can publicly refer to yourself as an Egeria contributor. Egeria's contributors are recognized in the contributors list","title":"Contributors"},{"location":"guides/project-operations/#maintainers","text":"Maintainers are members of the Egeria community that have permission to change the Egeria content. This may be content that they have created themselves, or has been provided by another member. Maintainers also have responsibility for helping other project members with their contributions. This includes: Monitoring email aliases. Monitoring Slack (delayed response is perfectly acceptable). Triage GitHub issues and perform pull request reviews for other maintainers and the community. Make sure that ongoing git pull requests are moving forward at the right pace or closing them. In general continue to be willing to spend at least 25% of one's time working on the project (approximately 1.25 business days per week). How to become a maintainer New maintainers are voted onto the maintainers list by the existing maintainers . A person wishing to become a maintainer sends a note to the existing mailing list at egeria-technical-discuss@lists.lfaidata.foundation, listing their Egeria contributions to date and requesting to be made a maintainer. The maintainers vote and if a majority agree then the requester is added to the maintainers list and given write access to the Egeria Git repositories . Once confirmed, you will receive an an Egeria Maintainer badge to add to your social profiles and/or website, and can publicly refer to yourself as an Egeria maintainer. Losing maintainer status If a maintainer is no longer interested or cannot perform the maintainer duties listed above, they should volunteer to be moved to emeritus status. In extreme cases this can also occur by a vote of the maintainers per the voting process below. Emeritus maintainers can rejoin the maintainer list through a vote of the existing maintainers.","title":"Maintainers"},{"location":"guides/project-operations/#leadership","text":"The leadership of Egeria is granted through a vote of the Egeria maintainers. Egeria is currently led by Mandy Chessell.","title":"Leadership"},{"location":"guides/project-operations/#project-meetings","text":"Some meetings are face-to-face, but most are conference calls. Attendance at meetings is open to all. Conference calls can be joined without an explicit invitation. However, due to physical security requirements at some venues we use, it is necessary to ensure you are added to the invitee list of any face-to-face meetings that you wish to attend and complete the necessary formalities for the venue. For example, the face-to-face meeting may be at a conference that requires you to register for the conference to attend, or a meeting may be at an organization's offices that are required to maintain a list of everyone on site. See details in the community guide Refer to the community guide for further details on the specific meetings that are planned, conference call links and dial-in numbers, as well as other communications channels like Slack and email.","title":"Project meetings"},{"location":"guides/project-operations/#releases","text":"The Egeria team aim to create an official release of the open metadata and governance capability every month. This release will be available to include in products and other technology through Maven's Central Repository , or through a download from the GitHub site. Details of the releases are maintained in the release notes . In between official releases, the latest build is also available to developers in GitHub. The process for creating a release is described in the developer guide","title":"Releases"},{"location":"guides/project-operations/#conflict-resolution-and-voting","text":"In general, we prefer that technical issues and maintainer membership are amicably worked out between the persons involved. If a dispute cannot be decided independently, the maintainers can be called in to decide an issue. If the maintainers themselves cannot decide an issue, the issue will be resolved by voting. The voting process is a simple majority in which each maintainer receives one vote.","title":"Conflict resolution and voting"},{"location":"guides/admin/configuring-the-omag-server-platform/","text":"Configuring the OMAG Server Platform \u00b6 The OMAG Server Platform is a JVM process that includes a Tomcat web server and uses Spring Boot to support REST APIs. Default setup \u00b6 REST APIs are registered at https://localhost:9443 . This address is called the platform's platform's URL root and is configured in a number of places in the OMAG Server's configuration. The platform supports no specific security authorization. All configuration is stored in encrypted files - one for each OMAG Server configured to run on it. Useful for development, be wary for production These defaults are suitable for a development environment; however, for production the platform should be configured with platform security because this ensures configuration is managed by authorized users. Configuring other options \u00b6 Configuration store \u00b6 The configuration document is the place where the configuration for a single OMAG Server is stored. This may include security certificates and passwords. By default, the configuration document is stored in its own encrypted file in the home directory of the OMAG Server Platform , named: omag.server.{serverName}.config As of v2.0 of Egeria, the contents of the configuration document are stored in an encrypted JSON format 1 . The clear-text contents of the configuration document can still be retrieved by accessing the admin services endpoint for retrieving the configuration document; however, this ensures security is applied before a user is able to retrieve the configuration document's contents: GET - retrieve configuration document {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/configuration/ You may also wish to: Move the location of the configuration documents Write you own alternative store for the configuration documents All of these options are possible because the configuration document store is implemented in a configuration document store connector . It is therefore possible to change the implementation or behavior of this connector with a simple configuration change to the OMAG Server Platform. The configuration document store connector is configured in the OMAG Server Platform using the following command: POST - configure the configuration store {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/stores/connection The request body should be a connection object used to create the connector to the configuration document store. Ensure the connector is available in the classpath In order to use any connector other than the default, you need to also ensure that the Connector and its ConnectorProvider class are available to the server platform (i.e. the jar file containing them is available in the LOADER_PATH location of the server platform). Exmaple: (unencrypted) file store connector For example, this connection would set up the (unencrypted) file store connector: 1 2 3 4 5 6 7 8 9 10 11 { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.adminservices.configurationstore.file.FileBasedServerConfigStoreProvider\" }, \"endpoint\" : { \"class\" : \"Endpoint\" , \"address\" : \"omag.server.{0}.config\" } } Example: encrypted JSON file store, non-default location As another example, this connection uses the default encrypted JSON file store, but the files are stored in a different location ( /my-config/omag.server.{0}.config ). 1 2 3 4 5 6 7 8 9 10 11 { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.adminservices.configurationstore.encryptedfile.EncryptedFileBasedServerConfigStoreProvider\" }, \"endpoint\" : { \"class\" : \"Endpoint\" , \"address\" : \"/my-config/omag.server.{0}.config\" } } Determine configured store \u00b6 It is possible to query the setting of the configuration document store connector using the following command: GET - retrieve configured configuration document store {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/stores/connection Response indicating default configuration store (encrypted JSON file store) { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used and the address shows where the configuration documents are stored. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.adminservices.configurationstore.file.FileBasedServerConfigStoreProvider\" }, \"endpoint\" : { \"class\" : \"Endpoint\" , \"address\" : \"omag.server.{0}.config\" } } } Remove configured store \u00b6 It is also possible to remove the configuration for the connector using the following command: DELETE - remove configured configuration store {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/stores/connection This reverts the store to the default encrypted JSON file store. Platform security \u00b6 The OMAG Server Platform provides both configuration and diagnostic services for OMAG Servers which in themselves provide access to a wide variety of information and control points. Therefore, it is necessary to provide authorization services relating to the use of the platform services. Egeria provides a platform security authorization capability . It is implemented in a platform security connector that is called whenever requests are made to the server platform services. Security is configured for a specific platform once it is running by using the following command. POST - configure platform security {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/platform/security/connection The request body should be a connection object used to create the platform security connector and the platform URL root of the platform. Example: sample platform security connector For example, this is the request body that would set up the sample platform security connector provided for the Coco Pharmaceuticals case study: { \"class\" : \"PlatformSecurityRequestBody\" , \"urlRoot\" : \"{{platformURLRoot}}\" , \"platformSecurityConnection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.CocoPharmaPlatformSecurityProvider\" } } } Determine configured security \u00b6 It is possible to query the setting of the platform security connector using the following command: GET - retrieve configured platform security {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/platform/security/connection Response indicating no security { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific security connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.CocoPharmaPlatformSecurityProvider\" } } } Remove configured security \u00b6 It is possible to remove the configuration for the connector using the following command: DELETE - remove configured platform security {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/platform/security/connection This removes all authorization checking from the platform services. application.properties \u00b6 Since the OMAG Server Platform is a Spring Boot application, there are other values that can be set in its application.properties file found in the resources subdirectory: Defining the port that the OMAG Server Platform will listen on for REST API calls. Controlling the level of developer logging that the platform produces when it is running. See Configuring logging options for more details. Defining where the connector implementations should be loaded from. Spring provides extensive documentation on its standard properties . Auto-starting servers \u00b6 An OMAG Server is typically started on the OMAG Server Platform using a command; however, it is possible to set up a list of servers that are automatically started whenever the platform is started. These servers are also automatically shutdown when the platform is shutdown. OMAG Servers can be automatically activated at startup by setting spring-boot property startup.server.list , typically in the application.properties file. The server names are listed without quotes. For example: startup.server.list = cocoMDS1, cocoMDS2 Each server is started with the administration user id set in the spring-boot property startup.user . For example: startup.user = garygeeke By default, this user id is set to the user id system . When the platform shuts down, if any of the servers that were in the startup list are still running, they will be shut down before the server completes. Default setting If startup.server.list is null then no servers are automatically started or stopped. startup.server.list = This is the default setting. Transport Layer Security (TLS) \u00b6 Transport layer security describes the security applied to API calls made between servers. The most commonly known transport layer security is SSL. Egeria uses TLS with certificates to secure the communication to and from the OMAG Server Platforms . Brief background on TLS \u00b6 Transport Layer Security (TLS) protects communications over network connections through encryption, authentication and integrity. It is just one layer of security of many. One-way SSL exchange \u00b6 One-way SSL exchange is typically in use when browsing the web - since as a user you are most concerned that the server you are connecting to is authentic. With this approach, the server is not able to guarantee your authenticity at the transport level. This means you can be assured of the identity of the server, but it cannot be sure of who you are. Two-way (mutual) SSL exchange \u00b6 With two-way (mutual) SSL exchange, trust is established both ways. This mutual authentication is more typical when interconnecting different systems and applications which are known in advance. Certificates for the OMAG Server Platform \u00b6 Important note on terminology Egeria's OMAG Server Platform is a Spring Boot based application. We refer to it as Egeria's \"platform\", which hosts Egeria OMAG Servers . However, in the documentation relating to network communications and SSL, a \"server\" is usually seen as an application listening on a network port. For Egeria, this \"server\" would be the OMAG Server Platform. It is important to be aware of this terminology difference when reading the links and references mentioned here. An instance of the OMAG Server Platform services requests over a REST based API from other OMAG Server Platforms, UIs, tools and engines. In this regard its role in SSL network requests described above is that of a network server , with its callers performing the role of a network client . However, since the OMAG Server Platform also makes requests to other systems (including other OMAG Server Platforms and Apache Kafka) it is also fulfilling a network client role. As a Spring application, the OMAG Server Platform's configuration for its network server role allows the following Spring properties to be set: Property Use server.ssl-key-store Used by Tomcat/Spring Boot to locate keys that identify the server server.ssl-key-alias Used by Tomcat/Spring Boot to identify the alias of the key tomcat should use for itself server.ssl.key-store-password Used by Tomcat/Spring Boot for the keystore password (2 way SSL) server.ssl.trust-store Used by Tomcat/Spring Boot to understand what clients it can trust (2 way SSL) server.ssl.trust-store-password Used by Tomcat/Spring Boot for the password of the truststore (2 way SSL) strict.ssl This is an additional parameter Egeria provides (non-standard Spring property) which if true causes SSL verification to be skipped entirely For further details on these and other less common configuration options, refer to the Spring Docs. Since the OMAG Server Platform is also a network client the settings in the next section for clients are also required. Egeria Java clients \u00b6 Standard Java properties need to be set within the JVM running the Egeria client code (this includes the OMAG Server Platform): Property Use javax.net.ssl.keyStore keyStore for client to use (2 way SSL needs this) javax.net.ssl.keyStorePassword password for the keystore (2 way SSL needs this) javax.net.ssl.trustStore trustStore for the client to use (always needs setting as Egeria makes client calls) javax.net.ssl.trustStorePassword password for the truststore (always - as above) strict.ssl For any executable jars provided by Egeria (such as samples), setting this additional parameter to true will cause SSL verification to be skipped. This is only recommended for test and development. Note that in the case of Java clients, these are system properties, and do not use Spring conventions. Other clients \u00b6 Similar principles to those documented for Java should apply. If you need further assistance, please contact the team on Slack . A pull request (or issue) with contributed documentation is also very welcome! Example to launch Egeria \u00b6 Examples certificates are provided in the codebase under open-metadata-resources/open-metadata-deployment/certificates As an example of running the Egeria server chassis with the certificates generated above, add the following options when launching the OMAG Server Platform jar file: -Dserver.ssl.key-store=${KS} -Dserver.ssl.key-alias=EgeriaServerChassis -Dserver.ssl.key-store-password=egeria -Dserver.ssl.trust-store=EgeriaCA.p12 -Dserver.ssl.trust-store-password=egeria -Djavax.net.ssl.keyStore=EgeriaServerChassis -Djavax.net.ssl.keyStorePassword=egeria -Djavax.net.ssl.trustStore=EgeriaCA.p12 -Djavax.net.ssl.trustStorePassword=egeria Detailed explanation We have to use both server.ssl and javax.net values since the former controls how the OMAG Server Platform works when accepting inbound connections and the latter are needed when it acts as a network client. We have assumed the default keystore passwords, and also that we will use the same key regardless of whether it is the one that the chassis sends back to its client after they connect, or the one the chassis may send to those other repositories. They could be distinct if needed. Creating your own certificates \u00b6 The example certificates are fine for development; however, it is important to have your own certificates for a production environment. An example script (MacOS/Linux) to create certificates is provided in gensamplecerts.sh . It is intended only as an example. It requires the openssl tool and keytool . Deployment frameworks in cloud services may also offer support to generate certificates, and it is likely an enterprise process will be in place in larger organizations. The script creates a Certificate Authority and then specific certificates for different Egeria components. It could be extended to create certificates for other clients especially if using 2 way SSL. When the script is run it also makes use of the configuration template openssl.cnf . Together, both set some important characteristics that are needed to allow the certificate to work properly, especially with current browsers: ensuring basicConstraints are specified ensuring the certificate expiry time is not too far in the future ensuring subjectAltName is specified. Additional notes on building diagrams The rendered image files are checked in, however when updating, the diagrams can be regenerated using PlantUML For example: plantuml -svg ssl-oneway.puml The diagrams are best rendered to svg, however notes do not render with a background if using the IntelliJ markdown plugin. They do render correctly if opened directly in IntelliJ, as well as in a browser It's also recommended to install the IntelliJ 'PlantUML' plugin to get a real-time preview whilst updating the diagrams. For more details on the encrypted format, see the encrypted file store connector . \u21a9","title":"Configure OMAG Server Platform"},{"location":"guides/admin/configuring-the-omag-server-platform/#configuring-the-omag-server-platform","text":"The OMAG Server Platform is a JVM process that includes a Tomcat web server and uses Spring Boot to support REST APIs.","title":"Configuring the OMAG Server Platform"},{"location":"guides/admin/configuring-the-omag-server-platform/#default-setup","text":"REST APIs are registered at https://localhost:9443 . This address is called the platform's platform's URL root and is configured in a number of places in the OMAG Server's configuration. The platform supports no specific security authorization. All configuration is stored in encrypted files - one for each OMAG Server configured to run on it. Useful for development, be wary for production These defaults are suitable for a development environment; however, for production the platform should be configured with platform security because this ensures configuration is managed by authorized users.","title":"Default setup"},{"location":"guides/admin/configuring-the-omag-server-platform/#configuring-other-options","text":"","title":"Configuring other options"},{"location":"guides/admin/configuring-the-omag-server-platform/#configuration-store","text":"The configuration document is the place where the configuration for a single OMAG Server is stored. This may include security certificates and passwords. By default, the configuration document is stored in its own encrypted file in the home directory of the OMAG Server Platform , named: omag.server.{serverName}.config As of v2.0 of Egeria, the contents of the configuration document are stored in an encrypted JSON format 1 . The clear-text contents of the configuration document can still be retrieved by accessing the admin services endpoint for retrieving the configuration document; however, this ensures security is applied before a user is able to retrieve the configuration document's contents: GET - retrieve configuration document {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/configuration/ You may also wish to: Move the location of the configuration documents Write you own alternative store for the configuration documents All of these options are possible because the configuration document store is implemented in a configuration document store connector . It is therefore possible to change the implementation or behavior of this connector with a simple configuration change to the OMAG Server Platform. The configuration document store connector is configured in the OMAG Server Platform using the following command: POST - configure the configuration store {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/stores/connection The request body should be a connection object used to create the connector to the configuration document store. Ensure the connector is available in the classpath In order to use any connector other than the default, you need to also ensure that the Connector and its ConnectorProvider class are available to the server platform (i.e. the jar file containing them is available in the LOADER_PATH location of the server platform). Exmaple: (unencrypted) file store connector For example, this connection would set up the (unencrypted) file store connector: 1 2 3 4 5 6 7 8 9 10 11 { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.adminservices.configurationstore.file.FileBasedServerConfigStoreProvider\" }, \"endpoint\" : { \"class\" : \"Endpoint\" , \"address\" : \"omag.server.{0}.config\" } } Example: encrypted JSON file store, non-default location As another example, this connection uses the default encrypted JSON file store, but the files are stored in a different location ( /my-config/omag.server.{0}.config ). 1 2 3 4 5 6 7 8 9 10 11 { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.adminservices.configurationstore.encryptedfile.EncryptedFileBasedServerConfigStoreProvider\" }, \"endpoint\" : { \"class\" : \"Endpoint\" , \"address\" : \"/my-config/omag.server.{0}.config\" } }","title":"Configuration store"},{"location":"guides/admin/configuring-the-omag-server-platform/#determine-configured-store","text":"It is possible to query the setting of the configuration document store connector using the following command: GET - retrieve configured configuration document store {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/stores/connection Response indicating default configuration store (encrypted JSON file store) { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used and the address shows where the configuration documents are stored. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.adminservices.configurationstore.file.FileBasedServerConfigStoreProvider\" }, \"endpoint\" : { \"class\" : \"Endpoint\" , \"address\" : \"omag.server.{0}.config\" } } }","title":"Determine configured store"},{"location":"guides/admin/configuring-the-omag-server-platform/#remove-configured-store","text":"It is also possible to remove the configuration for the connector using the following command: DELETE - remove configured configuration store {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/stores/connection This reverts the store to the default encrypted JSON file store.","title":"Remove configured store"},{"location":"guides/admin/configuring-the-omag-server-platform/#platform-security","text":"The OMAG Server Platform provides both configuration and diagnostic services for OMAG Servers which in themselves provide access to a wide variety of information and control points. Therefore, it is necessary to provide authorization services relating to the use of the platform services. Egeria provides a platform security authorization capability . It is implemented in a platform security connector that is called whenever requests are made to the server platform services. Security is configured for a specific platform once it is running by using the following command. POST - configure platform security {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/platform/security/connection The request body should be a connection object used to create the platform security connector and the platform URL root of the platform. Example: sample platform security connector For example, this is the request body that would set up the sample platform security connector provided for the Coco Pharmaceuticals case study: { \"class\" : \"PlatformSecurityRequestBody\" , \"urlRoot\" : \"{{platformURLRoot}}\" , \"platformSecurityConnection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.CocoPharmaPlatformSecurityProvider\" } } }","title":"Platform security"},{"location":"guides/admin/configuring-the-omag-server-platform/#determine-configured-security","text":"It is possible to query the setting of the platform security connector using the following command: GET - retrieve configured platform security {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/platform/security/connection Response indicating no security { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific security connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.CocoPharmaPlatformSecurityProvider\" } } }","title":"Determine configured security"},{"location":"guides/admin/configuring-the-omag-server-platform/#remove-configured-security","text":"It is possible to remove the configuration for the connector using the following command: DELETE - remove configured platform security {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/platform/security/connection This removes all authorization checking from the platform services.","title":"Remove configured security"},{"location":"guides/admin/configuring-the-omag-server-platform/#applicationproperties","text":"Since the OMAG Server Platform is a Spring Boot application, there are other values that can be set in its application.properties file found in the resources subdirectory: Defining the port that the OMAG Server Platform will listen on for REST API calls. Controlling the level of developer logging that the platform produces when it is running. See Configuring logging options for more details. Defining where the connector implementations should be loaded from. Spring provides extensive documentation on its standard properties .","title":"application.properties"},{"location":"guides/admin/configuring-the-omag-server-platform/#auto-starting-servers","text":"An OMAG Server is typically started on the OMAG Server Platform using a command; however, it is possible to set up a list of servers that are automatically started whenever the platform is started. These servers are also automatically shutdown when the platform is shutdown. OMAG Servers can be automatically activated at startup by setting spring-boot property startup.server.list , typically in the application.properties file. The server names are listed without quotes. For example: startup.server.list = cocoMDS1, cocoMDS2 Each server is started with the administration user id set in the spring-boot property startup.user . For example: startup.user = garygeeke By default, this user id is set to the user id system . When the platform shuts down, if any of the servers that were in the startup list are still running, they will be shut down before the server completes. Default setting If startup.server.list is null then no servers are automatically started or stopped. startup.server.list = This is the default setting.","title":"Auto-starting servers"},{"location":"guides/admin/configuring-the-omag-server-platform/#transport-layer-security-tls","text":"Transport layer security describes the security applied to API calls made between servers. The most commonly known transport layer security is SSL. Egeria uses TLS with certificates to secure the communication to and from the OMAG Server Platforms .","title":"Transport Layer Security (TLS)"},{"location":"guides/admin/configuring-the-omag-server-platform/#brief-background-on-tls","text":"Transport Layer Security (TLS) protects communications over network connections through encryption, authentication and integrity. It is just one layer of security of many.","title":"Brief background on TLS"},{"location":"guides/admin/configuring-the-omag-server-platform/#one-way-ssl-exchange","text":"One-way SSL exchange is typically in use when browsing the web - since as a user you are most concerned that the server you are connecting to is authentic. With this approach, the server is not able to guarantee your authenticity at the transport level. This means you can be assured of the identity of the server, but it cannot be sure of who you are.","title":"One-way SSL exchange"},{"location":"guides/admin/configuring-the-omag-server-platform/#two-way-mutual-ssl-exchange","text":"With two-way (mutual) SSL exchange, trust is established both ways. This mutual authentication is more typical when interconnecting different systems and applications which are known in advance.","title":"Two-way (mutual) SSL exchange"},{"location":"guides/admin/configuring-the-omag-server-platform/#certificates-for-the-omag-server-platform","text":"Important note on terminology Egeria's OMAG Server Platform is a Spring Boot based application. We refer to it as Egeria's \"platform\", which hosts Egeria OMAG Servers . However, in the documentation relating to network communications and SSL, a \"server\" is usually seen as an application listening on a network port. For Egeria, this \"server\" would be the OMAG Server Platform. It is important to be aware of this terminology difference when reading the links and references mentioned here. An instance of the OMAG Server Platform services requests over a REST based API from other OMAG Server Platforms, UIs, tools and engines. In this regard its role in SSL network requests described above is that of a network server , with its callers performing the role of a network client . However, since the OMAG Server Platform also makes requests to other systems (including other OMAG Server Platforms and Apache Kafka) it is also fulfilling a network client role. As a Spring application, the OMAG Server Platform's configuration for its network server role allows the following Spring properties to be set: Property Use server.ssl-key-store Used by Tomcat/Spring Boot to locate keys that identify the server server.ssl-key-alias Used by Tomcat/Spring Boot to identify the alias of the key tomcat should use for itself server.ssl.key-store-password Used by Tomcat/Spring Boot for the keystore password (2 way SSL) server.ssl.trust-store Used by Tomcat/Spring Boot to understand what clients it can trust (2 way SSL) server.ssl.trust-store-password Used by Tomcat/Spring Boot for the password of the truststore (2 way SSL) strict.ssl This is an additional parameter Egeria provides (non-standard Spring property) which if true causes SSL verification to be skipped entirely For further details on these and other less common configuration options, refer to the Spring Docs. Since the OMAG Server Platform is also a network client the settings in the next section for clients are also required.","title":"Certificates for the OMAG Server Platform"},{"location":"guides/admin/configuring-the-omag-server-platform/#egeria-java-clients","text":"Standard Java properties need to be set within the JVM running the Egeria client code (this includes the OMAG Server Platform): Property Use javax.net.ssl.keyStore keyStore for client to use (2 way SSL needs this) javax.net.ssl.keyStorePassword password for the keystore (2 way SSL needs this) javax.net.ssl.trustStore trustStore for the client to use (always needs setting as Egeria makes client calls) javax.net.ssl.trustStorePassword password for the truststore (always - as above) strict.ssl For any executable jars provided by Egeria (such as samples), setting this additional parameter to true will cause SSL verification to be skipped. This is only recommended for test and development. Note that in the case of Java clients, these are system properties, and do not use Spring conventions.","title":"Egeria Java clients"},{"location":"guides/admin/configuring-the-omag-server-platform/#other-clients","text":"Similar principles to those documented for Java should apply. If you need further assistance, please contact the team on Slack . A pull request (or issue) with contributed documentation is also very welcome!","title":"Other clients"},{"location":"guides/admin/configuring-the-omag-server-platform/#example-to-launch-egeria","text":"Examples certificates are provided in the codebase under open-metadata-resources/open-metadata-deployment/certificates As an example of running the Egeria server chassis with the certificates generated above, add the following options when launching the OMAG Server Platform jar file: -Dserver.ssl.key-store=${KS} -Dserver.ssl.key-alias=EgeriaServerChassis -Dserver.ssl.key-store-password=egeria -Dserver.ssl.trust-store=EgeriaCA.p12 -Dserver.ssl.trust-store-password=egeria -Djavax.net.ssl.keyStore=EgeriaServerChassis -Djavax.net.ssl.keyStorePassword=egeria -Djavax.net.ssl.trustStore=EgeriaCA.p12 -Djavax.net.ssl.trustStorePassword=egeria Detailed explanation We have to use both server.ssl and javax.net values since the former controls how the OMAG Server Platform works when accepting inbound connections and the latter are needed when it acts as a network client. We have assumed the default keystore passwords, and also that we will use the same key regardless of whether it is the one that the chassis sends back to its client after they connect, or the one the chassis may send to those other repositories. They could be distinct if needed.","title":"Example to launch Egeria"},{"location":"guides/admin/configuring-the-omag-server-platform/#creating-your-own-certificates","text":"The example certificates are fine for development; however, it is important to have your own certificates for a production environment. An example script (MacOS/Linux) to create certificates is provided in gensamplecerts.sh . It is intended only as an example. It requires the openssl tool and keytool . Deployment frameworks in cloud services may also offer support to generate certificates, and it is likely an enterprise process will be in place in larger organizations. The script creates a Certificate Authority and then specific certificates for different Egeria components. It could be extended to create certificates for other clients especially if using 2 way SSL. When the script is run it also makes use of the configuration template openssl.cnf . Together, both set some important characteristics that are needed to allow the certificate to work properly, especially with current browsers: ensuring basicConstraints are specified ensuring the certificate expiry time is not too far in the future ensuring subjectAltName is specified. Additional notes on building diagrams The rendered image files are checked in, however when updating, the diagrams can be regenerated using PlantUML For example: plantuml -svg ssl-oneway.puml The diagrams are best rendered to svg, however notes do not render with a background if using the IntelliJ markdown plugin. They do render correctly if opened directly in IntelliJ, as well as in a browser It's also recommended to install the IntelliJ 'PlantUML' plugin to get a real-time preview whilst updating the diagrams. For more details on the encrypted format, see the encrypted file store connector . \u21a9","title":"Creating your own certificates"},{"location":"guides/admin/guide/","text":"Administration Guide \u00b6 The Egeria technology principally runs on the Open Metadata and Governance ( OMAG ) Server Platform . This platform hosts one or more OMAG Servers , each supporting a variety of open metadata and governance capabilities. In the illustration above, the OMAG Server Platforms are the blue, rounded boxes and the orange circles are the OMAG Servers. The green clouds represent different cloud platforms or data centers. This guide explains how to configure the OMAG Server Platform and the different types of OMAG Servers that run on it. TL;DR If you are keen to get started right away then these are the links to the configuration instructions. Configuring the OMAG Server Platform Configuring an OMAG Server to run on an OMAG Server Platform Configuring the Presentation Server (for UIs) and once you have your OMAG Servers configured: Operating an OMAG Server OMAG subsystems \u00b6 An OMAG Server is a set of configured OMAG subsystems supported by the OMAG Server Platform . Each subsystem supports a particular type of technology, so it can exchange metadata with the open metadata ecosystem. Some technologies are sources of metadata, others just consume metadata and then there are technologies that have a two-way exchange of metadata with the open metadata ecosystem. The OMAG subsystems that are to be enabled in a specific instance of an OMAG Server are defined in a configuration document . When the configuration document is loaded into the OMAG Server Platform, the OMAG Server that it describes is started, and the subsystems defined in the configuration document are activated for that server. OMAG Servers \u00b6 In an open metadata landscape, there may be multiple instances of OMAG Servers running in an OMAG Server Platform , each performing a different role. Each of these server instances would have their own configuration document allowing them to have different subsystems active. Configuring an OMAG Server \u00b6 The configuration document for a specific OMAG Server is identified by the server's name. This is passed on the URL of every admin services API request along with the user id of the administrator. By default, the configuration is stored in a file called: omag.server.{serverName}.config The administration services that set up this file all begin with a URL like this: .../open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/... The serverName specified on these calls determines which configuration document is used, and hence which OMAG Server's configuration it is working with. The OMAG Server Platform typically starts up without any OMAG Servers active. Once it is running, it can be used to set up the configuration documents that describe the open metadata subsystems needed for each OMAG Server instance. Once the configuration document is in place, the OMAG Server can be activated and deactivated multiple times, across multiple restarts of the OMAG Server Platform. Further information Configuring the OMAG Server Platform Configuring an OMAG Server Operating the OMAG Server Migrating OMAG Server Configuration Documents Examples of configuration calls \u00b6 The admin-services modules has three Postman collections to illustrate many of the configuration and operation calls: Egeria-admin-services-platform-configuration.postman_environment.json - setting up and configuring the OMAG Server Platform. Egeria-admin-services-server-configuration.postman_environment.json - setting up and configuring the variety of OMAG Servers. Egeria-admin-services-operational.postman_environment.json - operating the OMAG Servers.","title":"Administration Guide"},{"location":"guides/admin/guide/#administration-guide","text":"The Egeria technology principally runs on the Open Metadata and Governance ( OMAG ) Server Platform . This platform hosts one or more OMAG Servers , each supporting a variety of open metadata and governance capabilities. In the illustration above, the OMAG Server Platforms are the blue, rounded boxes and the orange circles are the OMAG Servers. The green clouds represent different cloud platforms or data centers. This guide explains how to configure the OMAG Server Platform and the different types of OMAG Servers that run on it. TL;DR If you are keen to get started right away then these are the links to the configuration instructions. Configuring the OMAG Server Platform Configuring an OMAG Server to run on an OMAG Server Platform Configuring the Presentation Server (for UIs) and once you have your OMAG Servers configured: Operating an OMAG Server","title":"Administration Guide"},{"location":"guides/admin/guide/#omag-subsystems","text":"An OMAG Server is a set of configured OMAG subsystems supported by the OMAG Server Platform . Each subsystem supports a particular type of technology, so it can exchange metadata with the open metadata ecosystem. Some technologies are sources of metadata, others just consume metadata and then there are technologies that have a two-way exchange of metadata with the open metadata ecosystem. The OMAG subsystems that are to be enabled in a specific instance of an OMAG Server are defined in a configuration document . When the configuration document is loaded into the OMAG Server Platform, the OMAG Server that it describes is started, and the subsystems defined in the configuration document are activated for that server.","title":"OMAG subsystems"},{"location":"guides/admin/guide/#omag-servers","text":"In an open metadata landscape, there may be multiple instances of OMAG Servers running in an OMAG Server Platform , each performing a different role. Each of these server instances would have their own configuration document allowing them to have different subsystems active.","title":"OMAG Servers"},{"location":"guides/admin/guide/#configuring-an-omag-server","text":"The configuration document for a specific OMAG Server is identified by the server's name. This is passed on the URL of every admin services API request along with the user id of the administrator. By default, the configuration is stored in a file called: omag.server.{serverName}.config The administration services that set up this file all begin with a URL like this: .../open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/... The serverName specified on these calls determines which configuration document is used, and hence which OMAG Server's configuration it is working with. The OMAG Server Platform typically starts up without any OMAG Servers active. Once it is running, it can be used to set up the configuration documents that describe the open metadata subsystems needed for each OMAG Server instance. Once the configuration document is in place, the OMAG Server can be activated and deactivated multiple times, across multiple restarts of the OMAG Server Platform. Further information Configuring the OMAG Server Platform Configuring an OMAG Server Operating the OMAG Server Migrating OMAG Server Configuration Documents","title":"Configuring an OMAG Server"},{"location":"guides/admin/guide/#examples-of-configuration-calls","text":"The admin-services modules has three Postman collections to illustrate many of the configuration and operation calls: Egeria-admin-services-platform-configuration.postman_environment.json - setting up and configuring the OMAG Server Platform. Egeria-admin-services-server-configuration.postman_environment.json - setting up and configuring the variety of OMAG Servers. Egeria-admin-services-operational.postman_environment.json - operating the OMAG Servers.","title":"Examples of configuration calls"},{"location":"guides/admin/kubernetes/chart_base/","text":"Base Egeria (egeria-base) \u00b6 This is a simple deployment of Egeria which will just deploy a basic Egeria environment which is ready for you to experiment with. Specifically it sets up A single egeria platform which hosts An egeria metadata server with a persistent graph store A new server to support the UI A UI to allow browsing of types, instances & servers Apache Kafka & Zookeeper It does not provide access to our lab notebooks, the Polymer based UI, nor is it preloaded with any data. This chart may also be useful to understand how to deploy Egeria within kubernetes. In future we anticipate providing an operator which will be more flexible Prerequisites \u00b6 In order to use the labs, you'll first need to have the following installed: A Kubernetes cluster at 1.15 or above the kubectl tool in your path Helm 3.0 or above No configuration of the chart is required to use defaults, but information is provided below Installation \u00b6 From one directory level above the location of this README, run the following: helm dep update egeria-base helm install egeria egeria-base THE INSTALL WILL TAKE SEVERAL MINUTES This is because it is not only creating the required objects in Kubernetes to run the platforms, but also is configuring egeria itself - which involves waiting for everything to startup before configuring Egeria via REST API calls. Once installed the configured server is set to start automatically, storage is persisted, and so if your pod gets moved/restarted, egeria should come back automatically with the same data as before. Additional Install Configuration \u00b6 This section is optional - skip over if you're happy with defaults - a good idea to begin with. In a helm chart the configuration that has been externalised by the chart writer is specified in the values.yaml file which you can find in this directory. However rather than edit this file directly, it's recommended you create an additional file with the required overrides. As an example, in values.yaml we see a value 'serverName' which is set to mds1. If I want to override this I could do helm install --set-string egeria.serverName = myserver However this can get tedious with multiple values to override, and you need to know the correct types to use. Instead it may be easier to create an additional file. For example let's create a file in my home directory ~/egeria.yaml containing: egeria: serverName: metadataserver viewServerName: presentationview We can then install the chart with: helm install -f ~/egeria.yaml egeria egeria-base Up to now the installation has been called egeria but we could call it something else ie metadataserver helm install -f ~/egeria.yaml metadataserver egeria-base This is known as a release in Helm, and we can have multiple installed currently. To list these use helm list and to delete both names we've experimented with so far: helm delete metadataserver helm delete egeria Refer to the comments in values.yaml for further information on what can be configured - this includes: - server, organization, cohort names - storage options - k8s storage class/size - Egeria version - Kubernetes service setup (see below also) - roles & accounts - timeouts - names & repositories for docker images used by the chart - Kafka specific configuration (setup in the Bitnami chart we use) Using additional connectors \u00b6 If you have additional Egeria connectors that are needed in your deployment, you should soon be able to include the following when deploying the chart helm install --include-dir libs = ~/libs egeria egeria-base Where in this example ~/libs is the directory including the additional connectors you wish to use. However the support for this is awaiting a helm PR , so in the meantime please copy files directly into a 'libs' directory within the chart instead. For example mkdir egeria-base/libs cp ~/libs/*jar egeria-base/libs These files will be copied into a kubernetes config map, which is then made available as a mounted volume to the runtime image of egeria & added to the class loading path as /extlib . You still need to configure the egeria server(s) appropriately Accessing Egeria \u00b6 When this chart is installed, an initialization job is run to configure the egeria metadata server and UI. For example looking at kubernetes jobs will show something like: $ kubectl get pods [ 17 :27:11 ] NAME READY STATUS RESTARTS AGE egeria-base-config-crhrv 1 /1 Running 0 4m16s egeria-base-platform-0 1 /1 Running 0 4m16s egeria-base-presentation-699669cfd4-9swjb 1 /1 Running 0 4m16s egeria-kafka-0 1 /1 Running 2 4m16s egeria-zookeeper-0 1 /1 Running 0 4m16s You should wait until that first 'egeria-base-config' job completes, it will then disappear from the list. This job issues REST calls against the egeria serves to configure them for this simple environment (see scripts directory). The script will not run again, since we will have now configured the servers with persistent storage, and for the platform to autostart our servers. So even if a pod is removed and restarted, the egeria platform and servers should return in the same state. We now have egeria running within a Kubernetes cluster, but by default no services are exposed externally - they are all of type ClusterIP - we can see these with $ kubectl get services [ 12 :38:16 ] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE egeria-kafka ClusterIP 172 .21.42.105 <none> 9092 /TCP 33m egeria-kafka-headless ClusterIP None <none> 9092 /TCP,9093/TCP 33m egeria-platform ClusterIP 172 .21.40.79 <none> 9443 /TCP 33m egeria-presentation ClusterIP 172 .21.107.242 <none> 8091 /TCP 33m egeria-zookeeper ClusterIP 172 .21.126.56 <none> 2181 /TCP,2888/TCP,3888/TCP 33m egeria-zookeeper-headless ClusterIP None <none> 2181 /TCP,2888/TCP,3888/TCP 33m The egeria-presentation service is very useful to expose as this provides a useful UI where you can explore types and instances. Also egeria-platform is the service for egeria itself. In production you might want this only exposed very carefully to other systems - and not other users or the internet, but for experimenting with egeria let's assume you do. How these are exposed can be somewhat dependent on the specific kubernetes environment you are using. In the lab chart we provided an example of using kubectl port-forward . Here we use RedHat OpenShift in IBM Cloud, where you can expose these services via a LoadBalancer using kubectl expose service/egeria-presentation --type=LoadBalancer --port=8091 --target-port=8091 --name pres kubectl expose service/egeria-platform --type=LoadBalancer --port=9443 --target-port=9443 --name platform If I run these, and then look again at my services I see I now have 2 additional entries (modified for obfuscation): platform LoadBalancer 172.21.218.241 3bc644c3-eu-gb.lb.appdomain.cloud 9443:30640/TCP 25h pres LoadBalancer 172.21.18.10 42b0c7f5-eu-gb.lb.appdomain.cloud 8091:30311/TCP 22h So I can access them at their respective hosts. Note that where hosts are allocated dynamically, it can take up to an hour or more for DNS caches to refresh. Waiting up to 5 minutes then refreshing a local cache has proven sufficient for me, but your experience may differ. In the example above, the Egeria UI can be accessed at https://42b0c7f5-eu-gb.lb.appdomain.cloud:8091/org/ . Replace the hostname accordingly, and also the org is the organization name from the Values.yaml file we referred to above Once logged in, you should be able to login using our demo userid/password of garygeeke/admin & start browsing types and instances within Egeria. You can also issue REST API calls against egeria using a base URL for the platform of https://3bc644c3-eu-gb.lb.appdomain.cloud - Other material covers these REST API calls in more detail, but a simple api doc is available at https://3bc644c3-eu-gb.lb.appdomain.cloud/swagger-ui.html Cleaning up / removing Egeria \u00b6 To delete the deployment, simply run this for Helm3: $ helm delete egeria In addition, the egeria chart makes use of persistent storage. To remove these use $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-egeria-kafka-0 Bound pvc-d31e0012-8568-4e6f-ae5d-94832ebcd92d 10Gi RWO ibmc-vpc-block-10iops-tier 48m data-egeria-zookeeper-0 Bound pvc-90739fce-dce0-422b-8738-a38293d8fdfb 10Gi RWO ibmc-vpc-block-10iops-tier 48m egeria-egeria-data-egeria-base-platform-0 Bound pvc-197ba47e-a6d5-4f35-a7d8-7b7ec1ed1df3 10Gi RWO ibmc-vpc-block-10iops-tier 48m Identify those associated with egeria - which should be obvious from the name and then delete with kubectl delete pvc <id> See the section on Configuration for more details License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Chart base"},{"location":"guides/admin/kubernetes/chart_base/#base-egeria-egeria-base","text":"This is a simple deployment of Egeria which will just deploy a basic Egeria environment which is ready for you to experiment with. Specifically it sets up A single egeria platform which hosts An egeria metadata server with a persistent graph store A new server to support the UI A UI to allow browsing of types, instances & servers Apache Kafka & Zookeeper It does not provide access to our lab notebooks, the Polymer based UI, nor is it preloaded with any data. This chart may also be useful to understand how to deploy Egeria within kubernetes. In future we anticipate providing an operator which will be more flexible","title":"Base Egeria (egeria-base)"},{"location":"guides/admin/kubernetes/chart_base/#prerequisites","text":"In order to use the labs, you'll first need to have the following installed: A Kubernetes cluster at 1.15 or above the kubectl tool in your path Helm 3.0 or above No configuration of the chart is required to use defaults, but information is provided below","title":"Prerequisites"},{"location":"guides/admin/kubernetes/chart_base/#installation","text":"From one directory level above the location of this README, run the following: helm dep update egeria-base helm install egeria egeria-base THE INSTALL WILL TAKE SEVERAL MINUTES This is because it is not only creating the required objects in Kubernetes to run the platforms, but also is configuring egeria itself - which involves waiting for everything to startup before configuring Egeria via REST API calls. Once installed the configured server is set to start automatically, storage is persisted, and so if your pod gets moved/restarted, egeria should come back automatically with the same data as before.","title":"Installation"},{"location":"guides/admin/kubernetes/chart_base/#additional-install-configuration","text":"This section is optional - skip over if you're happy with defaults - a good idea to begin with. In a helm chart the configuration that has been externalised by the chart writer is specified in the values.yaml file which you can find in this directory. However rather than edit this file directly, it's recommended you create an additional file with the required overrides. As an example, in values.yaml we see a value 'serverName' which is set to mds1. If I want to override this I could do helm install --set-string egeria.serverName = myserver However this can get tedious with multiple values to override, and you need to know the correct types to use. Instead it may be easier to create an additional file. For example let's create a file in my home directory ~/egeria.yaml containing: egeria: serverName: metadataserver viewServerName: presentationview We can then install the chart with: helm install -f ~/egeria.yaml egeria egeria-base Up to now the installation has been called egeria but we could call it something else ie metadataserver helm install -f ~/egeria.yaml metadataserver egeria-base This is known as a release in Helm, and we can have multiple installed currently. To list these use helm list and to delete both names we've experimented with so far: helm delete metadataserver helm delete egeria Refer to the comments in values.yaml for further information on what can be configured - this includes: - server, organization, cohort names - storage options - k8s storage class/size - Egeria version - Kubernetes service setup (see below also) - roles & accounts - timeouts - names & repositories for docker images used by the chart - Kafka specific configuration (setup in the Bitnami chart we use)","title":"Additional Install Configuration"},{"location":"guides/admin/kubernetes/chart_base/#using-additional-connectors","text":"If you have additional Egeria connectors that are needed in your deployment, you should soon be able to include the following when deploying the chart helm install --include-dir libs = ~/libs egeria egeria-base Where in this example ~/libs is the directory including the additional connectors you wish to use. However the support for this is awaiting a helm PR , so in the meantime please copy files directly into a 'libs' directory within the chart instead. For example mkdir egeria-base/libs cp ~/libs/*jar egeria-base/libs These files will be copied into a kubernetes config map, which is then made available as a mounted volume to the runtime image of egeria & added to the class loading path as /extlib . You still need to configure the egeria server(s) appropriately","title":"Using additional connectors"},{"location":"guides/admin/kubernetes/chart_base/#accessing-egeria","text":"When this chart is installed, an initialization job is run to configure the egeria metadata server and UI. For example looking at kubernetes jobs will show something like: $ kubectl get pods [ 17 :27:11 ] NAME READY STATUS RESTARTS AGE egeria-base-config-crhrv 1 /1 Running 0 4m16s egeria-base-platform-0 1 /1 Running 0 4m16s egeria-base-presentation-699669cfd4-9swjb 1 /1 Running 0 4m16s egeria-kafka-0 1 /1 Running 2 4m16s egeria-zookeeper-0 1 /1 Running 0 4m16s You should wait until that first 'egeria-base-config' job completes, it will then disappear from the list. This job issues REST calls against the egeria serves to configure them for this simple environment (see scripts directory). The script will not run again, since we will have now configured the servers with persistent storage, and for the platform to autostart our servers. So even if a pod is removed and restarted, the egeria platform and servers should return in the same state. We now have egeria running within a Kubernetes cluster, but by default no services are exposed externally - they are all of type ClusterIP - we can see these with $ kubectl get services [ 12 :38:16 ] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE egeria-kafka ClusterIP 172 .21.42.105 <none> 9092 /TCP 33m egeria-kafka-headless ClusterIP None <none> 9092 /TCP,9093/TCP 33m egeria-platform ClusterIP 172 .21.40.79 <none> 9443 /TCP 33m egeria-presentation ClusterIP 172 .21.107.242 <none> 8091 /TCP 33m egeria-zookeeper ClusterIP 172 .21.126.56 <none> 2181 /TCP,2888/TCP,3888/TCP 33m egeria-zookeeper-headless ClusterIP None <none> 2181 /TCP,2888/TCP,3888/TCP 33m The egeria-presentation service is very useful to expose as this provides a useful UI where you can explore types and instances. Also egeria-platform is the service for egeria itself. In production you might want this only exposed very carefully to other systems - and not other users or the internet, but for experimenting with egeria let's assume you do. How these are exposed can be somewhat dependent on the specific kubernetes environment you are using. In the lab chart we provided an example of using kubectl port-forward . Here we use RedHat OpenShift in IBM Cloud, where you can expose these services via a LoadBalancer using kubectl expose service/egeria-presentation --type=LoadBalancer --port=8091 --target-port=8091 --name pres kubectl expose service/egeria-platform --type=LoadBalancer --port=9443 --target-port=9443 --name platform If I run these, and then look again at my services I see I now have 2 additional entries (modified for obfuscation): platform LoadBalancer 172.21.218.241 3bc644c3-eu-gb.lb.appdomain.cloud 9443:30640/TCP 25h pres LoadBalancer 172.21.18.10 42b0c7f5-eu-gb.lb.appdomain.cloud 8091:30311/TCP 22h So I can access them at their respective hosts. Note that where hosts are allocated dynamically, it can take up to an hour or more for DNS caches to refresh. Waiting up to 5 minutes then refreshing a local cache has proven sufficient for me, but your experience may differ. In the example above, the Egeria UI can be accessed at https://42b0c7f5-eu-gb.lb.appdomain.cloud:8091/org/ . Replace the hostname accordingly, and also the org is the organization name from the Values.yaml file we referred to above Once logged in, you should be able to login using our demo userid/password of garygeeke/admin & start browsing types and instances within Egeria. You can also issue REST API calls against egeria using a base URL for the platform of https://3bc644c3-eu-gb.lb.appdomain.cloud - Other material covers these REST API calls in more detail, but a simple api doc is available at https://3bc644c3-eu-gb.lb.appdomain.cloud/swagger-ui.html","title":"Accessing Egeria"},{"location":"guides/admin/kubernetes/chart_base/#cleaning-up-removing-egeria","text":"To delete the deployment, simply run this for Helm3: $ helm delete egeria In addition, the egeria chart makes use of persistent storage. To remove these use $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-egeria-kafka-0 Bound pvc-d31e0012-8568-4e6f-ae5d-94832ebcd92d 10Gi RWO ibmc-vpc-block-10iops-tier 48m data-egeria-zookeeper-0 Bound pvc-90739fce-dce0-422b-8738-a38293d8fdfb 10Gi RWO ibmc-vpc-block-10iops-tier 48m egeria-egeria-data-egeria-base-platform-0 Bound pvc-197ba47e-a6d5-4f35-a7d8-7b7ec1ed1df3 10Gi RWO ibmc-vpc-block-10iops-tier 48m Identify those associated with egeria - which should be obvious from the name and then delete with kubectl delete pvc <id> See the section on Configuration for more details License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Cleaning up / removing Egeria"},{"location":"guides/admin/kubernetes/chart_lab/","text":"Lab - Coco Pharmaceuticals (odpi-egeria-lab) \u00b6 This example is intended to replicate the metadata landscape of a hypothetical company, Coco Pharmaceuticals, and allow you to understand a little more about how Egeria can help, and how to use it. This forms our 'Hands on Lab'. The Helm chart will deploy the following components, all in a self-contained environment, allowing you to explore Egeria and its concepts safely and repeatably: Multiple Egeria servers Apache Kafka (and its Zookeeper dependency) Example Jupyter Notebooks Jupyter runtime Egeria's React based UI Prerequisites \u00b6 In order to use the labs, you'll first need to have the following installed: Kubernetes 1.15 or above Helm 3.0 or above Egeria chart repository configured & updated 6GB RAM minimum is recommended for your k8s environment. You no longer need a git clone of this repository to install the chart. Installation \u00b6 helm install lab egeria/odpi-egeria-lab In the following examples we're using microk8s. $ helm install lab egeria/odpi-egeria-lab [ 11 :47:04 ] WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /var/snap/microk8s/2346/credentials/client.config NAME: lab LAST DEPLOYED: Tue Aug 10 11 :47:19 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ODPi Egeria lab tutorial --- Some additional help text is also output, which is truncated for brevity. It can take a few seconds for the various components to all spin-up. You can monitor the readiness by running kubectl get all -- when ready, you should see output like the following: $ kubectl get all [ 13 :53:43 ] NAME READY STATUS RESTARTS AGE pod/lab-odpi-egeria-lab-ui-74cc464575-cf8rm 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-datalake-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-nginx-7b96949b4f-7dff4 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-presentation-bd9789747-rbv69 1 /1 Running 0 126m pod/lab-kafka-0 1 /1 Running 0 126m pod/lab-zookeeper-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-dev-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-uistatic-7b98d4bf9b-sf9bj 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-core-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-factory-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-jupyter-77b6868c4-9hnlp 1 /1 Running 0 126m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .152.183.1 <none> 443 /TCP 20h service/lab-kafka-headless ClusterIP None <none> 9092 /TCP,9093/TCP 126m service/lab-zookeeper-headless ClusterIP None <none> 2181 /TCP,2888/TCP,3888/TCP 126m service/lab-zookeeper ClusterIP 10 .152.183.25 <none> 2181 /TCP,2888/TCP,3888/TCP 126m service/lab-jupyter ClusterIP 10 .152.183.172 <none> 8888 /TCP 126m service/lab-core ClusterIP 10 .152.183.248 <none> 9443 /TCP 126m service/lab-nginx ClusterIP 10 .152.183.155 <none> 443 /TCP 126m service/lab-datalake ClusterIP 10 .152.183.191 <none> 9443 /TCP 126m service/lab-dev ClusterIP 10 .152.183.122 <none> 9443 /TCP 126m service/lab-kafka ClusterIP 10 .152.183.79 <none> 9092 /TCP 126m service/lab-odpi-egeria-lab-factory ClusterIP 10 .152.183.158 <none> 9443 /TCP 126m service/lab-uistatic ClusterIP 10 .152.183.156 <none> 80 /TCP 126m service/lab-presentation ClusterIP 10 .152.183.61 <none> 8091 /TCP 126m service/lab-ui ClusterIP 10 .152.183.186 <none> 8443 /TCP 126m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/lab-odpi-egeria-lab-presentation 1 /1 1 1 126m deployment.apps/lab-odpi-egeria-lab-nginx 1 /1 1 1 126m deployment.apps/lab-odpi-egeria-lab-ui 1 /1 1 1 126m deployment.apps/lab-odpi-egeria-lab-uistatic 1 /1 1 1 126m deployment.apps/lab-odpi-egeria-lab-jupyter 1 /1 1 1 126m NAME DESIRED CURRENT READY AGE replicaset.apps/lab-odpi-egeria-lab-presentation-bd9789747 1 1 1 126m replicaset.apps/lab-odpi-egeria-lab-nginx-7b96949b4f 1 1 1 126m replicaset.apps/lab-odpi-egeria-lab-ui-74cc464575 1 1 1 126m replicaset.apps/lab-odpi-egeria-lab-uistatic-7b98d4bf9b 1 1 1 126m replicaset.apps/lab-odpi-egeria-lab-jupyter-77b6868c4 1 1 1 126m NAME READY AGE statefulset.apps/lab-odpi-egeria-lab-dev 1 /1 126m statefulset.apps/lab-zookeeper 1 /1 126m statefulset.apps/lab-kafka 1 /1 126m statefulset.apps/lab-odpi-egeria-lab-core 1 /1 126m statefulset.apps/lab-odpi-egeria-lab-datalake 1 /1 126m statefulset.apps/lab-odpi-egeria-lab-factory 1 /1 126m ```` All of the ` pod/... ` listed at the top have ` Running ` as their ` STATUS ` and ` 1 /1 ` under ` READY ` . ) ## Accessing the Jupyter notebooks We now need to get connectivity to the interesting pods, such as that running the Jupyer notebook server, as this is where you 'll get to the examples. Since k8s implementations vary one simple approach for local testing is to use `kubectl port-forward` to connect to the relevant service. If you look in the list of services above (`kubectl get services`) we have one named ' service/lab-jupyter ' so let' s try that ( with microk8s ) : ``` shell $ kubectl port-forward service/lab-jupyter 8888 :8888 [ 13 :53:52 ] Forwarding from 127 .0.0.1:8888 -> 8888 Forwarding from [ ::1 ] :8888 -> 8888 This command will not return - the port forwarding is active whilst it's running. In this example we're forwarding all requests to local port 8888 to the k8s service for jupyter At this point you should be able to access your notebooks by going to this forwarded port ie 'http://localhost:8888' Accessing the React UI \u00b6 We repeat the port forwarding above, this time for another service $ kubectl port-forward service/lab-presentation 8091 :8091 [ 14 :15:37 ] Forwarding from 127 .0.0.1:8091 -> 8091 Forwarding from [ ::1 ] :8091 -> 8091 As before, you can define an Ingress, or use nodeports instead if preferred. Now go to https://localhost:8091/coco to access the React UI. Login as 'garygeeke',password 'admin'. Starting over \u00b6 Because the environment is entirely self-contained, you can easily start over the labs simply by deleting the deployment and running the installation again. This will wipe out all of the metadata across the lab Egeria servers, remove all messages from the Kafka bus used in the cohort, reset the Jupyter notebooks to their original clean state, etc. To delete the deployment, simply run this: $ helm delete lab Where lab is the name you used in your original deployment. (You can see what it's called by first running helm list and reviewing the output.) (Then just re-run the last command in the Installation section above to get a fresh environment.) Overriding Configuration \u00b6 The chart is configured to use a default set of parameters. You can override these by creating a file such as lab.yaml with the contents of any values you wish to modify, for example: service: type: NodePort nodeport: jupyter: 30888 core: 30080 datalake: 30081 dev: 30082 factory: 30083 ui: 30443 Refer to the existing values file for additional ports in this section that may reflect new components as added You can then deploy using helm install lab odpi-egeria-lab -f lab.yaml which will override standard defaults with your choices Enabling persistence \u00b6 Support has been added to use persistence in these charts. See 'values.yaml' for more information on this option. You may also wish to refer to the 'egeria-base' helm chart which is a deployment of a single, persistent, autostart server with UI. Note however that since this will save the state of your configuration done from the tutorial notebooks it may be confusing - as such this is disabled by default. It may be useful if you are modifying the charts for your own use. You will also need to delete all storage associated with the chart manually if you want to cleanup/reset - for example kubectl delete pvc --all kubectl delete pv --all License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Chart lab"},{"location":"guides/admin/kubernetes/chart_lab/#lab-coco-pharmaceuticals-odpi-egeria-lab","text":"This example is intended to replicate the metadata landscape of a hypothetical company, Coco Pharmaceuticals, and allow you to understand a little more about how Egeria can help, and how to use it. This forms our 'Hands on Lab'. The Helm chart will deploy the following components, all in a self-contained environment, allowing you to explore Egeria and its concepts safely and repeatably: Multiple Egeria servers Apache Kafka (and its Zookeeper dependency) Example Jupyter Notebooks Jupyter runtime Egeria's React based UI","title":"Lab - Coco Pharmaceuticals (odpi-egeria-lab)"},{"location":"guides/admin/kubernetes/chart_lab/#prerequisites","text":"In order to use the labs, you'll first need to have the following installed: Kubernetes 1.15 or above Helm 3.0 or above Egeria chart repository configured & updated 6GB RAM minimum is recommended for your k8s environment. You no longer need a git clone of this repository to install the chart.","title":"Prerequisites"},{"location":"guides/admin/kubernetes/chart_lab/#installation","text":"helm install lab egeria/odpi-egeria-lab In the following examples we're using microk8s. $ helm install lab egeria/odpi-egeria-lab [ 11 :47:04 ] WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /var/snap/microk8s/2346/credentials/client.config NAME: lab LAST DEPLOYED: Tue Aug 10 11 :47:19 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ODPi Egeria lab tutorial --- Some additional help text is also output, which is truncated for brevity. It can take a few seconds for the various components to all spin-up. You can monitor the readiness by running kubectl get all -- when ready, you should see output like the following: $ kubectl get all [ 13 :53:43 ] NAME READY STATUS RESTARTS AGE pod/lab-odpi-egeria-lab-ui-74cc464575-cf8rm 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-datalake-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-nginx-7b96949b4f-7dff4 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-presentation-bd9789747-rbv69 1 /1 Running 0 126m pod/lab-kafka-0 1 /1 Running 0 126m pod/lab-zookeeper-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-dev-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-uistatic-7b98d4bf9b-sf9bj 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-core-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-factory-0 1 /1 Running 0 126m pod/lab-odpi-egeria-lab-jupyter-77b6868c4-9hnlp 1 /1 Running 0 126m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 10 .152.183.1 <none> 443 /TCP 20h service/lab-kafka-headless ClusterIP None <none> 9092 /TCP,9093/TCP 126m service/lab-zookeeper-headless ClusterIP None <none> 2181 /TCP,2888/TCP,3888/TCP 126m service/lab-zookeeper ClusterIP 10 .152.183.25 <none> 2181 /TCP,2888/TCP,3888/TCP 126m service/lab-jupyter ClusterIP 10 .152.183.172 <none> 8888 /TCP 126m service/lab-core ClusterIP 10 .152.183.248 <none> 9443 /TCP 126m service/lab-nginx ClusterIP 10 .152.183.155 <none> 443 /TCP 126m service/lab-datalake ClusterIP 10 .152.183.191 <none> 9443 /TCP 126m service/lab-dev ClusterIP 10 .152.183.122 <none> 9443 /TCP 126m service/lab-kafka ClusterIP 10 .152.183.79 <none> 9092 /TCP 126m service/lab-odpi-egeria-lab-factory ClusterIP 10 .152.183.158 <none> 9443 /TCP 126m service/lab-uistatic ClusterIP 10 .152.183.156 <none> 80 /TCP 126m service/lab-presentation ClusterIP 10 .152.183.61 <none> 8091 /TCP 126m service/lab-ui ClusterIP 10 .152.183.186 <none> 8443 /TCP 126m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/lab-odpi-egeria-lab-presentation 1 /1 1 1 126m deployment.apps/lab-odpi-egeria-lab-nginx 1 /1 1 1 126m deployment.apps/lab-odpi-egeria-lab-ui 1 /1 1 1 126m deployment.apps/lab-odpi-egeria-lab-uistatic 1 /1 1 1 126m deployment.apps/lab-odpi-egeria-lab-jupyter 1 /1 1 1 126m NAME DESIRED CURRENT READY AGE replicaset.apps/lab-odpi-egeria-lab-presentation-bd9789747 1 1 1 126m replicaset.apps/lab-odpi-egeria-lab-nginx-7b96949b4f 1 1 1 126m replicaset.apps/lab-odpi-egeria-lab-ui-74cc464575 1 1 1 126m replicaset.apps/lab-odpi-egeria-lab-uistatic-7b98d4bf9b 1 1 1 126m replicaset.apps/lab-odpi-egeria-lab-jupyter-77b6868c4 1 1 1 126m NAME READY AGE statefulset.apps/lab-odpi-egeria-lab-dev 1 /1 126m statefulset.apps/lab-zookeeper 1 /1 126m statefulset.apps/lab-kafka 1 /1 126m statefulset.apps/lab-odpi-egeria-lab-core 1 /1 126m statefulset.apps/lab-odpi-egeria-lab-datalake 1 /1 126m statefulset.apps/lab-odpi-egeria-lab-factory 1 /1 126m ```` All of the ` pod/... ` listed at the top have ` Running ` as their ` STATUS ` and ` 1 /1 ` under ` READY ` . ) ## Accessing the Jupyter notebooks We now need to get connectivity to the interesting pods, such as that running the Jupyer notebook server, as this is where you 'll get to the examples. Since k8s implementations vary one simple approach for local testing is to use `kubectl port-forward` to connect to the relevant service. If you look in the list of services above (`kubectl get services`) we have one named ' service/lab-jupyter ' so let' s try that ( with microk8s ) : ``` shell $ kubectl port-forward service/lab-jupyter 8888 :8888 [ 13 :53:52 ] Forwarding from 127 .0.0.1:8888 -> 8888 Forwarding from [ ::1 ] :8888 -> 8888 This command will not return - the port forwarding is active whilst it's running. In this example we're forwarding all requests to local port 8888 to the k8s service for jupyter At this point you should be able to access your notebooks by going to this forwarded port ie 'http://localhost:8888'","title":"Installation"},{"location":"guides/admin/kubernetes/chart_lab/#accessing-the-react-ui","text":"We repeat the port forwarding above, this time for another service $ kubectl port-forward service/lab-presentation 8091 :8091 [ 14 :15:37 ] Forwarding from 127 .0.0.1:8091 -> 8091 Forwarding from [ ::1 ] :8091 -> 8091 As before, you can define an Ingress, or use nodeports instead if preferred. Now go to https://localhost:8091/coco to access the React UI. Login as 'garygeeke',password 'admin'.","title":"Accessing the React UI"},{"location":"guides/admin/kubernetes/chart_lab/#starting-over","text":"Because the environment is entirely self-contained, you can easily start over the labs simply by deleting the deployment and running the installation again. This will wipe out all of the metadata across the lab Egeria servers, remove all messages from the Kafka bus used in the cohort, reset the Jupyter notebooks to their original clean state, etc. To delete the deployment, simply run this: $ helm delete lab Where lab is the name you used in your original deployment. (You can see what it's called by first running helm list and reviewing the output.) (Then just re-run the last command in the Installation section above to get a fresh environment.)","title":"Starting over"},{"location":"guides/admin/kubernetes/chart_lab/#overriding-configuration","text":"The chart is configured to use a default set of parameters. You can override these by creating a file such as lab.yaml with the contents of any values you wish to modify, for example: service: type: NodePort nodeport: jupyter: 30888 core: 30080 datalake: 30081 dev: 30082 factory: 30083 ui: 30443 Refer to the existing values file for additional ports in this section that may reflect new components as added You can then deploy using helm install lab odpi-egeria-lab -f lab.yaml which will override standard defaults with your choices","title":"Overriding Configuration"},{"location":"guides/admin/kubernetes/chart_lab/#enabling-persistence","text":"Support has been added to use persistence in these charts. See 'values.yaml' for more information on this option. You may also wish to refer to the 'egeria-base' helm chart which is a deployment of a single, persistent, autostart server with UI. Note however that since this will save the state of your configuration done from the tutorial notebooks it may be confusing - as such this is disabled by default. It may be useful if you are modifying the charts for your own use. You will also need to delete all storage associated with the chart manually if you want to cleanup/reset - for example kubectl delete pvc --all kubectl delete pv --all License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Enabling persistence"},{"location":"guides/admin/kubernetes/container-images/","text":"Container Images \u00b6 This section will list the current images we produce for Egeria to aid those who wish to setup their own container-based deployment of Egeria. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Container images"},{"location":"guides/admin/kubernetes/container-images/#container-images","text":"This section will list the current images we produce for Egeria to aid those who wish to setup their own container-based deployment of Egeria. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Container Images"},{"location":"guides/admin/kubernetes/custom_deployment/","text":"Creating your own Helm chart and deployment \u00b6 This section will add useful tips on extending the provided charts to deploy Egeria in your own environment. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Custom deployment"},{"location":"guides/admin/kubernetes/custom_deployment/#creating-your-own-helm-chart-and-deployment","text":"This section will add useful tips on extending the provided charts to deploy Egeria in your own environment. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Creating your own Helm chart and deployment"},{"location":"guides/admin/kubernetes/helm/","text":"Helm \u00b6 Helm is the best way to find, share, and use software built for Kubernetes. - https://helm.sh Deploying apps in Kubernetes \u00b6 In Kubernetes, resources such as pods (to run code) or services (for network accessibility) are defined in yaml. One or more documents can be submitted at a time. So we might have one yaml file that defines our pod - with information about which container images to run, and another to setup a network service. finally we may have another that describes our storage requirements (a persistent volume claim ) What does Helm do? \u00b6 Helm provides a way of bunding yaml files together into an archive, together with a templating mechanism to allow reuse of common patterns & ensure different yamls are consistent and can reference each other. These archives are known as 'Charts' and can be hosted in a known format as a 'chart repository'. The archives are versioned. Helm is basically focussed on creating yaml documents that are submitted to Kubernetes - it is not involved in the runtime of a Kubernetes environment. Once a helm app is installed, interaction is just with the regular kubernetes objects. Helm Commands include options to * install * uninstall * list * search * upgrade * rollback helm under microk8s \u00b6 microk8s qualifies the Helm command helm in that you need to use microk8s helm3 so either * When docs refer you to type helm then just use microk8s helm3 * add a shell alias ie alias helm='microk8s helm3' into ~/.zshrc or equivilent shell startup script - if this doesn't clash with your other usage of k8s. If using this approach you not need to explicitly install Helm. Installing Helm \u00b6 Some Kubernetes environments may install helm as part of their client tooling, refer to the docs to see if this is the case, and run helm version to check - expect to use v3 or above. If so, install can be skipped. MacOS \u00b6 If using macOS with HomeBrew installed, helm can be simply installed with brew install helm Other platforms (Linux, Windows) \u00b6 See the Installation Guide for more ways to install Helm Accessing the egeria charts repository \u00b6 Our helm charts for Egeria are stored in a repository hosted on GitHub. The source for these is at https://github.com/odpi/egeria-charts , and as charts are updated they are automatically published to a GitHub pages Website (in fact this one!) Run the following to add this repository helm repo add egeria https://odpi.github.io/egeria-charts Before searching or installing, always update your local copy of the repository helm repo update egeria You can now list released charts: helm search repo egeria or development charts (being worked on, or using code from master) helm search repo egeria --devel and install a chart that looks interesting - helm install egeria/<chart> License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Helm"},{"location":"guides/admin/kubernetes/helm/#helm","text":"Helm is the best way to find, share, and use software built for Kubernetes. - https://helm.sh","title":"Helm"},{"location":"guides/admin/kubernetes/helm/#deploying-apps-in-kubernetes","text":"In Kubernetes, resources such as pods (to run code) or services (for network accessibility) are defined in yaml. One or more documents can be submitted at a time. So we might have one yaml file that defines our pod - with information about which container images to run, and another to setup a network service. finally we may have another that describes our storage requirements (a persistent volume claim )","title":"Deploying apps in Kubernetes"},{"location":"guides/admin/kubernetes/helm/#what-does-helm-do","text":"Helm provides a way of bunding yaml files together into an archive, together with a templating mechanism to allow reuse of common patterns & ensure different yamls are consistent and can reference each other. These archives are known as 'Charts' and can be hosted in a known format as a 'chart repository'. The archives are versioned. Helm is basically focussed on creating yaml documents that are submitted to Kubernetes - it is not involved in the runtime of a Kubernetes environment. Once a helm app is installed, interaction is just with the regular kubernetes objects. Helm Commands include options to * install * uninstall * list * search * upgrade * rollback","title":"What does Helm do?"},{"location":"guides/admin/kubernetes/helm/#helm-under-microk8s","text":"microk8s qualifies the Helm command helm in that you need to use microk8s helm3 so either * When docs refer you to type helm then just use microk8s helm3 * add a shell alias ie alias helm='microk8s helm3' into ~/.zshrc or equivilent shell startup script - if this doesn't clash with your other usage of k8s. If using this approach you not need to explicitly install Helm.","title":"helm under microk8s"},{"location":"guides/admin/kubernetes/helm/#installing-helm","text":"Some Kubernetes environments may install helm as part of their client tooling, refer to the docs to see if this is the case, and run helm version to check - expect to use v3 or above. If so, install can be skipped.","title":"Installing Helm"},{"location":"guides/admin/kubernetes/helm/#macos","text":"If using macOS with HomeBrew installed, helm can be simply installed with brew install helm","title":"MacOS"},{"location":"guides/admin/kubernetes/helm/#other-platforms-linux-windows","text":"See the Installation Guide for more ways to install Helm","title":"Other platforms (Linux, Windows)"},{"location":"guides/admin/kubernetes/helm/#accessing-the-egeria-charts-repository","text":"Our helm charts for Egeria are stored in a repository hosted on GitHub. The source for these is at https://github.com/odpi/egeria-charts , and as charts are updated they are automatically published to a GitHub pages Website (in fact this one!) Run the following to add this repository helm repo add egeria https://odpi.github.io/egeria-charts Before searching or installing, always update your local copy of the repository helm repo update egeria You can now list released charts: helm search repo egeria or development charts (being worked on, or using code from master) helm search repo egeria --devel and install a chart that looks interesting - helm install egeria/<chart> License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing the egeria charts repository"},{"location":"guides/admin/kubernetes/intro/","text":"Egeria & Kubernetes \u00b6 Kubernetes offers one standard way of deploying the Egeria platform into a variety of environments including a developer desktop, or a public cloud environment: Introduction to Kubernetes Introduction to Helm Sample chart - the Coco Pharmaceuticals lab Sample chart - base egeria setup Container images Developing a custom deployment Egeria Operator License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Egeria in Kubernetes"},{"location":"guides/admin/kubernetes/intro/#egeria-kubernetes","text":"Kubernetes offers one standard way of deploying the Egeria platform into a variety of environments including a developer desktop, or a public cloud environment: Introduction to Kubernetes Introduction to Helm Sample chart - the Coco Pharmaceuticals lab Sample chart - base egeria setup Container images Developing a custom deployment Egeria Operator License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Egeria &amp; Kubernetes"},{"location":"guides/admin/kubernetes/k8s/","text":"What is Kubernetes? \u00b6 Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. - https://kubernetes.io This is how the official website describes it. It's effectively a standardized way of deploying applications in a very scalable way - from everything such as development prototyping through to massive highly available enterprise solutions. In this document I'll give a very brief summary that should help those of you new to Kubernetes to make your first steps with Egeria. What are the key concepts in Kubernetes? \u00b6 These are just some of the concepts that can help to understand what's going on. This isn't a complete list. Api \u00b6 Kubernetes using a standard API which is oriented around manipulating Objects. The commands are therefore very standard, it's all about the objects. Making it so \u00b6 The system is always observing the state of the system through these objects, and where there are discrepancies, taking action to 'Make it So' as Captain Picard would say. The approach is imperitive. So we think of these objects as describing the desired state of the system. Namespace \u00b6 A namespace provides a way of seperating out kubernetes resources by users or applications as a convenience. It keeps names more understandable, and avoids global duplicates. For example a developer working on a k8s cluster may have a namespace of their own to experiment in. Container \u00b6 A Container is what runs stuff. It's similar to a Virtual Machine in some ways, but much more lightweight. Containers use code Images which may be custom built, or very standard off-the-shelf reusable items. Containers are typically very focussed on a single need or application. Pod \u00b6 A Pod is a single group of one or more containers. Typically a single main container runs in a pod, but this may be supported by additional containers for log, audit, security, initialization etc. Think of this as an atomic unit that can run a workload. Pods are disposeable - they will come and go. Other objects are concerned with providing a reliable service. Service \u00b6 A service provides network accessibility to one or more pods. The service name will be added into local Domain Name Service (DNS) for easy accessibility from other pods. Load can be shared across multiple pods Ingres \u00b6 Think of Ingress as the entry point to Kubernetes services from an external network perspective - so it is these addresses external users would be aware of. Deployment \u00b6 A deployment keeps a set of pods running - including replica copies, ie restarted if stopped, matching resource requirements, handling node failure . Stateful Set \u00b6 A stateful set goes further than a deployment in that it keeps a well known identifier for each identical replica. This helps in allocating persistent storage & network resources to a replica ConfigMap \u00b6 A config map is a way of keeping configuration (exposed as files or environment variables) seperate to an application. Secret \u00b6 A secret is used to keep information secret, as the name might suggest ... This might be a password or an API key & the data is encrypted Custom Objects \u00b6 In addition to this list -- and many more covered in the official documentation -- Kubernetes also supports custom resources. These form a key part of Kubernetes Operators . Storage \u00b6 Pods can request storage - which is known as a persistent volume claim (PVC), which are either manually or automatically resolved to a persistent volume. See the k8s docs Persistent Volumes Why are we using Kubernetes? \u00b6 All sizes of systems can run kubernetes applications - from a small raspberry pi through desktops and workstations through to huge cloud deployments. Whilst the details around storage, security, networking etc do vary by implementation, the core concepts, and configurations work across all. Some may be more concerned about an easy way to play with development code, try out new ideas, whilst at the far end of the spectrum enterprises want something super scalable and reliable, and easy to monitor. For egeria we want to achieve two main things * Provide easy to use demos and tutorials that show how Egeria can be used and worked with without requiring too much complex setup. * Provide examples that show how Egeria can be deployed in k8s, and then adapted for the organization's needs. Other alternatives that might come to mind include * Docker -- whilst simple, this is more geared around running a single container, and building complex environment means a lot of work combining application stacks together, often resulting in something that isn't usable. We do of course still have container images, which are essential to k8s, but these are simple & self contained. * docker-compose -- this builds on docker in allowing multiple containers and servers to be orchestrated, but it's much less flexible & scalable than kubernetes. How do I get access to Kubernetes? \u00b6 Getting Started provides links to setting up Kubernetes in many environments. Below we'll take a quick look at some of the simpler examples, especially for new users. microk8s (Linux, Windows, macOS) \u00b6 Official microk8s site 4GB is recommended as a minimum memory requirement. As with most k8s implementations, when running some ongoing cpu will be used, so if running on your laptop/low power device it's recommended to refer to the relevant docs & stop k8s when not in use. When running on a separate server or a cloud service this isn't a concern. When using microk8s, note that the standard k8s commands are renamed to avoid clashes, so use the microk8s ones in the remainder of the Egeria documentation kubectl -> microk8s kubectl helm -> microk8s helm They can also be aliased on some platforms. MacOS \u00b6 The macos install docs cover the steps needed to install microk8s. Most of the Egeria development team use MacOS, so the instructions are elaborated and qualified here: The recommended approach uses HomeBrew . This offers a suite of tools often found on linux which are easy to setup on macOS. See install docs IMPORTANT: Before installing, go into System Preferences->Security & Privacy. Click the lock to get into Admin mode. Then ensure Firewall Options->Enable Stealth Mode is NOT enabled (no tick). If it is, microk8s will not work properly . More If you are concerned over the firewall change, or homebrew requirement, refer back to the official k8s documentation & choose another k8s implementation that works for you. Ensure you turn on the following services: storage, dns, helm3 . dashboard is also useful to understand more about k8s and what is running. However it is currently failing as described in issue 2507 As an example, the following commands should get you set up, but always check the official docs for current details brew install ubuntu/microk8s/microk8s microk8s install microk8s status --wait-ready microk8s enable dns storage helm3 microk8s kubectl get all --all-namespaces Kubernetes is now running. Windows \u00b6 Follow the official instructions (untested) Linux \u00b6 Follow the official instructions (untested) kubectl command under microk8s \u00b6 microk8s qualifies the core k8s command kubectl in that you need to use microk8s kubectl so either * When docs refer you to type kubectl then just use microk8s kubectl * add a shell alias ie alias kubectl='microk8s kubectl' into ~/.zshrc or equivilent shell startup script Docker Desktop (Windows, macOS) \u00b6 Docker Desktop supports Kubernetes After installing, go into Docker Desktop 'settings and select 'Kubernetes'. Make sure 'Enable Kubernetes' is checked. Also under resources ensure at least 4GB is allocated to Docker Cloud \u00b6 Many cloud providers offer Kubernetes deployments which can be used for experimentation or production. This include Redhat OpenShift on multiple cloud providers including on IBMCloud Kubernetes on IBMCloud Azure Kubernetes Service Google Kubernetes Engine (GKE) In addition to a cloud install, ensure you have installed the relevant cloud provider's tooling to manage their k8s environment, including having access to the standard kubernetes command kubectl . Note that in the team's testing we mostly are running Redhat OpenShift on IBMCloud as a managed service. We welcome feedback of running our examples on other environments, especially as some of the specifics around ingress rules, storage, security can vary. Accessing applications in your cluster \u00b6 See also kubernetes docs NodePort \u00b6 In the sample charts provided an option to use a NodePort is usually provided. This is often easiest when running k8s locally, as any of the ip addressible worker nodes in your cluster can service a request on the port provided. This is why it's named a 'node port' ie a port on your node.. kubectl port-forward \u00b6 This can be run at a command line, and directly sets up forwarding from local ports into services running in your cluster. It requires no additional configuration beforehand, and lasts only as long as the port forwarding is running. See port forwarding for more info Ingress \u00b6 Ingress rules define how traffic directed at your k8s cluster is directed. Their definition tends to vary substantially between different k8s implementations but often is the easiest approach when running with a cloud service. * microk8s ingress \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"K8s"},{"location":"guides/admin/kubernetes/k8s/#what-is-kubernetes","text":"Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. - https://kubernetes.io This is how the official website describes it. It's effectively a standardized way of deploying applications in a very scalable way - from everything such as development prototyping through to massive highly available enterprise solutions. In this document I'll give a very brief summary that should help those of you new to Kubernetes to make your first steps with Egeria.","title":"What is Kubernetes?"},{"location":"guides/admin/kubernetes/k8s/#what-are-the-key-concepts-in-kubernetes","text":"These are just some of the concepts that can help to understand what's going on. This isn't a complete list.","title":"What are the key concepts in Kubernetes?"},{"location":"guides/admin/kubernetes/k8s/#api","text":"Kubernetes using a standard API which is oriented around manipulating Objects. The commands are therefore very standard, it's all about the objects.","title":"Api"},{"location":"guides/admin/kubernetes/k8s/#making-it-so","text":"The system is always observing the state of the system through these objects, and where there are discrepancies, taking action to 'Make it So' as Captain Picard would say. The approach is imperitive. So we think of these objects as describing the desired state of the system.","title":"Making it so"},{"location":"guides/admin/kubernetes/k8s/#namespace","text":"A namespace provides a way of seperating out kubernetes resources by users or applications as a convenience. It keeps names more understandable, and avoids global duplicates. For example a developer working on a k8s cluster may have a namespace of their own to experiment in.","title":"Namespace"},{"location":"guides/admin/kubernetes/k8s/#container","text":"A Container is what runs stuff. It's similar to a Virtual Machine in some ways, but much more lightweight. Containers use code Images which may be custom built, or very standard off-the-shelf reusable items. Containers are typically very focussed on a single need or application.","title":"Container"},{"location":"guides/admin/kubernetes/k8s/#pod","text":"A Pod is a single group of one or more containers. Typically a single main container runs in a pod, but this may be supported by additional containers for log, audit, security, initialization etc. Think of this as an atomic unit that can run a workload. Pods are disposeable - they will come and go. Other objects are concerned with providing a reliable service.","title":"Pod"},{"location":"guides/admin/kubernetes/k8s/#service","text":"A service provides network accessibility to one or more pods. The service name will be added into local Domain Name Service (DNS) for easy accessibility from other pods. Load can be shared across multiple pods","title":"Service"},{"location":"guides/admin/kubernetes/k8s/#ingres","text":"Think of Ingress as the entry point to Kubernetes services from an external network perspective - so it is these addresses external users would be aware of.","title":"Ingres"},{"location":"guides/admin/kubernetes/k8s/#deployment","text":"A deployment keeps a set of pods running - including replica copies, ie restarted if stopped, matching resource requirements, handling node failure .","title":"Deployment"},{"location":"guides/admin/kubernetes/k8s/#stateful-set","text":"A stateful set goes further than a deployment in that it keeps a well known identifier for each identical replica. This helps in allocating persistent storage & network resources to a replica","title":"Stateful Set"},{"location":"guides/admin/kubernetes/k8s/#configmap","text":"A config map is a way of keeping configuration (exposed as files or environment variables) seperate to an application.","title":"ConfigMap"},{"location":"guides/admin/kubernetes/k8s/#secret","text":"A secret is used to keep information secret, as the name might suggest ... This might be a password or an API key & the data is encrypted","title":"Secret"},{"location":"guides/admin/kubernetes/k8s/#custom-objects","text":"In addition to this list -- and many more covered in the official documentation -- Kubernetes also supports custom resources. These form a key part of Kubernetes Operators .","title":"Custom Objects"},{"location":"guides/admin/kubernetes/k8s/#storage","text":"Pods can request storage - which is known as a persistent volume claim (PVC), which are either manually or automatically resolved to a persistent volume. See the k8s docs Persistent Volumes","title":"Storage"},{"location":"guides/admin/kubernetes/k8s/#why-are-we-using-kubernetes","text":"All sizes of systems can run kubernetes applications - from a small raspberry pi through desktops and workstations through to huge cloud deployments. Whilst the details around storage, security, networking etc do vary by implementation, the core concepts, and configurations work across all. Some may be more concerned about an easy way to play with development code, try out new ideas, whilst at the far end of the spectrum enterprises want something super scalable and reliable, and easy to monitor. For egeria we want to achieve two main things * Provide easy to use demos and tutorials that show how Egeria can be used and worked with without requiring too much complex setup. * Provide examples that show how Egeria can be deployed in k8s, and then adapted for the organization's needs. Other alternatives that might come to mind include * Docker -- whilst simple, this is more geared around running a single container, and building complex environment means a lot of work combining application stacks together, often resulting in something that isn't usable. We do of course still have container images, which are essential to k8s, but these are simple & self contained. * docker-compose -- this builds on docker in allowing multiple containers and servers to be orchestrated, but it's much less flexible & scalable than kubernetes.","title":"Why are we using Kubernetes?"},{"location":"guides/admin/kubernetes/k8s/#how-do-i-get-access-to-kubernetes","text":"Getting Started provides links to setting up Kubernetes in many environments. Below we'll take a quick look at some of the simpler examples, especially for new users.","title":"How do I get access to Kubernetes?"},{"location":"guides/admin/kubernetes/k8s/#microk8s-linux-windows-macos","text":"Official microk8s site 4GB is recommended as a minimum memory requirement. As with most k8s implementations, when running some ongoing cpu will be used, so if running on your laptop/low power device it's recommended to refer to the relevant docs & stop k8s when not in use. When running on a separate server or a cloud service this isn't a concern. When using microk8s, note that the standard k8s commands are renamed to avoid clashes, so use the microk8s ones in the remainder of the Egeria documentation kubectl -> microk8s kubectl helm -> microk8s helm They can also be aliased on some platforms.","title":"microk8s (Linux, Windows, macOS)"},{"location":"guides/admin/kubernetes/k8s/#macos","text":"The macos install docs cover the steps needed to install microk8s. Most of the Egeria development team use MacOS, so the instructions are elaborated and qualified here: The recommended approach uses HomeBrew . This offers a suite of tools often found on linux which are easy to setup on macOS. See install docs IMPORTANT: Before installing, go into System Preferences->Security & Privacy. Click the lock to get into Admin mode. Then ensure Firewall Options->Enable Stealth Mode is NOT enabled (no tick). If it is, microk8s will not work properly . More If you are concerned over the firewall change, or homebrew requirement, refer back to the official k8s documentation & choose another k8s implementation that works for you. Ensure you turn on the following services: storage, dns, helm3 . dashboard is also useful to understand more about k8s and what is running. However it is currently failing as described in issue 2507 As an example, the following commands should get you set up, but always check the official docs for current details brew install ubuntu/microk8s/microk8s microk8s install microk8s status --wait-ready microk8s enable dns storage helm3 microk8s kubectl get all --all-namespaces Kubernetes is now running.","title":"MacOS"},{"location":"guides/admin/kubernetes/k8s/#windows","text":"Follow the official instructions (untested)","title":"Windows"},{"location":"guides/admin/kubernetes/k8s/#linux","text":"Follow the official instructions (untested)","title":"Linux"},{"location":"guides/admin/kubernetes/k8s/#kubectl-command-under-microk8s","text":"microk8s qualifies the core k8s command kubectl in that you need to use microk8s kubectl so either * When docs refer you to type kubectl then just use microk8s kubectl * add a shell alias ie alias kubectl='microk8s kubectl' into ~/.zshrc or equivilent shell startup script","title":"kubectl command under microk8s"},{"location":"guides/admin/kubernetes/k8s/#docker-desktop-windows-macos","text":"Docker Desktop supports Kubernetes After installing, go into Docker Desktop 'settings and select 'Kubernetes'. Make sure 'Enable Kubernetes' is checked. Also under resources ensure at least 4GB is allocated to Docker","title":"Docker Desktop (Windows, macOS)"},{"location":"guides/admin/kubernetes/k8s/#cloud","text":"Many cloud providers offer Kubernetes deployments which can be used for experimentation or production. This include Redhat OpenShift on multiple cloud providers including on IBMCloud Kubernetes on IBMCloud Azure Kubernetes Service Google Kubernetes Engine (GKE) In addition to a cloud install, ensure you have installed the relevant cloud provider's tooling to manage their k8s environment, including having access to the standard kubernetes command kubectl . Note that in the team's testing we mostly are running Redhat OpenShift on IBMCloud as a managed service. We welcome feedback of running our examples on other environments, especially as some of the specifics around ingress rules, storage, security can vary.","title":"Cloud"},{"location":"guides/admin/kubernetes/k8s/#accessing-applications-in-your-cluster","text":"See also kubernetes docs","title":"Accessing applications in your cluster"},{"location":"guides/admin/kubernetes/k8s/#nodeport","text":"In the sample charts provided an option to use a NodePort is usually provided. This is often easiest when running k8s locally, as any of the ip addressible worker nodes in your cluster can service a request on the port provided. This is why it's named a 'node port' ie a port on your node..","title":"NodePort"},{"location":"guides/admin/kubernetes/k8s/#kubectl-port-forward","text":"This can be run at a command line, and directly sets up forwarding from local ports into services running in your cluster. It requires no additional configuration beforehand, and lasts only as long as the port forwarding is running. See port forwarding for more info","title":"kubectl port-forward"},{"location":"guides/admin/kubernetes/k8s/#ingress","text":"Ingress rules define how traffic directed at your k8s cluster is directed. Their definition tends to vary substantially between different k8s implementations but often is the easiest approach when running with a cloud service.","title":"Ingress"},{"location":"guides/admin/kubernetes/k8s/#microk8s-ingress","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"* microk8s ingress"},{"location":"guides/admin/kubernetes/operator/","text":"What is an Operator? \u00b6 An operator is a way of extending Kubernetes with application specific capababilities. The notion of 'operator' comes from managing an application through it's lifecycle, just as a human operator would, but here we do it automatically. Operator functionality could include: - deploying an application - upgrading an applications, perhaps migrating data - taking backups - mapping application specific behaviour into more standard kubernetes commands A summary of some interesting operators can be found here Custom Resources \u00b6 Kubernetes has many standard resource definitions, such as for pods, services, deployments. Instances of each define the intended state of the object it represents. Custom Resource Definitions ( CRD s) extend Kubernetes to allow anything to be represented, such as the intended state of an application stack. They are defined in yaml. Resource definitions include: - apiVersion - consider this the namespace - metadata - useful for searching, selecting - spec - the intended state we are describing - status - used to capture current state An instance of a CRD is known as a Custom Resource ( CR ) Controller \u00b6 There is no intrinsic behaviour associated with a custom resource - it is purely a document with a defined schema. The behaviour is defined by the Controller , which is code running within the Kubernetes cluster. The controller's objective is to make reality match the configured Custom Resource. It will watch particular resource types - the primary being the CRD above - and as changes are observed will do whatever is needed to configure application to match the desired intent. It is primarily a reconcilation loop. The controller will likely use a combination of Kubernetes APIs, perhaps defining and updating other resources, as well as interacting directly with application code within, or external to the cluster. Further information \u00b6 Operator Pattern (Docs, kubernetes.io) Kubernetes Operator 101 (Blog, Red Hat) - Part1 | Part2 Kubernetes Operator (Video, IBM) Operators over easy: an introduction to Kubernetes Operators (Video, Red Hat) How does it differ from Helm? \u00b6 Helm is a popular way of installing an application stack through charts, and sometimes upgrading. However at it's core, Helm is a templating engine. It takes templates, runs them through a processor to expand variables (across multiple documents). It then submits the documents to the Kubernetes API server just the same as if they'd be created standalone. Helm charts can be stored in a repository, and make putting together a complex stack relatively easy. (we already use them successfully for Egeria). More complex behaviour can be done by deploying additional containers, jobs etc. This is very different to an operator. Rather than create standard Kubernetes resources, with an operator we are trying to model our application, define it's intended state, and then have active code always running to monitor and achieve this. Operators are thus much more sophisticated, but also complex. Operators and custom resources can themselves be deployed using Helm - so we don't think of the Operators are replacing Helm, rather they are very complementary techniques which can be used together. For example, whist an operator might focus on a single application, a helm chart may deploy a demonstation environment. Operator Lifecycle Manager \u00b6 The Operator Lifecycle Manager ( OLM ) is an optional extension that can be used alongside operators to manage them. OLM allows for operator discovery and manages dependencies between operators. OLM is standard in Red Hat OpenShift OLM is not necessary to use operators - and unlike operators themselves, the framework is not present in all Kubernetes distibutions . Catalogs \u00b6 A catalog of operators can be found on OperatorHub and authors can submit their own operator to help with findability. Building an operator \u00b6 Operators can be built in any language than can support making API calls to Kubernetes. For example it would be possible to write a Java based operator. This is complex, and uses low level APIs, with few examples published. The Operator Framework is a toolkit to make it easier to develop an operator by including higher level APIs, and tools for code generation. It supports: * Helm - this wraps up a Helm chart as an operator, but as such it only delivers install/upgrade capability. They can be managed and catalogued as an operator. * Ansible - an ansible operator can respond to events & run ansible scripts accordingly. This makes them significantly more flexible * Go - These are full 'native' operators with full access to the Kubernetes API. Kubernetes itself is written in Go. They are therefore very flexible, though more complex to develop. They do however represent the majority of development in this area, providing also the best scope for help from peers. A more detailed comparison can be found in this blog For another project aiming to support java see https://github.com/java-operator-sdk/java-operator-sdk Egeria is implementing a Go operator, using the Operator Framework. This is the most established & flexible approach. Egeria Operator Development \u00b6 The Egeria operator is being developed at https://github.com/odpi/egeria-k8s-operator - see here for * Issues * prereqs/dependencies * feedback, getting involved * build, install, and usage instructions Design considerations \u00b6 Egeria configurations \u00b6 Custom Resource definitions allow for a very detailed specification of what we want our Egeria deployment to be. However Egeria itself already has a sophisticated approach to configuration - some elements of which are still evolving. It could be possible to try and fully expose this configuration as a CRD. However due to the complexity, fluidity, and duplication the Operator instead will add k8s deployment specific information around existing Egeria configurations. It's therefore imperative we keep the Authoring of server configuration distinct from Deployment . The deployment environment will be different in a Kubernetes environment (hostnames, service names etc) Initially the egeria config document will be used verbatim, however if processing is needed, a Admission Webhook could be used to validate ( validating ) & convert ( mutating ) the config before storing in k8s. This approach could also be used for CR validation. Example server configuration documents Scaling & failover \u00b6 Kubernetes is a useful container solution that scales from a raspberry pi to huge cloud deployments. As such it may be used for anything from development to full enterprise production - and this is a strong part of it's appeal. So the operator must support everything from a small development environment, through to a scalable, reliable cloud environment. Egeria Platform \u00b6 The OMAG server platform (aka 'server chassis') is effectively a java process that is launched and acts as a container for running Egeria servers (below). In itself it has relatively little configuration but does include - TLS configuration ie passwords, keys, certs - Spring configuration (security, plugins) It offers an admin interface for defining servers (below) and starting/stopping them. It also provides the base URL which is extended for server access. The platform's connectors are mostly limited to configuration & security. Egeria Servers \u00b6 The OMAG Server comes in a number of different forms including a repository proxy, a metadata repository, a view server etc A 'server' is a logical concept that actually executes code within an Egeria Platform but is basically what is defined, started, stopped, and also hosts the myriad of REST APIs that egeria supports such as for creating the definition of a data asset. So how do we scale? \u00b6 Three main options are: Model both servers & platforms as different Kubernetes resources. This provides the most accurate mapping, but would require the operator to manage 'scheduling' servers on platforms, and allowing for this to change dynamically as workload & requiremenets change. This would also require having logical URLs unique to each server. Define servers, and automatically create platforms as needed, resulting in a 1:1 relationship between server and platform. This is simple, though server configuration would need to be overloaded with platform configuration (like ssl keys). This would also lead to creating many more platforms which comes at a resource cost (ram, cpu), as this is a process. Servers however are just logical constructs and have minimal overhead. Handle replication at the level of platform only. This is close to how Kubernetes works with most apps. Whilst initially persuing the first option, due to complexity, the last of these has now been chosen for simplicity. This has also resulted in the Egeria Platform being the first-class resource we focus on defining for the operator, at least initially. Stateful set vs Deployment \u00b6 A Deployment manages scaling of a set of containers (such as egeria platform) across multiple pods. A Statefulset extends this to provide a unique identity which can be used for unique service definitions, or for maintaining an association with dedicated storage. In the current Egeria helm chart we use a stateful set that that persistent storage can be allocated uniquely to each pod & retained across restarts. However the assumption in the operator environment is that Egeria is stateless (beyond the config & CR), and that any persistent state is managed via the connections to metadata storage etc, and that a unique service address is not needed. This latter point may need to be revisited if the Egeria operator needs to actively interact with the individual replicas for control/config, but this is not yet the case, hence the choice of a simpler deployment initially. Configuration updates \u00b6 If a server config document is changed the initially the platform will be restarted. Later we will try to just restart/stop the modified server and/or perform a rolling change. Care will need to be taken if the config change leads to a conflict. Admin requests \u00b6 The operator has no knowledge of the state of egeria servers, discovery engines etc. Attempts to perform any admin ops should be blocked (via security plugins to both platform & server) To refine this will require modeling those concepts in the operator Metadata repositories \u00b6 The only metadata repository that offers a suitable HA/remote environment & supported by the Egeria open source project is XTDB , using the Egeria XTDB connector . Note: At this point, the deployment of XTDB itself is outside the scope of the operator. Connectors \u00b6 Much of the capability in Egeria is pluggable. For example we have a connector to the message bus, which could be kafka, but equally may be RabbitMQ (if a connector were written). We have connectors that understand data sources such as for Postgres. These are implemented as java jars, and are added into the classpath of the egeria platform. Thus the platform can provide a certain capability with these additional connectors, but it is the server which defines which ones to use (and refers to the class name to locate) An Egeria server configuration document therefore contains many references to connectors. The references libraries must be available in the runtime environment ie platform. This is done by ensuring they are pointed to within the spring loader's 'LOADER_PATH' environment variable. Several approaches are possible: * Build a custom container image based on the Egeria docker image including the desired connectors, and either placing the required additional files into /deployments/server/lib, or placing them elsewhere in the image and ensuring LOADER_PATH is set * Dynamically download or mount the required libraries - and dependencies - when the server platform is set up by the operator, for example through an init job. Currently the operator takes the former, simpler approach. Therefore specifying a custom container image as part of the platform configuration will often be required. Connectors also often tend to refer to endpoints - for example the location of a Kafka message bus, a server's own address. Currently the server configuration document passed to the operator must have correct addresses. As yet there is no manipulation or templating of these addresses, though this may change in future. Operator Development \u00b6 Prerelease \u00b6 deploy Egeria platforms with a list of servers uses Kubernetes ConfigMap to store individual server configuration requires the user to use repositories capable of supporting replication (like XTDB) Still to do for an initial release \u00b6 Helm chart to deploy a complete demo environment (coco pharma) with Egeria operator & XTDB backend. working through the full server authoring lifecycle (alongside Server Author UI & View Services) - including templating of connections which contain host:ip from the authoring environment Evaluating alternative stores of server configuration (at a minimum may need to be a secret as we include auth info - but decided to use ConfigMaps initially for clarity during initial design) Automated testing & packaging (to both a full k8s cluster & using a test framework) Further end-user docs on using the operator publish on Operator Hub Future enhancements \u00b6 Operator Lifecycle Manager integration publish on Operator Hub refinement of configurations (consolidation, admission webhooks) consideration of more detailed mapping between Egeria Concepts and operator rolling platform restart (config update, upgrade etc) Handling upgrades/migration between egeria versions Integration with Prometheus License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Operator"},{"location":"guides/admin/kubernetes/operator/#what-is-an-operator","text":"An operator is a way of extending Kubernetes with application specific capababilities. The notion of 'operator' comes from managing an application through it's lifecycle, just as a human operator would, but here we do it automatically. Operator functionality could include: - deploying an application - upgrading an applications, perhaps migrating data - taking backups - mapping application specific behaviour into more standard kubernetes commands A summary of some interesting operators can be found here","title":"What is an Operator?"},{"location":"guides/admin/kubernetes/operator/#custom-resources","text":"Kubernetes has many standard resource definitions, such as for pods, services, deployments. Instances of each define the intended state of the object it represents. Custom Resource Definitions ( CRD s) extend Kubernetes to allow anything to be represented, such as the intended state of an application stack. They are defined in yaml. Resource definitions include: - apiVersion - consider this the namespace - metadata - useful for searching, selecting - spec - the intended state we are describing - status - used to capture current state An instance of a CRD is known as a Custom Resource ( CR )","title":"Custom Resources"},{"location":"guides/admin/kubernetes/operator/#controller","text":"There is no intrinsic behaviour associated with a custom resource - it is purely a document with a defined schema. The behaviour is defined by the Controller , which is code running within the Kubernetes cluster. The controller's objective is to make reality match the configured Custom Resource. It will watch particular resource types - the primary being the CRD above - and as changes are observed will do whatever is needed to configure application to match the desired intent. It is primarily a reconcilation loop. The controller will likely use a combination of Kubernetes APIs, perhaps defining and updating other resources, as well as interacting directly with application code within, or external to the cluster.","title":"Controller"},{"location":"guides/admin/kubernetes/operator/#further-information","text":"Operator Pattern (Docs, kubernetes.io) Kubernetes Operator 101 (Blog, Red Hat) - Part1 | Part2 Kubernetes Operator (Video, IBM) Operators over easy: an introduction to Kubernetes Operators (Video, Red Hat)","title":"Further information"},{"location":"guides/admin/kubernetes/operator/#how-does-it-differ-from-helm","text":"Helm is a popular way of installing an application stack through charts, and sometimes upgrading. However at it's core, Helm is a templating engine. It takes templates, runs them through a processor to expand variables (across multiple documents). It then submits the documents to the Kubernetes API server just the same as if they'd be created standalone. Helm charts can be stored in a repository, and make putting together a complex stack relatively easy. (we already use them successfully for Egeria). More complex behaviour can be done by deploying additional containers, jobs etc. This is very different to an operator. Rather than create standard Kubernetes resources, with an operator we are trying to model our application, define it's intended state, and then have active code always running to monitor and achieve this. Operators are thus much more sophisticated, but also complex. Operators and custom resources can themselves be deployed using Helm - so we don't think of the Operators are replacing Helm, rather they are very complementary techniques which can be used together. For example, whist an operator might focus on a single application, a helm chart may deploy a demonstation environment.","title":"How does it differ from Helm?"},{"location":"guides/admin/kubernetes/operator/#operator-lifecycle-manager","text":"The Operator Lifecycle Manager ( OLM ) is an optional extension that can be used alongside operators to manage them. OLM allows for operator discovery and manages dependencies between operators. OLM is standard in Red Hat OpenShift OLM is not necessary to use operators - and unlike operators themselves, the framework is not present in all Kubernetes distibutions .","title":"Operator Lifecycle Manager"},{"location":"guides/admin/kubernetes/operator/#catalogs","text":"A catalog of operators can be found on OperatorHub and authors can submit their own operator to help with findability.","title":"Catalogs"},{"location":"guides/admin/kubernetes/operator/#building-an-operator","text":"Operators can be built in any language than can support making API calls to Kubernetes. For example it would be possible to write a Java based operator. This is complex, and uses low level APIs, with few examples published. The Operator Framework is a toolkit to make it easier to develop an operator by including higher level APIs, and tools for code generation. It supports: * Helm - this wraps up a Helm chart as an operator, but as such it only delivers install/upgrade capability. They can be managed and catalogued as an operator. * Ansible - an ansible operator can respond to events & run ansible scripts accordingly. This makes them significantly more flexible * Go - These are full 'native' operators with full access to the Kubernetes API. Kubernetes itself is written in Go. They are therefore very flexible, though more complex to develop. They do however represent the majority of development in this area, providing also the best scope for help from peers. A more detailed comparison can be found in this blog For another project aiming to support java see https://github.com/java-operator-sdk/java-operator-sdk Egeria is implementing a Go operator, using the Operator Framework. This is the most established & flexible approach.","title":"Building an operator"},{"location":"guides/admin/kubernetes/operator/#egeria-operator-development","text":"The Egeria operator is being developed at https://github.com/odpi/egeria-k8s-operator - see here for * Issues * prereqs/dependencies * feedback, getting involved * build, install, and usage instructions","title":"Egeria Operator Development"},{"location":"guides/admin/kubernetes/operator/#design-considerations","text":"","title":"Design considerations"},{"location":"guides/admin/kubernetes/operator/#egeria-configurations","text":"Custom Resource definitions allow for a very detailed specification of what we want our Egeria deployment to be. However Egeria itself already has a sophisticated approach to configuration - some elements of which are still evolving. It could be possible to try and fully expose this configuration as a CRD. However due to the complexity, fluidity, and duplication the Operator instead will add k8s deployment specific information around existing Egeria configurations. It's therefore imperative we keep the Authoring of server configuration distinct from Deployment . The deployment environment will be different in a Kubernetes environment (hostnames, service names etc) Initially the egeria config document will be used verbatim, however if processing is needed, a Admission Webhook could be used to validate ( validating ) & convert ( mutating ) the config before storing in k8s. This approach could also be used for CR validation. Example server configuration documents","title":"Egeria configurations"},{"location":"guides/admin/kubernetes/operator/#scaling-failover","text":"Kubernetes is a useful container solution that scales from a raspberry pi to huge cloud deployments. As such it may be used for anything from development to full enterprise production - and this is a strong part of it's appeal. So the operator must support everything from a small development environment, through to a scalable, reliable cloud environment.","title":"Scaling &amp; failover"},{"location":"guides/admin/kubernetes/operator/#egeria-platform","text":"The OMAG server platform (aka 'server chassis') is effectively a java process that is launched and acts as a container for running Egeria servers (below). In itself it has relatively little configuration but does include - TLS configuration ie passwords, keys, certs - Spring configuration (security, plugins) It offers an admin interface for defining servers (below) and starting/stopping them. It also provides the base URL which is extended for server access. The platform's connectors are mostly limited to configuration & security.","title":"Egeria Platform"},{"location":"guides/admin/kubernetes/operator/#egeria-servers","text":"The OMAG Server comes in a number of different forms including a repository proxy, a metadata repository, a view server etc A 'server' is a logical concept that actually executes code within an Egeria Platform but is basically what is defined, started, stopped, and also hosts the myriad of REST APIs that egeria supports such as for creating the definition of a data asset.","title":"Egeria Servers"},{"location":"guides/admin/kubernetes/operator/#so-how-do-we-scale","text":"Three main options are: Model both servers & platforms as different Kubernetes resources. This provides the most accurate mapping, but would require the operator to manage 'scheduling' servers on platforms, and allowing for this to change dynamically as workload & requiremenets change. This would also require having logical URLs unique to each server. Define servers, and automatically create platforms as needed, resulting in a 1:1 relationship between server and platform. This is simple, though server configuration would need to be overloaded with platform configuration (like ssl keys). This would also lead to creating many more platforms which comes at a resource cost (ram, cpu), as this is a process. Servers however are just logical constructs and have minimal overhead. Handle replication at the level of platform only. This is close to how Kubernetes works with most apps. Whilst initially persuing the first option, due to complexity, the last of these has now been chosen for simplicity. This has also resulted in the Egeria Platform being the first-class resource we focus on defining for the operator, at least initially.","title":"So how do we scale?"},{"location":"guides/admin/kubernetes/operator/#stateful-set-vs-deployment","text":"A Deployment manages scaling of a set of containers (such as egeria platform) across multiple pods. A Statefulset extends this to provide a unique identity which can be used for unique service definitions, or for maintaining an association with dedicated storage. In the current Egeria helm chart we use a stateful set that that persistent storage can be allocated uniquely to each pod & retained across restarts. However the assumption in the operator environment is that Egeria is stateless (beyond the config & CR), and that any persistent state is managed via the connections to metadata storage etc, and that a unique service address is not needed. This latter point may need to be revisited if the Egeria operator needs to actively interact with the individual replicas for control/config, but this is not yet the case, hence the choice of a simpler deployment initially.","title":"Stateful set vs Deployment"},{"location":"guides/admin/kubernetes/operator/#configuration-updates","text":"If a server config document is changed the initially the platform will be restarted. Later we will try to just restart/stop the modified server and/or perform a rolling change. Care will need to be taken if the config change leads to a conflict.","title":"Configuration updates"},{"location":"guides/admin/kubernetes/operator/#admin-requests","text":"The operator has no knowledge of the state of egeria servers, discovery engines etc. Attempts to perform any admin ops should be blocked (via security plugins to both platform & server) To refine this will require modeling those concepts in the operator","title":"Admin requests"},{"location":"guides/admin/kubernetes/operator/#metadata-repositories","text":"The only metadata repository that offers a suitable HA/remote environment & supported by the Egeria open source project is XTDB , using the Egeria XTDB connector . Note: At this point, the deployment of XTDB itself is outside the scope of the operator.","title":"Metadata repositories"},{"location":"guides/admin/kubernetes/operator/#connectors","text":"Much of the capability in Egeria is pluggable. For example we have a connector to the message bus, which could be kafka, but equally may be RabbitMQ (if a connector were written). We have connectors that understand data sources such as for Postgres. These are implemented as java jars, and are added into the classpath of the egeria platform. Thus the platform can provide a certain capability with these additional connectors, but it is the server which defines which ones to use (and refers to the class name to locate) An Egeria server configuration document therefore contains many references to connectors. The references libraries must be available in the runtime environment ie platform. This is done by ensuring they are pointed to within the spring loader's 'LOADER_PATH' environment variable. Several approaches are possible: * Build a custom container image based on the Egeria docker image including the desired connectors, and either placing the required additional files into /deployments/server/lib, or placing them elsewhere in the image and ensuring LOADER_PATH is set * Dynamically download or mount the required libraries - and dependencies - when the server platform is set up by the operator, for example through an init job. Currently the operator takes the former, simpler approach. Therefore specifying a custom container image as part of the platform configuration will often be required. Connectors also often tend to refer to endpoints - for example the location of a Kafka message bus, a server's own address. Currently the server configuration document passed to the operator must have correct addresses. As yet there is no manipulation or templating of these addresses, though this may change in future.","title":"Connectors"},{"location":"guides/admin/kubernetes/operator/#operator-development","text":"","title":"Operator Development"},{"location":"guides/admin/kubernetes/operator/#prerelease","text":"deploy Egeria platforms with a list of servers uses Kubernetes ConfigMap to store individual server configuration requires the user to use repositories capable of supporting replication (like XTDB)","title":"Prerelease"},{"location":"guides/admin/kubernetes/operator/#still-to-do-for-an-initial-release","text":"Helm chart to deploy a complete demo environment (coco pharma) with Egeria operator & XTDB backend. working through the full server authoring lifecycle (alongside Server Author UI & View Services) - including templating of connections which contain host:ip from the authoring environment Evaluating alternative stores of server configuration (at a minimum may need to be a secret as we include auth info - but decided to use ConfigMaps initially for clarity during initial design) Automated testing & packaging (to both a full k8s cluster & using a test framework) Further end-user docs on using the operator publish on Operator Hub","title":"Still to do for an initial release"},{"location":"guides/admin/kubernetes/operator/#future-enhancements","text":"Operator Lifecycle Manager integration publish on Operator Hub refinement of configurations (consolidation, admission webhooks) consideration of more detailed mapping between Egeria Concepts and operator rolling platform restart (config update, upgrade etc) Handling upgrades/migration between egeria versions Integration with Prometheus License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Future enhancements"},{"location":"guides/admin/servers/","text":"Configuring an OMAG Server \u00b6 An OMAG Server is a configured set of services and connectors that support the integration of a particular type of technology. There are different types of OMAG Server for each type of technology. Each are configured separately and then linked together to form a connected ecosystem. The configuration document for the OMAG Server determines which OMAG subsystems (and hence the types of open metadata and governance services) should be activated in the OMAG Server. For example: Setting basic descriptive properties of the server that are used in logging and events originating from the server. What type of local repository to use. Whether the Open Metadata Access Services (OMASs) should be started. Which cohorts to connect to. Each of the configuration commands builds up sections in the configuration document. This document is stored in the configuration store after each configuration request, so it is immediately available for use each time the open metadata services are activated in the OMAG Server. Many of the configuration values are connections to allow the server to create the connectors to the resources it needs. These connectors enable Egeria to run in different deployment environments and to connect to different third party technologies. In the descriptions of the configuration commands, there are placeholders for the specific configuration values: they are names of the value in double curly braces. For example: {{platformURLRoot}} - the network address where the OMAG Server Platform is registered, such as https://localhost:9443 . {{adminUserId}} - the user id of the administrator, for example garygeeke . {{serverName}} - the name of the OMAG Server, for example cocoMDS1 . Retrieving the configuration \u00b6 GET - retrieve the configuration document for a specific OMAG Server {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/configuration GET - retrieve the origin of the server It is also possible to query the origin of the server supporting the open metadata services. For the Egeria OMAG Server Platform, the response is Egeria OMAG Server Platform (version 3.1-SNAPSHOT) . {{platformURLRoot}}/open-metadata/platform-services/users/{{adminUserId}}/servers/{{serverName}}/server-platform-origin Migrating configuration documents \u00b6 As Egeria evolves, the content of the configuration document expands. Many of the changes are pure additions and are therefore backward compatible. OMAG Server Platform fails to load configuration document From time to time the structure of the configuration document needs to change. When this happens, the OMAG Server Platform is not able to load the configuration document and a message similar to this is returned: { \"class\" : \"VoidResponse\" , \"relatedHTTPCode\" : 400 , \"exceptionClassName\" : \"org.odpi.openmetadata.adminservices.ffdc.exception.OMAGInvalidParameterException\" , \"exceptionErrorMessage\" : \"OMAG-ADMIN-400-022 The configuration document for OMAG Server cocoMDS1 is at version V1.0 which is not compatible with this OMAG Server Platform which supports versions [V2.0]\" , \"exceptionSystemAction\" : \"The system is unable to configure the local server because it can not read the configuration document.\" , \"exceptionUserAction\" : \"Migrate the configuration document to a compatible version (or delete and recreate it). See https://egeria.odpi.org/open-metadata-implementation/admin-services/docs/user/migrating-configuration-documents.html\" } Migrating a configuration document from V1.0 to V2.0 \u00b6 The additionalProperties property name changed to configurationProperties . To migrate the configuration document, make a global change from additionalProperties to configurationProperties throughout the configuration document. Release 2.x+ of Egeria \u00b6 Release 2.0 encrypts the configuration document by default. This includes automatically detecting and encrypting any clear-text (unencrypted) configuration document that may already exist. No user action is required for this migration, the encryption will be handled automatically when the clear-text configuration document is first opened by the platform in these releases.","title":"Configuring an OMAG Server"},{"location":"guides/admin/servers/#configuring-an-omag-server","text":"An OMAG Server is a configured set of services and connectors that support the integration of a particular type of technology. There are different types of OMAG Server for each type of technology. Each are configured separately and then linked together to form a connected ecosystem. The configuration document for the OMAG Server determines which OMAG subsystems (and hence the types of open metadata and governance services) should be activated in the OMAG Server. For example: Setting basic descriptive properties of the server that are used in logging and events originating from the server. What type of local repository to use. Whether the Open Metadata Access Services (OMASs) should be started. Which cohorts to connect to. Each of the configuration commands builds up sections in the configuration document. This document is stored in the configuration store after each configuration request, so it is immediately available for use each time the open metadata services are activated in the OMAG Server. Many of the configuration values are connections to allow the server to create the connectors to the resources it needs. These connectors enable Egeria to run in different deployment environments and to connect to different third party technologies. In the descriptions of the configuration commands, there are placeholders for the specific configuration values: they are names of the value in double curly braces. For example: {{platformURLRoot}} - the network address where the OMAG Server Platform is registered, such as https://localhost:9443 . {{adminUserId}} - the user id of the administrator, for example garygeeke . {{serverName}} - the name of the OMAG Server, for example cocoMDS1 .","title":"Configuring an OMAG Server"},{"location":"guides/admin/servers/#retrieving-the-configuration","text":"GET - retrieve the configuration document for a specific OMAG Server {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/configuration GET - retrieve the origin of the server It is also possible to query the origin of the server supporting the open metadata services. For the Egeria OMAG Server Platform, the response is Egeria OMAG Server Platform (version 3.1-SNAPSHOT) . {{platformURLRoot}}/open-metadata/platform-services/users/{{adminUserId}}/servers/{{serverName}}/server-platform-origin","title":"Retrieving the configuration"},{"location":"guides/admin/servers/#migrating-configuration-documents","text":"As Egeria evolves, the content of the configuration document expands. Many of the changes are pure additions and are therefore backward compatible. OMAG Server Platform fails to load configuration document From time to time the structure of the configuration document needs to change. When this happens, the OMAG Server Platform is not able to load the configuration document and a message similar to this is returned: { \"class\" : \"VoidResponse\" , \"relatedHTTPCode\" : 400 , \"exceptionClassName\" : \"org.odpi.openmetadata.adminservices.ffdc.exception.OMAGInvalidParameterException\" , \"exceptionErrorMessage\" : \"OMAG-ADMIN-400-022 The configuration document for OMAG Server cocoMDS1 is at version V1.0 which is not compatible with this OMAG Server Platform which supports versions [V2.0]\" , \"exceptionSystemAction\" : \"The system is unable to configure the local server because it can not read the configuration document.\" , \"exceptionUserAction\" : \"Migrate the configuration document to a compatible version (or delete and recreate it). See https://egeria.odpi.org/open-metadata-implementation/admin-services/docs/user/migrating-configuration-documents.html\" }","title":"Migrating configuration documents"},{"location":"guides/admin/servers/#migrating-a-configuration-document-from-v10-to-v20","text":"The additionalProperties property name changed to configurationProperties . To migrate the configuration document, make a global change from additionalProperties to configurationProperties throughout the configuration document.","title":"Migrating a configuration document from V1.0 to V2.0"},{"location":"guides/admin/servers/#release-2x-of-egeria","text":"Release 2.0 encrypts the configuration document by default. This includes automatically detecting and encrypting any clear-text (unencrypted) configuration document that may already exist. No user action is required for this migration, the encryption will be handled automatically when the clear-text configuration document is first opened by the platform in these releases.","title":"Release 2.x+ of Egeria"},{"location":"guides/admin/servers/configuring-a-conformance-test-server/","text":"Configuring a conformance test server \u00b6 Each type of OMAG Server is configured by creating a configuration document . Set up the default event bus \u00b6 An OMAG Server uses an event bus such as Apache Kafka to exchange events with other servers and tools. Egeria manages the specific topic names and the event payloads; however, it needs to know where the event bus is deployed and any properties needed to configure it. Since the event bus is used in multiple places, the configuration document allows you to set up the details of the event bus which are then incorporated into all the places where the event bus is needed. Important sequencing information You need to set up this information before configuring any of the following: Using an event topic as the destination for the audit log . Configuring the access services in a metadata server or a metadata access point . Configuring registration to a cohort in a metadata server , a metadata access point , a repository proxy or a conformance test server . The following command creates information about the event bus. This information is used on the subsequent configuration of the OMAG Server subsystems. It does not affect any subsystems that have already been configured in the configuration document and if the event bus is not needed, its values are ignored. It is possible to add arbitrary name/value pairs as JSON in the request body. The correct properties to use are defined in the connector type for the event bus. POST - configure event bus {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/event-bus Example: Apache Kafka For example, when using Apache Kafka as your event bus you may want to configure properties that control the behavior of the consumer that receives events and the producer that sends events. This is a typical set of producer and consumer properties: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"producer\" : { \"bootstrap.servers\" : \"localhost:9092\" , \"acks\" : \"all\" , \"retries\" : \"0\" , \"batch.size\" : \"16384\" , \"linger.ms\" : \"1\" , \"buffer.memory\" : \"33554432\" , \"max.request.size\" : \"10485760\" , \"key.serializer\" : \"org.apache.kafka.common.serialization.StringSerializer\" , \"value.serializer\" : \"org.apache.kafka.common.serialization.StringSerializer\" , \"kafka.omrs.topic.id\" : \"cocoCohort\" }, \"consumer\" : { \"bootstrap.servers\" : \"localhost:9092\" , \"zookeeper.session.timeout.ms\" : \"400\" , \"zookeeper.sync.time.ms\" : \"200\" , \"fetch.message.max.bytes\" : \"10485760\" , \"max.partition.fetch.bytes\" : \"10485760\" , \"key.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"value.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"kafka.omrs.topic.id\" : \"cocoCohort\" } } A different type of event bus would use different properties. Set the server URL root \u00b6 Configure the local server URL root with the value of the OMAG Server Platform where the service will run: in particular if the configuration document will be deployed to a different OMAG Server Platform from the one used to maintain the configuration document. POST - set server URL root {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-url-root?url={{targetPlatformURLRoot}} Detailed explanation The {{targetPlatformURLRoot}} gives the location of the OMAG Server Platform on which this configured service is intended to run, while the {{platformURLRoot}} gives the location of the OMAG Server Platform in which this configuration document is maintained. They could be, but do not need to be, the same location. Configure the basic properties \u00b6 The basic properties of the OMAG Server are used in logging and events originating from the server. They help to document the purpose of the server (which helps with problem determination) and enable performance improvements by allowing the server to ignore activity or metadata that is not relevant to its operation. The basic properties include two unique identifiers: Property Description localServerId Unique identifier for this server. By default, this is initialized to a randomly generated Universal Unique identifier (UUID). localServerName Meaningful name for the server for use in messages and UIs. Ideally this value is unique to aid administrators in understanding the source of messages and events from the server. This value is set to the server name assigned when the configuration is created. The other basic properties have values that can be changed through the admin services API: Property Description localServerType Descriptive type name for the server. Again this is useful information for the administrator to understand the role of the server. The default value is Open Metadata and Governance Server . organizationName Descriptive name for the organization that owns the local server/repository. This is useful when the open metadata repository cluster consists of metadata servers from different organizations, or different departments of an enterprise. The default value is null . localServerUserId UserId to use for server-initiated REST calls. The default is OMAGServer . localServerPassword Password to use for server-initiated REST calls. The default is null . This means that only the userId is sent in the HTTP header. maxPageSize The maximum page size that can be set on requests to the server. The default value is 1000 . A value of zero means unlimited page size. Although supported, the zero value is not recommended because it provides no protection from a large request denial of service attack. The sections that follow cover how to set up these values. Set server type name \u00b6 The server type name should be set to something that describes the OMAG Server's role. It may be the name of a specific product that it is enabling, or a role in the metadata and governance landscape. POST - set server type {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-type?typeName=\"{{serverTypeName}}\" Set organization name \u00b6 The organization name may be the owning organization or department or team supported by the server. POST - set organization name {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/organization-name?name=\"{{organizationName}}\" Set the server's user ID and optional password \u00b6 The server's user ID is used when processing requests that do not have an end user, such as receiving an event from a topic. The default value is OMAGServer . Ideally each server should have its own user ID so it is possible to restrict the resources that each server has access to. If the password is specified as well, the userId and password combination are used to provide authentication information on each REST call made by the server. POST - set server's userId {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-id?id=\"{{serverUserId}}\" POST - set server's password {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-password?password=\"{{serverUserPassword}}\" Set the maximum page size for REST API requests \u00b6 The maximum page size value sets an upper limit on the number of results that a caller can request on any paging REST API to this server. Setting maximum page size helps to prevent a denial of service attack that uses very large requests to overwhelm the server. A value of 0 means no limit, and leaves the server open to such attacks. POST - set maximum page size {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/max-page-size?limit={{maxPageSize}} Configure the audit log \u00b6 Egeria's audit log provides a configurable set of destinations for audit records and other diagnostic logging for an OMAG Server . Some destinations also support a query interface to allow an administrator to understand how the server is running. If the server is a development or test server, then the default audit log configuration is probably sufficient: the console audit log destination. POST - set default audit log destination {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/default Using this option overrides all previous audit log destinations. If this server is a production server then you will probably want to set up the audit log destinations explicitly. You can add multiple destinations and each one can be set up to process specific severities of log records. The audit log severities are as follows: Severity Description Information The server is providing information about its normal operation. Event An event was received from another member of the open metadata repository cohort. Decision A decision has been made related to the interaction of the local metadata repository and the rest of the cohort. Action An Action is required by the administrator. At a minimum, the situation needs to be investigated and if necessary, corrective action taken. Error An error occurred, possibly caused by an incompatibility between the local metadata repository and one of the remote repositories. The local repository may restrict some of the metadata interchange functions as a result. Exception An unexpected exception occurred. This means that the server needs some administration attention to correct configuration or fix a logic error because it is not operating as a proper peer in the open metadata repository cohort. Security Unauthorized access to a service or metadata instance has been attempted. Startup A new component is starting up. Shutdown An existing component is shutting down. Asset An auditable action relating to an asset has been taken. Types Activity is occurring that relates to the open metadata types in use by this server. Cohort The server is exchanging registration information about an open metadata repository cohort that it is connecting to. Trace This is additional information on the operation of the server that may be of assistance in debugging a problem. It is not normally logged to any destination, but can be added when needed. PerfMon This log record contains performance monitoring timing information for specific types of processing. It is not normally logged to any destination, but can be added when needed. <Unknown> Uninitialized Severity The body of the request should be a list of severities If an empty list is passed as the request body then all severities are supported by the destination. Add audit log destinations \u00b6 There are various destinations that can be configured for the audit log: console POST - add console audit log destination This writes selected parts of each audit log record to stdout. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/console slf4j POST - add slf4j audit log destination This writes full log records to the slf4j ecosystem. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/slf4j When configuring slf4j as destination you also need to specify auditlog logger category via the application properties. This is described in Connecting the OMAG Audit Log Framework section of the developer logging guide. file POST - add JSON file-based audit log destination This writes JSON files in a shared directory. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/files event POST - add event-based audit log destination This writes each log record as an event on the supplied event topic. It assumes that the event bus is set up first. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/event-topic connection POST - add connection-based audit log destination This sets up an audit log destination that is described though a connection . In this case, the connection is passed in the request body and the supported severities can be supplied in the connection's configuration properties. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/connection list POST - add a list of connection-based audit log destinations It is also possible to set up the audit log destinations as a list of connections. Using this option overrides all previous audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations Remove audit logs \u00b6 The following will remove all audit log destinations: POST - clear all audit log destinations Clears the list of audit log destinations from the configuration enabling you to add a new set of audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/none Configure the server security connector \u00b6 Metadata that is being aggregated from different sources is likely to need comprehensive access controls. Egeria provides fine-grained security control for metadata access . It is implemented in a server security connector that is called whenever requests are made for to the server. Security is configured for a specific OMAG Server by adding a connection for this connector to the server's configuration document using the following command. POST - configure security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This passes in a connection used to create the server security connector in the request body. { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } Example: set up the sample server security connector For example, this is the connection that would set up the sample server security connector provided for the Coco Pharmaceuticals case study: { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.OpenMetadataServerSecurityProvider\" } } Determine configured security \u00b6 GET - query the server security connector setting {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection Response indicating no security { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific security connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } } Remove configured security \u00b6 DELETE - remove configured security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This removes all authorization checking from the server. Configure the workbenches \u00b6 The workbenches are configured using the OMAG Server Platform Administration Services. This defines which workbenches to run and how to connect to the technology to test. This configuration defines an OMAG Server that will run the requested conformance suite tests. Configure the OMAG Server that will run the requested conformance suite tests. The requested workbenches will begin to execute their tests as soon as the OMAG Server is started. Repository workbench \u00b6 To run a metadata repository through the Repository Workbench, first configure a CTS server in the OMAG Server Platform by configuring its general properties like server type, event bus, cohort, etc. Before starting the CTS server instance, configure the repository workbench within it using the following command: POST - configure repository workbench {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/cts/conformance-suite-workbenches/repository-workbench/repositories Send the repository workbench configuration as the request body, similar to the following: { \"class\" : \"RepositoryConformanceWorkbenchConfig\" , \"tutRepositoryServerName\" : \"myserver\" , \"maxSearchResults\" : 5 } The required tutRepositoryServerName parameter defines the name of the repository server you wish to test, while the optional maxSearchResults parameter controls the sizing of the tests: both the number of instances the tests will attempt to create to carry out its tests and how extensive the search-based tests are. Start the technology under test after the CTS server This repository server to test ( myserver in the example above) should be configured and started after starting the CTS repository workbench instance. Once the CTS server instance is started it will wait for the technology under test (the server named by the tutRepositoryServerName parameter) to be up and running before then starting its suite of tests. The OMAG Server also supports a REST API for querying the results of running the conformance suite tests. These commands include: Retrieve the results from a single named workbench. Retrieve the results from all workbenches and test cases (beware that the response can be 100's of MB in size, and may overflow your JVM heap). Retrieve the results from all failed test cases. Retrieve the IDs of all test cases. Retrieve the results from a specific test cases (for example, iterating through the above call's response). Retrieve the names of all profiles. Retrieve the details of a single profile's results (for example, iterating through the above call's response). The resulting reports can be large Ensure the jvm running the CTS server has at least 1GB heap to avoid any Java heap errors. The Open Metadata Conformance Suite also has a client called OpenMetadataConformanceTestReport that will retrieve the conformance report and all details. It will store a summarized report in openmetadata_cts_summary.json , and the full details of each profile and test case in profile-details and test-cases sub-directories, respectively. The client also outputs a summary of the test run. Example output for a successful CTS run This output is an example of a successful run: $ OpenMetadataConformanceTestReport cSuiteServer https://localhost:9444 ======================================= Open Metadata Conformance Test Report ======================================= Contacting conformance suite server: cts (https://localhost:9443) Saving full profile details into 'profile-details' directory... Summary of profile results: ... Metadata sharing: CONFORMANT_FULL_SUPPORT ... Reference copies: CONFORMANT_FULL_SUPPORT ... Metadata maintenance: CONFORMANT_FULL_SUPPORT ... Dynamic types: UNKNOWN_STATUS ... Graph queries: CONFORMANT_FULL_SUPPORT ... Historical search: CONFORMANT_FULL_SUPPORT ... Entity proxies: CONFORMANT_FULL_SUPPORT ... Soft-delete and restore: CONFORMANT_FULL_SUPPORT ... Undo an update: CONFORMANT_FULL_SUPPORT ... Reidentify instance: CONFORMANT_FULL_SUPPORT ... Retype instance: CONFORMANT_FULL_SUPPORT ... Rehome instance: CONFORMANT_FULL_SUPPORT ... Entity search: CONFORMANT_FULL_SUPPORT ... Relationship search: CONFORMANT_FULL_SUPPORT ... Entity advanced search: CONFORMANT_FULL_SUPPORT ... Relationship advanced search: CONFORMANT_FULL_SUPPORT Saving full test case details into 'test-case-details' directory (can take 1-2 minutes)... Summary: ... number of tests: 4965 ... number of tests passed: 4965 ... number of tests failed: 0 ... number of tests skipped: 0 Congratulations, technology under test is conformant Process finished with exit code 0 Example output for an unsuccessful CTS run The example below is for an unsuccessful run (where one of the Entity search tests has failed): $ OpenMetadataConformanceTestReport cSuiteServer https://localhost:9444 ======================================= Open Metadata Conformance Test Report ======================================= Contacting conformance suite server: cts (https://localhost:9443) Saving full profile details into 'profile-details' directory... Summary of profile results: ... Metadata sharing: CONFORMANT_FULL_SUPPORT ... Reference copies: CONFORMANT_FULL_SUPPORT ... Metadata maintenance: CONFORMANT_FULL_SUPPORT ... Dynamic types: UNKNOWN_STATUS ... Graph queries: CONFORMANT_FULL_SUPPORT ... Historical search: CONFORMANT_FULL_SUPPORT ... Entity proxies: CONFORMANT_FULL_SUPPORT ... Soft-delete and restore: CONFORMANT_FULL_SUPPORT ... Undo an update: CONFORMANT_FULL_SUPPORT ... Reidentify instance: CONFORMANT_FULL_SUPPORT ... Retype instance: CONFORMANT_FULL_SUPPORT ... Rehome instance: CONFORMANT_FULL_SUPPORT ... Entity search: NOT_CONFORMANT ... Relationship search: CONFORMANT_FULL_SUPPORT ... Entity advanced search: CONFORMANT_FULL_SUPPORT ... Relationship advanced search: CONFORMANT_FULL_SUPPORT Saving full test case details into 'test-case-details' directory (can take 1-2 minutes)... Summary: ... number of tests: 4965 ... number of tests passed: 4964 ... number of tests failed: 1 ... number of tests skipped: 0 Technology under test is not yet conformant Process finished with exit code 1","title":"Configure Conformance Test Server"},{"location":"guides/admin/servers/configuring-a-conformance-test-server/#configuring-a-conformance-test-server","text":"Each type of OMAG Server is configured by creating a configuration document .","title":"Configuring a conformance test server"},{"location":"guides/admin/servers/configuring-a-conformance-test-server/#set-up-the-default-event-bus","text":"An OMAG Server uses an event bus such as Apache Kafka to exchange events with other servers and tools. Egeria manages the specific topic names and the event payloads; however, it needs to know where the event bus is deployed and any properties needed to configure it. Since the event bus is used in multiple places, the configuration document allows you to set up the details of the event bus which are then incorporated into all the places where the event bus is needed. Important sequencing information You need to set up this information before configuring any of the following: Using an event topic as the destination for the audit log . Configuring the access services in a metadata server or a metadata access point . Configuring registration to a cohort in a metadata server , a metadata access point , a repository proxy or a conformance test server . The following command creates information about the event bus. This information is used on the subsequent configuration of the OMAG Server subsystems. It does not affect any subsystems that have already been configured in the configuration document and if the event bus is not needed, its values are ignored. It is possible to add arbitrary name/value pairs as JSON in the request body. The correct properties to use are defined in the connector type for the event bus. POST - configure event bus {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/event-bus Example: Apache Kafka For example, when using Apache Kafka as your event bus you may want to configure properties that control the behavior of the consumer that receives events and the producer that sends events. This is a typical set of producer and consumer properties: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"producer\" : { \"bootstrap.servers\" : \"localhost:9092\" , \"acks\" : \"all\" , \"retries\" : \"0\" , \"batch.size\" : \"16384\" , \"linger.ms\" : \"1\" , \"buffer.memory\" : \"33554432\" , \"max.request.size\" : \"10485760\" , \"key.serializer\" : \"org.apache.kafka.common.serialization.StringSerializer\" , \"value.serializer\" : \"org.apache.kafka.common.serialization.StringSerializer\" , \"kafka.omrs.topic.id\" : \"cocoCohort\" }, \"consumer\" : { \"bootstrap.servers\" : \"localhost:9092\" , \"zookeeper.session.timeout.ms\" : \"400\" , \"zookeeper.sync.time.ms\" : \"200\" , \"fetch.message.max.bytes\" : \"10485760\" , \"max.partition.fetch.bytes\" : \"10485760\" , \"key.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"value.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"kafka.omrs.topic.id\" : \"cocoCohort\" } } A different type of event bus would use different properties.","title":"Set up the default event bus"},{"location":"guides/admin/servers/configuring-a-conformance-test-server/#set-the-server-url-root","text":"Configure the local server URL root with the value of the OMAG Server Platform where the service will run: in particular if the configuration document will be deployed to a different OMAG Server Platform from the one used to maintain the configuration document. POST - set server URL root {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-url-root?url={{targetPlatformURLRoot}} Detailed explanation The {{targetPlatformURLRoot}} gives the location of the OMAG Server Platform on which this configured service is intended to run, while the {{platformURLRoot}} gives the location of the OMAG Server Platform in which this configuration document is maintained. They could be, but do not need to be, the same location.","title":"Set the server URL root"},{"location":"guides/admin/servers/configuring-a-conformance-test-server/#configure-the-basic-properties","text":"The basic properties of the OMAG Server are used in logging and events originating from the server. They help to document the purpose of the server (which helps with problem determination) and enable performance improvements by allowing the server to ignore activity or metadata that is not relevant to its operation. The basic properties include two unique identifiers: Property Description localServerId Unique identifier for this server. By default, this is initialized to a randomly generated Universal Unique identifier (UUID). localServerName Meaningful name for the server for use in messages and UIs. Ideally this value is unique to aid administrators in understanding the source of messages and events from the server. This value is set to the server name assigned when the configuration is created. The other basic properties have values that can be changed through the admin services API: Property Description localServerType Descriptive type name for the server. Again this is useful information for the administrator to understand the role of the server. The default value is Open Metadata and Governance Server . organizationName Descriptive name for the organization that owns the local server/repository. This is useful when the open metadata repository cluster consists of metadata servers from different organizations, or different departments of an enterprise. The default value is null . localServerUserId UserId to use for server-initiated REST calls. The default is OMAGServer . localServerPassword Password to use for server-initiated REST calls. The default is null . This means that only the userId is sent in the HTTP header. maxPageSize The maximum page size that can be set on requests to the server. The default value is 1000 . A value of zero means unlimited page size. Although supported, the zero value is not recommended because it provides no protection from a large request denial of service attack. The sections that follow cover how to set up these values.","title":"Configure the basic properties"},{"location":"guides/admin/servers/configuring-a-conformance-test-server/#set-server-type-name","text":"The server type name should be set to something that describes the OMAG Server's role. It may be the name of a specific product that it is enabling, or a role in the metadata and governance landscape. POST - set server type {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-type?typeName=\"{{serverTypeName}}\"","title":"Set server type name"},{"location":"guides/admin/servers/configuring-a-conformance-test-server/#set-organization-name","text":"The organization name may be the owning organization or department or team supported by the server. POST - set organization name {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/organization-name?name=\"{{organizationName}}\"","title":"Set organization name"},{"location":"guides/admin/servers/configuring-a-conformance-test-server/#set-the-servers-user-id-and-optional-password","text":"The server's user ID is used when processing requests that do not have an end user, such as receiving an event from a topic. The default value is OMAGServer . Ideally each server should have its own user ID so it is possible to restrict the resources that each server has access to. If the password is specified as well, the userId and password combination are used to provide authentication information on each REST call made by the server. POST - set server's userId {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-id?id=\"{{serverUserId}}\" POST - set server's password {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-password?password=\"{{serverUserPassword}}\"","title":"Set the server's user ID and optional password"},{"location":"guides/admin/servers/configuring-a-conformance-test-server/#set-the-maximum-page-size-for-rest-api-requests","text":"The maximum page size value sets an upper limit on the number of results that a caller can request on any paging REST API to this server. Setting maximum page size helps to prevent a denial of service attack that uses very large requests to overwhelm the server. A value of 0 means no limit, and leaves the server open to such attacks. POST - set maximum page size {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/max-page-size?limit={{maxPageSize}}","title":"Set the maximum page size for REST API requests"},{"location":"guides/admin/servers/configuring-a-conformance-test-server/#configure-the-audit-log","text":"Egeria's audit log provides a configurable set of destinations for audit records and other diagnostic logging for an OMAG Server . Some destinations also support a query interface to allow an administrator to understand how the server is running. If the server is a development or test server, then the default audit log configuration is probably sufficient: the console audit log destination. POST - set default audit log destination {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/default Using this option overrides all previous audit log destinations. If this server is a production server then you will probably want to set up the audit log destinations explicitly. You can add multiple destinations and each one can be set up to process specific severities of log records. The audit log severities are as follows: Severity Description Information The server is providing information about its normal operation. Event An event was received from another member of the open metadata repository cohort. Decision A decision has been made related to the interaction of the local metadata repository and the rest of the cohort. Action An Action is required by the administrator. At a minimum, the situation needs to be investigated and if necessary, corrective action taken. Error An error occurred, possibly caused by an incompatibility between the local metadata repository and one of the remote repositories. The local repository may restrict some of the metadata interchange functions as a result. Exception An unexpected exception occurred. This means that the server needs some administration attention to correct configuration or fix a logic error because it is not operating as a proper peer in the open metadata repository cohort. Security Unauthorized access to a service or metadata instance has been attempted. Startup A new component is starting up. Shutdown An existing component is shutting down. Asset An auditable action relating to an asset has been taken. Types Activity is occurring that relates to the open metadata types in use by this server. Cohort The server is exchanging registration information about an open metadata repository cohort that it is connecting to. Trace This is additional information on the operation of the server that may be of assistance in debugging a problem. It is not normally logged to any destination, but can be added when needed. PerfMon This log record contains performance monitoring timing information for specific types of processing. It is not normally logged to any destination, but can be added when needed. <Unknown> Uninitialized Severity The body of the request should be a list of severities If an empty list is passed as the request body then all severities are supported by the destination.","title":"Configure the audit log"},{"location":"guides/admin/servers/configuring-a-conformance-test-server/#add-audit-log-destinations","text":"There are various destinations that can be configured for the audit log: console POST - add console audit log destination This writes selected parts of each audit log record to stdout. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/console slf4j POST - add slf4j audit log destination This writes full log records to the slf4j ecosystem. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/slf4j When configuring slf4j as destination you also need to specify auditlog logger category via the application properties. This is described in Connecting the OMAG Audit Log Framework section of the developer logging guide. file POST - add JSON file-based audit log destination This writes JSON files in a shared directory. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/files event POST - add event-based audit log destination This writes each log record as an event on the supplied event topic. It assumes that the event bus is set up first. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/event-topic connection POST - add connection-based audit log destination This sets up an audit log destination that is described though a connection . In this case, the connection is passed in the request body and the supported severities can be supplied in the connection's configuration properties. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/connection list POST - add a list of connection-based audit log destinations It is also possible to set up the audit log destinations as a list of connections. Using this option overrides all previous audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations","title":"Add audit log destinations"},{"location":"guides/admin/servers/configuring-a-conformance-test-server/#remove-audit-logs","text":"The following will remove all audit log destinations: POST - clear all audit log destinations Clears the list of audit log destinations from the configuration enabling you to add a new set of audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/none","title":"Remove audit logs"},{"location":"guides/admin/servers/configuring-a-conformance-test-server/#configure-the-server-security-connector","text":"Metadata that is being aggregated from different sources is likely to need comprehensive access controls. Egeria provides fine-grained security control for metadata access . It is implemented in a server security connector that is called whenever requests are made for to the server. Security is configured for a specific OMAG Server by adding a connection for this connector to the server's configuration document using the following command. POST - configure security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This passes in a connection used to create the server security connector in the request body. { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } Example: set up the sample server security connector For example, this is the connection that would set up the sample server security connector provided for the Coco Pharmaceuticals case study: { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.OpenMetadataServerSecurityProvider\" } }","title":"Configure the server security connector"},{"location":"guides/admin/servers/configuring-a-conformance-test-server/#determine-configured-security","text":"GET - query the server security connector setting {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection Response indicating no security { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific security connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } }","title":"Determine configured security"},{"location":"guides/admin/servers/configuring-a-conformance-test-server/#remove-configured-security","text":"DELETE - remove configured security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This removes all authorization checking from the server.","title":"Remove configured security"},{"location":"guides/admin/servers/configuring-a-conformance-test-server/#configure-the-workbenches","text":"The workbenches are configured using the OMAG Server Platform Administration Services. This defines which workbenches to run and how to connect to the technology to test. This configuration defines an OMAG Server that will run the requested conformance suite tests. Configure the OMAG Server that will run the requested conformance suite tests. The requested workbenches will begin to execute their tests as soon as the OMAG Server is started.","title":"Configure the workbenches"},{"location":"guides/admin/servers/configuring-a-conformance-test-server/#repository-workbench","text":"To run a metadata repository through the Repository Workbench, first configure a CTS server in the OMAG Server Platform by configuring its general properties like server type, event bus, cohort, etc. Before starting the CTS server instance, configure the repository workbench within it using the following command: POST - configure repository workbench {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/cts/conformance-suite-workbenches/repository-workbench/repositories Send the repository workbench configuration as the request body, similar to the following: { \"class\" : \"RepositoryConformanceWorkbenchConfig\" , \"tutRepositoryServerName\" : \"myserver\" , \"maxSearchResults\" : 5 } The required tutRepositoryServerName parameter defines the name of the repository server you wish to test, while the optional maxSearchResults parameter controls the sizing of the tests: both the number of instances the tests will attempt to create to carry out its tests and how extensive the search-based tests are. Start the technology under test after the CTS server This repository server to test ( myserver in the example above) should be configured and started after starting the CTS repository workbench instance. Once the CTS server instance is started it will wait for the technology under test (the server named by the tutRepositoryServerName parameter) to be up and running before then starting its suite of tests. The OMAG Server also supports a REST API for querying the results of running the conformance suite tests. These commands include: Retrieve the results from a single named workbench. Retrieve the results from all workbenches and test cases (beware that the response can be 100's of MB in size, and may overflow your JVM heap). Retrieve the results from all failed test cases. Retrieve the IDs of all test cases. Retrieve the results from a specific test cases (for example, iterating through the above call's response). Retrieve the names of all profiles. Retrieve the details of a single profile's results (for example, iterating through the above call's response). The resulting reports can be large Ensure the jvm running the CTS server has at least 1GB heap to avoid any Java heap errors. The Open Metadata Conformance Suite also has a client called OpenMetadataConformanceTestReport that will retrieve the conformance report and all details. It will store a summarized report in openmetadata_cts_summary.json , and the full details of each profile and test case in profile-details and test-cases sub-directories, respectively. The client also outputs a summary of the test run. Example output for a successful CTS run This output is an example of a successful run: $ OpenMetadataConformanceTestReport cSuiteServer https://localhost:9444 ======================================= Open Metadata Conformance Test Report ======================================= Contacting conformance suite server: cts (https://localhost:9443) Saving full profile details into 'profile-details' directory... Summary of profile results: ... Metadata sharing: CONFORMANT_FULL_SUPPORT ... Reference copies: CONFORMANT_FULL_SUPPORT ... Metadata maintenance: CONFORMANT_FULL_SUPPORT ... Dynamic types: UNKNOWN_STATUS ... Graph queries: CONFORMANT_FULL_SUPPORT ... Historical search: CONFORMANT_FULL_SUPPORT ... Entity proxies: CONFORMANT_FULL_SUPPORT ... Soft-delete and restore: CONFORMANT_FULL_SUPPORT ... Undo an update: CONFORMANT_FULL_SUPPORT ... Reidentify instance: CONFORMANT_FULL_SUPPORT ... Retype instance: CONFORMANT_FULL_SUPPORT ... Rehome instance: CONFORMANT_FULL_SUPPORT ... Entity search: CONFORMANT_FULL_SUPPORT ... Relationship search: CONFORMANT_FULL_SUPPORT ... Entity advanced search: CONFORMANT_FULL_SUPPORT ... Relationship advanced search: CONFORMANT_FULL_SUPPORT Saving full test case details into 'test-case-details' directory (can take 1-2 minutes)... Summary: ... number of tests: 4965 ... number of tests passed: 4965 ... number of tests failed: 0 ... number of tests skipped: 0 Congratulations, technology under test is conformant Process finished with exit code 0 Example output for an unsuccessful CTS run The example below is for an unsuccessful run (where one of the Entity search tests has failed): $ OpenMetadataConformanceTestReport cSuiteServer https://localhost:9444 ======================================= Open Metadata Conformance Test Report ======================================= Contacting conformance suite server: cts (https://localhost:9443) Saving full profile details into 'profile-details' directory... Summary of profile results: ... Metadata sharing: CONFORMANT_FULL_SUPPORT ... Reference copies: CONFORMANT_FULL_SUPPORT ... Metadata maintenance: CONFORMANT_FULL_SUPPORT ... Dynamic types: UNKNOWN_STATUS ... Graph queries: CONFORMANT_FULL_SUPPORT ... Historical search: CONFORMANT_FULL_SUPPORT ... Entity proxies: CONFORMANT_FULL_SUPPORT ... Soft-delete and restore: CONFORMANT_FULL_SUPPORT ... Undo an update: CONFORMANT_FULL_SUPPORT ... Reidentify instance: CONFORMANT_FULL_SUPPORT ... Retype instance: CONFORMANT_FULL_SUPPORT ... Rehome instance: CONFORMANT_FULL_SUPPORT ... Entity search: NOT_CONFORMANT ... Relationship search: CONFORMANT_FULL_SUPPORT ... Entity advanced search: CONFORMANT_FULL_SUPPORT ... Relationship advanced search: CONFORMANT_FULL_SUPPORT Saving full test case details into 'test-case-details' directory (can take 1-2 minutes)... Summary: ... number of tests: 4965 ... number of tests passed: 4964 ... number of tests failed: 1 ... number of tests skipped: 0 Technology under test is not yet conformant Process finished with exit code 1","title":"Repository workbench"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/","text":"Configuring a metadata access point \u00b6 Each type of OMAG Server is configured by creating a configuration document . For a metadata access point, the following can be configured: Set up the default event bus \u00b6 An OMAG Server uses an event bus such as Apache Kafka to exchange events with other servers and tools. Egeria manages the specific topic names and the event payloads; however, it needs to know where the event bus is deployed and any properties needed to configure it. Since the event bus is used in multiple places, the configuration document allows you to set up the details of the event bus which are then incorporated into all the places where the event bus is needed. Important sequencing information You need to set up this information before configuring any of the following: Using an event topic as the destination for the audit log . Configuring the access services in a metadata server or a metadata access point . Configuring registration to a cohort in a metadata server , a metadata access point , a repository proxy or a conformance test server . The following command creates information about the event bus. This information is used on the subsequent configuration of the OMAG Server subsystems. It does not affect any subsystems that have already been configured in the configuration document and if the event bus is not needed, its values are ignored. It is possible to add arbitrary name/value pairs as JSON in the request body. The correct properties to use are defined in the connector type for the event bus. POST - configure event bus {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/event-bus Example: Apache Kafka For example, when using Apache Kafka as your event bus you may want to configure properties that control the behavior of the consumer that receives events and the producer that sends events. This is a typical set of producer and consumer properties: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"producer\" : { \"bootstrap.servers\" : \"localhost:9092\" , \"acks\" : \"all\" , \"retries\" : \"0\" , \"batch.size\" : \"16384\" , \"linger.ms\" : \"1\" , \"buffer.memory\" : \"33554432\" , \"max.request.size\" : \"10485760\" , \"key.serializer\" : \"org.apache.kafka.common.serialization.StringSerializer\" , \"value.serializer\" : \"org.apache.kafka.common.serialization.StringSerializer\" , \"kafka.omrs.topic.id\" : \"cocoCohort\" }, \"consumer\" : { \"bootstrap.servers\" : \"localhost:9092\" , \"zookeeper.session.timeout.ms\" : \"400\" , \"zookeeper.sync.time.ms\" : \"200\" , \"fetch.message.max.bytes\" : \"10485760\" , \"max.partition.fetch.bytes\" : \"10485760\" , \"key.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"value.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"kafka.omrs.topic.id\" : \"cocoCohort\" } } A different type of event bus would use different properties. Set the server URL root \u00b6 Configure the local server URL root with the value of the OMAG Server Platform where the service will run: in particular if the configuration document will be deployed to a different OMAG Server Platform from the one used to maintain the configuration document. POST - set server URL root {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-url-root?url={{targetPlatformURLRoot}} Detailed explanation The {{targetPlatformURLRoot}} gives the location of the OMAG Server Platform on which this configured service is intended to run, while the {{platformURLRoot}} gives the location of the OMAG Server Platform in which this configuration document is maintained. They could be, but do not need to be, the same location. Configure the basic properties \u00b6 The basic properties of the OMAG Server are used in logging and events originating from the server. They help to document the purpose of the server (which helps with problem determination) and enable performance improvements by allowing the server to ignore activity or metadata that is not relevant to its operation. The basic properties include two unique identifiers: Property Description localServerId Unique identifier for this server. By default, this is initialized to a randomly generated Universal Unique identifier (UUID). localServerName Meaningful name for the server for use in messages and UIs. Ideally this value is unique to aid administrators in understanding the source of messages and events from the server. This value is set to the server name assigned when the configuration is created. The other basic properties have values that can be changed through the admin services API: Property Description localServerType Descriptive type name for the server. Again this is useful information for the administrator to understand the role of the server. The default value is Open Metadata and Governance Server . organizationName Descriptive name for the organization that owns the local server/repository. This is useful when the open metadata repository cluster consists of metadata servers from different organizations, or different departments of an enterprise. The default value is null . localServerUserId UserId to use for server-initiated REST calls. The default is OMAGServer . localServerPassword Password to use for server-initiated REST calls. The default is null . This means that only the userId is sent in the HTTP header. maxPageSize The maximum page size that can be set on requests to the server. The default value is 1000 . A value of zero means unlimited page size. Although supported, the zero value is not recommended because it provides no protection from a large request denial of service attack. The sections that follow cover how to set up these values. Set server type name \u00b6 The server type name should be set to something that describes the OMAG Server's role. It may be the name of a specific product that it is enabling, or a role in the metadata and governance landscape. POST - set server type {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-type?typeName=\"{{serverTypeName}}\" Set organization name \u00b6 The organization name may be the owning organization or department or team supported by the server. POST - set organization name {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/organization-name?name=\"{{organizationName}}\" Set the server's user ID and optional password \u00b6 The server's user ID is used when processing requests that do not have an end user, such as receiving an event from a topic. The default value is OMAGServer . Ideally each server should have its own user ID so it is possible to restrict the resources that each server has access to. If the password is specified as well, the userId and password combination are used to provide authentication information on each REST call made by the server. POST - set server's userId {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-id?id=\"{{serverUserId}}\" POST - set server's password {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-password?password=\"{{serverUserPassword}}\" Set the maximum page size for REST API requests \u00b6 The maximum page size value sets an upper limit on the number of results that a caller can request on any paging REST API to this server. Setting maximum page size helps to prevent a denial of service attack that uses very large requests to overwhelm the server. A value of 0 means no limit, and leaves the server open to such attacks. POST - set maximum page size {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/max-page-size?limit={{maxPageSize}} Configure the audit log \u00b6 Egeria's audit log provides a configurable set of destinations for audit records and other diagnostic logging for an OMAG Server . Some destinations also support a query interface to allow an administrator to understand how the server is running. If the server is a development or test server, then the default audit log configuration is probably sufficient: the console audit log destination. POST - set default audit log destination {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/default Using this option overrides all previous audit log destinations. If this server is a production server then you will probably want to set up the audit log destinations explicitly. You can add multiple destinations and each one can be set up to process specific severities of log records. The audit log severities are as follows: Severity Description Information The server is providing information about its normal operation. Event An event was received from another member of the open metadata repository cohort. Decision A decision has been made related to the interaction of the local metadata repository and the rest of the cohort. Action An Action is required by the administrator. At a minimum, the situation needs to be investigated and if necessary, corrective action taken. Error An error occurred, possibly caused by an incompatibility between the local metadata repository and one of the remote repositories. The local repository may restrict some of the metadata interchange functions as a result. Exception An unexpected exception occurred. This means that the server needs some administration attention to correct configuration or fix a logic error because it is not operating as a proper peer in the open metadata repository cohort. Security Unauthorized access to a service or metadata instance has been attempted. Startup A new component is starting up. Shutdown An existing component is shutting down. Asset An auditable action relating to an asset has been taken. Types Activity is occurring that relates to the open metadata types in use by this server. Cohort The server is exchanging registration information about an open metadata repository cohort that it is connecting to. Trace This is additional information on the operation of the server that may be of assistance in debugging a problem. It is not normally logged to any destination, but can be added when needed. PerfMon This log record contains performance monitoring timing information for specific types of processing. It is not normally logged to any destination, but can be added when needed. <Unknown> Uninitialized Severity The body of the request should be a list of severities If an empty list is passed as the request body then all severities are supported by the destination. Add audit log destinations \u00b6 There are various destinations that can be configured for the audit log: console POST - add console audit log destination This writes selected parts of each audit log record to stdout. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/console slf4j POST - add slf4j audit log destination This writes full log records to the slf4j ecosystem. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/slf4j When configuring slf4j as destination you also need to specify auditlog logger category via the application properties. This is described in Connecting the OMAG Audit Log Framework section of the developer logging guide. file POST - add JSON file-based audit log destination This writes JSON files in a shared directory. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/files event POST - add event-based audit log destination This writes each log record as an event on the supplied event topic. It assumes that the event bus is set up first. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/event-topic connection POST - add connection-based audit log destination This sets up an audit log destination that is described though a connection . In this case, the connection is passed in the request body and the supported severities can be supplied in the connection's configuration properties. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/connection list POST - add a list of connection-based audit log destinations It is also possible to set up the audit log destinations as a list of connections. Using this option overrides all previous audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations Remove audit logs \u00b6 The following will remove all audit log destinations: POST - clear all audit log destinations Clears the list of audit log destinations from the configuration enabling you to add a new set of audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/none Configure the server security connector \u00b6 Metadata that is being aggregated from different sources is likely to need comprehensive access controls. Egeria provides fine-grained security control for metadata access . It is implemented in a server security connector that is called whenever requests are made for to the server. Security is configured for a specific OMAG Server by adding a connection for this connector to the server's configuration document using the following command. POST - configure security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This passes in a connection used to create the server security connector in the request body. { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } Example: set up the sample server security connector For example, this is the connection that would set up the sample server security connector provided for the Coco Pharmaceuticals case study: { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.OpenMetadataServerSecurityProvider\" } } Determine configured security \u00b6 GET - query the server security connector setting {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection Response indicating no security { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific security connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } } Remove configured security \u00b6 DELETE - remove configured security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This removes all authorization checking from the server. Configure the access services \u00b6 The Open Metadata Access Services (OMASs) provide the domain-specific APIs for metadata management and governance. They run in a metadata server or metadata access point and typically offer a REST API, Java client and an event-based interface for asynchronous interaction. Prerequisite configuration The access service configuration depends on the definitions of the event bus and the local server's userId . List available access services \u00b6 GET - list all available access services {{platformURLRoot}}/open-metadata/platform-services/users/{{adminUserId}}/server-platform/registered-services/access-services Response listing available access services { \"relatedHTTPCode\" : 200 , \"services\" : [ { \"serviceName\" : \"Asset Owner\" , \"serviceURLMarker\" : \"asset-owner\" , \"serviceDescription\" : \"Manage an asset\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/asset-owner/\" }, { \"serviceName\" : \"Stewardship Action\" , \"serviceURLMarker\" : \"stewardship-action\" , \"serviceDescription\" : \"Manage exceptions and actions from open governance\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/stewardship-action/\" }, { \"serviceName\" : \"Subject Area\" , \"serviceURLMarker\" : \"subject-area\" , \"serviceDescription\" : \"Document knowledge about a subject area\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/subject-area/\" }, { \"serviceName\" : \"Governance Program\" , \"serviceURLMarker\" : \"governance-program\" , \"serviceDescription\" : \"Manage the governance program\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/governance-program/\" }, { \"serviceName\" : \"Asset Lineage\" , \"serviceURLMarker\" : \"asset-lineage\" , \"serviceDescription\" : \"Store asset lineage\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/asset-lineage/\" }, { \"serviceName\" : \"Design Model\" , \"serviceURLMarker\" : \"design-model\" , \"serviceDescription\" : \"Exchange design model content with tools and standard packages\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/design-model/\" }, { \"serviceName\" : \"Glossary View\" , \"serviceURLMarker\" : \"glossary-view\" , \"serviceDescription\" : \"Support glossary terms visualization\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/glossary-view/\" }, { \"serviceName\" : \"Security Manager\" , \"serviceURLMarker\" : \"security-officer\" , \"serviceDescription\" : \"Set up rules to protect data\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/security-officer/\" }, { \"serviceName\" : \"Asset Consumer\" , \"serviceURLMarker\" : \"asset-consumer\" , \"serviceDescription\" : \"Access assets through connectors\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/asset-consumer/\" }, { \"serviceName\" : \"IT Infrastructure\" , \"serviceURLMarker\" : \"it-infrastructure\" , \"serviceDescription\" : \"Manage information about the deployed IT infrastructure\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/it-infrastructure/\" }, { \"serviceName\" : \"Asset Catalog\" , \"serviceURLMarker\" : \"asset-catalog\" , \"serviceDescription\" : \"Search and understand your assets\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/asset-catalog/\" }, { \"serviceName\" : \"Data Science\" , \"serviceURLMarker\" : \"data-science\" , \"serviceDescription\" : \"Create and manage data science definitions and models\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/data-science/\" }, { \"serviceName\" : \"Community Profile\" , \"serviceURLMarker\" : \"community-profile\" , \"serviceDescription\" : \"Define personal profile and collaborate\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/community-profile/\" }, { \"serviceName\" : \"DevOps\" , \"serviceURLMarker\" : \"devops\" , \"serviceDescription\" : \"Manage a DevOps pipeline\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/dev-ops/\" }, { \"serviceName\" : \"Software Developer\" , \"serviceURLMarker\" : \"software-developer\" , \"serviceDescription\" : \"Interact with software development tools\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/software-developer/\" }, { \"serviceName\" : \"Discovery Engine\" , \"serviceURLMarker\" : \"discovery-engine\" , \"serviceDescription\" : \"Support for automated metadata discovery engines\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/discovery-engine/\" }, { \"serviceName\" : \"Data Engine\" , \"serviceURLMarker\" : \"data-engine\" , \"serviceDescription\" : \"Exchange process models and lineage with a data engine\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/data-engine/\" }, { \"serviceName\" : \"Project Management\" , \"serviceURLMarker\" : \"project-management\" , \"serviceDescription\" : \"Manage data projects\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/project-management/\" }, { \"serviceName\" : \"Governance Engine\" , \"serviceURLMarker\" : \"governance-engine\" , \"serviceDescription\" : \"Set up an operational governance engine\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/governance-engine/\" }, { \"serviceName\" : \"Digital Architecture\" , \"serviceURLMarker\" : \"digital-architecture\" , \"serviceDescription\" : \"Design of the digital services for an organization\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/digital-architecture/\" }, { \"serviceName\" : \"Data Privacy\" , \"serviceURLMarker\" : \"data-privacy\" , \"serviceDescription\" : \"Manage governance of privacy\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/data-privacy/\" }, { \"serviceName\" : \"Data Manager\" , \"serviceURLMarker\" : \"data-manager\" , \"serviceDescription\" : \"Capture changes to the data stores and data set managed by a technology managing collections of data\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/data-manager/\" } ] } These access services are available to configure either all together or individually. Enable access services \u00b6 The access services can either all be enabled (with default configuration values) or individually enabled: all, with defaults To enable all the access services (and the enterprise repository services that support them) with default configuration values use the following command. POST - enable all access services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services individually Alternatively, each service can be configured individually with the following command: POST - configure an individual access service {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services/{{serviceURLMarker}} The service URL marker for each service is shown in the example response given above. In both cases, it is possible to pass a list of properties to the access service that controls the behavior of each access service. These are sent in the request body. More details of which properties are supported are documented with each access service. Disable the access services \u00b6 The access services can be disabled with the following command. This also disables the enterprise repository services since they are not being used. DELETE - disable the access services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services Review configuration \u00b6 GET - retrieve current configuration for the access services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services/configuration POST - save changes back to the configuration {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services/configuration","title":"Configure Metadata Access Point"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#configuring-a-metadata-access-point","text":"Each type of OMAG Server is configured by creating a configuration document . For a metadata access point, the following can be configured:","title":"Configuring a metadata access point"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#set-up-the-default-event-bus","text":"An OMAG Server uses an event bus such as Apache Kafka to exchange events with other servers and tools. Egeria manages the specific topic names and the event payloads; however, it needs to know where the event bus is deployed and any properties needed to configure it. Since the event bus is used in multiple places, the configuration document allows you to set up the details of the event bus which are then incorporated into all the places where the event bus is needed. Important sequencing information You need to set up this information before configuring any of the following: Using an event topic as the destination for the audit log . Configuring the access services in a metadata server or a metadata access point . Configuring registration to a cohort in a metadata server , a metadata access point , a repository proxy or a conformance test server . The following command creates information about the event bus. This information is used on the subsequent configuration of the OMAG Server subsystems. It does not affect any subsystems that have already been configured in the configuration document and if the event bus is not needed, its values are ignored. It is possible to add arbitrary name/value pairs as JSON in the request body. The correct properties to use are defined in the connector type for the event bus. POST - configure event bus {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/event-bus Example: Apache Kafka For example, when using Apache Kafka as your event bus you may want to configure properties that control the behavior of the consumer that receives events and the producer that sends events. This is a typical set of producer and consumer properties: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"producer\" : { \"bootstrap.servers\" : \"localhost:9092\" , \"acks\" : \"all\" , \"retries\" : \"0\" , \"batch.size\" : \"16384\" , \"linger.ms\" : \"1\" , \"buffer.memory\" : \"33554432\" , \"max.request.size\" : \"10485760\" , \"key.serializer\" : \"org.apache.kafka.common.serialization.StringSerializer\" , \"value.serializer\" : \"org.apache.kafka.common.serialization.StringSerializer\" , \"kafka.omrs.topic.id\" : \"cocoCohort\" }, \"consumer\" : { \"bootstrap.servers\" : \"localhost:9092\" , \"zookeeper.session.timeout.ms\" : \"400\" , \"zookeeper.sync.time.ms\" : \"200\" , \"fetch.message.max.bytes\" : \"10485760\" , \"max.partition.fetch.bytes\" : \"10485760\" , \"key.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"value.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"kafka.omrs.topic.id\" : \"cocoCohort\" } } A different type of event bus would use different properties.","title":"Set up the default event bus"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#set-the-server-url-root","text":"Configure the local server URL root with the value of the OMAG Server Platform where the service will run: in particular if the configuration document will be deployed to a different OMAG Server Platform from the one used to maintain the configuration document. POST - set server URL root {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-url-root?url={{targetPlatformURLRoot}} Detailed explanation The {{targetPlatformURLRoot}} gives the location of the OMAG Server Platform on which this configured service is intended to run, while the {{platformURLRoot}} gives the location of the OMAG Server Platform in which this configuration document is maintained. They could be, but do not need to be, the same location.","title":"Set the server URL root"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#configure-the-basic-properties","text":"The basic properties of the OMAG Server are used in logging and events originating from the server. They help to document the purpose of the server (which helps with problem determination) and enable performance improvements by allowing the server to ignore activity or metadata that is not relevant to its operation. The basic properties include two unique identifiers: Property Description localServerId Unique identifier for this server. By default, this is initialized to a randomly generated Universal Unique identifier (UUID). localServerName Meaningful name for the server for use in messages and UIs. Ideally this value is unique to aid administrators in understanding the source of messages and events from the server. This value is set to the server name assigned when the configuration is created. The other basic properties have values that can be changed through the admin services API: Property Description localServerType Descriptive type name for the server. Again this is useful information for the administrator to understand the role of the server. The default value is Open Metadata and Governance Server . organizationName Descriptive name for the organization that owns the local server/repository. This is useful when the open metadata repository cluster consists of metadata servers from different organizations, or different departments of an enterprise. The default value is null . localServerUserId UserId to use for server-initiated REST calls. The default is OMAGServer . localServerPassword Password to use for server-initiated REST calls. The default is null . This means that only the userId is sent in the HTTP header. maxPageSize The maximum page size that can be set on requests to the server. The default value is 1000 . A value of zero means unlimited page size. Although supported, the zero value is not recommended because it provides no protection from a large request denial of service attack. The sections that follow cover how to set up these values.","title":"Configure the basic properties"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#set-server-type-name","text":"The server type name should be set to something that describes the OMAG Server's role. It may be the name of a specific product that it is enabling, or a role in the metadata and governance landscape. POST - set server type {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-type?typeName=\"{{serverTypeName}}\"","title":"Set server type name"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#set-organization-name","text":"The organization name may be the owning organization or department or team supported by the server. POST - set organization name {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/organization-name?name=\"{{organizationName}}\"","title":"Set organization name"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#set-the-servers-user-id-and-optional-password","text":"The server's user ID is used when processing requests that do not have an end user, such as receiving an event from a topic. The default value is OMAGServer . Ideally each server should have its own user ID so it is possible to restrict the resources that each server has access to. If the password is specified as well, the userId and password combination are used to provide authentication information on each REST call made by the server. POST - set server's userId {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-id?id=\"{{serverUserId}}\" POST - set server's password {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-password?password=\"{{serverUserPassword}}\"","title":"Set the server's user ID and optional password"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#set-the-maximum-page-size-for-rest-api-requests","text":"The maximum page size value sets an upper limit on the number of results that a caller can request on any paging REST API to this server. Setting maximum page size helps to prevent a denial of service attack that uses very large requests to overwhelm the server. A value of 0 means no limit, and leaves the server open to such attacks. POST - set maximum page size {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/max-page-size?limit={{maxPageSize}}","title":"Set the maximum page size for REST API requests"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#configure-the-audit-log","text":"Egeria's audit log provides a configurable set of destinations for audit records and other diagnostic logging for an OMAG Server . Some destinations also support a query interface to allow an administrator to understand how the server is running. If the server is a development or test server, then the default audit log configuration is probably sufficient: the console audit log destination. POST - set default audit log destination {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/default Using this option overrides all previous audit log destinations. If this server is a production server then you will probably want to set up the audit log destinations explicitly. You can add multiple destinations and each one can be set up to process specific severities of log records. The audit log severities are as follows: Severity Description Information The server is providing information about its normal operation. Event An event was received from another member of the open metadata repository cohort. Decision A decision has been made related to the interaction of the local metadata repository and the rest of the cohort. Action An Action is required by the administrator. At a minimum, the situation needs to be investigated and if necessary, corrective action taken. Error An error occurred, possibly caused by an incompatibility between the local metadata repository and one of the remote repositories. The local repository may restrict some of the metadata interchange functions as a result. Exception An unexpected exception occurred. This means that the server needs some administration attention to correct configuration or fix a logic error because it is not operating as a proper peer in the open metadata repository cohort. Security Unauthorized access to a service or metadata instance has been attempted. Startup A new component is starting up. Shutdown An existing component is shutting down. Asset An auditable action relating to an asset has been taken. Types Activity is occurring that relates to the open metadata types in use by this server. Cohort The server is exchanging registration information about an open metadata repository cohort that it is connecting to. Trace This is additional information on the operation of the server that may be of assistance in debugging a problem. It is not normally logged to any destination, but can be added when needed. PerfMon This log record contains performance monitoring timing information for specific types of processing. It is not normally logged to any destination, but can be added when needed. <Unknown> Uninitialized Severity The body of the request should be a list of severities If an empty list is passed as the request body then all severities are supported by the destination.","title":"Configure the audit log"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#add-audit-log-destinations","text":"There are various destinations that can be configured for the audit log: console POST - add console audit log destination This writes selected parts of each audit log record to stdout. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/console slf4j POST - add slf4j audit log destination This writes full log records to the slf4j ecosystem. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/slf4j When configuring slf4j as destination you also need to specify auditlog logger category via the application properties. This is described in Connecting the OMAG Audit Log Framework section of the developer logging guide. file POST - add JSON file-based audit log destination This writes JSON files in a shared directory. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/files event POST - add event-based audit log destination This writes each log record as an event on the supplied event topic. It assumes that the event bus is set up first. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/event-topic connection POST - add connection-based audit log destination This sets up an audit log destination that is described though a connection . In this case, the connection is passed in the request body and the supported severities can be supplied in the connection's configuration properties. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/connection list POST - add a list of connection-based audit log destinations It is also possible to set up the audit log destinations as a list of connections. Using this option overrides all previous audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations","title":"Add audit log destinations"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#remove-audit-logs","text":"The following will remove all audit log destinations: POST - clear all audit log destinations Clears the list of audit log destinations from the configuration enabling you to add a new set of audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/none","title":"Remove audit logs"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#configure-the-server-security-connector","text":"Metadata that is being aggregated from different sources is likely to need comprehensive access controls. Egeria provides fine-grained security control for metadata access . It is implemented in a server security connector that is called whenever requests are made for to the server. Security is configured for a specific OMAG Server by adding a connection for this connector to the server's configuration document using the following command. POST - configure security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This passes in a connection used to create the server security connector in the request body. { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } Example: set up the sample server security connector For example, this is the connection that would set up the sample server security connector provided for the Coco Pharmaceuticals case study: { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.OpenMetadataServerSecurityProvider\" } }","title":"Configure the server security connector"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#determine-configured-security","text":"GET - query the server security connector setting {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection Response indicating no security { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific security connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } }","title":"Determine configured security"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#remove-configured-security","text":"DELETE - remove configured security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This removes all authorization checking from the server.","title":"Remove configured security"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#configure-the-access-services","text":"The Open Metadata Access Services (OMASs) provide the domain-specific APIs for metadata management and governance. They run in a metadata server or metadata access point and typically offer a REST API, Java client and an event-based interface for asynchronous interaction. Prerequisite configuration The access service configuration depends on the definitions of the event bus and the local server's userId .","title":"Configure the access services"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#list-available-access-services","text":"GET - list all available access services {{platformURLRoot}}/open-metadata/platform-services/users/{{adminUserId}}/server-platform/registered-services/access-services Response listing available access services { \"relatedHTTPCode\" : 200 , \"services\" : [ { \"serviceName\" : \"Asset Owner\" , \"serviceURLMarker\" : \"asset-owner\" , \"serviceDescription\" : \"Manage an asset\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/asset-owner/\" }, { \"serviceName\" : \"Stewardship Action\" , \"serviceURLMarker\" : \"stewardship-action\" , \"serviceDescription\" : \"Manage exceptions and actions from open governance\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/stewardship-action/\" }, { \"serviceName\" : \"Subject Area\" , \"serviceURLMarker\" : \"subject-area\" , \"serviceDescription\" : \"Document knowledge about a subject area\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/subject-area/\" }, { \"serviceName\" : \"Governance Program\" , \"serviceURLMarker\" : \"governance-program\" , \"serviceDescription\" : \"Manage the governance program\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/governance-program/\" }, { \"serviceName\" : \"Asset Lineage\" , \"serviceURLMarker\" : \"asset-lineage\" , \"serviceDescription\" : \"Store asset lineage\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/asset-lineage/\" }, { \"serviceName\" : \"Design Model\" , \"serviceURLMarker\" : \"design-model\" , \"serviceDescription\" : \"Exchange design model content with tools and standard packages\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/design-model/\" }, { \"serviceName\" : \"Glossary View\" , \"serviceURLMarker\" : \"glossary-view\" , \"serviceDescription\" : \"Support glossary terms visualization\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/glossary-view/\" }, { \"serviceName\" : \"Security Manager\" , \"serviceURLMarker\" : \"security-officer\" , \"serviceDescription\" : \"Set up rules to protect data\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/security-officer/\" }, { \"serviceName\" : \"Asset Consumer\" , \"serviceURLMarker\" : \"asset-consumer\" , \"serviceDescription\" : \"Access assets through connectors\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/asset-consumer/\" }, { \"serviceName\" : \"IT Infrastructure\" , \"serviceURLMarker\" : \"it-infrastructure\" , \"serviceDescription\" : \"Manage information about the deployed IT infrastructure\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/it-infrastructure/\" }, { \"serviceName\" : \"Asset Catalog\" , \"serviceURLMarker\" : \"asset-catalog\" , \"serviceDescription\" : \"Search and understand your assets\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/asset-catalog/\" }, { \"serviceName\" : \"Data Science\" , \"serviceURLMarker\" : \"data-science\" , \"serviceDescription\" : \"Create and manage data science definitions and models\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/data-science/\" }, { \"serviceName\" : \"Community Profile\" , \"serviceURLMarker\" : \"community-profile\" , \"serviceDescription\" : \"Define personal profile and collaborate\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/community-profile/\" }, { \"serviceName\" : \"DevOps\" , \"serviceURLMarker\" : \"devops\" , \"serviceDescription\" : \"Manage a DevOps pipeline\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/dev-ops/\" }, { \"serviceName\" : \"Software Developer\" , \"serviceURLMarker\" : \"software-developer\" , \"serviceDescription\" : \"Interact with software development tools\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/software-developer/\" }, { \"serviceName\" : \"Discovery Engine\" , \"serviceURLMarker\" : \"discovery-engine\" , \"serviceDescription\" : \"Support for automated metadata discovery engines\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/discovery-engine/\" }, { \"serviceName\" : \"Data Engine\" , \"serviceURLMarker\" : \"data-engine\" , \"serviceDescription\" : \"Exchange process models and lineage with a data engine\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/data-engine/\" }, { \"serviceName\" : \"Project Management\" , \"serviceURLMarker\" : \"project-management\" , \"serviceDescription\" : \"Manage data projects\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/project-management/\" }, { \"serviceName\" : \"Governance Engine\" , \"serviceURLMarker\" : \"governance-engine\" , \"serviceDescription\" : \"Set up an operational governance engine\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/governance-engine/\" }, { \"serviceName\" : \"Digital Architecture\" , \"serviceURLMarker\" : \"digital-architecture\" , \"serviceDescription\" : \"Design of the digital services for an organization\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/digital-architecture/\" }, { \"serviceName\" : \"Data Privacy\" , \"serviceURLMarker\" : \"data-privacy\" , \"serviceDescription\" : \"Manage governance of privacy\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/data-privacy/\" }, { \"serviceName\" : \"Data Manager\" , \"serviceURLMarker\" : \"data-manager\" , \"serviceDescription\" : \"Capture changes to the data stores and data set managed by a technology managing collections of data\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/data-manager/\" } ] } These access services are available to configure either all together or individually.","title":"List available access services"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#enable-access-services","text":"The access services can either all be enabled (with default configuration values) or individually enabled: all, with defaults To enable all the access services (and the enterprise repository services that support them) with default configuration values use the following command. POST - enable all access services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services individually Alternatively, each service can be configured individually with the following command: POST - configure an individual access service {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services/{{serviceURLMarker}} The service URL marker for each service is shown in the example response given above. In both cases, it is possible to pass a list of properties to the access service that controls the behavior of each access service. These are sent in the request body. More details of which properties are supported are documented with each access service.","title":"Enable access services"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#disable-the-access-services","text":"The access services can be disabled with the following command. This also disables the enterprise repository services since they are not being used. DELETE - disable the access services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services","title":"Disable the access services"},{"location":"guides/admin/servers/configuring-a-metadata-access-point/#review-configuration","text":"GET - retrieve current configuration for the access services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services/configuration POST - save changes back to the configuration {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services/configuration","title":"Review configuration"},{"location":"guides/admin/servers/configuring-a-metadata-server/","text":"Configuring a metadata server \u00b6 Each type of OMAG Server is configured by creating a configuration document . A metadata server can be run standalone, without connecting to a cohort: Adding the cohort configuration enables the metadata server to communicate with other metadata servers: Set up the default event bus \u00b6 An OMAG Server uses an event bus such as Apache Kafka to exchange events with other servers and tools. Egeria manages the specific topic names and the event payloads; however, it needs to know where the event bus is deployed and any properties needed to configure it. Since the event bus is used in multiple places, the configuration document allows you to set up the details of the event bus which are then incorporated into all the places where the event bus is needed. Important sequencing information You need to set up this information before configuring any of the following: Using an event topic as the destination for the audit log . Configuring the access services in a metadata server or a metadata access point . Configuring registration to a cohort in a metadata server , a metadata access point , a repository proxy or a conformance test server . The following command creates information about the event bus. This information is used on the subsequent configuration of the OMAG Server subsystems. It does not affect any subsystems that have already been configured in the configuration document and if the event bus is not needed, its values are ignored. It is possible to add arbitrary name/value pairs as JSON in the request body. The correct properties to use are defined in the connector type for the event bus. POST - configure event bus {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/event-bus Example: Apache Kafka For example, when using Apache Kafka as your event bus you may want to configure properties that control the behavior of the consumer that receives events and the producer that sends events. This is a typical set of producer and consumer properties: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"producer\" : { \"bootstrap.servers\" : \"localhost:9092\" , \"acks\" : \"all\" , \"retries\" : \"0\" , \"batch.size\" : \"16384\" , \"linger.ms\" : \"1\" , \"buffer.memory\" : \"33554432\" , \"max.request.size\" : \"10485760\" , \"key.serializer\" : \"org.apache.kafka.common.serialization.StringSerializer\" , \"value.serializer\" : \"org.apache.kafka.common.serialization.StringSerializer\" , \"kafka.omrs.topic.id\" : \"cocoCohort\" }, \"consumer\" : { \"bootstrap.servers\" : \"localhost:9092\" , \"zookeeper.session.timeout.ms\" : \"400\" , \"zookeeper.sync.time.ms\" : \"200\" , \"fetch.message.max.bytes\" : \"10485760\" , \"max.partition.fetch.bytes\" : \"10485760\" , \"key.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"value.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"kafka.omrs.topic.id\" : \"cocoCohort\" } } A different type of event bus would use different properties. Set the server URL root \u00b6 Configure the local server URL root with the value of the OMAG Server Platform where the service will run: in particular if the configuration document will be deployed to a different OMAG Server Platform from the one used to maintain the configuration document. POST - set server URL root {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-url-root?url={{targetPlatformURLRoot}} Detailed explanation The {{targetPlatformURLRoot}} gives the location of the OMAG Server Platform on which this configured service is intended to run, while the {{platformURLRoot}} gives the location of the OMAG Server Platform in which this configuration document is maintained. They could be, but do not need to be, the same location. Configure the basic properties \u00b6 The basic properties of the OMAG Server are used in logging and events originating from the server. They help to document the purpose of the server (which helps with problem determination) and enable performance improvements by allowing the server to ignore activity or metadata that is not relevant to its operation. The basic properties include two unique identifiers: Property Description localServerId Unique identifier for this server. By default, this is initialized to a randomly generated Universal Unique identifier (UUID). localServerName Meaningful name for the server for use in messages and UIs. Ideally this value is unique to aid administrators in understanding the source of messages and events from the server. This value is set to the server name assigned when the configuration is created. The other basic properties have values that can be changed through the admin services API: Property Description localServerType Descriptive type name for the server. Again this is useful information for the administrator to understand the role of the server. The default value is Open Metadata and Governance Server . organizationName Descriptive name for the organization that owns the local server/repository. This is useful when the open metadata repository cluster consists of metadata servers from different organizations, or different departments of an enterprise. The default value is null . localServerUserId UserId to use for server-initiated REST calls. The default is OMAGServer . localServerPassword Password to use for server-initiated REST calls. The default is null . This means that only the userId is sent in the HTTP header. maxPageSize The maximum page size that can be set on requests to the server. The default value is 1000 . A value of zero means unlimited page size. Although supported, the zero value is not recommended because it provides no protection from a large request denial of service attack. The sections that follow cover how to set up these values. Set server type name \u00b6 The server type name should be set to something that describes the OMAG Server's role. It may be the name of a specific product that it is enabling, or a role in the metadata and governance landscape. POST - set server type {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-type?typeName=\"{{serverTypeName}}\" Set organization name \u00b6 The organization name may be the owning organization or department or team supported by the server. POST - set organization name {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/organization-name?name=\"{{organizationName}}\" Set the server's user ID and optional password \u00b6 The server's user ID is used when processing requests that do not have an end user, such as receiving an event from a topic. The default value is OMAGServer . Ideally each server should have its own user ID so it is possible to restrict the resources that each server has access to. If the password is specified as well, the userId and password combination are used to provide authentication information on each REST call made by the server. POST - set server's userId {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-id?id=\"{{serverUserId}}\" POST - set server's password {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-password?password=\"{{serverUserPassword}}\" Set the maximum page size for REST API requests \u00b6 The maximum page size value sets an upper limit on the number of results that a caller can request on any paging REST API to this server. Setting maximum page size helps to prevent a denial of service attack that uses very large requests to overwhelm the server. A value of 0 means no limit, and leaves the server open to such attacks. POST - set maximum page size {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/max-page-size?limit={{maxPageSize}} Configure the audit log \u00b6 Egeria's audit log provides a configurable set of destinations for audit records and other diagnostic logging for an OMAG Server . Some destinations also support a query interface to allow an administrator to understand how the server is running. If the server is a development or test server, then the default audit log configuration is probably sufficient: the console audit log destination. POST - set default audit log destination {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/default Using this option overrides all previous audit log destinations. If this server is a production server then you will probably want to set up the audit log destinations explicitly. You can add multiple destinations and each one can be set up to process specific severities of log records. The audit log severities are as follows: Severity Description Information The server is providing information about its normal operation. Event An event was received from another member of the open metadata repository cohort. Decision A decision has been made related to the interaction of the local metadata repository and the rest of the cohort. Action An Action is required by the administrator. At a minimum, the situation needs to be investigated and if necessary, corrective action taken. Error An error occurred, possibly caused by an incompatibility between the local metadata repository and one of the remote repositories. The local repository may restrict some of the metadata interchange functions as a result. Exception An unexpected exception occurred. This means that the server needs some administration attention to correct configuration or fix a logic error because it is not operating as a proper peer in the open metadata repository cohort. Security Unauthorized access to a service or metadata instance has been attempted. Startup A new component is starting up. Shutdown An existing component is shutting down. Asset An auditable action relating to an asset has been taken. Types Activity is occurring that relates to the open metadata types in use by this server. Cohort The server is exchanging registration information about an open metadata repository cohort that it is connecting to. Trace This is additional information on the operation of the server that may be of assistance in debugging a problem. It is not normally logged to any destination, but can be added when needed. PerfMon This log record contains performance monitoring timing information for specific types of processing. It is not normally logged to any destination, but can be added when needed. <Unknown> Uninitialized Severity The body of the request should be a list of severities If an empty list is passed as the request body then all severities are supported by the destination. Add audit log destinations \u00b6 There are various destinations that can be configured for the audit log: console POST - add console audit log destination This writes selected parts of each audit log record to stdout. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/console slf4j POST - add slf4j audit log destination This writes full log records to the slf4j ecosystem. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/slf4j When configuring slf4j as destination you also need to specify auditlog logger category via the application properties. This is described in Connecting the OMAG Audit Log Framework section of the developer logging guide. file POST - add JSON file-based audit log destination This writes JSON files in a shared directory. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/files event POST - add event-based audit log destination This writes each log record as an event on the supplied event topic. It assumes that the event bus is set up first. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/event-topic connection POST - add connection-based audit log destination This sets up an audit log destination that is described though a connection . In this case, the connection is passed in the request body and the supported severities can be supplied in the connection's configuration properties. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/connection list POST - add a list of connection-based audit log destinations It is also possible to set up the audit log destinations as a list of connections. Using this option overrides all previous audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations Remove audit logs \u00b6 The following will remove all audit log destinations: POST - clear all audit log destinations Clears the list of audit log destinations from the configuration enabling you to add a new set of audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/none Configure the server security connector \u00b6 Metadata that is being aggregated from different sources is likely to need comprehensive access controls. Egeria provides fine-grained security control for metadata access . It is implemented in a server security connector that is called whenever requests are made for to the server. Security is configured for a specific OMAG Server by adding a connection for this connector to the server's configuration document using the following command. POST - configure security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This passes in a connection used to create the server security connector in the request body. { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } Example: set up the sample server security connector For example, this is the connection that would set up the sample server security connector provided for the Coco Pharmaceuticals case study: { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.OpenMetadataServerSecurityProvider\" } } Determine configured security \u00b6 GET - query the server security connector setting {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection Response indicating no security { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific security connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } } Remove configured security \u00b6 DELETE - remove configured security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This removes all authorization checking from the server. Configure the local repository \u00b6 A metadata server supports a local metadata repository that has native support for the Open Metadata Repository Services ( OMRS ) types and instances . Choose a repository \u00b6 Egeria provides a number of implementations of such a repository -- only one of these options can be configured for a given metadata server at a time. bi-temporal graph This command enables a XTDB-based metadata repository, which itself has a number of pluggable back-end options for persistence and other configuration options . This plugin repository is currently the highest-performing, most fully-functional repository for Egeria, supporting all metadata operations including historical metadata as well as being highly-available through clustered deployment . POST - enable the bi-temporal graph repository {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/local-repository/mode/plugin-repository/connection { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.egeria.connectors.juxt.xtdb.repositoryconnector.XtdbOMRSRepositoryConnectorProvider\" } } May require additional driver libraries Note that depending on the persistence you configure, you may need to obtain additional driver libraries for your back-end service , as not every driver is embedded in the XTDB connector itself. non-temporal graph This command enables a JanusGraph-based metadata repository that is embedded in the metadata server and uses the local disk to store the metadata, but does not manage any historical metadata. POST - enable the graph repository {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/local-repository/mode/local-graph-repository in-memory The in-memory repository maintains an in-memory store of metadata. It is useful for demos and testing. No metadata is kept if the open metadata services are deactivated, or the server is shutdown. It should nto be used in a production environment. POST - enable the in-memory repository {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/local-repository/mode/in-memory-repository read-only The read-only repository connector provides a compliant implementation of a local repository that can be configured into a metadata server. It does not support the interfaces for create, update, delete. However, it does support the search interfaces and is able to cache metadata. This means it can be loaded with metadata from an open metadata archive and connected to a cohort. The content from the archive will be shared with other members of the cohort. POST - enable the read-only repository {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/local-repository/mode/read-only-repository Remove the local repository \u00b6 This command removes all configuration for the local repository. This includes the local metadata collection id . If a new local repository is added, it will have a new local metadata collection id and will not be able to automatically re-register with its cohort(s). DELETE - remove the local repository {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/local-repository Load metadata \u00b6 Open metadata archives contain pre-canned metadata types and instances for cohort members . Archives can be added to the configuration document of a server to ensure their content is loaded each time the server is started. This is intended for repositories that do not store the archive content but keep it in memory. Archives can also be loaded to a running server . Loading the same archive multiple times If an archive is loaded multiple times, its content is only added to the local repository if the repository does not have the content already. Add to a running server \u00b6 Typically, an open metadata archive is stored as JSON format in a file. To load such a file use the following command: POST - load file {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/instance/open-metadata-archives/file The body of the request should be the fully-qualified path name or path relative to the startup directory of the OMAG Server Platform -- and the file name should not have any quotes around it. Alternatively it is possible to set up the list of open metadata archives as a list of connections . These connections refer to open metadata archive connectors that can read and retrieve the open metadata archive content. POST - load from connection(s) {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/instance/open-metadata-archives/connection The body of the request should be the list of connections from which to load archives. This option can be used when the open metadata archives are not stored in a file, or a different file format from the default one for the OMAG Server Platform is required. Configure metadata to load on startup \u00b6 Typically, an open metadata archive is stored as JSON format in a file. To configure the load of such a file use the following command: POST - specify file to load POST {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/open-metadata-archives/file The body of the request should be the fully-qualified path name or path relative to the startup directory of the OMAG Server Platform -- and the file name should not have any quotes around it. Alternatively it is possible to set up the list of open metadata archives as a list of connections . These connections refer to connectors that can read and retrieve the open metadata archive content. POST - specify connection(s) to load {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/open-metadata-archives The body of the request should be the list of connections from which to load archives. This option can be used when the open metadata archives are not stored in a file, or a different file connector from the default one for the OMAG Server Platform is required. Remove metadata load on startup \u00b6 Finally, this is how to remove the archives from the configuration document. DELETE - remove archives from configuration document {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/open-metadata-archives The body of the request should be the path to the metadata archive file. Configure the access services \u00b6 The Open Metadata Access Services (OMASs) provide the domain-specific APIs for metadata management and governance. They run in a metadata server or metadata access point and typically offer a REST API, Java client and an event-based interface for asynchronous interaction. Prerequisite configuration The access service configuration depends on the definitions of the event bus and the local server's userId . List available access services \u00b6 GET - list all available access services {{platformURLRoot}}/open-metadata/platform-services/users/{{adminUserId}}/server-platform/registered-services/access-services Response listing available access services { \"relatedHTTPCode\" : 200 , \"services\" : [ { \"serviceName\" : \"Asset Owner\" , \"serviceURLMarker\" : \"asset-owner\" , \"serviceDescription\" : \"Manage an asset\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/asset-owner/\" }, { \"serviceName\" : \"Stewardship Action\" , \"serviceURLMarker\" : \"stewardship-action\" , \"serviceDescription\" : \"Manage exceptions and actions from open governance\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/stewardship-action/\" }, { \"serviceName\" : \"Subject Area\" , \"serviceURLMarker\" : \"subject-area\" , \"serviceDescription\" : \"Document knowledge about a subject area\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/subject-area/\" }, { \"serviceName\" : \"Governance Program\" , \"serviceURLMarker\" : \"governance-program\" , \"serviceDescription\" : \"Manage the governance program\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/governance-program/\" }, { \"serviceName\" : \"Asset Lineage\" , \"serviceURLMarker\" : \"asset-lineage\" , \"serviceDescription\" : \"Store asset lineage\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/asset-lineage/\" }, { \"serviceName\" : \"Design Model\" , \"serviceURLMarker\" : \"design-model\" , \"serviceDescription\" : \"Exchange design model content with tools and standard packages\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/design-model/\" }, { \"serviceName\" : \"Glossary View\" , \"serviceURLMarker\" : \"glossary-view\" , \"serviceDescription\" : \"Support glossary terms visualization\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/glossary-view/\" }, { \"serviceName\" : \"Security Manager\" , \"serviceURLMarker\" : \"security-officer\" , \"serviceDescription\" : \"Set up rules to protect data\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/security-officer/\" }, { \"serviceName\" : \"Asset Consumer\" , \"serviceURLMarker\" : \"asset-consumer\" , \"serviceDescription\" : \"Access assets through connectors\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/asset-consumer/\" }, { \"serviceName\" : \"IT Infrastructure\" , \"serviceURLMarker\" : \"it-infrastructure\" , \"serviceDescription\" : \"Manage information about the deployed IT infrastructure\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/it-infrastructure/\" }, { \"serviceName\" : \"Asset Catalog\" , \"serviceURLMarker\" : \"asset-catalog\" , \"serviceDescription\" : \"Search and understand your assets\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/asset-catalog/\" }, { \"serviceName\" : \"Data Science\" , \"serviceURLMarker\" : \"data-science\" , \"serviceDescription\" : \"Create and manage data science definitions and models\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/data-science/\" }, { \"serviceName\" : \"Community Profile\" , \"serviceURLMarker\" : \"community-profile\" , \"serviceDescription\" : \"Define personal profile and collaborate\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/community-profile/\" }, { \"serviceName\" : \"DevOps\" , \"serviceURLMarker\" : \"devops\" , \"serviceDescription\" : \"Manage a DevOps pipeline\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/dev-ops/\" }, { \"serviceName\" : \"Software Developer\" , \"serviceURLMarker\" : \"software-developer\" , \"serviceDescription\" : \"Interact with software development tools\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/software-developer/\" }, { \"serviceName\" : \"Discovery Engine\" , \"serviceURLMarker\" : \"discovery-engine\" , \"serviceDescription\" : \"Support for automated metadata discovery engines\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/discovery-engine/\" }, { \"serviceName\" : \"Data Engine\" , \"serviceURLMarker\" : \"data-engine\" , \"serviceDescription\" : \"Exchange process models and lineage with a data engine\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/data-engine/\" }, { \"serviceName\" : \"Project Management\" , \"serviceURLMarker\" : \"project-management\" , \"serviceDescription\" : \"Manage data projects\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/project-management/\" }, { \"serviceName\" : \"Governance Engine\" , \"serviceURLMarker\" : \"governance-engine\" , \"serviceDescription\" : \"Set up an operational governance engine\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/governance-engine/\" }, { \"serviceName\" : \"Digital Architecture\" , \"serviceURLMarker\" : \"digital-architecture\" , \"serviceDescription\" : \"Design of the digital services for an organization\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/digital-architecture/\" }, { \"serviceName\" : \"Data Privacy\" , \"serviceURLMarker\" : \"data-privacy\" , \"serviceDescription\" : \"Manage governance of privacy\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/data-privacy/\" }, { \"serviceName\" : \"Data Manager\" , \"serviceURLMarker\" : \"data-manager\" , \"serviceDescription\" : \"Capture changes to the data stores and data set managed by a technology managing collections of data\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/data-manager/\" } ] } These access services are available to configure either all together or individually. Enable access services \u00b6 The access services can either all be enabled (with default configuration values) or individually enabled: all, with defaults To enable all the access services (and the enterprise repository services that support them) with default configuration values use the following command. POST - enable all access services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services individually Alternatively, each service can be configured individually with the following command: POST - configure an individual access service {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services/{{serviceURLMarker}} The service URL marker for each service is shown in the example response given above. In both cases, it is possible to pass a list of properties to the access service that controls the behavior of each access service. These are sent in the request body. More details of which properties are supported are documented with each access service. Disable the access services \u00b6 The access services can be disabled with the following command. This also disables the enterprise repository services since they are not being used. DELETE - disable the access services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services Review configuration \u00b6 GET - retrieve current configuration for the access services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services/configuration POST - save changes back to the configuration {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services/configuration","title":"Configure Metadata Server"},{"location":"guides/admin/servers/configuring-a-metadata-server/#configuring-a-metadata-server","text":"Each type of OMAG Server is configured by creating a configuration document . A metadata server can be run standalone, without connecting to a cohort: Adding the cohort configuration enables the metadata server to communicate with other metadata servers:","title":"Configuring a metadata server"},{"location":"guides/admin/servers/configuring-a-metadata-server/#set-up-the-default-event-bus","text":"An OMAG Server uses an event bus such as Apache Kafka to exchange events with other servers and tools. Egeria manages the specific topic names and the event payloads; however, it needs to know where the event bus is deployed and any properties needed to configure it. Since the event bus is used in multiple places, the configuration document allows you to set up the details of the event bus which are then incorporated into all the places where the event bus is needed. Important sequencing information You need to set up this information before configuring any of the following: Using an event topic as the destination for the audit log . Configuring the access services in a metadata server or a metadata access point . Configuring registration to a cohort in a metadata server , a metadata access point , a repository proxy or a conformance test server . The following command creates information about the event bus. This information is used on the subsequent configuration of the OMAG Server subsystems. It does not affect any subsystems that have already been configured in the configuration document and if the event bus is not needed, its values are ignored. It is possible to add arbitrary name/value pairs as JSON in the request body. The correct properties to use are defined in the connector type for the event bus. POST - configure event bus {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/event-bus Example: Apache Kafka For example, when using Apache Kafka as your event bus you may want to configure properties that control the behavior of the consumer that receives events and the producer that sends events. This is a typical set of producer and consumer properties: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"producer\" : { \"bootstrap.servers\" : \"localhost:9092\" , \"acks\" : \"all\" , \"retries\" : \"0\" , \"batch.size\" : \"16384\" , \"linger.ms\" : \"1\" , \"buffer.memory\" : \"33554432\" , \"max.request.size\" : \"10485760\" , \"key.serializer\" : \"org.apache.kafka.common.serialization.StringSerializer\" , \"value.serializer\" : \"org.apache.kafka.common.serialization.StringSerializer\" , \"kafka.omrs.topic.id\" : \"cocoCohort\" }, \"consumer\" : { \"bootstrap.servers\" : \"localhost:9092\" , \"zookeeper.session.timeout.ms\" : \"400\" , \"zookeeper.sync.time.ms\" : \"200\" , \"fetch.message.max.bytes\" : \"10485760\" , \"max.partition.fetch.bytes\" : \"10485760\" , \"key.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"value.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"kafka.omrs.topic.id\" : \"cocoCohort\" } } A different type of event bus would use different properties.","title":"Set up the default event bus"},{"location":"guides/admin/servers/configuring-a-metadata-server/#set-the-server-url-root","text":"Configure the local server URL root with the value of the OMAG Server Platform where the service will run: in particular if the configuration document will be deployed to a different OMAG Server Platform from the one used to maintain the configuration document. POST - set server URL root {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-url-root?url={{targetPlatformURLRoot}} Detailed explanation The {{targetPlatformURLRoot}} gives the location of the OMAG Server Platform on which this configured service is intended to run, while the {{platformURLRoot}} gives the location of the OMAG Server Platform in which this configuration document is maintained. They could be, but do not need to be, the same location.","title":"Set the server URL root"},{"location":"guides/admin/servers/configuring-a-metadata-server/#configure-the-basic-properties","text":"The basic properties of the OMAG Server are used in logging and events originating from the server. They help to document the purpose of the server (which helps with problem determination) and enable performance improvements by allowing the server to ignore activity or metadata that is not relevant to its operation. The basic properties include two unique identifiers: Property Description localServerId Unique identifier for this server. By default, this is initialized to a randomly generated Universal Unique identifier (UUID). localServerName Meaningful name for the server for use in messages and UIs. Ideally this value is unique to aid administrators in understanding the source of messages and events from the server. This value is set to the server name assigned when the configuration is created. The other basic properties have values that can be changed through the admin services API: Property Description localServerType Descriptive type name for the server. Again this is useful information for the administrator to understand the role of the server. The default value is Open Metadata and Governance Server . organizationName Descriptive name for the organization that owns the local server/repository. This is useful when the open metadata repository cluster consists of metadata servers from different organizations, or different departments of an enterprise. The default value is null . localServerUserId UserId to use for server-initiated REST calls. The default is OMAGServer . localServerPassword Password to use for server-initiated REST calls. The default is null . This means that only the userId is sent in the HTTP header. maxPageSize The maximum page size that can be set on requests to the server. The default value is 1000 . A value of zero means unlimited page size. Although supported, the zero value is not recommended because it provides no protection from a large request denial of service attack. The sections that follow cover how to set up these values.","title":"Configure the basic properties"},{"location":"guides/admin/servers/configuring-a-metadata-server/#set-server-type-name","text":"The server type name should be set to something that describes the OMAG Server's role. It may be the name of a specific product that it is enabling, or a role in the metadata and governance landscape. POST - set server type {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-type?typeName=\"{{serverTypeName}}\"","title":"Set server type name"},{"location":"guides/admin/servers/configuring-a-metadata-server/#set-organization-name","text":"The organization name may be the owning organization or department or team supported by the server. POST - set organization name {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/organization-name?name=\"{{organizationName}}\"","title":"Set organization name"},{"location":"guides/admin/servers/configuring-a-metadata-server/#set-the-servers-user-id-and-optional-password","text":"The server's user ID is used when processing requests that do not have an end user, such as receiving an event from a topic. The default value is OMAGServer . Ideally each server should have its own user ID so it is possible to restrict the resources that each server has access to. If the password is specified as well, the userId and password combination are used to provide authentication information on each REST call made by the server. POST - set server's userId {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-id?id=\"{{serverUserId}}\" POST - set server's password {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-password?password=\"{{serverUserPassword}}\"","title":"Set the server's user ID and optional password"},{"location":"guides/admin/servers/configuring-a-metadata-server/#set-the-maximum-page-size-for-rest-api-requests","text":"The maximum page size value sets an upper limit on the number of results that a caller can request on any paging REST API to this server. Setting maximum page size helps to prevent a denial of service attack that uses very large requests to overwhelm the server. A value of 0 means no limit, and leaves the server open to such attacks. POST - set maximum page size {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/max-page-size?limit={{maxPageSize}}","title":"Set the maximum page size for REST API requests"},{"location":"guides/admin/servers/configuring-a-metadata-server/#configure-the-audit-log","text":"Egeria's audit log provides a configurable set of destinations for audit records and other diagnostic logging for an OMAG Server . Some destinations also support a query interface to allow an administrator to understand how the server is running. If the server is a development or test server, then the default audit log configuration is probably sufficient: the console audit log destination. POST - set default audit log destination {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/default Using this option overrides all previous audit log destinations. If this server is a production server then you will probably want to set up the audit log destinations explicitly. You can add multiple destinations and each one can be set up to process specific severities of log records. The audit log severities are as follows: Severity Description Information The server is providing information about its normal operation. Event An event was received from another member of the open metadata repository cohort. Decision A decision has been made related to the interaction of the local metadata repository and the rest of the cohort. Action An Action is required by the administrator. At a minimum, the situation needs to be investigated and if necessary, corrective action taken. Error An error occurred, possibly caused by an incompatibility between the local metadata repository and one of the remote repositories. The local repository may restrict some of the metadata interchange functions as a result. Exception An unexpected exception occurred. This means that the server needs some administration attention to correct configuration or fix a logic error because it is not operating as a proper peer in the open metadata repository cohort. Security Unauthorized access to a service or metadata instance has been attempted. Startup A new component is starting up. Shutdown An existing component is shutting down. Asset An auditable action relating to an asset has been taken. Types Activity is occurring that relates to the open metadata types in use by this server. Cohort The server is exchanging registration information about an open metadata repository cohort that it is connecting to. Trace This is additional information on the operation of the server that may be of assistance in debugging a problem. It is not normally logged to any destination, but can be added when needed. PerfMon This log record contains performance monitoring timing information for specific types of processing. It is not normally logged to any destination, but can be added when needed. <Unknown> Uninitialized Severity The body of the request should be a list of severities If an empty list is passed as the request body then all severities are supported by the destination.","title":"Configure the audit log"},{"location":"guides/admin/servers/configuring-a-metadata-server/#add-audit-log-destinations","text":"There are various destinations that can be configured for the audit log: console POST - add console audit log destination This writes selected parts of each audit log record to stdout. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/console slf4j POST - add slf4j audit log destination This writes full log records to the slf4j ecosystem. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/slf4j When configuring slf4j as destination you also need to specify auditlog logger category via the application properties. This is described in Connecting the OMAG Audit Log Framework section of the developer logging guide. file POST - add JSON file-based audit log destination This writes JSON files in a shared directory. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/files event POST - add event-based audit log destination This writes each log record as an event on the supplied event topic. It assumes that the event bus is set up first. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/event-topic connection POST - add connection-based audit log destination This sets up an audit log destination that is described though a connection . In this case, the connection is passed in the request body and the supported severities can be supplied in the connection's configuration properties. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/connection list POST - add a list of connection-based audit log destinations It is also possible to set up the audit log destinations as a list of connections. Using this option overrides all previous audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations","title":"Add audit log destinations"},{"location":"guides/admin/servers/configuring-a-metadata-server/#remove-audit-logs","text":"The following will remove all audit log destinations: POST - clear all audit log destinations Clears the list of audit log destinations from the configuration enabling you to add a new set of audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/none","title":"Remove audit logs"},{"location":"guides/admin/servers/configuring-a-metadata-server/#configure-the-server-security-connector","text":"Metadata that is being aggregated from different sources is likely to need comprehensive access controls. Egeria provides fine-grained security control for metadata access . It is implemented in a server security connector that is called whenever requests are made for to the server. Security is configured for a specific OMAG Server by adding a connection for this connector to the server's configuration document using the following command. POST - configure security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This passes in a connection used to create the server security connector in the request body. { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } Example: set up the sample server security connector For example, this is the connection that would set up the sample server security connector provided for the Coco Pharmaceuticals case study: { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.OpenMetadataServerSecurityProvider\" } }","title":"Configure the server security connector"},{"location":"guides/admin/servers/configuring-a-metadata-server/#determine-configured-security","text":"GET - query the server security connector setting {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection Response indicating no security { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific security connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } }","title":"Determine configured security"},{"location":"guides/admin/servers/configuring-a-metadata-server/#remove-configured-security","text":"DELETE - remove configured security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This removes all authorization checking from the server.","title":"Remove configured security"},{"location":"guides/admin/servers/configuring-a-metadata-server/#configure-the-local-repository","text":"A metadata server supports a local metadata repository that has native support for the Open Metadata Repository Services ( OMRS ) types and instances .","title":"Configure the local repository"},{"location":"guides/admin/servers/configuring-a-metadata-server/#choose-a-repository","text":"Egeria provides a number of implementations of such a repository -- only one of these options can be configured for a given metadata server at a time. bi-temporal graph This command enables a XTDB-based metadata repository, which itself has a number of pluggable back-end options for persistence and other configuration options . This plugin repository is currently the highest-performing, most fully-functional repository for Egeria, supporting all metadata operations including historical metadata as well as being highly-available through clustered deployment . POST - enable the bi-temporal graph repository {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/local-repository/mode/plugin-repository/connection { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.egeria.connectors.juxt.xtdb.repositoryconnector.XtdbOMRSRepositoryConnectorProvider\" } } May require additional driver libraries Note that depending on the persistence you configure, you may need to obtain additional driver libraries for your back-end service , as not every driver is embedded in the XTDB connector itself. non-temporal graph This command enables a JanusGraph-based metadata repository that is embedded in the metadata server and uses the local disk to store the metadata, but does not manage any historical metadata. POST - enable the graph repository {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/local-repository/mode/local-graph-repository in-memory The in-memory repository maintains an in-memory store of metadata. It is useful for demos and testing. No metadata is kept if the open metadata services are deactivated, or the server is shutdown. It should nto be used in a production environment. POST - enable the in-memory repository {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/local-repository/mode/in-memory-repository read-only The read-only repository connector provides a compliant implementation of a local repository that can be configured into a metadata server. It does not support the interfaces for create, update, delete. However, it does support the search interfaces and is able to cache metadata. This means it can be loaded with metadata from an open metadata archive and connected to a cohort. The content from the archive will be shared with other members of the cohort. POST - enable the read-only repository {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/local-repository/mode/read-only-repository","title":"Choose a repository"},{"location":"guides/admin/servers/configuring-a-metadata-server/#remove-the-local-repository","text":"This command removes all configuration for the local repository. This includes the local metadata collection id . If a new local repository is added, it will have a new local metadata collection id and will not be able to automatically re-register with its cohort(s). DELETE - remove the local repository {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/local-repository","title":"Remove the local repository"},{"location":"guides/admin/servers/configuring-a-metadata-server/#load-metadata","text":"Open metadata archives contain pre-canned metadata types and instances for cohort members . Archives can be added to the configuration document of a server to ensure their content is loaded each time the server is started. This is intended for repositories that do not store the archive content but keep it in memory. Archives can also be loaded to a running server . Loading the same archive multiple times If an archive is loaded multiple times, its content is only added to the local repository if the repository does not have the content already.","title":"Load metadata"},{"location":"guides/admin/servers/configuring-a-metadata-server/#add-to-a-running-server","text":"Typically, an open metadata archive is stored as JSON format in a file. To load such a file use the following command: POST - load file {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/instance/open-metadata-archives/file The body of the request should be the fully-qualified path name or path relative to the startup directory of the OMAG Server Platform -- and the file name should not have any quotes around it. Alternatively it is possible to set up the list of open metadata archives as a list of connections . These connections refer to open metadata archive connectors that can read and retrieve the open metadata archive content. POST - load from connection(s) {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/instance/open-metadata-archives/connection The body of the request should be the list of connections from which to load archives. This option can be used when the open metadata archives are not stored in a file, or a different file format from the default one for the OMAG Server Platform is required.","title":"Add to a running server"},{"location":"guides/admin/servers/configuring-a-metadata-server/#configure-metadata-to-load-on-startup","text":"Typically, an open metadata archive is stored as JSON format in a file. To configure the load of such a file use the following command: POST - specify file to load POST {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/open-metadata-archives/file The body of the request should be the fully-qualified path name or path relative to the startup directory of the OMAG Server Platform -- and the file name should not have any quotes around it. Alternatively it is possible to set up the list of open metadata archives as a list of connections . These connections refer to connectors that can read and retrieve the open metadata archive content. POST - specify connection(s) to load {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/open-metadata-archives The body of the request should be the list of connections from which to load archives. This option can be used when the open metadata archives are not stored in a file, or a different file connector from the default one for the OMAG Server Platform is required.","title":"Configure metadata to load on startup"},{"location":"guides/admin/servers/configuring-a-metadata-server/#remove-metadata-load-on-startup","text":"Finally, this is how to remove the archives from the configuration document. DELETE - remove archives from configuration document {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/open-metadata-archives The body of the request should be the path to the metadata archive file.","title":"Remove metadata load on startup"},{"location":"guides/admin/servers/configuring-a-metadata-server/#configure-the-access-services","text":"The Open Metadata Access Services (OMASs) provide the domain-specific APIs for metadata management and governance. They run in a metadata server or metadata access point and typically offer a REST API, Java client and an event-based interface for asynchronous interaction. Prerequisite configuration The access service configuration depends on the definitions of the event bus and the local server's userId .","title":"Configure the access services"},{"location":"guides/admin/servers/configuring-a-metadata-server/#list-available-access-services","text":"GET - list all available access services {{platformURLRoot}}/open-metadata/platform-services/users/{{adminUserId}}/server-platform/registered-services/access-services Response listing available access services { \"relatedHTTPCode\" : 200 , \"services\" : [ { \"serviceName\" : \"Asset Owner\" , \"serviceURLMarker\" : \"asset-owner\" , \"serviceDescription\" : \"Manage an asset\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/asset-owner/\" }, { \"serviceName\" : \"Stewardship Action\" , \"serviceURLMarker\" : \"stewardship-action\" , \"serviceDescription\" : \"Manage exceptions and actions from open governance\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/stewardship-action/\" }, { \"serviceName\" : \"Subject Area\" , \"serviceURLMarker\" : \"subject-area\" , \"serviceDescription\" : \"Document knowledge about a subject area\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/subject-area/\" }, { \"serviceName\" : \"Governance Program\" , \"serviceURLMarker\" : \"governance-program\" , \"serviceDescription\" : \"Manage the governance program\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/governance-program/\" }, { \"serviceName\" : \"Asset Lineage\" , \"serviceURLMarker\" : \"asset-lineage\" , \"serviceDescription\" : \"Store asset lineage\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/asset-lineage/\" }, { \"serviceName\" : \"Design Model\" , \"serviceURLMarker\" : \"design-model\" , \"serviceDescription\" : \"Exchange design model content with tools and standard packages\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/design-model/\" }, { \"serviceName\" : \"Glossary View\" , \"serviceURLMarker\" : \"glossary-view\" , \"serviceDescription\" : \"Support glossary terms visualization\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/glossary-view/\" }, { \"serviceName\" : \"Security Manager\" , \"serviceURLMarker\" : \"security-officer\" , \"serviceDescription\" : \"Set up rules to protect data\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/security-officer/\" }, { \"serviceName\" : \"Asset Consumer\" , \"serviceURLMarker\" : \"asset-consumer\" , \"serviceDescription\" : \"Access assets through connectors\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/asset-consumer/\" }, { \"serviceName\" : \"IT Infrastructure\" , \"serviceURLMarker\" : \"it-infrastructure\" , \"serviceDescription\" : \"Manage information about the deployed IT infrastructure\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/it-infrastructure/\" }, { \"serviceName\" : \"Asset Catalog\" , \"serviceURLMarker\" : \"asset-catalog\" , \"serviceDescription\" : \"Search and understand your assets\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/asset-catalog/\" }, { \"serviceName\" : \"Data Science\" , \"serviceURLMarker\" : \"data-science\" , \"serviceDescription\" : \"Create and manage data science definitions and models\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/data-science/\" }, { \"serviceName\" : \"Community Profile\" , \"serviceURLMarker\" : \"community-profile\" , \"serviceDescription\" : \"Define personal profile and collaborate\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/community-profile/\" }, { \"serviceName\" : \"DevOps\" , \"serviceURLMarker\" : \"devops\" , \"serviceDescription\" : \"Manage a DevOps pipeline\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/dev-ops/\" }, { \"serviceName\" : \"Software Developer\" , \"serviceURLMarker\" : \"software-developer\" , \"serviceDescription\" : \"Interact with software development tools\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/software-developer/\" }, { \"serviceName\" : \"Discovery Engine\" , \"serviceURLMarker\" : \"discovery-engine\" , \"serviceDescription\" : \"Support for automated metadata discovery engines\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/discovery-engine/\" }, { \"serviceName\" : \"Data Engine\" , \"serviceURLMarker\" : \"data-engine\" , \"serviceDescription\" : \"Exchange process models and lineage with a data engine\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/data-engine/\" }, { \"serviceName\" : \"Project Management\" , \"serviceURLMarker\" : \"project-management\" , \"serviceDescription\" : \"Manage data projects\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/project-management/\" }, { \"serviceName\" : \"Governance Engine\" , \"serviceURLMarker\" : \"governance-engine\" , \"serviceDescription\" : \"Set up an operational governance engine\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/governance-engine/\" }, { \"serviceName\" : \"Digital Architecture\" , \"serviceURLMarker\" : \"digital-architecture\" , \"serviceDescription\" : \"Design of the digital services for an organization\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/digital-architecture/\" }, { \"serviceName\" : \"Data Privacy\" , \"serviceURLMarker\" : \"data-privacy\" , \"serviceDescription\" : \"Manage governance of privacy\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/data-privacy/\" }, { \"serviceName\" : \"Data Manager\" , \"serviceURLMarker\" : \"data-manager\" , \"serviceDescription\" : \"Capture changes to the data stores and data set managed by a technology managing collections of data\" , \"serviceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/access-services/data-manager/\" } ] } These access services are available to configure either all together or individually.","title":"List available access services"},{"location":"guides/admin/servers/configuring-a-metadata-server/#enable-access-services","text":"The access services can either all be enabled (with default configuration values) or individually enabled: all, with defaults To enable all the access services (and the enterprise repository services that support them) with default configuration values use the following command. POST - enable all access services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services individually Alternatively, each service can be configured individually with the following command: POST - configure an individual access service {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services/{{serviceURLMarker}} The service URL marker for each service is shown in the example response given above. In both cases, it is possible to pass a list of properties to the access service that controls the behavior of each access service. These are sent in the request body. More details of which properties are supported are documented with each access service.","title":"Enable access services"},{"location":"guides/admin/servers/configuring-a-metadata-server/#disable-the-access-services","text":"The access services can be disabled with the following command. This also disables the enterprise repository services since they are not being used. DELETE - disable the access services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services","title":"Disable the access services"},{"location":"guides/admin/servers/configuring-a-metadata-server/#review-configuration","text":"GET - retrieve current configuration for the access services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services/configuration POST - save changes back to the configuration {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/access-services/configuration","title":"Review configuration"},{"location":"guides/admin/servers/configuring-a-repository-proxy/","text":"Configuring a repository proxy \u00b6 Each type of OMAG Server is configured by creating a configuration document . Set up the default event bus \u00b6 An OMAG Server uses an event bus such as Apache Kafka to exchange events with other servers and tools. Egeria manages the specific topic names and the event payloads; however, it needs to know where the event bus is deployed and any properties needed to configure it. Since the event bus is used in multiple places, the configuration document allows you to set up the details of the event bus which are then incorporated into all the places where the event bus is needed. Important sequencing information You need to set up this information before configuring any of the following: Using an event topic as the destination for the audit log . Configuring the access services in a metadata server or a metadata access point . Configuring registration to a cohort in a metadata server , a metadata access point , a repository proxy or a conformance test server . The following command creates information about the event bus. This information is used on the subsequent configuration of the OMAG Server subsystems. It does not affect any subsystems that have already been configured in the configuration document and if the event bus is not needed, its values are ignored. It is possible to add arbitrary name/value pairs as JSON in the request body. The correct properties to use are defined in the connector type for the event bus. POST - configure event bus {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/event-bus Example: Apache Kafka For example, when using Apache Kafka as your event bus you may want to configure properties that control the behavior of the consumer that receives events and the producer that sends events. This is a typical set of producer and consumer properties: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"producer\" : { \"bootstrap.servers\" : \"localhost:9092\" , \"acks\" : \"all\" , \"retries\" : \"0\" , \"batch.size\" : \"16384\" , \"linger.ms\" : \"1\" , \"buffer.memory\" : \"33554432\" , \"max.request.size\" : \"10485760\" , \"key.serializer\" : \"org.apache.kafka.common.serialization.StringSerializer\" , \"value.serializer\" : \"org.apache.kafka.common.serialization.StringSerializer\" , \"kafka.omrs.topic.id\" : \"cocoCohort\" }, \"consumer\" : { \"bootstrap.servers\" : \"localhost:9092\" , \"zookeeper.session.timeout.ms\" : \"400\" , \"zookeeper.sync.time.ms\" : \"200\" , \"fetch.message.max.bytes\" : \"10485760\" , \"max.partition.fetch.bytes\" : \"10485760\" , \"key.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"value.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"kafka.omrs.topic.id\" : \"cocoCohort\" } } A different type of event bus would use different properties. Set the server URL root \u00b6 Configure the local server URL root with the value of the OMAG Server Platform where the service will run: in particular if the configuration document will be deployed to a different OMAG Server Platform from the one used to maintain the configuration document. POST - set server URL root {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-url-root?url={{targetPlatformURLRoot}} Detailed explanation The {{targetPlatformURLRoot}} gives the location of the OMAG Server Platform on which this configured service is intended to run, while the {{platformURLRoot}} gives the location of the OMAG Server Platform in which this configuration document is maintained. They could be, but do not need to be, the same location. Configure the basic properties \u00b6 The basic properties of the OMAG Server are used in logging and events originating from the server. They help to document the purpose of the server (which helps with problem determination) and enable performance improvements by allowing the server to ignore activity or metadata that is not relevant to its operation. The basic properties include two unique identifiers: Property Description localServerId Unique identifier for this server. By default, this is initialized to a randomly generated Universal Unique identifier (UUID). localServerName Meaningful name for the server for use in messages and UIs. Ideally this value is unique to aid administrators in understanding the source of messages and events from the server. This value is set to the server name assigned when the configuration is created. The other basic properties have values that can be changed through the admin services API: Property Description localServerType Descriptive type name for the server. Again this is useful information for the administrator to understand the role of the server. The default value is Open Metadata and Governance Server . organizationName Descriptive name for the organization that owns the local server/repository. This is useful when the open metadata repository cluster consists of metadata servers from different organizations, or different departments of an enterprise. The default value is null . localServerUserId UserId to use for server-initiated REST calls. The default is OMAGServer . localServerPassword Password to use for server-initiated REST calls. The default is null . This means that only the userId is sent in the HTTP header. maxPageSize The maximum page size that can be set on requests to the server. The default value is 1000 . A value of zero means unlimited page size. Although supported, the zero value is not recommended because it provides no protection from a large request denial of service attack. The sections that follow cover how to set up these values. Set server type name \u00b6 The server type name should be set to something that describes the OMAG Server's role. It may be the name of a specific product that it is enabling, or a role in the metadata and governance landscape. POST - set server type {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-type?typeName=\"{{serverTypeName}}\" Set organization name \u00b6 The organization name may be the owning organization or department or team supported by the server. POST - set organization name {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/organization-name?name=\"{{organizationName}}\" Set the server's user ID and optional password \u00b6 The server's user ID is used when processing requests that do not have an end user, such as receiving an event from a topic. The default value is OMAGServer . Ideally each server should have its own user ID so it is possible to restrict the resources that each server has access to. If the password is specified as well, the userId and password combination are used to provide authentication information on each REST call made by the server. POST - set server's userId {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-id?id=\"{{serverUserId}}\" POST - set server's password {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-password?password=\"{{serverUserPassword}}\" Set the maximum page size for REST API requests \u00b6 The maximum page size value sets an upper limit on the number of results that a caller can request on any paging REST API to this server. Setting maximum page size helps to prevent a denial of service attack that uses very large requests to overwhelm the server. A value of 0 means no limit, and leaves the server open to such attacks. POST - set maximum page size {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/max-page-size?limit={{maxPageSize}} Configure the audit log \u00b6 Egeria's audit log provides a configurable set of destinations for audit records and other diagnostic logging for an OMAG Server . Some destinations also support a query interface to allow an administrator to understand how the server is running. If the server is a development or test server, then the default audit log configuration is probably sufficient: the console audit log destination. POST - set default audit log destination {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/default Using this option overrides all previous audit log destinations. If this server is a production server then you will probably want to set up the audit log destinations explicitly. You can add multiple destinations and each one can be set up to process specific severities of log records. The audit log severities are as follows: Severity Description Information The server is providing information about its normal operation. Event An event was received from another member of the open metadata repository cohort. Decision A decision has been made related to the interaction of the local metadata repository and the rest of the cohort. Action An Action is required by the administrator. At a minimum, the situation needs to be investigated and if necessary, corrective action taken. Error An error occurred, possibly caused by an incompatibility between the local metadata repository and one of the remote repositories. The local repository may restrict some of the metadata interchange functions as a result. Exception An unexpected exception occurred. This means that the server needs some administration attention to correct configuration or fix a logic error because it is not operating as a proper peer in the open metadata repository cohort. Security Unauthorized access to a service or metadata instance has been attempted. Startup A new component is starting up. Shutdown An existing component is shutting down. Asset An auditable action relating to an asset has been taken. Types Activity is occurring that relates to the open metadata types in use by this server. Cohort The server is exchanging registration information about an open metadata repository cohort that it is connecting to. Trace This is additional information on the operation of the server that may be of assistance in debugging a problem. It is not normally logged to any destination, but can be added when needed. PerfMon This log record contains performance monitoring timing information for specific types of processing. It is not normally logged to any destination, but can be added when needed. <Unknown> Uninitialized Severity The body of the request should be a list of severities If an empty list is passed as the request body then all severities are supported by the destination. Add audit log destinations \u00b6 There are various destinations that can be configured for the audit log: console POST - add console audit log destination This writes selected parts of each audit log record to stdout. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/console slf4j POST - add slf4j audit log destination This writes full log records to the slf4j ecosystem. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/slf4j When configuring slf4j as destination you also need to specify auditlog logger category via the application properties. This is described in Connecting the OMAG Audit Log Framework section of the developer logging guide. file POST - add JSON file-based audit log destination This writes JSON files in a shared directory. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/files event POST - add event-based audit log destination This writes each log record as an event on the supplied event topic. It assumes that the event bus is set up first. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/event-topic connection POST - add connection-based audit log destination This sets up an audit log destination that is described though a connection . In this case, the connection is passed in the request body and the supported severities can be supplied in the connection's configuration properties. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/connection list POST - add a list of connection-based audit log destinations It is also possible to set up the audit log destinations as a list of connections. Using this option overrides all previous audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations Remove audit logs \u00b6 The following will remove all audit log destinations: POST - clear all audit log destinations Clears the list of audit log destinations from the configuration enabling you to add a new set of audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/none Configure the server security connector \u00b6 Metadata that is being aggregated from different sources is likely to need comprehensive access controls. Egeria provides fine-grained security control for metadata access . It is implemented in a server security connector that is called whenever requests are made for to the server. Security is configured for a specific OMAG Server by adding a connection for this connector to the server's configuration document using the following command. POST - configure security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This passes in a connection used to create the server security connector in the request body. { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } Example: set up the sample server security connector For example, this is the connection that would set up the sample server security connector provided for the Coco Pharmaceuticals case study: { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.OpenMetadataServerSecurityProvider\" } } Determine configured security \u00b6 GET - query the server security connector setting {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection Response indicating no security { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific security connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } } Remove configured security \u00b6 DELETE - remove configured security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This removes all authorization checking from the server. Load metadata \u00b6 Open metadata archives contain pre-canned metadata types and instances for cohort members . Archives can be added to the configuration document of a server to ensure their content is loaded each time the server is started. This is intended for repositories that do not store the archive content but keep it in memory. Archives can also be loaded to a running server . Loading the same archive multiple times If an archive is loaded multiple times, its content is only added to the local repository if the repository does not have the content already. Add to a running server \u00b6 Typically, an open metadata archive is stored as JSON format in a file. To load such a file use the following command: POST - load file {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/instance/open-metadata-archives/file The body of the request should be the fully-qualified path name or path relative to the startup directory of the OMAG Server Platform -- and the file name should not have any quotes around it. Alternatively it is possible to set up the list of open metadata archives as a list of connections . These connections refer to open metadata archive connectors that can read and retrieve the open metadata archive content. POST - load from connection(s) {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/instance/open-metadata-archives/connection The body of the request should be the list of connections from which to load archives. This option can be used when the open metadata archives are not stored in a file, or a different file format from the default one for the OMAG Server Platform is required. Configure metadata to load on startup \u00b6 Typically, an open metadata archive is stored as JSON format in a file. To configure the load of such a file use the following command: POST - specify file to load POST {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/open-metadata-archives/file The body of the request should be the fully-qualified path name or path relative to the startup directory of the OMAG Server Platform -- and the file name should not have any quotes around it. Alternatively it is possible to set up the list of open metadata archives as a list of connections . These connections refer to connectors that can read and retrieve the open metadata archive content. POST - specify connection(s) to load {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/open-metadata-archives The body of the request should be the list of connections from which to load archives. This option can be used when the open metadata archives are not stored in a file, or a different file connector from the default one for the OMAG Server Platform is required. Remove metadata load on startup \u00b6 Finally, this is how to remove the archives from the configuration document. DELETE - remove archives from configuration document {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/open-metadata-archives The body of the request should be the path to the metadata archive file. Configure the repository proxy connector \u00b6 The mapping between a third party metadata repository and the open metadata protocols in a repository proxy is implemented in two connectors: An OMRS repository connector An OMRS event mapper connector They are configured as follows. Configure the repository connector \u00b6 The OMAG Server can act as a proxy to a vendor's repository. This is done by adding the connection for the repository proxy as the local repository. POST - configure the repository connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/local-repository/mode/repository-proxy/connection The request body should be the connection option to use for configuring the connector, which can include any connector-specific options as well as general details like the endpoint and credentials for the third party repository. Example: connection to configure the IBM Information Governance Catalog repository connector 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.egeria.connectors.ibm.igc.repositoryconnector.IGCOMRSRepositoryConnectorProvider\" }, \"endpoint\" : { \"class\" : \"Endpoint\" , \"address\" : \"infosvr:9446\" , \"protocol\" : \"https\" }, \"userId\" : \"a-user\" , \"clearPassword\" : \"a-password\" , \"configurationProperties\" : { \"defaultZones\" : [ \"default\" ] } } In this example, the type of connector to configure is given by the class name on line 5, and details about how to connect to the third party repository (IBM Information Governance Catalog) itself are provided on lines 9 (hostname and port), and 12-13 (credentials). Configure the repository's event mapper \u00b6 Any open metadata repository that supports its own API may also implement an event mapper to ensure the Open Metadata Repository Services ( OMRS ) is notified when metadata is added to the repository without going through the open metadata APIs. The event mapper is a connector that listens for proprietary events from the repository and converts them into calls to the OMRS . The OMRS then distributes this new metadata. POST - configure event mapper {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/local-repository/event-mapper-details?connectorProvider={{fullyQualifiedJavaClassName}}&eventSource={{resourceName}} The connectorProvider should be set to the fully-qualified Java class name for the connector provider , and the eventSource should give the details for how to access the events (for example, the hostname and port number of an Apache Kafka bootstrap server).","title":"Configure Repository Proxy"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#configuring-a-repository-proxy","text":"Each type of OMAG Server is configured by creating a configuration document .","title":"Configuring a repository proxy"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#set-up-the-default-event-bus","text":"An OMAG Server uses an event bus such as Apache Kafka to exchange events with other servers and tools. Egeria manages the specific topic names and the event payloads; however, it needs to know where the event bus is deployed and any properties needed to configure it. Since the event bus is used in multiple places, the configuration document allows you to set up the details of the event bus which are then incorporated into all the places where the event bus is needed. Important sequencing information You need to set up this information before configuring any of the following: Using an event topic as the destination for the audit log . Configuring the access services in a metadata server or a metadata access point . Configuring registration to a cohort in a metadata server , a metadata access point , a repository proxy or a conformance test server . The following command creates information about the event bus. This information is used on the subsequent configuration of the OMAG Server subsystems. It does not affect any subsystems that have already been configured in the configuration document and if the event bus is not needed, its values are ignored. It is possible to add arbitrary name/value pairs as JSON in the request body. The correct properties to use are defined in the connector type for the event bus. POST - configure event bus {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/event-bus Example: Apache Kafka For example, when using Apache Kafka as your event bus you may want to configure properties that control the behavior of the consumer that receives events and the producer that sends events. This is a typical set of producer and consumer properties: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"producer\" : { \"bootstrap.servers\" : \"localhost:9092\" , \"acks\" : \"all\" , \"retries\" : \"0\" , \"batch.size\" : \"16384\" , \"linger.ms\" : \"1\" , \"buffer.memory\" : \"33554432\" , \"max.request.size\" : \"10485760\" , \"key.serializer\" : \"org.apache.kafka.common.serialization.StringSerializer\" , \"value.serializer\" : \"org.apache.kafka.common.serialization.StringSerializer\" , \"kafka.omrs.topic.id\" : \"cocoCohort\" }, \"consumer\" : { \"bootstrap.servers\" : \"localhost:9092\" , \"zookeeper.session.timeout.ms\" : \"400\" , \"zookeeper.sync.time.ms\" : \"200\" , \"fetch.message.max.bytes\" : \"10485760\" , \"max.partition.fetch.bytes\" : \"10485760\" , \"key.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"value.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"kafka.omrs.topic.id\" : \"cocoCohort\" } } A different type of event bus would use different properties.","title":"Set up the default event bus"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#set-the-server-url-root","text":"Configure the local server URL root with the value of the OMAG Server Platform where the service will run: in particular if the configuration document will be deployed to a different OMAG Server Platform from the one used to maintain the configuration document. POST - set server URL root {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-url-root?url={{targetPlatformURLRoot}} Detailed explanation The {{targetPlatformURLRoot}} gives the location of the OMAG Server Platform on which this configured service is intended to run, while the {{platformURLRoot}} gives the location of the OMAG Server Platform in which this configuration document is maintained. They could be, but do not need to be, the same location.","title":"Set the server URL root"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#configure-the-basic-properties","text":"The basic properties of the OMAG Server are used in logging and events originating from the server. They help to document the purpose of the server (which helps with problem determination) and enable performance improvements by allowing the server to ignore activity or metadata that is not relevant to its operation. The basic properties include two unique identifiers: Property Description localServerId Unique identifier for this server. By default, this is initialized to a randomly generated Universal Unique identifier (UUID). localServerName Meaningful name for the server for use in messages and UIs. Ideally this value is unique to aid administrators in understanding the source of messages and events from the server. This value is set to the server name assigned when the configuration is created. The other basic properties have values that can be changed through the admin services API: Property Description localServerType Descriptive type name for the server. Again this is useful information for the administrator to understand the role of the server. The default value is Open Metadata and Governance Server . organizationName Descriptive name for the organization that owns the local server/repository. This is useful when the open metadata repository cluster consists of metadata servers from different organizations, or different departments of an enterprise. The default value is null . localServerUserId UserId to use for server-initiated REST calls. The default is OMAGServer . localServerPassword Password to use for server-initiated REST calls. The default is null . This means that only the userId is sent in the HTTP header. maxPageSize The maximum page size that can be set on requests to the server. The default value is 1000 . A value of zero means unlimited page size. Although supported, the zero value is not recommended because it provides no protection from a large request denial of service attack. The sections that follow cover how to set up these values.","title":"Configure the basic properties"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#set-server-type-name","text":"The server type name should be set to something that describes the OMAG Server's role. It may be the name of a specific product that it is enabling, or a role in the metadata and governance landscape. POST - set server type {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-type?typeName=\"{{serverTypeName}}\"","title":"Set server type name"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#set-organization-name","text":"The organization name may be the owning organization or department or team supported by the server. POST - set organization name {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/organization-name?name=\"{{organizationName}}\"","title":"Set organization name"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#set-the-servers-user-id-and-optional-password","text":"The server's user ID is used when processing requests that do not have an end user, such as receiving an event from a topic. The default value is OMAGServer . Ideally each server should have its own user ID so it is possible to restrict the resources that each server has access to. If the password is specified as well, the userId and password combination are used to provide authentication information on each REST call made by the server. POST - set server's userId {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-id?id=\"{{serverUserId}}\" POST - set server's password {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-password?password=\"{{serverUserPassword}}\"","title":"Set the server's user ID and optional password"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#set-the-maximum-page-size-for-rest-api-requests","text":"The maximum page size value sets an upper limit on the number of results that a caller can request on any paging REST API to this server. Setting maximum page size helps to prevent a denial of service attack that uses very large requests to overwhelm the server. A value of 0 means no limit, and leaves the server open to such attacks. POST - set maximum page size {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/max-page-size?limit={{maxPageSize}}","title":"Set the maximum page size for REST API requests"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#configure-the-audit-log","text":"Egeria's audit log provides a configurable set of destinations for audit records and other diagnostic logging for an OMAG Server . Some destinations also support a query interface to allow an administrator to understand how the server is running. If the server is a development or test server, then the default audit log configuration is probably sufficient: the console audit log destination. POST - set default audit log destination {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/default Using this option overrides all previous audit log destinations. If this server is a production server then you will probably want to set up the audit log destinations explicitly. You can add multiple destinations and each one can be set up to process specific severities of log records. The audit log severities are as follows: Severity Description Information The server is providing information about its normal operation. Event An event was received from another member of the open metadata repository cohort. Decision A decision has been made related to the interaction of the local metadata repository and the rest of the cohort. Action An Action is required by the administrator. At a minimum, the situation needs to be investigated and if necessary, corrective action taken. Error An error occurred, possibly caused by an incompatibility between the local metadata repository and one of the remote repositories. The local repository may restrict some of the metadata interchange functions as a result. Exception An unexpected exception occurred. This means that the server needs some administration attention to correct configuration or fix a logic error because it is not operating as a proper peer in the open metadata repository cohort. Security Unauthorized access to a service or metadata instance has been attempted. Startup A new component is starting up. Shutdown An existing component is shutting down. Asset An auditable action relating to an asset has been taken. Types Activity is occurring that relates to the open metadata types in use by this server. Cohort The server is exchanging registration information about an open metadata repository cohort that it is connecting to. Trace This is additional information on the operation of the server that may be of assistance in debugging a problem. It is not normally logged to any destination, but can be added when needed. PerfMon This log record contains performance monitoring timing information for specific types of processing. It is not normally logged to any destination, but can be added when needed. <Unknown> Uninitialized Severity The body of the request should be a list of severities If an empty list is passed as the request body then all severities are supported by the destination.","title":"Configure the audit log"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#add-audit-log-destinations","text":"There are various destinations that can be configured for the audit log: console POST - add console audit log destination This writes selected parts of each audit log record to stdout. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/console slf4j POST - add slf4j audit log destination This writes full log records to the slf4j ecosystem. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/slf4j When configuring slf4j as destination you also need to specify auditlog logger category via the application properties. This is described in Connecting the OMAG Audit Log Framework section of the developer logging guide. file POST - add JSON file-based audit log destination This writes JSON files in a shared directory. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/files event POST - add event-based audit log destination This writes each log record as an event on the supplied event topic. It assumes that the event bus is set up first. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/event-topic connection POST - add connection-based audit log destination This sets up an audit log destination that is described though a connection . In this case, the connection is passed in the request body and the supported severities can be supplied in the connection's configuration properties. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/connection list POST - add a list of connection-based audit log destinations It is also possible to set up the audit log destinations as a list of connections. Using this option overrides all previous audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations","title":"Add audit log destinations"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#remove-audit-logs","text":"The following will remove all audit log destinations: POST - clear all audit log destinations Clears the list of audit log destinations from the configuration enabling you to add a new set of audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/none","title":"Remove audit logs"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#configure-the-server-security-connector","text":"Metadata that is being aggregated from different sources is likely to need comprehensive access controls. Egeria provides fine-grained security control for metadata access . It is implemented in a server security connector that is called whenever requests are made for to the server. Security is configured for a specific OMAG Server by adding a connection for this connector to the server's configuration document using the following command. POST - configure security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This passes in a connection used to create the server security connector in the request body. { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } Example: set up the sample server security connector For example, this is the connection that would set up the sample server security connector provided for the Coco Pharmaceuticals case study: { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.OpenMetadataServerSecurityProvider\" } }","title":"Configure the server security connector"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#determine-configured-security","text":"GET - query the server security connector setting {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection Response indicating no security { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific security connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } }","title":"Determine configured security"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#remove-configured-security","text":"DELETE - remove configured security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This removes all authorization checking from the server.","title":"Remove configured security"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#load-metadata","text":"Open metadata archives contain pre-canned metadata types and instances for cohort members . Archives can be added to the configuration document of a server to ensure their content is loaded each time the server is started. This is intended for repositories that do not store the archive content but keep it in memory. Archives can also be loaded to a running server . Loading the same archive multiple times If an archive is loaded multiple times, its content is only added to the local repository if the repository does not have the content already.","title":"Load metadata"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#add-to-a-running-server","text":"Typically, an open metadata archive is stored as JSON format in a file. To load such a file use the following command: POST - load file {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/instance/open-metadata-archives/file The body of the request should be the fully-qualified path name or path relative to the startup directory of the OMAG Server Platform -- and the file name should not have any quotes around it. Alternatively it is possible to set up the list of open metadata archives as a list of connections . These connections refer to open metadata archive connectors that can read and retrieve the open metadata archive content. POST - load from connection(s) {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/instance/open-metadata-archives/connection The body of the request should be the list of connections from which to load archives. This option can be used when the open metadata archives are not stored in a file, or a different file format from the default one for the OMAG Server Platform is required.","title":"Add to a running server"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#configure-metadata-to-load-on-startup","text":"Typically, an open metadata archive is stored as JSON format in a file. To configure the load of such a file use the following command: POST - specify file to load POST {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/open-metadata-archives/file The body of the request should be the fully-qualified path name or path relative to the startup directory of the OMAG Server Platform -- and the file name should not have any quotes around it. Alternatively it is possible to set up the list of open metadata archives as a list of connections . These connections refer to connectors that can read and retrieve the open metadata archive content. POST - specify connection(s) to load {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/open-metadata-archives The body of the request should be the list of connections from which to load archives. This option can be used when the open metadata archives are not stored in a file, or a different file connector from the default one for the OMAG Server Platform is required.","title":"Configure metadata to load on startup"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#remove-metadata-load-on-startup","text":"Finally, this is how to remove the archives from the configuration document. DELETE - remove archives from configuration document {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/open-metadata-archives The body of the request should be the path to the metadata archive file.","title":"Remove metadata load on startup"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#configure-the-repository-proxy-connector","text":"The mapping between a third party metadata repository and the open metadata protocols in a repository proxy is implemented in two connectors: An OMRS repository connector An OMRS event mapper connector They are configured as follows.","title":"Configure the repository proxy connector"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#configure-the-repository-connector","text":"The OMAG Server can act as a proxy to a vendor's repository. This is done by adding the connection for the repository proxy as the local repository. POST - configure the repository connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/local-repository/mode/repository-proxy/connection The request body should be the connection option to use for configuring the connector, which can include any connector-specific options as well as general details like the endpoint and credentials for the third party repository. Example: connection to configure the IBM Information Governance Catalog repository connector 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.egeria.connectors.ibm.igc.repositoryconnector.IGCOMRSRepositoryConnectorProvider\" }, \"endpoint\" : { \"class\" : \"Endpoint\" , \"address\" : \"infosvr:9446\" , \"protocol\" : \"https\" }, \"userId\" : \"a-user\" , \"clearPassword\" : \"a-password\" , \"configurationProperties\" : { \"defaultZones\" : [ \"default\" ] } } In this example, the type of connector to configure is given by the class name on line 5, and details about how to connect to the third party repository (IBM Information Governance Catalog) itself are provided on lines 9 (hostname and port), and 12-13 (credentials).","title":"Configure the repository connector"},{"location":"guides/admin/servers/configuring-a-repository-proxy/#configure-the-repositorys-event-mapper","text":"Any open metadata repository that supports its own API may also implement an event mapper to ensure the Open Metadata Repository Services ( OMRS ) is notified when metadata is added to the repository without going through the open metadata APIs. The event mapper is a connector that listens for proprietary events from the repository and converts them into calls to the OMRS . The OMRS then distributes this new metadata. POST - configure event mapper {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/local-repository/event-mapper-details?connectorProvider={{fullyQualifiedJavaClassName}}&eventSource={{resourceName}} The connectorProvider should be set to the fully-qualified Java class name for the connector provider , and the eventSource should give the details for how to access the events (for example, the hostname and port number of an Apache Kafka bootstrap server).","title":"Configure the repository's event mapper"},{"location":"guides/admin/servers/configuring-a-view-server/","text":"Configuring a view server \u00b6 Each type of OMAG Server is configured by creating a configuration document . Configure the basic properties \u00b6 The basic properties of the OMAG Server are used in logging and events originating from the server. They help to document the purpose of the server (which helps with problem determination) and enable performance improvements by allowing the server to ignore activity or metadata that is not relevant to its operation. The basic properties include two unique identifiers: Property Description localServerId Unique identifier for this server. By default, this is initialized to a randomly generated Universal Unique identifier (UUID). localServerName Meaningful name for the server for use in messages and UIs. Ideally this value is unique to aid administrators in understanding the source of messages and events from the server. This value is set to the server name assigned when the configuration is created. The other basic properties have values that can be changed through the admin services API: Property Description localServerType Descriptive type name for the server. Again this is useful information for the administrator to understand the role of the server. The default value is Open Metadata and Governance Server . organizationName Descriptive name for the organization that owns the local server/repository. This is useful when the open metadata repository cluster consists of metadata servers from different organizations, or different departments of an enterprise. The default value is null . localServerUserId UserId to use for server-initiated REST calls. The default is OMAGServer . localServerPassword Password to use for server-initiated REST calls. The default is null . This means that only the userId is sent in the HTTP header. maxPageSize The maximum page size that can be set on requests to the server. The default value is 1000 . A value of zero means unlimited page size. Although supported, the zero value is not recommended because it provides no protection from a large request denial of service attack. The sections that follow cover how to set up these values. Set server type name \u00b6 The server type name should be set to something that describes the OMAG Server's role. It may be the name of a specific product that it is enabling, or a role in the metadata and governance landscape. POST - set server type {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-type?typeName=\"{{serverTypeName}}\" Set organization name \u00b6 The organization name may be the owning organization or department or team supported by the server. POST - set organization name {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/organization-name?name=\"{{organizationName}}\" Set the server's user ID and optional password \u00b6 The server's user ID is used when processing requests that do not have an end user, such as receiving an event from a topic. The default value is OMAGServer . Ideally each server should have its own user ID so it is possible to restrict the resources that each server has access to. If the password is specified as well, the userId and password combination are used to provide authentication information on each REST call made by the server. POST - set server's userId {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-id?id=\"{{serverUserId}}\" POST - set server's password {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-password?password=\"{{serverUserPassword}}\" Set the maximum page size for REST API requests \u00b6 The maximum page size value sets an upper limit on the number of results that a caller can request on any paging REST API to this server. Setting maximum page size helps to prevent a denial of service attack that uses very large requests to overwhelm the server. A value of 0 means no limit, and leaves the server open to such attacks. POST - set maximum page size {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/max-page-size?limit={{maxPageSize}} Configure the audit log \u00b6 Egeria's audit log provides a configurable set of destinations for audit records and other diagnostic logging for an OMAG Server . Some destinations also support a query interface to allow an administrator to understand how the server is running. If the server is a development or test server, then the default audit log configuration is probably sufficient: the console audit log destination. POST - set default audit log destination {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/default Using this option overrides all previous audit log destinations. If this server is a production server then you will probably want to set up the audit log destinations explicitly. You can add multiple destinations and each one can be set up to process specific severities of log records. The audit log severities are as follows: Severity Description Information The server is providing information about its normal operation. Event An event was received from another member of the open metadata repository cohort. Decision A decision has been made related to the interaction of the local metadata repository and the rest of the cohort. Action An Action is required by the administrator. At a minimum, the situation needs to be investigated and if necessary, corrective action taken. Error An error occurred, possibly caused by an incompatibility between the local metadata repository and one of the remote repositories. The local repository may restrict some of the metadata interchange functions as a result. Exception An unexpected exception occurred. This means that the server needs some administration attention to correct configuration or fix a logic error because it is not operating as a proper peer in the open metadata repository cohort. Security Unauthorized access to a service or metadata instance has been attempted. Startup A new component is starting up. Shutdown An existing component is shutting down. Asset An auditable action relating to an asset has been taken. Types Activity is occurring that relates to the open metadata types in use by this server. Cohort The server is exchanging registration information about an open metadata repository cohort that it is connecting to. Trace This is additional information on the operation of the server that may be of assistance in debugging a problem. It is not normally logged to any destination, but can be added when needed. PerfMon This log record contains performance monitoring timing information for specific types of processing. It is not normally logged to any destination, but can be added when needed. <Unknown> Uninitialized Severity The body of the request should be a list of severities If an empty list is passed as the request body then all severities are supported by the destination. Add audit log destinations \u00b6 There are various destinations that can be configured for the audit log: console POST - add console audit log destination This writes selected parts of each audit log record to stdout. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/console slf4j POST - add slf4j audit log destination This writes full log records to the slf4j ecosystem. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/slf4j When configuring slf4j as destination you also need to specify auditlog logger category via the application properties. This is described in Connecting the OMAG Audit Log Framework section of the developer logging guide. file POST - add JSON file-based audit log destination This writes JSON files in a shared directory. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/files event POST - add event-based audit log destination This writes each log record as an event on the supplied event topic. It assumes that the event bus is set up first. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/event-topic connection POST - add connection-based audit log destination This sets up an audit log destination that is described though a connection . In this case, the connection is passed in the request body and the supported severities can be supplied in the connection's configuration properties. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/connection list POST - add a list of connection-based audit log destinations It is also possible to set up the audit log destinations as a list of connections. Using this option overrides all previous audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations Remove audit logs \u00b6 The following will remove all audit log destinations: POST - clear all audit log destinations Clears the list of audit log destinations from the configuration enabling you to add a new set of audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/none Configure the server security connector \u00b6 Metadata that is being aggregated from different sources is likely to need comprehensive access controls. Egeria provides fine-grained security control for metadata access . It is implemented in a server security connector that is called whenever requests are made for to the server. Security is configured for a specific OMAG Server by adding a connection for this connector to the server's configuration document using the following command. POST - configure security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This passes in a connection used to create the server security connector in the request body. { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } Example: set up the sample server security connector For example, this is the connection that would set up the sample server security connector provided for the Coco Pharmaceuticals case study: { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.OpenMetadataServerSecurityProvider\" } } Determine configured security \u00b6 GET - query the server security connector setting {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection Response indicating no security { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific security connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } } Remove configured security \u00b6 DELETE - remove configured security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This removes all authorization checking from the server. Configure the view services \u00b6 The Open Metadata View Services ( OMVS 's) run in a view server . View services provide task-oriented, domain-specific services for user interfaces that integrate with open metadata. View services are part of a multi-tier architecture for the provision of multi-tenant user interfaces. The front tier consists of web components that are rendered in a web browser and served by a web application called the presentation server . The presentation server in turn delegates requests to a set of view services that form a second tier running in the view server. Each view service exposes a REST API that supports the domain-specific operations relevant to the service and issues queries and commands to other OMAG Servers. To get a description of each of the registered view services, and each service's viewServiceURLMarker , see list view services instructions below. To activate a specific view service in a view server, it is necessary to add an entry for the view service to the view server's configuration document. The descriptive information and operational status are filled out automatically by the administration services based on the viewServiceURLMarker value that you supply. The other values are supplied on the configuration call. There are two types of view services, each with a different type of view service configuration object: Solution view services \u00b6 A solution view service supports operations needed by a solution-oriented user interface. These are typically geared toward a specific Open Metadata Access Service ( OMAS ) . For example, the Glossary Author view service supports a user interface for creation and management of glossaries using the Subject Area OMAS . A solution view service is configured using a SolutionViewServiceConfig object which has the following properties: Property Use viewServiceId required property, set to a unique numeric identifier viewServiceAdminClass required property, set to the admin class of the view service viewServiceName required property, set to the name of the view service being configured viewServiceFullName required property, set to the full name of view service viewServiceURLMarker required property, set to the serviceURL Marker of the service - this can be discovered by listing the registered view services viewServiceDescription optional property that describes the view service viewServiceWiki optional property specifying the location of the view service documentation viewServiceOperationalStatus required property, set to ENABLED or DISABLED viewServiceOptions optional property that specifies options needed by a specific view service (refer to the documentation for the specific service for details) omagServerPlatformRootURL required property (see below) omagServerName required property (see below) A solution view service configuration must include omagServerPlatformRootURL and omagServerName properties(defined in OMAGServerClientConfig ). These properties specify the OMAG Server to which to send downstream REST calls to an OMAG Server that is running the OMAS needed by the view service. Example solution view service configuration Below is an example of a configuration object for a solution view service. In this example, the view service is Glossary Author View Service . It would be similar for the other solution view services. The configuration contains the name and status of the view service and contains the name and rootURL of the OMAG Server to which 'downstream' requests will be sent. In this example the 'downstream' server is the server running the Subject Area OMAS , required by the Glossary Author view service. { \"class\":\"SolutionViewServiceConfig\", \"viewServiceAdminClass\":\"org.odpi.openmetadata.viewservices.glossaryauthor.admin.GlossaryAuthorViewAdmin\", \"viewServiceFullName\":\"Glossary Author\", \"viewServiceOperationalStatus\":\"ENABLED\", \"omagserverName\":\"Subject_Area_Server\", \"omagserverPlatformRootURL\":\"https://localhost:8083\" } Integration view services \u00b6 An integration view service supports operations needed by an integration-oriented user interface. Examples include the Repository Explorer View Service , Type Explorer View Service or Dino View Service for operational management. It additionally has the following configuration properties: Property Use viewServiceId required property, set to a unique numeric identifier viewServiceAdminClass required property, set to the admin class of the view service viewServiceName required property, set to the name of the view service being configured viewServiceFullName required property, set to the full name of view service viewServiceURLMarker required property, set to the serviceURL marker of the service - this can be discovered by listing the registered view services viewServiceDescription optional property that describes the view service viewServiceWiki optional property specifying the location of the view service documentation viewServiceOperationalStatus required property, set to ENABLED or DISABLED viewServiceOptions optional property that specifies options needed by a specific view service (refer to the documentation for the specific service for details) resourceEndpoints required property that lists the platform and server endpoints of the OMAG Platforms or Servers to which to send downstream REST calls, for example to query metadata repositories (see below) An integration view service configuration does not need the omagServerPlatformRootURL and omagServerName properties that are required for a solution view service configuration. This is because an integration view service will generally need to perform operations routed to a variety of open metadata servers, selected by the user at runtime. The set of platforms and servers that the user can select are configured by the resourceEndpoints configuration property. The resourceEndpoints property is a list of ResourceEndpointConfig objects, which each have the following properties: Property Use resourceCategory required property, set to either \"Platform\" or \"Server\" platformName required property, a unique name given to a \"Platform\" resource, or a reference to a named \"Platform\" resource endpoint from a \"Server\" resource serverName required property for a \"Server\" resource, set to the name of the OMAG Server. Not used for a \"Platform\" resource. serverInstanceName required property for a \"Server\" resource, a unique name for the combination of server and platform. description optional property that is displayed by some integration view services In an Egeria deployment, a server may be deployed to multiple platforms; this is typically used for clustering. A \"Server\" ResourceEndpointConfig must possess a serverInstanceName property which contains a unique name that refers to the specific instance of the server identified by the serverName property hosted by the platform identified by the platformName property. For example, you could configure a pair of server resource endpoints called Server1@PlatformA and Server1@PlatformB : both are Server1 , but hosted on different platforms ( PlatformA and PlatformB ). The serverInstanceName is used to display the resource in the user interface selector lists. Example integration view service configuration Below is an example of a configuration object for an Integration View Service. In this example, the view service is Dino View Service . It would be similar for the other integration view services. The configuration contains the name and status of the view service and contains a list of the resources that will appear in the platform and server selectors in the user interface. All requests to the view service REST API are based on these configured named resources. When a user selects a platform name or server name from the selector lists, the interface sends the resource name to the view service, which resolves the platform or server name to a resource endpoint to identify the URL needed to send a request to the platform or server. In the example configuration, the list of ResourceEndpointConfig objects represents two platforms and two servers. Every ResourceEndpointConfig has a resourceCategory , set to either \"Platform\" or \"Server\" . Each platform ResourceEndpointConfig has a unique platformName and platformRootURL and an optional description property. Each server ResourceEndpointConfig has a serverInstanceName , serverName and the platformName of one of the configured platform resource endpoints. Each server also has an optional description property. You would need to replace the <hostname> and <port> variables with your own values: { \"class\" : \"IntegrationViewServiceConfig\" , \"viewServiceAdminClass\" : \"org.odpi.openmetadata.viewservices.rex.admin.RexViewAdmin\" , \"viewServiceFullName\" : \"RepositoryExplorer\" , \"viewServiceOperationalStatus\" : \"ENABLED\" , \"resourceEndpoints\" : [ { \"class\" : \"ResourceEndpointConfig\" , \"resourceCategory\" : \"Platform\" , \"platformName\" : \"Platform1\" , \"platformRootURL\" : \"https://<hostname>:<port>\" , \"description\" : \"This platform is running in the development cloud\" }, { \"class\" : \"ResourceEndpointConfig\" , \"resourceCategory\" : \"Platform\" , \"platformName\" : \"Platform2\" , \"platformRootURL\" : \"https://<hostname>:<port>\" , \"description\" : \"This platform is running in the departmental test cluster\" }, { \"class\" : \"ResourceEndpointConfig\" , \"resourceCategory\" : \"Server\" , \"serverInstanceName\" : \"Central Metadata Server\" , \"serverName\" : \"Metadata_Server1\" , \"platformName\" : \"Platform1\" , \"description\" : \"Metadata server with home reopsitory for schema artefacts\" }, { \"class\" : \"ResourceEndpointConfig\" , \"resourceCategory\" : \"Server\" , \"serverInstanceName\" : \"Supplementary Metadata Server\" , \"serverName\" : \"Metadata_Server2\" , \"platformName\" : \"Platform2\" , \"description\" : \"Metadata server with home repository for review artefacts\" } ] } Administrative operations \u00b6 List view services \u00b6 It is possible to list the registered view services for an OMAG Server Platform using the following command: GET - list view services {{platformURLRoot}}/open-metadata/platform-services/users/{{adminUserId}}/server-platform/registered-services/view-services Response from listing view services { \"class\" : \"RegisteredOMAGServicesResponse\" , \"relatedHTTPCode\" : 200 , \"services\" : [ { \"serviceName\" : \"Glossary Author\" , \"serviceURLMarker\" : \"glossary-author\" , \"serviceDescription\" : \"View Service for glossary authoring.\" , \"serviceWiki\" : \"https://odpi.github.io/egeria/open-metadata-implementation/access-services/subject-area/\" }, { \"serviceName\" : \"Repository Explorer\" , \"serviceURLMarker\" : \"rex\" , \"serviceDescription\" : \"Explore open metadata instances.\" , \"serviceWiki\" : \"https://odpi.github.io/egeria/open-metadata-implementation/view-services/rex-view/\" }, { \"serviceName\" : \"Type Explorer\" , \"serviceURLMarker\" : \"tex\" , \"serviceDescription\" : \"Explore the open metadata types.\" , \"serviceWiki\" : \"https://odpi.github.io/egeria/open-metadata-implementation/view-services/tex-view/\" }, { \"serviceName\" : \"Dino\" , \"serviceURLMarker\" : \"dino\" , \"serviceDescription\" : \"Operate an open metadata topology.\" , \"serviceWiki\" : \"https://odpi.github.io/egeria/open-metadata-implementation/view-services/dino-view/\" } ] } These view services are available to configure either together or individually. This operation is a good way to discover the serviceURLMarker property for each view service, which is needed for various operations described below. List configured view services \u00b6 It is possible to list the configured view services for an OMAG Server using the following command: GET - list configured view services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/view-services/configuration Response from listing configured view services The response will be a RegisteredOMAGServicesResponse which contains a list of RegisteredOMAGService objects, that will look something like the following: { \"class\" : \"RegisteredOMAGServicesResponse\" , \"relatedHTTPCode\" : 200 , \"services\" : [ { \"serviceName\" : \"Glossary Author\" , \"serviceURLMarker\" : \"glossary-author\" , \"serviceDescription\" : \"View Service for glossary authoring.\" , \"serviceWiki\" : \"https://odpi.github.io/egeria/open-metadata-implementation/access-services/subject-area/\" }, { \"serviceName\" : \"Repository Explorer\" , \"serviceURLMarker\" : \"rex\" , \"serviceDescription\" : \"Explore open metadata instances.\" , \"serviceWiki\" : \"https://odpi.github.io/egeria/open-metadata-implementation/view-services/rex-view/\" }, { \"serviceName\" : \"Type Explorer\" , \"serviceURLMarker\" : \"tex\" , \"serviceDescription\" : \"Explore the open metadata types.\" , \"serviceWiki\" : \"https://odpi.github.io/egeria/open-metadata-implementation/view-services/tex-view/\" } ] } These view services are available to configure either together or individually. Retrieve view service configuration \u00b6 individually Retrieve a specific view service's configuration: GET - retrieve a specific view service's configuration {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/view-services/{{serviceURLMarker}} The response will be a ViewServiceConfigResponse containing a ViewServiceConfig object. multiple It is also possible to retrieve the current configuration for all configured view services: GET - retrieve current configuration for all configured view services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/view-services This will return a ViewServicesResponse which will contain a list of ViewServiceConfig objects. Configure view services \u00b6 individually A specific view service can be individually configured with the following command: POST - configure a specific view service {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/view-services/{{serviceURLMarker}} The request body must contain a ViewServiceConfig object, while the serviceURLMarker can be found by listing the configured view services . multiple It is also possible to configurate a set of view services at the same time, using the following command: POST - configure multiple view services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/view-services/configuration The request body must contain a list of ViewServiceConfig objects. Remove view services \u00b6 individually A specific view service can be individually cleared with the following command. This will remove the view service's configuration from the server. DELETE - remove configuration for a specific view service {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/view-services/{{serviceURLMarker}} The serviceURLMarker can be found by listing the configured view services . multiple All the view services configured on a server can be cleared with the following command: DELETE - remove configured view services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/view-services Configure the presentation server \u00b6 The presentation server is a multi-tenant web application that calls view services running in a view server to retrieve information and perform operations relating to metadata servers. A presentation server tenant is designed to support an organization. These may be independent organizations or divisions/departments within an organization. The tenant routes requests to the appropriate view server and then on to the metadata servers behind. Therefore, each tenant sees a different collection of metadata. Information for configuring the presentation server is provided in a separate GitHub repository .","title":"Configure View Server"},{"location":"guides/admin/servers/configuring-a-view-server/#configuring-a-view-server","text":"Each type of OMAG Server is configured by creating a configuration document .","title":"Configuring a view server"},{"location":"guides/admin/servers/configuring-a-view-server/#configure-the-basic-properties","text":"The basic properties of the OMAG Server are used in logging and events originating from the server. They help to document the purpose of the server (which helps with problem determination) and enable performance improvements by allowing the server to ignore activity or metadata that is not relevant to its operation. The basic properties include two unique identifiers: Property Description localServerId Unique identifier for this server. By default, this is initialized to a randomly generated Universal Unique identifier (UUID). localServerName Meaningful name for the server for use in messages and UIs. Ideally this value is unique to aid administrators in understanding the source of messages and events from the server. This value is set to the server name assigned when the configuration is created. The other basic properties have values that can be changed through the admin services API: Property Description localServerType Descriptive type name for the server. Again this is useful information for the administrator to understand the role of the server. The default value is Open Metadata and Governance Server . organizationName Descriptive name for the organization that owns the local server/repository. This is useful when the open metadata repository cluster consists of metadata servers from different organizations, or different departments of an enterprise. The default value is null . localServerUserId UserId to use for server-initiated REST calls. The default is OMAGServer . localServerPassword Password to use for server-initiated REST calls. The default is null . This means that only the userId is sent in the HTTP header. maxPageSize The maximum page size that can be set on requests to the server. The default value is 1000 . A value of zero means unlimited page size. Although supported, the zero value is not recommended because it provides no protection from a large request denial of service attack. The sections that follow cover how to set up these values.","title":"Configure the basic properties"},{"location":"guides/admin/servers/configuring-a-view-server/#set-server-type-name","text":"The server type name should be set to something that describes the OMAG Server's role. It may be the name of a specific product that it is enabling, or a role in the metadata and governance landscape. POST - set server type {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-type?typeName=\"{{serverTypeName}}\"","title":"Set server type name"},{"location":"guides/admin/servers/configuring-a-view-server/#set-organization-name","text":"The organization name may be the owning organization or department or team supported by the server. POST - set organization name {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/organization-name?name=\"{{organizationName}}\"","title":"Set organization name"},{"location":"guides/admin/servers/configuring-a-view-server/#set-the-servers-user-id-and-optional-password","text":"The server's user ID is used when processing requests that do not have an end user, such as receiving an event from a topic. The default value is OMAGServer . Ideally each server should have its own user ID so it is possible to restrict the resources that each server has access to. If the password is specified as well, the userId and password combination are used to provide authentication information on each REST call made by the server. POST - set server's userId {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-id?id=\"{{serverUserId}}\" POST - set server's password {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-password?password=\"{{serverUserPassword}}\"","title":"Set the server's user ID and optional password"},{"location":"guides/admin/servers/configuring-a-view-server/#set-the-maximum-page-size-for-rest-api-requests","text":"The maximum page size value sets an upper limit on the number of results that a caller can request on any paging REST API to this server. Setting maximum page size helps to prevent a denial of service attack that uses very large requests to overwhelm the server. A value of 0 means no limit, and leaves the server open to such attacks. POST - set maximum page size {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/max-page-size?limit={{maxPageSize}}","title":"Set the maximum page size for REST API requests"},{"location":"guides/admin/servers/configuring-a-view-server/#configure-the-audit-log","text":"Egeria's audit log provides a configurable set of destinations for audit records and other diagnostic logging for an OMAG Server . Some destinations also support a query interface to allow an administrator to understand how the server is running. If the server is a development or test server, then the default audit log configuration is probably sufficient: the console audit log destination. POST - set default audit log destination {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/default Using this option overrides all previous audit log destinations. If this server is a production server then you will probably want to set up the audit log destinations explicitly. You can add multiple destinations and each one can be set up to process specific severities of log records. The audit log severities are as follows: Severity Description Information The server is providing information about its normal operation. Event An event was received from another member of the open metadata repository cohort. Decision A decision has been made related to the interaction of the local metadata repository and the rest of the cohort. Action An Action is required by the administrator. At a minimum, the situation needs to be investigated and if necessary, corrective action taken. Error An error occurred, possibly caused by an incompatibility between the local metadata repository and one of the remote repositories. The local repository may restrict some of the metadata interchange functions as a result. Exception An unexpected exception occurred. This means that the server needs some administration attention to correct configuration or fix a logic error because it is not operating as a proper peer in the open metadata repository cohort. Security Unauthorized access to a service or metadata instance has been attempted. Startup A new component is starting up. Shutdown An existing component is shutting down. Asset An auditable action relating to an asset has been taken. Types Activity is occurring that relates to the open metadata types in use by this server. Cohort The server is exchanging registration information about an open metadata repository cohort that it is connecting to. Trace This is additional information on the operation of the server that may be of assistance in debugging a problem. It is not normally logged to any destination, but can be added when needed. PerfMon This log record contains performance monitoring timing information for specific types of processing. It is not normally logged to any destination, but can be added when needed. <Unknown> Uninitialized Severity The body of the request should be a list of severities If an empty list is passed as the request body then all severities are supported by the destination.","title":"Configure the audit log"},{"location":"guides/admin/servers/configuring-a-view-server/#add-audit-log-destinations","text":"There are various destinations that can be configured for the audit log: console POST - add console audit log destination This writes selected parts of each audit log record to stdout. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/console slf4j POST - add slf4j audit log destination This writes full log records to the slf4j ecosystem. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/slf4j When configuring slf4j as destination you also need to specify auditlog logger category via the application properties. This is described in Connecting the OMAG Audit Log Framework section of the developer logging guide. file POST - add JSON file-based audit log destination This writes JSON files in a shared directory. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/files event POST - add event-based audit log destination This writes each log record as an event on the supplied event topic. It assumes that the event bus is set up first. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/event-topic connection POST - add connection-based audit log destination This sets up an audit log destination that is described though a connection . In this case, the connection is passed in the request body and the supported severities can be supplied in the connection's configuration properties. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/connection list POST - add a list of connection-based audit log destinations It is also possible to set up the audit log destinations as a list of connections. Using this option overrides all previous audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations","title":"Add audit log destinations"},{"location":"guides/admin/servers/configuring-a-view-server/#remove-audit-logs","text":"The following will remove all audit log destinations: POST - clear all audit log destinations Clears the list of audit log destinations from the configuration enabling you to add a new set of audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/none","title":"Remove audit logs"},{"location":"guides/admin/servers/configuring-a-view-server/#configure-the-server-security-connector","text":"Metadata that is being aggregated from different sources is likely to need comprehensive access controls. Egeria provides fine-grained security control for metadata access . It is implemented in a server security connector that is called whenever requests are made for to the server. Security is configured for a specific OMAG Server by adding a connection for this connector to the server's configuration document using the following command. POST - configure security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This passes in a connection used to create the server security connector in the request body. { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } Example: set up the sample server security connector For example, this is the connection that would set up the sample server security connector provided for the Coco Pharmaceuticals case study: { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.OpenMetadataServerSecurityProvider\" } }","title":"Configure the server security connector"},{"location":"guides/admin/servers/configuring-a-view-server/#determine-configured-security","text":"GET - query the server security connector setting {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection Response indicating no security { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific security connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } }","title":"Determine configured security"},{"location":"guides/admin/servers/configuring-a-view-server/#remove-configured-security","text":"DELETE - remove configured security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This removes all authorization checking from the server.","title":"Remove configured security"},{"location":"guides/admin/servers/configuring-a-view-server/#configure-the-view-services","text":"The Open Metadata View Services ( OMVS 's) run in a view server . View services provide task-oriented, domain-specific services for user interfaces that integrate with open metadata. View services are part of a multi-tier architecture for the provision of multi-tenant user interfaces. The front tier consists of web components that are rendered in a web browser and served by a web application called the presentation server . The presentation server in turn delegates requests to a set of view services that form a second tier running in the view server. Each view service exposes a REST API that supports the domain-specific operations relevant to the service and issues queries and commands to other OMAG Servers. To get a description of each of the registered view services, and each service's viewServiceURLMarker , see list view services instructions below. To activate a specific view service in a view server, it is necessary to add an entry for the view service to the view server's configuration document. The descriptive information and operational status are filled out automatically by the administration services based on the viewServiceURLMarker value that you supply. The other values are supplied on the configuration call. There are two types of view services, each with a different type of view service configuration object:","title":"Configure the view services"},{"location":"guides/admin/servers/configuring-a-view-server/#solution-view-services","text":"A solution view service supports operations needed by a solution-oriented user interface. These are typically geared toward a specific Open Metadata Access Service ( OMAS ) . For example, the Glossary Author view service supports a user interface for creation and management of glossaries using the Subject Area OMAS . A solution view service is configured using a SolutionViewServiceConfig object which has the following properties: Property Use viewServiceId required property, set to a unique numeric identifier viewServiceAdminClass required property, set to the admin class of the view service viewServiceName required property, set to the name of the view service being configured viewServiceFullName required property, set to the full name of view service viewServiceURLMarker required property, set to the serviceURL Marker of the service - this can be discovered by listing the registered view services viewServiceDescription optional property that describes the view service viewServiceWiki optional property specifying the location of the view service documentation viewServiceOperationalStatus required property, set to ENABLED or DISABLED viewServiceOptions optional property that specifies options needed by a specific view service (refer to the documentation for the specific service for details) omagServerPlatformRootURL required property (see below) omagServerName required property (see below) A solution view service configuration must include omagServerPlatformRootURL and omagServerName properties(defined in OMAGServerClientConfig ). These properties specify the OMAG Server to which to send downstream REST calls to an OMAG Server that is running the OMAS needed by the view service. Example solution view service configuration Below is an example of a configuration object for a solution view service. In this example, the view service is Glossary Author View Service . It would be similar for the other solution view services. The configuration contains the name and status of the view service and contains the name and rootURL of the OMAG Server to which 'downstream' requests will be sent. In this example the 'downstream' server is the server running the Subject Area OMAS , required by the Glossary Author view service. { \"class\":\"SolutionViewServiceConfig\", \"viewServiceAdminClass\":\"org.odpi.openmetadata.viewservices.glossaryauthor.admin.GlossaryAuthorViewAdmin\", \"viewServiceFullName\":\"Glossary Author\", \"viewServiceOperationalStatus\":\"ENABLED\", \"omagserverName\":\"Subject_Area_Server\", \"omagserverPlatformRootURL\":\"https://localhost:8083\" }","title":"Solution view services"},{"location":"guides/admin/servers/configuring-a-view-server/#integration-view-services","text":"An integration view service supports operations needed by an integration-oriented user interface. Examples include the Repository Explorer View Service , Type Explorer View Service or Dino View Service for operational management. It additionally has the following configuration properties: Property Use viewServiceId required property, set to a unique numeric identifier viewServiceAdminClass required property, set to the admin class of the view service viewServiceName required property, set to the name of the view service being configured viewServiceFullName required property, set to the full name of view service viewServiceURLMarker required property, set to the serviceURL marker of the service - this can be discovered by listing the registered view services viewServiceDescription optional property that describes the view service viewServiceWiki optional property specifying the location of the view service documentation viewServiceOperationalStatus required property, set to ENABLED or DISABLED viewServiceOptions optional property that specifies options needed by a specific view service (refer to the documentation for the specific service for details) resourceEndpoints required property that lists the platform and server endpoints of the OMAG Platforms or Servers to which to send downstream REST calls, for example to query metadata repositories (see below) An integration view service configuration does not need the omagServerPlatformRootURL and omagServerName properties that are required for a solution view service configuration. This is because an integration view service will generally need to perform operations routed to a variety of open metadata servers, selected by the user at runtime. The set of platforms and servers that the user can select are configured by the resourceEndpoints configuration property. The resourceEndpoints property is a list of ResourceEndpointConfig objects, which each have the following properties: Property Use resourceCategory required property, set to either \"Platform\" or \"Server\" platformName required property, a unique name given to a \"Platform\" resource, or a reference to a named \"Platform\" resource endpoint from a \"Server\" resource serverName required property for a \"Server\" resource, set to the name of the OMAG Server. Not used for a \"Platform\" resource. serverInstanceName required property for a \"Server\" resource, a unique name for the combination of server and platform. description optional property that is displayed by some integration view services In an Egeria deployment, a server may be deployed to multiple platforms; this is typically used for clustering. A \"Server\" ResourceEndpointConfig must possess a serverInstanceName property which contains a unique name that refers to the specific instance of the server identified by the serverName property hosted by the platform identified by the platformName property. For example, you could configure a pair of server resource endpoints called Server1@PlatformA and Server1@PlatformB : both are Server1 , but hosted on different platforms ( PlatformA and PlatformB ). The serverInstanceName is used to display the resource in the user interface selector lists. Example integration view service configuration Below is an example of a configuration object for an Integration View Service. In this example, the view service is Dino View Service . It would be similar for the other integration view services. The configuration contains the name and status of the view service and contains a list of the resources that will appear in the platform and server selectors in the user interface. All requests to the view service REST API are based on these configured named resources. When a user selects a platform name or server name from the selector lists, the interface sends the resource name to the view service, which resolves the platform or server name to a resource endpoint to identify the URL needed to send a request to the platform or server. In the example configuration, the list of ResourceEndpointConfig objects represents two platforms and two servers. Every ResourceEndpointConfig has a resourceCategory , set to either \"Platform\" or \"Server\" . Each platform ResourceEndpointConfig has a unique platformName and platformRootURL and an optional description property. Each server ResourceEndpointConfig has a serverInstanceName , serverName and the platformName of one of the configured platform resource endpoints. Each server also has an optional description property. You would need to replace the <hostname> and <port> variables with your own values: { \"class\" : \"IntegrationViewServiceConfig\" , \"viewServiceAdminClass\" : \"org.odpi.openmetadata.viewservices.rex.admin.RexViewAdmin\" , \"viewServiceFullName\" : \"RepositoryExplorer\" , \"viewServiceOperationalStatus\" : \"ENABLED\" , \"resourceEndpoints\" : [ { \"class\" : \"ResourceEndpointConfig\" , \"resourceCategory\" : \"Platform\" , \"platformName\" : \"Platform1\" , \"platformRootURL\" : \"https://<hostname>:<port>\" , \"description\" : \"This platform is running in the development cloud\" }, { \"class\" : \"ResourceEndpointConfig\" , \"resourceCategory\" : \"Platform\" , \"platformName\" : \"Platform2\" , \"platformRootURL\" : \"https://<hostname>:<port>\" , \"description\" : \"This platform is running in the departmental test cluster\" }, { \"class\" : \"ResourceEndpointConfig\" , \"resourceCategory\" : \"Server\" , \"serverInstanceName\" : \"Central Metadata Server\" , \"serverName\" : \"Metadata_Server1\" , \"platformName\" : \"Platform1\" , \"description\" : \"Metadata server with home reopsitory for schema artefacts\" }, { \"class\" : \"ResourceEndpointConfig\" , \"resourceCategory\" : \"Server\" , \"serverInstanceName\" : \"Supplementary Metadata Server\" , \"serverName\" : \"Metadata_Server2\" , \"platformName\" : \"Platform2\" , \"description\" : \"Metadata server with home repository for review artefacts\" } ] }","title":"Integration view services"},{"location":"guides/admin/servers/configuring-a-view-server/#administrative-operations","text":"","title":"Administrative operations"},{"location":"guides/admin/servers/configuring-a-view-server/#list-view-services","text":"It is possible to list the registered view services for an OMAG Server Platform using the following command: GET - list view services {{platformURLRoot}}/open-metadata/platform-services/users/{{adminUserId}}/server-platform/registered-services/view-services Response from listing view services { \"class\" : \"RegisteredOMAGServicesResponse\" , \"relatedHTTPCode\" : 200 , \"services\" : [ { \"serviceName\" : \"Glossary Author\" , \"serviceURLMarker\" : \"glossary-author\" , \"serviceDescription\" : \"View Service for glossary authoring.\" , \"serviceWiki\" : \"https://odpi.github.io/egeria/open-metadata-implementation/access-services/subject-area/\" }, { \"serviceName\" : \"Repository Explorer\" , \"serviceURLMarker\" : \"rex\" , \"serviceDescription\" : \"Explore open metadata instances.\" , \"serviceWiki\" : \"https://odpi.github.io/egeria/open-metadata-implementation/view-services/rex-view/\" }, { \"serviceName\" : \"Type Explorer\" , \"serviceURLMarker\" : \"tex\" , \"serviceDescription\" : \"Explore the open metadata types.\" , \"serviceWiki\" : \"https://odpi.github.io/egeria/open-metadata-implementation/view-services/tex-view/\" }, { \"serviceName\" : \"Dino\" , \"serviceURLMarker\" : \"dino\" , \"serviceDescription\" : \"Operate an open metadata topology.\" , \"serviceWiki\" : \"https://odpi.github.io/egeria/open-metadata-implementation/view-services/dino-view/\" } ] } These view services are available to configure either together or individually. This operation is a good way to discover the serviceURLMarker property for each view service, which is needed for various operations described below.","title":"List view services"},{"location":"guides/admin/servers/configuring-a-view-server/#list-configured-view-services","text":"It is possible to list the configured view services for an OMAG Server using the following command: GET - list configured view services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/view-services/configuration Response from listing configured view services The response will be a RegisteredOMAGServicesResponse which contains a list of RegisteredOMAGService objects, that will look something like the following: { \"class\" : \"RegisteredOMAGServicesResponse\" , \"relatedHTTPCode\" : 200 , \"services\" : [ { \"serviceName\" : \"Glossary Author\" , \"serviceURLMarker\" : \"glossary-author\" , \"serviceDescription\" : \"View Service for glossary authoring.\" , \"serviceWiki\" : \"https://odpi.github.io/egeria/open-metadata-implementation/access-services/subject-area/\" }, { \"serviceName\" : \"Repository Explorer\" , \"serviceURLMarker\" : \"rex\" , \"serviceDescription\" : \"Explore open metadata instances.\" , \"serviceWiki\" : \"https://odpi.github.io/egeria/open-metadata-implementation/view-services/rex-view/\" }, { \"serviceName\" : \"Type Explorer\" , \"serviceURLMarker\" : \"tex\" , \"serviceDescription\" : \"Explore the open metadata types.\" , \"serviceWiki\" : \"https://odpi.github.io/egeria/open-metadata-implementation/view-services/tex-view/\" } ] } These view services are available to configure either together or individually.","title":"List configured view services"},{"location":"guides/admin/servers/configuring-a-view-server/#retrieve-view-service-configuration","text":"individually Retrieve a specific view service's configuration: GET - retrieve a specific view service's configuration {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/view-services/{{serviceURLMarker}} The response will be a ViewServiceConfigResponse containing a ViewServiceConfig object. multiple It is also possible to retrieve the current configuration for all configured view services: GET - retrieve current configuration for all configured view services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/view-services This will return a ViewServicesResponse which will contain a list of ViewServiceConfig objects.","title":"Retrieve view service configuration"},{"location":"guides/admin/servers/configuring-a-view-server/#configure-view-services","text":"individually A specific view service can be individually configured with the following command: POST - configure a specific view service {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/view-services/{{serviceURLMarker}} The request body must contain a ViewServiceConfig object, while the serviceURLMarker can be found by listing the configured view services . multiple It is also possible to configurate a set of view services at the same time, using the following command: POST - configure multiple view services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/view-services/configuration The request body must contain a list of ViewServiceConfig objects.","title":"Configure view services"},{"location":"guides/admin/servers/configuring-a-view-server/#remove-view-services","text":"individually A specific view service can be individually cleared with the following command. This will remove the view service's configuration from the server. DELETE - remove configuration for a specific view service {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/view-services/{{serviceURLMarker}} The serviceURLMarker can be found by listing the configured view services . multiple All the view services configured on a server can be cleared with the following command: DELETE - remove configured view services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/view-services","title":"Remove view services"},{"location":"guides/admin/servers/configuring-a-view-server/#configure-the-presentation-server","text":"The presentation server is a multi-tenant web application that calls view services running in a view server to retrieve information and perform operations relating to metadata servers. A presentation server tenant is designed to support an organization. These may be independent organizations or divisions/departments within an organization. The tenant routes requests to the appropriate view server and then on to the metadata servers behind. Therefore, each tenant sees a different collection of metadata. Information for configuring the presentation server is provided in a separate GitHub repository .","title":"Configure the presentation server"},{"location":"guides/admin/servers/configuring-an-engine-host/","text":"Configuring an engine host \u00b6 Each type of OMAG Server is configured by creating a configuration document . Example configuration of a minimal engine host server Below is an example of the configuration for a minimal engine host server. It has a single engine service ( Asset Analysis OMES ) and the default audit log. Both the Governance Engine OMAS used by the engine host services and the Discovery Engine OMAS used by the Asset Analysis OMES are running on the metadata server called myMetadataServer . { \"class\" : \"OMAGServerConfigResponse\" , \"relatedHTTPCode\" : 200 , \"omagserverConfig\" : { \"class\" : \"OMAGServerConfig\" , \"versionId\" : \"V2.0\" , \"localServerId\" : \"8b745d03-5ffc-4978-81ab-bd3d5156eebe\" , \"localServerName\" : \"myserver\" , \"localServerType\" : \"Open Metadata and Governance Server\" , \"localServerURL\" : \"https://localhost:9443\" , \"localServerUserId\" : \"OMAGServer\" , \"maxPageSize\" : 1000 , \"engineHostServicesConfig\" : { \"omagserverPlatformRootURL\" : \"https://localhost:9443\" , \"omagserverName\" : \"myMetadataServer\" , \"engineServices\" : [ { \"class\" : \"EngineServiceConfig\" , \"engineId\" : 6000 , \"engineQualifiedName\" : \"Asset Analysis\" , \"engineServiceFullName\" : \"Asset Analysis OMES\" , \"engineServiceURLMarker\" : \"asset-analysis\" , \"engineServiceDescription\" : \"Analyses the content of an asset's real world counterpart, generates annotations in an open discovery report that is attached to the asset in the open metadata repositories .\" , \"engineServiceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/engine-services/asset-analysis/\" , \"engines\" : [ { \"engineId\" : \"daff1dca-984b-4b8a-8a8f-febaf72b82a8\" , \"engineName\" : \"engine1\" , \"engineUserId\" : \"engine1UserId\" }, { \"engineId\" : \"a80aa0f8-2ea0-4f84-b613-d68becba2693\" , \"engineName\" : \"engine2\" , \"engineUserId\" : \"engine2UserId\" } ], \"engineServiceOperationalStatus\" : \"ENABLED\" , \"engineServiceAdminClass\" : \"org.odpi.openmetadata.engineservices.assetanalysis.admin.AssetAnalysisAdmin\" , \"omagserverPlatformRootURL\" : \"https://localhost:9443\" , \"omagserverName\" : \"myMetadataServer\" } ]}, \"repositoryServicesConfig\" : { \"class\" : \"RepositoryServicesConfig\" , \"auditLogConnections\" : [ { \"class\" : \"Connection\" , \"headerVersion\" : 0 , \"displayName\" : \"Console\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"headerVersion\" : 0 , \"type\" : { \"class\" : \"ElementType\" , \"headerVersion\" : 0 , \"elementOrigin\" : \"LOCAL_COHORT\" , \"elementVersion\" : 0 , \"elementTypeId\" : \"954421eb-33a6-462d-a8ca-b5709a1bd0d4\" , \"elementTypeName\" : \"ConnectorType\" , \"elementTypeVersion\" : 1 , \"elementTypeDescription\" : \"A set of properties describing a type of connector.\" }, \"guid\" : \"4afac741-3dcc-4c60-a4ca-a6dede994e3f\" , \"qualifiedName\" : \"Console Audit Log Store Connector\" , \"displayName\" : \"Console Audit Log Store Connector\" , \"description\" : \"Connector supports logging of audit log messages to stdout.\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.repositoryservices.auditlogstore.console.ConsoleAuditLogStoreProvider\" }, \"configurationProperties\" : { \"supportedSeverities\" : [ \"<Unknown>\" , \"Information\" , \"Event\" , \"Decision\" , \"Action\" , \"Error\" , \"Exception\" , \"Security\" , \"Startup\" , \"Shutdown\" , \"Asset\" , \"Types\" , \"Cohort\" ] } } ] }, \"auditTrail\" : [ \"Tue Dec 08 18:38:32 GMT 2020 me updated configuration for engine service asset-analysis.\" , \"Tue Dec 08 18:43:47 GMT 2020 me set up default audit log destinations.\" ] } } Configure the basic properties \u00b6 The basic properties of the OMAG Server are used in logging and events originating from the server. They help to document the purpose of the server (which helps with problem determination) and enable performance improvements by allowing the server to ignore activity or metadata that is not relevant to its operation. The basic properties include two unique identifiers: Property Description localServerId Unique identifier for this server. By default, this is initialized to a randomly generated Universal Unique identifier (UUID). localServerName Meaningful name for the server for use in messages and UIs. Ideally this value is unique to aid administrators in understanding the source of messages and events from the server. This value is set to the server name assigned when the configuration is created. The other basic properties have values that can be changed through the admin services API: Property Description localServerType Descriptive type name for the server. Again this is useful information for the administrator to understand the role of the server. The default value is Open Metadata and Governance Server . organizationName Descriptive name for the organization that owns the local server/repository. This is useful when the open metadata repository cluster consists of metadata servers from different organizations, or different departments of an enterprise. The default value is null . localServerUserId UserId to use for server-initiated REST calls. The default is OMAGServer . localServerPassword Password to use for server-initiated REST calls. The default is null . This means that only the userId is sent in the HTTP header. maxPageSize The maximum page size that can be set on requests to the server. The default value is 1000 . A value of zero means unlimited page size. Although supported, the zero value is not recommended because it provides no protection from a large request denial of service attack. The sections that follow cover how to set up these values. Set server type name \u00b6 The server type name should be set to something that describes the OMAG Server's role. It may be the name of a specific product that it is enabling, or a role in the metadata and governance landscape. POST - set server type {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-type?typeName=\"{{serverTypeName}}\" Set organization name \u00b6 The organization name may be the owning organization or department or team supported by the server. POST - set organization name {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/organization-name?name=\"{{organizationName}}\" Set the server's user ID and optional password \u00b6 The server's user ID is used when processing requests that do not have an end user, such as receiving an event from a topic. The default value is OMAGServer . Ideally each server should have its own user ID so it is possible to restrict the resources that each server has access to. If the password is specified as well, the userId and password combination are used to provide authentication information on each REST call made by the server. POST - set server's userId {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-id?id=\"{{serverUserId}}\" POST - set server's password {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-password?password=\"{{serverUserPassword}}\" Set the maximum page size for REST API requests \u00b6 The maximum page size value sets an upper limit on the number of results that a caller can request on any paging REST API to this server. Setting maximum page size helps to prevent a denial of service attack that uses very large requests to overwhelm the server. A value of 0 means no limit, and leaves the server open to such attacks. POST - set maximum page size {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/max-page-size?limit={{maxPageSize}} Configure the audit log \u00b6 Egeria's audit log provides a configurable set of destinations for audit records and other diagnostic logging for an OMAG Server . Some destinations also support a query interface to allow an administrator to understand how the server is running. If the server is a development or test server, then the default audit log configuration is probably sufficient: the console audit log destination. POST - set default audit log destination {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/default Using this option overrides all previous audit log destinations. If this server is a production server then you will probably want to set up the audit log destinations explicitly. You can add multiple destinations and each one can be set up to process specific severities of log records. The audit log severities are as follows: Severity Description Information The server is providing information about its normal operation. Event An event was received from another member of the open metadata repository cohort. Decision A decision has been made related to the interaction of the local metadata repository and the rest of the cohort. Action An Action is required by the administrator. At a minimum, the situation needs to be investigated and if necessary, corrective action taken. Error An error occurred, possibly caused by an incompatibility between the local metadata repository and one of the remote repositories. The local repository may restrict some of the metadata interchange functions as a result. Exception An unexpected exception occurred. This means that the server needs some administration attention to correct configuration or fix a logic error because it is not operating as a proper peer in the open metadata repository cohort. Security Unauthorized access to a service or metadata instance has been attempted. Startup A new component is starting up. Shutdown An existing component is shutting down. Asset An auditable action relating to an asset has been taken. Types Activity is occurring that relates to the open metadata types in use by this server. Cohort The server is exchanging registration information about an open metadata repository cohort that it is connecting to. Trace This is additional information on the operation of the server that may be of assistance in debugging a problem. It is not normally logged to any destination, but can be added when needed. PerfMon This log record contains performance monitoring timing information for specific types of processing. It is not normally logged to any destination, but can be added when needed. <Unknown> Uninitialized Severity The body of the request should be a list of severities If an empty list is passed as the request body then all severities are supported by the destination. Add audit log destinations \u00b6 There are various destinations that can be configured for the audit log: console POST - add console audit log destination This writes selected parts of each audit log record to stdout. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/console slf4j POST - add slf4j audit log destination This writes full log records to the slf4j ecosystem. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/slf4j When configuring slf4j as destination you also need to specify auditlog logger category via the application properties. This is described in Connecting the OMAG Audit Log Framework section of the developer logging guide. file POST - add JSON file-based audit log destination This writes JSON files in a shared directory. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/files event POST - add event-based audit log destination This writes each log record as an event on the supplied event topic. It assumes that the event bus is set up first. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/event-topic connection POST - add connection-based audit log destination This sets up an audit log destination that is described though a connection . In this case, the connection is passed in the request body and the supported severities can be supplied in the connection's configuration properties. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/connection list POST - add a list of connection-based audit log destinations It is also possible to set up the audit log destinations as a list of connections. Using this option overrides all previous audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations Remove audit logs \u00b6 The following will remove all audit log destinations: POST - clear all audit log destinations Clears the list of audit log destinations from the configuration enabling you to add a new set of audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/none Configure the server security connector \u00b6 Metadata that is being aggregated from different sources is likely to need comprehensive access controls. Egeria provides fine-grained security control for metadata access . It is implemented in a server security connector that is called whenever requests are made for to the server. Security is configured for a specific OMAG Server by adding a connection for this connector to the server's configuration document using the following command. POST - configure security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This passes in a connection used to create the server security connector in the request body. { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } Example: set up the sample server security connector For example, this is the connection that would set up the sample server security connector provided for the Coco Pharmaceuticals case study: { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.OpenMetadataServerSecurityProvider\" } } Determine configured security \u00b6 GET - query the server security connector setting {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection Response indicating no security { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific security connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } } Remove configured security \u00b6 DELETE - remove configured security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This removes all authorization checking from the server. Configure the engine host services \u00b6 The engine host services provide the base implementation of the engine host OMAG Server. There are two parts to configuring the engine host services: Specify location of governance engine \u00b6 The location of the metadata server (or metadata access point ) running the Governance Engine OMAS , which will supply the definitions of the governance engines that will run in the engine services, is configured using two properties: the server url root of the metadata server's OMAG Server Platform, and the name of the metadata server . POST - specify location of governance engine {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{engineHostServerName}}/engine-definitions/client-config With a request body like the following: { \"class\" : \"OMAGServerClientConfig\" , \"omagserverPlatformRootURL\" : {{ MDServerURLRoo t }}, \"omagserverName\" : \"{{MDServerName}}\" } Configure the engines services \u00b6 The engine services (or Open Metadata Engine Services ( OMES ) to give them their full name) also run in the engine host. Each engine service provides support for a particular type of governance engine: Open Discovery Engines Governance Action Engines Each engine service hosts one or more governance engines. A governance engine is a collection of governance services of a specific type: Asset analysis hosts open discovery services that analyze the content of an asset's real world counterpart, generates annotations in an open discovery analysis report that is attached to the asset in the open metadata repositories. Governance action hosts governance action services that monitor changes in the metadata and initiate updates and other actions as a result. List engine services \u00b6 It is possible to get a description of each of the registered engine services using the following command: GET - list engine services {{platformURLRoot}}/open-metadata/platform-services/users/{{userId}}/server-platform/registered-services/engine-services Note the engineServiceURLMarker for the engine service that you want to configure. Configure engine service \u00b6 The descriptive information and operational status are filled out automatically by the administration services based on the engineServiceURLMarker value that you supply. The other values are supplied on the configuration call. Each engine service is configured with the network location of the metadata access point / metadata server running the appropriate OMAS . There are a set of options that the engine service supports along with the list of configuration properties for the governance engines that will be run in the engine service. The governance engine's configuration properties identify which governance engine to run. The governance engine's definition, including the services it supports are retrieved from the metadata access point / metadata server when the engine service starts up. POST - configure engine service {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/engine-services/{{engineServiceURLMarker}} With a request body like the following: { \"class\" : \"EngineServiceRequestBody\" , \"omagserverPlatformRootURL\" : {{ MDServerURLRoo t }}, \"omagserverName\" : \"{{MDServerName}}\" , [ { \"class\" : \"EngineConfig\" , \"engineQualifiedName\" : \" ... \" \"engineUserId\" : \" ... \" } ] } Where: engineQualifiedName - set up the qualified name of the governance engine stored in the metadata servers. connectorUserId - set up the user id for the engine: if this is null, the engine host's userId is used on requests to the Open Metadata Access Service ( OMAS ). Further Information \u00b6 The definition of the governance services that are supported by these governance engines are retrieved from the open metadata server when the engine host server starts up. Maintaining these definitions is described: For discovery engines and services see Discovery Engine OMAS For governance action engines and services see Governance Engine OMAS Remove engine host services \u00b6 The following command removes the configuration for the engine host services from an OMAG Server's configuration document. This may be used if the engine host services have been added in error. DELETE - remove engine host services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{engineHostServerName}}/engine-host-services","title":"Configure Engine Host"},{"location":"guides/admin/servers/configuring-an-engine-host/#configuring-an-engine-host","text":"Each type of OMAG Server is configured by creating a configuration document . Example configuration of a minimal engine host server Below is an example of the configuration for a minimal engine host server. It has a single engine service ( Asset Analysis OMES ) and the default audit log. Both the Governance Engine OMAS used by the engine host services and the Discovery Engine OMAS used by the Asset Analysis OMES are running on the metadata server called myMetadataServer . { \"class\" : \"OMAGServerConfigResponse\" , \"relatedHTTPCode\" : 200 , \"omagserverConfig\" : { \"class\" : \"OMAGServerConfig\" , \"versionId\" : \"V2.0\" , \"localServerId\" : \"8b745d03-5ffc-4978-81ab-bd3d5156eebe\" , \"localServerName\" : \"myserver\" , \"localServerType\" : \"Open Metadata and Governance Server\" , \"localServerURL\" : \"https://localhost:9443\" , \"localServerUserId\" : \"OMAGServer\" , \"maxPageSize\" : 1000 , \"engineHostServicesConfig\" : { \"omagserverPlatformRootURL\" : \"https://localhost:9443\" , \"omagserverName\" : \"myMetadataServer\" , \"engineServices\" : [ { \"class\" : \"EngineServiceConfig\" , \"engineId\" : 6000 , \"engineQualifiedName\" : \"Asset Analysis\" , \"engineServiceFullName\" : \"Asset Analysis OMES\" , \"engineServiceURLMarker\" : \"asset-analysis\" , \"engineServiceDescription\" : \"Analyses the content of an asset's real world counterpart, generates annotations in an open discovery report that is attached to the asset in the open metadata repositories .\" , \"engineServiceWiki\" : \"https://egeria.odpi.org/open-metadata-implementation/engine-services/asset-analysis/\" , \"engines\" : [ { \"engineId\" : \"daff1dca-984b-4b8a-8a8f-febaf72b82a8\" , \"engineName\" : \"engine1\" , \"engineUserId\" : \"engine1UserId\" }, { \"engineId\" : \"a80aa0f8-2ea0-4f84-b613-d68becba2693\" , \"engineName\" : \"engine2\" , \"engineUserId\" : \"engine2UserId\" } ], \"engineServiceOperationalStatus\" : \"ENABLED\" , \"engineServiceAdminClass\" : \"org.odpi.openmetadata.engineservices.assetanalysis.admin.AssetAnalysisAdmin\" , \"omagserverPlatformRootURL\" : \"https://localhost:9443\" , \"omagserverName\" : \"myMetadataServer\" } ]}, \"repositoryServicesConfig\" : { \"class\" : \"RepositoryServicesConfig\" , \"auditLogConnections\" : [ { \"class\" : \"Connection\" , \"headerVersion\" : 0 , \"displayName\" : \"Console\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"headerVersion\" : 0 , \"type\" : { \"class\" : \"ElementType\" , \"headerVersion\" : 0 , \"elementOrigin\" : \"LOCAL_COHORT\" , \"elementVersion\" : 0 , \"elementTypeId\" : \"954421eb-33a6-462d-a8ca-b5709a1bd0d4\" , \"elementTypeName\" : \"ConnectorType\" , \"elementTypeVersion\" : 1 , \"elementTypeDescription\" : \"A set of properties describing a type of connector.\" }, \"guid\" : \"4afac741-3dcc-4c60-a4ca-a6dede994e3f\" , \"qualifiedName\" : \"Console Audit Log Store Connector\" , \"displayName\" : \"Console Audit Log Store Connector\" , \"description\" : \"Connector supports logging of audit log messages to stdout.\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.repositoryservices.auditlogstore.console.ConsoleAuditLogStoreProvider\" }, \"configurationProperties\" : { \"supportedSeverities\" : [ \"<Unknown>\" , \"Information\" , \"Event\" , \"Decision\" , \"Action\" , \"Error\" , \"Exception\" , \"Security\" , \"Startup\" , \"Shutdown\" , \"Asset\" , \"Types\" , \"Cohort\" ] } } ] }, \"auditTrail\" : [ \"Tue Dec 08 18:38:32 GMT 2020 me updated configuration for engine service asset-analysis.\" , \"Tue Dec 08 18:43:47 GMT 2020 me set up default audit log destinations.\" ] } }","title":"Configuring an engine host"},{"location":"guides/admin/servers/configuring-an-engine-host/#configure-the-basic-properties","text":"The basic properties of the OMAG Server are used in logging and events originating from the server. They help to document the purpose of the server (which helps with problem determination) and enable performance improvements by allowing the server to ignore activity or metadata that is not relevant to its operation. The basic properties include two unique identifiers: Property Description localServerId Unique identifier for this server. By default, this is initialized to a randomly generated Universal Unique identifier (UUID). localServerName Meaningful name for the server for use in messages and UIs. Ideally this value is unique to aid administrators in understanding the source of messages and events from the server. This value is set to the server name assigned when the configuration is created. The other basic properties have values that can be changed through the admin services API: Property Description localServerType Descriptive type name for the server. Again this is useful information for the administrator to understand the role of the server. The default value is Open Metadata and Governance Server . organizationName Descriptive name for the organization that owns the local server/repository. This is useful when the open metadata repository cluster consists of metadata servers from different organizations, or different departments of an enterprise. The default value is null . localServerUserId UserId to use for server-initiated REST calls. The default is OMAGServer . localServerPassword Password to use for server-initiated REST calls. The default is null . This means that only the userId is sent in the HTTP header. maxPageSize The maximum page size that can be set on requests to the server. The default value is 1000 . A value of zero means unlimited page size. Although supported, the zero value is not recommended because it provides no protection from a large request denial of service attack. The sections that follow cover how to set up these values.","title":"Configure the basic properties"},{"location":"guides/admin/servers/configuring-an-engine-host/#set-server-type-name","text":"The server type name should be set to something that describes the OMAG Server's role. It may be the name of a specific product that it is enabling, or a role in the metadata and governance landscape. POST - set server type {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-type?typeName=\"{{serverTypeName}}\"","title":"Set server type name"},{"location":"guides/admin/servers/configuring-an-engine-host/#set-organization-name","text":"The organization name may be the owning organization or department or team supported by the server. POST - set organization name {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/organization-name?name=\"{{organizationName}}\"","title":"Set organization name"},{"location":"guides/admin/servers/configuring-an-engine-host/#set-the-servers-user-id-and-optional-password","text":"The server's user ID is used when processing requests that do not have an end user, such as receiving an event from a topic. The default value is OMAGServer . Ideally each server should have its own user ID so it is possible to restrict the resources that each server has access to. If the password is specified as well, the userId and password combination are used to provide authentication information on each REST call made by the server. POST - set server's userId {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-id?id=\"{{serverUserId}}\" POST - set server's password {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-password?password=\"{{serverUserPassword}}\"","title":"Set the server's user ID and optional password"},{"location":"guides/admin/servers/configuring-an-engine-host/#set-the-maximum-page-size-for-rest-api-requests","text":"The maximum page size value sets an upper limit on the number of results that a caller can request on any paging REST API to this server. Setting maximum page size helps to prevent a denial of service attack that uses very large requests to overwhelm the server. A value of 0 means no limit, and leaves the server open to such attacks. POST - set maximum page size {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/max-page-size?limit={{maxPageSize}}","title":"Set the maximum page size for REST API requests"},{"location":"guides/admin/servers/configuring-an-engine-host/#configure-the-audit-log","text":"Egeria's audit log provides a configurable set of destinations for audit records and other diagnostic logging for an OMAG Server . Some destinations also support a query interface to allow an administrator to understand how the server is running. If the server is a development or test server, then the default audit log configuration is probably sufficient: the console audit log destination. POST - set default audit log destination {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/default Using this option overrides all previous audit log destinations. If this server is a production server then you will probably want to set up the audit log destinations explicitly. You can add multiple destinations and each one can be set up to process specific severities of log records. The audit log severities are as follows: Severity Description Information The server is providing information about its normal operation. Event An event was received from another member of the open metadata repository cohort. Decision A decision has been made related to the interaction of the local metadata repository and the rest of the cohort. Action An Action is required by the administrator. At a minimum, the situation needs to be investigated and if necessary, corrective action taken. Error An error occurred, possibly caused by an incompatibility between the local metadata repository and one of the remote repositories. The local repository may restrict some of the metadata interchange functions as a result. Exception An unexpected exception occurred. This means that the server needs some administration attention to correct configuration or fix a logic error because it is not operating as a proper peer in the open metadata repository cohort. Security Unauthorized access to a service or metadata instance has been attempted. Startup A new component is starting up. Shutdown An existing component is shutting down. Asset An auditable action relating to an asset has been taken. Types Activity is occurring that relates to the open metadata types in use by this server. Cohort The server is exchanging registration information about an open metadata repository cohort that it is connecting to. Trace This is additional information on the operation of the server that may be of assistance in debugging a problem. It is not normally logged to any destination, but can be added when needed. PerfMon This log record contains performance monitoring timing information for specific types of processing. It is not normally logged to any destination, but can be added when needed. <Unknown> Uninitialized Severity The body of the request should be a list of severities If an empty list is passed as the request body then all severities are supported by the destination.","title":"Configure the audit log"},{"location":"guides/admin/servers/configuring-an-engine-host/#add-audit-log-destinations","text":"There are various destinations that can be configured for the audit log: console POST - add console audit log destination This writes selected parts of each audit log record to stdout. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/console slf4j POST - add slf4j audit log destination This writes full log records to the slf4j ecosystem. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/slf4j When configuring slf4j as destination you also need to specify auditlog logger category via the application properties. This is described in Connecting the OMAG Audit Log Framework section of the developer logging guide. file POST - add JSON file-based audit log destination This writes JSON files in a shared directory. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/files event POST - add event-based audit log destination This writes each log record as an event on the supplied event topic. It assumes that the event bus is set up first. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/event-topic connection POST - add connection-based audit log destination This sets up an audit log destination that is described though a connection . In this case, the connection is passed in the request body and the supported severities can be supplied in the connection's configuration properties. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/connection list POST - add a list of connection-based audit log destinations It is also possible to set up the audit log destinations as a list of connections. Using this option overrides all previous audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations","title":"Add audit log destinations"},{"location":"guides/admin/servers/configuring-an-engine-host/#remove-audit-logs","text":"The following will remove all audit log destinations: POST - clear all audit log destinations Clears the list of audit log destinations from the configuration enabling you to add a new set of audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/none","title":"Remove audit logs"},{"location":"guides/admin/servers/configuring-an-engine-host/#configure-the-server-security-connector","text":"Metadata that is being aggregated from different sources is likely to need comprehensive access controls. Egeria provides fine-grained security control for metadata access . It is implemented in a server security connector that is called whenever requests are made for to the server. Security is configured for a specific OMAG Server by adding a connection for this connector to the server's configuration document using the following command. POST - configure security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This passes in a connection used to create the server security connector in the request body. { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } Example: set up the sample server security connector For example, this is the connection that would set up the sample server security connector provided for the Coco Pharmaceuticals case study: { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.OpenMetadataServerSecurityProvider\" } }","title":"Configure the server security connector"},{"location":"guides/admin/servers/configuring-an-engine-host/#determine-configured-security","text":"GET - query the server security connector setting {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection Response indicating no security { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific security connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } }","title":"Determine configured security"},{"location":"guides/admin/servers/configuring-an-engine-host/#remove-configured-security","text":"DELETE - remove configured security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This removes all authorization checking from the server.","title":"Remove configured security"},{"location":"guides/admin/servers/configuring-an-engine-host/#configure-the-engine-host-services","text":"The engine host services provide the base implementation of the engine host OMAG Server. There are two parts to configuring the engine host services:","title":"Configure the engine host services"},{"location":"guides/admin/servers/configuring-an-engine-host/#specify-location-of-governance-engine","text":"The location of the metadata server (or metadata access point ) running the Governance Engine OMAS , which will supply the definitions of the governance engines that will run in the engine services, is configured using two properties: the server url root of the metadata server's OMAG Server Platform, and the name of the metadata server . POST - specify location of governance engine {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{engineHostServerName}}/engine-definitions/client-config With a request body like the following: { \"class\" : \"OMAGServerClientConfig\" , \"omagserverPlatformRootURL\" : {{ MDServerURLRoo t }}, \"omagserverName\" : \"{{MDServerName}}\" }","title":"Specify location of governance engine"},{"location":"guides/admin/servers/configuring-an-engine-host/#configure-the-engines-services","text":"The engine services (or Open Metadata Engine Services ( OMES ) to give them their full name) also run in the engine host. Each engine service provides support for a particular type of governance engine: Open Discovery Engines Governance Action Engines Each engine service hosts one or more governance engines. A governance engine is a collection of governance services of a specific type: Asset analysis hosts open discovery services that analyze the content of an asset's real world counterpart, generates annotations in an open discovery analysis report that is attached to the asset in the open metadata repositories. Governance action hosts governance action services that monitor changes in the metadata and initiate updates and other actions as a result.","title":"Configure the engines services"},{"location":"guides/admin/servers/configuring-an-engine-host/#list-engine-services","text":"It is possible to get a description of each of the registered engine services using the following command: GET - list engine services {{platformURLRoot}}/open-metadata/platform-services/users/{{userId}}/server-platform/registered-services/engine-services Note the engineServiceURLMarker for the engine service that you want to configure.","title":"List engine services"},{"location":"guides/admin/servers/configuring-an-engine-host/#configure-engine-service","text":"The descriptive information and operational status are filled out automatically by the administration services based on the engineServiceURLMarker value that you supply. The other values are supplied on the configuration call. Each engine service is configured with the network location of the metadata access point / metadata server running the appropriate OMAS . There are a set of options that the engine service supports along with the list of configuration properties for the governance engines that will be run in the engine service. The governance engine's configuration properties identify which governance engine to run. The governance engine's definition, including the services it supports are retrieved from the metadata access point / metadata server when the engine service starts up. POST - configure engine service {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/engine-services/{{engineServiceURLMarker}} With a request body like the following: { \"class\" : \"EngineServiceRequestBody\" , \"omagserverPlatformRootURL\" : {{ MDServerURLRoo t }}, \"omagserverName\" : \"{{MDServerName}}\" , [ { \"class\" : \"EngineConfig\" , \"engineQualifiedName\" : \" ... \" \"engineUserId\" : \" ... \" } ] } Where: engineQualifiedName - set up the qualified name of the governance engine stored in the metadata servers. connectorUserId - set up the user id for the engine: if this is null, the engine host's userId is used on requests to the Open Metadata Access Service ( OMAS ).","title":"Configure engine service"},{"location":"guides/admin/servers/configuring-an-engine-host/#further-information","text":"The definition of the governance services that are supported by these governance engines are retrieved from the open metadata server when the engine host server starts up. Maintaining these definitions is described: For discovery engines and services see Discovery Engine OMAS For governance action engines and services see Governance Engine OMAS","title":"Further Information"},{"location":"guides/admin/servers/configuring-an-engine-host/#remove-engine-host-services","text":"The following command removes the configuration for the engine host services from an OMAG Server's configuration document. This may be used if the engine host services have been added in error. DELETE - remove engine host services {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{engineHostServerName}}/engine-host-services","title":"Remove engine host services"},{"location":"guides/admin/servers/configuring-an-integration-daemon/","text":"Configuring an integration daemon \u00b6 Each type of OMAG Server is configured by creating a configuration document . Configure the basic properties \u00b6 The basic properties of the OMAG Server are used in logging and events originating from the server. They help to document the purpose of the server (which helps with problem determination) and enable performance improvements by allowing the server to ignore activity or metadata that is not relevant to its operation. The basic properties include two unique identifiers: Property Description localServerId Unique identifier for this server. By default, this is initialized to a randomly generated Universal Unique identifier (UUID). localServerName Meaningful name for the server for use in messages and UIs. Ideally this value is unique to aid administrators in understanding the source of messages and events from the server. This value is set to the server name assigned when the configuration is created. The other basic properties have values that can be changed through the admin services API: Property Description localServerType Descriptive type name for the server. Again this is useful information for the administrator to understand the role of the server. The default value is Open Metadata and Governance Server . organizationName Descriptive name for the organization that owns the local server/repository. This is useful when the open metadata repository cluster consists of metadata servers from different organizations, or different departments of an enterprise. The default value is null . localServerUserId UserId to use for server-initiated REST calls. The default is OMAGServer . localServerPassword Password to use for server-initiated REST calls. The default is null . This means that only the userId is sent in the HTTP header. maxPageSize The maximum page size that can be set on requests to the server. The default value is 1000 . A value of zero means unlimited page size. Although supported, the zero value is not recommended because it provides no protection from a large request denial of service attack. The sections that follow cover how to set up these values. Set server type name \u00b6 The server type name should be set to something that describes the OMAG Server's role. It may be the name of a specific product that it is enabling, or a role in the metadata and governance landscape. POST - set server type {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-type?typeName=\"{{serverTypeName}}\" Set organization name \u00b6 The organization name may be the owning organization or department or team supported by the server. POST - set organization name {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/organization-name?name=\"{{organizationName}}\" Set the server's user ID and optional password \u00b6 The server's user ID is used when processing requests that do not have an end user, such as receiving an event from a topic. The default value is OMAGServer . Ideally each server should have its own user ID so it is possible to restrict the resources that each server has access to. If the password is specified as well, the userId and password combination are used to provide authentication information on each REST call made by the server. POST - set server's userId {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-id?id=\"{{serverUserId}}\" POST - set server's password {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-password?password=\"{{serverUserPassword}}\" Set the maximum page size for REST API requests \u00b6 The maximum page size value sets an upper limit on the number of results that a caller can request on any paging REST API to this server. Setting maximum page size helps to prevent a denial of service attack that uses very large requests to overwhelm the server. A value of 0 means no limit, and leaves the server open to such attacks. POST - set maximum page size {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/max-page-size?limit={{maxPageSize}} Configure the audit log \u00b6 Egeria's audit log provides a configurable set of destinations for audit records and other diagnostic logging for an OMAG Server . Some destinations also support a query interface to allow an administrator to understand how the server is running. If the server is a development or test server, then the default audit log configuration is probably sufficient: the console audit log destination. POST - set default audit log destination {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/default Using this option overrides all previous audit log destinations. If this server is a production server then you will probably want to set up the audit log destinations explicitly. You can add multiple destinations and each one can be set up to process specific severities of log records. The audit log severities are as follows: Severity Description Information The server is providing information about its normal operation. Event An event was received from another member of the open metadata repository cohort. Decision A decision has been made related to the interaction of the local metadata repository and the rest of the cohort. Action An Action is required by the administrator. At a minimum, the situation needs to be investigated and if necessary, corrective action taken. Error An error occurred, possibly caused by an incompatibility between the local metadata repository and one of the remote repositories. The local repository may restrict some of the metadata interchange functions as a result. Exception An unexpected exception occurred. This means that the server needs some administration attention to correct configuration or fix a logic error because it is not operating as a proper peer in the open metadata repository cohort. Security Unauthorized access to a service or metadata instance has been attempted. Startup A new component is starting up. Shutdown An existing component is shutting down. Asset An auditable action relating to an asset has been taken. Types Activity is occurring that relates to the open metadata types in use by this server. Cohort The server is exchanging registration information about an open metadata repository cohort that it is connecting to. Trace This is additional information on the operation of the server that may be of assistance in debugging a problem. It is not normally logged to any destination, but can be added when needed. PerfMon This log record contains performance monitoring timing information for specific types of processing. It is not normally logged to any destination, but can be added when needed. <Unknown> Uninitialized Severity The body of the request should be a list of severities If an empty list is passed as the request body then all severities are supported by the destination. Add audit log destinations \u00b6 There are various destinations that can be configured for the audit log: console POST - add console audit log destination This writes selected parts of each audit log record to stdout. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/console slf4j POST - add slf4j audit log destination This writes full log records to the slf4j ecosystem. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/slf4j When configuring slf4j as destination you also need to specify auditlog logger category via the application properties. This is described in Connecting the OMAG Audit Log Framework section of the developer logging guide. file POST - add JSON file-based audit log destination This writes JSON files in a shared directory. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/files event POST - add event-based audit log destination This writes each log record as an event on the supplied event topic. It assumes that the event bus is set up first. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/event-topic connection POST - add connection-based audit log destination This sets up an audit log destination that is described though a connection . In this case, the connection is passed in the request body and the supported severities can be supplied in the connection's configuration properties. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/connection list POST - add a list of connection-based audit log destinations It is also possible to set up the audit log destinations as a list of connections. Using this option overrides all previous audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations Remove audit logs \u00b6 The following will remove all audit log destinations: POST - clear all audit log destinations Clears the list of audit log destinations from the configuration enabling you to add a new set of audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/none Configure the server security connector \u00b6 Metadata that is being aggregated from different sources is likely to need comprehensive access controls. Egeria provides fine-grained security control for metadata access . It is implemented in a server security connector that is called whenever requests are made for to the server. Security is configured for a specific OMAG Server by adding a connection for this connector to the server's configuration document using the following command. POST - configure security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This passes in a connection used to create the server security connector in the request body. { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } Example: set up the sample server security connector For example, this is the connection that would set up the sample server security connector provided for the Coco Pharmaceuticals case study: { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.OpenMetadataServerSecurityProvider\" } } Determine configured security \u00b6 GET - query the server security connector setting {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection Response indicating no security { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific security connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } } Remove configured security \u00b6 DELETE - remove configured security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This removes all authorization checking from the server. Configure the integration services \u00b6 The integration services (or Open Metadata Integration Services ( OMIS ) to give them their full name) run in an integration daemon . Each integration service hosts one or more integration connectors . An integration connector is responsible for the exchange of metadata with a specific deployment of a third party technology. For example, the database integrator integration service supports integration connectors that work with relational databases (RDBMS). A deployment of this integration service in an integration daemon may host, say, two integration connectors each loading metadata from their own relational database server. The descriptive information and operational status are filled out automatically by the administration services based on the integrationServiceURLMarker value that you supply. The other values are supplied on the configuration call. List integration services \u00b6 It is possible to get a description of each of the registered integration services using the following command: GET - list integration services {{platformURLRoot}}/open-metadata/platform-services/users/{{adminUserId}}/server-platform/registered-services/integration-services Note the integrationServiceURLMarker for the integration service that you want to configure. Configure an integration service \u00b6 Each integration service is configured with the network location of the metadata access point / metadata server running the appropriate OMAS . There are a set of options that the integration service supports along with the list of configuration properties for the integration connectors that will be run in the integration service. The integration connector's configuration properties defines which connector implementation to use and how it should be operated. POST - configure an integration service {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/integration-services/{{integrationServiceURLMarker}} With a request body like the following: { \"class\" : \"IntegrationServiceRequestBody\" , \"omagserverPlatformRootURL\" : \"{MDServerURLRoot}\" , \"omagserverName\" : \"{MDServerName}\" , \"integrationConnectorConfigs\" : [ { \"class\" : \"IntegrationConnectorConfig\" , \"connectorName\" : \" ... \" , \"connectorUserId\" : \" ... \" , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{connector provider class name}\" }, \"endpoint\" : { \"class\" : \"Endpoint\" , \"address\" : \"...\" } }, \"metadataSourceQualifiedName\" : \" ... \" , \"refreshTimeInterval\" : \"60\" , \"usesBlockingCalls\" : \"false\" , \"permittedSynchronization\" : \" ... \" } ] } Detailed description of the properties: connectorName sets up the name of the connector. This name is used for routing refresh calls to the connector as well as being used for diagnostics. Ideally it should be unique amongst the connectors for the integration service. connectorUserId sets up the user id for the connector - if this is null, the integration daemon's userId is used on requests to the Open Metadata Access Service ( OMAS ). connection sets up the connection for the integration connector. metadataSourceQualifiedName sets up the qualified name of the metadata source for this integration connector. This is the qualified name of an appropriate software server capability stored in open metadata. This software server capability is accessed via the partner OMAS . refreshTimeInterval sets up the number of minutes between each call to the connector to refresh the metadata. Zero means that refresh is only called at server start up and whenever the refresh REST API request is made to the integration daemon. If the refresh time interval is greater than 0 then additional calls to refresh are added spaced out by the refresh time interval. usesBlockingCalls sets up whether the connector should be started in its own thread to allow it to block on a listening call. permittedSynchronization is an optional property that defines the permitted directions of metadata flow between the third party technology and open metadata. If the integration connector attempts to flow metadata in a direction that is not permitted, it receives the UserNotAuthorizedException . The default for this value is set up automatically in the integration service's descriptive information so this value only needs to be set if it is necessary to restrict the behavior of the connector. These are the different values for this property and their effect: TO_THIRD_PARTY - The third party technology is logically downstream of open metadata. This means the open metadata ecosystem is the originator and owner of the metadata being synchronized. Any updates detected in the third technology are overridden by the latest open metadata values. FROM_THIRD_PARTY - The third party technology is logically upstream (the originator and owner of the metadata). Any updates made in open metadata are not passed to the third party technology and the third party technology is requested to refresh the open metadata version. BOTH_DIRECTIONS - Metadata exchange is permitted in both directions. Synchronization is halted on a specific element if potentially clashing updates have occurred both in the third party technology and open metadata. Such conflicts are logged on the audit log and resolved through manual stewardship. Further information \u00b6 For help in fixing any error you find using the integration daemon, visit the integration daemon diagnostic guide . Link to the Egeria solutions to see the integration daemon in action. Link to the integration daemon services to understand how the integration daemon is implemented.","title":"Configure Integration Daemon"},{"location":"guides/admin/servers/configuring-an-integration-daemon/#configuring-an-integration-daemon","text":"Each type of OMAG Server is configured by creating a configuration document .","title":"Configuring an integration daemon"},{"location":"guides/admin/servers/configuring-an-integration-daemon/#configure-the-basic-properties","text":"The basic properties of the OMAG Server are used in logging and events originating from the server. They help to document the purpose of the server (which helps with problem determination) and enable performance improvements by allowing the server to ignore activity or metadata that is not relevant to its operation. The basic properties include two unique identifiers: Property Description localServerId Unique identifier for this server. By default, this is initialized to a randomly generated Universal Unique identifier (UUID). localServerName Meaningful name for the server for use in messages and UIs. Ideally this value is unique to aid administrators in understanding the source of messages and events from the server. This value is set to the server name assigned when the configuration is created. The other basic properties have values that can be changed through the admin services API: Property Description localServerType Descriptive type name for the server. Again this is useful information for the administrator to understand the role of the server. The default value is Open Metadata and Governance Server . organizationName Descriptive name for the organization that owns the local server/repository. This is useful when the open metadata repository cluster consists of metadata servers from different organizations, or different departments of an enterprise. The default value is null . localServerUserId UserId to use for server-initiated REST calls. The default is OMAGServer . localServerPassword Password to use for server-initiated REST calls. The default is null . This means that only the userId is sent in the HTTP header. maxPageSize The maximum page size that can be set on requests to the server. The default value is 1000 . A value of zero means unlimited page size. Although supported, the zero value is not recommended because it provides no protection from a large request denial of service attack. The sections that follow cover how to set up these values.","title":"Configure the basic properties"},{"location":"guides/admin/servers/configuring-an-integration-daemon/#set-server-type-name","text":"The server type name should be set to something that describes the OMAG Server's role. It may be the name of a specific product that it is enabling, or a role in the metadata and governance landscape. POST - set server type {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-type?typeName=\"{{serverTypeName}}\"","title":"Set server type name"},{"location":"guides/admin/servers/configuring-an-integration-daemon/#set-organization-name","text":"The organization name may be the owning organization or department or team supported by the server. POST - set organization name {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/organization-name?name=\"{{organizationName}}\"","title":"Set organization name"},{"location":"guides/admin/servers/configuring-an-integration-daemon/#set-the-servers-user-id-and-optional-password","text":"The server's user ID is used when processing requests that do not have an end user, such as receiving an event from a topic. The default value is OMAGServer . Ideally each server should have its own user ID so it is possible to restrict the resources that each server has access to. If the password is specified as well, the userId and password combination are used to provide authentication information on each REST call made by the server. POST - set server's userId {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-id?id=\"{{serverUserId}}\" POST - set server's password {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/server-user-password?password=\"{{serverUserPassword}}\"","title":"Set the server's user ID and optional password"},{"location":"guides/admin/servers/configuring-an-integration-daemon/#set-the-maximum-page-size-for-rest-api-requests","text":"The maximum page size value sets an upper limit on the number of results that a caller can request on any paging REST API to this server. Setting maximum page size helps to prevent a denial of service attack that uses very large requests to overwhelm the server. A value of 0 means no limit, and leaves the server open to such attacks. POST - set maximum page size {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/max-page-size?limit={{maxPageSize}}","title":"Set the maximum page size for REST API requests"},{"location":"guides/admin/servers/configuring-an-integration-daemon/#configure-the-audit-log","text":"Egeria's audit log provides a configurable set of destinations for audit records and other diagnostic logging for an OMAG Server . Some destinations also support a query interface to allow an administrator to understand how the server is running. If the server is a development or test server, then the default audit log configuration is probably sufficient: the console audit log destination. POST - set default audit log destination {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/default Using this option overrides all previous audit log destinations. If this server is a production server then you will probably want to set up the audit log destinations explicitly. You can add multiple destinations and each one can be set up to process specific severities of log records. The audit log severities are as follows: Severity Description Information The server is providing information about its normal operation. Event An event was received from another member of the open metadata repository cohort. Decision A decision has been made related to the interaction of the local metadata repository and the rest of the cohort. Action An Action is required by the administrator. At a minimum, the situation needs to be investigated and if necessary, corrective action taken. Error An error occurred, possibly caused by an incompatibility between the local metadata repository and one of the remote repositories. The local repository may restrict some of the metadata interchange functions as a result. Exception An unexpected exception occurred. This means that the server needs some administration attention to correct configuration or fix a logic error because it is not operating as a proper peer in the open metadata repository cohort. Security Unauthorized access to a service or metadata instance has been attempted. Startup A new component is starting up. Shutdown An existing component is shutting down. Asset An auditable action relating to an asset has been taken. Types Activity is occurring that relates to the open metadata types in use by this server. Cohort The server is exchanging registration information about an open metadata repository cohort that it is connecting to. Trace This is additional information on the operation of the server that may be of assistance in debugging a problem. It is not normally logged to any destination, but can be added when needed. PerfMon This log record contains performance monitoring timing information for specific types of processing. It is not normally logged to any destination, but can be added when needed. <Unknown> Uninitialized Severity The body of the request should be a list of severities If an empty list is passed as the request body then all severities are supported by the destination.","title":"Configure the audit log"},{"location":"guides/admin/servers/configuring-an-integration-daemon/#add-audit-log-destinations","text":"There are various destinations that can be configured for the audit log: console POST - add console audit log destination This writes selected parts of each audit log record to stdout. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/console slf4j POST - add slf4j audit log destination This writes full log records to the slf4j ecosystem. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/slf4j When configuring slf4j as destination you also need to specify auditlog logger category via the application properties. This is described in Connecting the OMAG Audit Log Framework section of the developer logging guide. file POST - add JSON file-based audit log destination This writes JSON files in a shared directory. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/files event POST - add event-based audit log destination This writes each log record as an event on the supplied event topic. It assumes that the event bus is set up first. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/event-topic connection POST - add connection-based audit log destination This sets up an audit log destination that is described though a connection . In this case, the connection is passed in the request body and the supported severities can be supplied in the connection's configuration properties. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/connection list POST - add a list of connection-based audit log destinations It is also possible to set up the audit log destinations as a list of connections. Using this option overrides all previous audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations","title":"Add audit log destinations"},{"location":"guides/admin/servers/configuring-an-integration-daemon/#remove-audit-logs","text":"The following will remove all audit log destinations: POST - clear all audit log destinations Clears the list of audit log destinations from the configuration enabling you to add a new set of audit log destinations. {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/audit-log-destinations/none","title":"Remove audit logs"},{"location":"guides/admin/servers/configuring-an-integration-daemon/#configure-the-server-security-connector","text":"Metadata that is being aggregated from different sources is likely to need comprehensive access controls. Egeria provides fine-grained security control for metadata access . It is implemented in a server security connector that is called whenever requests are made for to the server. Security is configured for a specific OMAG Server by adding a connection for this connector to the server's configuration document using the following command. POST - configure security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This passes in a connection used to create the server security connector in the request body. { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } Example: set up the sample server security connector For example, this is the connection that would set up the sample server security connector provided for the Coco Pharmaceuticals case study: { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.metadatasecurity.samples.OpenMetadataServerSecurityProvider\" } }","title":"Configure the server security connector"},{"location":"guides/admin/servers/configuring-an-integration-daemon/#determine-configured-security","text":"GET - query the server security connector setting {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection Response indicating no security { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 } Response indicating a specific security connector If the response looks more like the JSON below, a connector is configured. The connectorProviderClassName tells you which connector is being used. { \"class\" : \"ConnectionResponse\" , \"relatedHTTPCode\" : 200 , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{fullyQualifiedJavaClassName}\" } } }","title":"Determine configured security"},{"location":"guides/admin/servers/configuring-an-integration-daemon/#remove-configured-security","text":"DELETE - remove configured security connector {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/security/connection This removes all authorization checking from the server.","title":"Remove configured security"},{"location":"guides/admin/servers/configuring-an-integration-daemon/#configure-the-integration-services","text":"The integration services (or Open Metadata Integration Services ( OMIS ) to give them their full name) run in an integration daemon . Each integration service hosts one or more integration connectors . An integration connector is responsible for the exchange of metadata with a specific deployment of a third party technology. For example, the database integrator integration service supports integration connectors that work with relational databases (RDBMS). A deployment of this integration service in an integration daemon may host, say, two integration connectors each loading metadata from their own relational database server. The descriptive information and operational status are filled out automatically by the administration services based on the integrationServiceURLMarker value that you supply. The other values are supplied on the configuration call.","title":"Configure the integration services"},{"location":"guides/admin/servers/configuring-an-integration-daemon/#list-integration-services","text":"It is possible to get a description of each of the registered integration services using the following command: GET - list integration services {{platformURLRoot}}/open-metadata/platform-services/users/{{adminUserId}}/server-platform/registered-services/integration-services Note the integrationServiceURLMarker for the integration service that you want to configure.","title":"List integration services"},{"location":"guides/admin/servers/configuring-an-integration-daemon/#configure-an-integration-service","text":"Each integration service is configured with the network location of the metadata access point / metadata server running the appropriate OMAS . There are a set of options that the integration service supports along with the list of configuration properties for the integration connectors that will be run in the integration service. The integration connector's configuration properties defines which connector implementation to use and how it should be operated. POST - configure an integration service {{platformURLRoot}}/open-metadata/admin-services/users/{{adminUserId}}/servers/{{serverName}}/integration-services/{{integrationServiceURLMarker}} With a request body like the following: { \"class\" : \"IntegrationServiceRequestBody\" , \"omagserverPlatformRootURL\" : \"{MDServerURLRoot}\" , \"omagserverName\" : \"{MDServerName}\" , \"integrationConnectorConfigs\" : [ { \"class\" : \"IntegrationConnectorConfig\" , \"connectorName\" : \" ... \" , \"connectorUserId\" : \" ... \" , \"connection\" : { \"class\" : \"Connection\" , \"connectorType\" : { \"class\" : \"ConnectorType\" , \"connectorProviderClassName\" : \"{connector provider class name}\" }, \"endpoint\" : { \"class\" : \"Endpoint\" , \"address\" : \"...\" } }, \"metadataSourceQualifiedName\" : \" ... \" , \"refreshTimeInterval\" : \"60\" , \"usesBlockingCalls\" : \"false\" , \"permittedSynchronization\" : \" ... \" } ] } Detailed description of the properties: connectorName sets up the name of the connector. This name is used for routing refresh calls to the connector as well as being used for diagnostics. Ideally it should be unique amongst the connectors for the integration service. connectorUserId sets up the user id for the connector - if this is null, the integration daemon's userId is used on requests to the Open Metadata Access Service ( OMAS ). connection sets up the connection for the integration connector. metadataSourceQualifiedName sets up the qualified name of the metadata source for this integration connector. This is the qualified name of an appropriate software server capability stored in open metadata. This software server capability is accessed via the partner OMAS . refreshTimeInterval sets up the number of minutes between each call to the connector to refresh the metadata. Zero means that refresh is only called at server start up and whenever the refresh REST API request is made to the integration daemon. If the refresh time interval is greater than 0 then additional calls to refresh are added spaced out by the refresh time interval. usesBlockingCalls sets up whether the connector should be started in its own thread to allow it to block on a listening call. permittedSynchronization is an optional property that defines the permitted directions of metadata flow between the third party technology and open metadata. If the integration connector attempts to flow metadata in a direction that is not permitted, it receives the UserNotAuthorizedException . The default for this value is set up automatically in the integration service's descriptive information so this value only needs to be set if it is necessary to restrict the behavior of the connector. These are the different values for this property and their effect: TO_THIRD_PARTY - The third party technology is logically downstream of open metadata. This means the open metadata ecosystem is the originator and owner of the metadata being synchronized. Any updates detected in the third technology are overridden by the latest open metadata values. FROM_THIRD_PARTY - The third party technology is logically upstream (the originator and owner of the metadata). Any updates made in open metadata are not passed to the third party technology and the third party technology is requested to refresh the open metadata version. BOTH_DIRECTIONS - Metadata exchange is permitted in both directions. Synchronization is halted on a specific element if potentially clashing updates have occurred both in the third party technology and open metadata. Such conflicts are logged on the audit log and resolved through manual stewardship.","title":"Configure an integration service"},{"location":"guides/admin/servers/configuring-an-integration-daemon/#further-information","text":"For help in fixing any error you find using the integration daemon, visit the integration daemon diagnostic guide . Link to the Egeria solutions to see the integration daemon in action. Link to the integration daemon services to understand how the integration daemon is implemented.","title":"Further information"},{"location":"guides/cts/overview/","text":"Conformance Test Suite Overview \u00b6 The open metadata conformance suite provides a testing framework to help the developers integrate a specific technology into the open metadata ecosystem. The actual tests are run by an open metadata conformance workbench within the open metadata conformance suite server. Each workbench focuses on testing a specific type of technology, and typically define the set of functionality being tested in a profile . Test cases within profiles The profiles are supported by one or more test cases (described in detail within each profile). Each test case typically focuses on a specific requirement within a profile. However, it may verify other requirements from either the same of different profiles if it is efficient to do so. When a test case encounters errors, it will log them and if possible it will continue testing. However, some failures are blocking and the test case will end when one of these is encountered. Platform workbench \u00b6 The open metadata conformance platform workbench is responsible for testing the various APIs supported by an Open Metadata and Governance ( OMAG ) Server Platform . This workbench supports the following profiles: Profile Description Platform origin Does the platform support the server-platform-origin API. Repository workbench \u00b6 The open metadata conformance repository workbench is responsible for testing the ability of an open metadata repository to connect and interact with other open metadata repositories in a conformant way. It tests both the repository's repository services API and its ability to exchange events with the OMRS cohort event topic . The workbench uses the registration information that is passed when the technology under test registers with the same open metadata repository cohort as the conformance suite. It will confirm that the information received in the events matches the information returned by the technology under test's repository services. This workbench works as a pipeline processor, accumulating information from one test and using it to seed subsequent tests. A failure early on in the pipeline may prevent other tests from running. In addition, this workbench dynamically generates tests based on the types returned by the repository. So for example, the repository TypeDef test case runs for each TypeDef returned by the repository. A failure in the early set up test cases will prevent the repository workbench from generating the full suite of test cases for the repository under test. The functions expected of an open metadata repository are numerous. These functions are broken down into the profiles listed below. An open metadata repository needs to support at least one profile to be conformant: in practice, metadata sharing is required in order to support any of the other profiles, so it is mandatory. Profile Description Metadata sharing The technology under test is able to share metadata with other members of the cohort. Reference copies The technology under test is able to store reference copies of metadata from other members of the cohort. Metadata maintenance The technology under test supports requests to create, update and purge metadata instances. Effectivity dating The technology under test supports effectivity dating properties. Dynamic types The technology under test supports changes to the list of its supported types while it is running. Graph queries The technology under test supports graph-like queries that return collections of metadata instances. Historical search The technology under test supports search for the state of the metadata instances at a specific time in the past. Entity proxies The technology under test is able to store stubs for entities to use on relationships when the full entity is not available. Soft-delete and restore The technology under test allows an instance to be soft-deleted and restored. Undo an update The technology under test is able to restore an instance to its previous version (although the version number is updated). Reidentify instance The technology under test supports the command to change the unique identifier (guid) of a metadata instance. Retype instance The technology under test supports the command to change the type of a metadata instance to either its super type or a subtype. Rehome instance The technology under test supports the command to update the metadata collection id for a metadata instance. Performance workbench \u00b6 The open metadata conformance performance workbench is responsible for testing the performance of various APIs supported by an Open Metadata and Governance ( OMAG ) server repository. Focused purely on measuring performance Note that the workbench is focused purely on measuring performance, and will not extensively test for correctness of results across a variety of edge cases, etc. For that, use the normal repository workbench . This workbench runs the following profiles, in the following order: Profile Description Entity creation tests the performance of addEntity and saveEntityReferenceCopy methods Entity search tests the performance of findEntities , findEntitiesByProperty and findEntitiesByPropertyValue methods Relationship creation tests the performance of addRelationship and saveRelationshipReferenceCopy methods Relationship search tests the performance of findRelationships , findRelationshipsByProperty and findRelationshipsByPropertyValue methods Entity classification tests the performance of classifyEntity and saveClassificationReferenceCopy methods Classification search tests the performance of findEntitiesByClassification method Entity update tests the performance of updateEntityProperties method Relationship update tests the performance of updateRelationshipProperties method Classification update tests the performance of updateEntityClassification method Entity undo tests the performance of undoEntityUpdate method Relationship undo tests the performance of undoRelationshipUpdate method Entity retrieval tests the performance of isEntityKnown , getEntitySummary and getEntityDetail methods Entity history retrieval tests the performance of getEntityDetail (with non-null asOfTime ) and getEntityDetailHistory methods Relationship retrieval tests the performance of isRelationshipKnown and getRelationship methods Relationship history retrieval tests the performance of getRelationship (with non-null asOfTime ) and getRelationshipHistory methods Entity history search tests the performance of the same search operations as Entity Search, but in each case with a non-null asOfTime Relationship history search tests the performance of the same search operations as Relationship Search, but in each case with a non-null asOfTime Graph queries tests the performance of getRelationshipsForEntity , getEntityNeighborhood , getRelatedEntities and getLinkingEntities methods Graph history queries tests the performance of the same operations as Graph Queries, but in each case with a non-null asOfTime Entity re-home tests the performance of reHomeEntity method Relationship re-home tests the performance of reHomeRelationship method Entity declassify tests the performance of declassifyEntity and purgeClassificationReferenceCopy methods Entity re-type tests the performance of reTypeEntity method Relationship re-type tests the performance of reTypeRelationship method Entity re-identify tests the performance of reIdentifyEntity method Relationship re-identify tests the performance of reIdentifyRelationship method Relationship delete tests the performance of deleteRelationship method Entity delete tests the performance of deleteEntity method Entity restore tests the performance of restoreEntity method Relationship restore tests the performance of restoreRelationship method Relationship purge tests the performance of purgeRelationship and purgeRelationshipReferenceCopy methods Entity purge tests the performance of purgeEntity and purgeEntityReferenceCopy methods Environment does not actually perform any tests, but rather gives statistics about the environment in which the tests were performed (instance counts, etc) In each profile, the methods being tested will be executed a number of times and the elapsed time of each execution captured. These elapsed times are available through the detailed profile results of the Conformance Test Suite reports, and can be extracted to calculate more detailed statistics (min, max, median, mean, etc). Configuration of the performance test can be done through the properties passed in to the admin services prior to executing it: Property Use instancesPerType controls how many instances of metadata to create (per type supported by the repository) (defaults to 50 ) maxSearchResults controls how many results to retrieve per page for any search operations (defaults to 10 ) waitBetweenScenarios controls an optional wait-point between write and read scenarios, in case you are testing a repository that has an eventually-consistent index (defaults to 0 to avoid any wait) profilesToSkip is an optional array of strings of the profile names that should be skipped during performance testing (for example, to skip very long-running profiles like the graph queries at the larger scales, where thousands or more relationships and entities could be returned by each query)","title":"CTS Overview"},{"location":"guides/cts/overview/#conformance-test-suite-overview","text":"The open metadata conformance suite provides a testing framework to help the developers integrate a specific technology into the open metadata ecosystem. The actual tests are run by an open metadata conformance workbench within the open metadata conformance suite server. Each workbench focuses on testing a specific type of technology, and typically define the set of functionality being tested in a profile . Test cases within profiles The profiles are supported by one or more test cases (described in detail within each profile). Each test case typically focuses on a specific requirement within a profile. However, it may verify other requirements from either the same of different profiles if it is efficient to do so. When a test case encounters errors, it will log them and if possible it will continue testing. However, some failures are blocking and the test case will end when one of these is encountered.","title":"Conformance Test Suite Overview"},{"location":"guides/cts/overview/#platform-workbench","text":"The open metadata conformance platform workbench is responsible for testing the various APIs supported by an Open Metadata and Governance ( OMAG ) Server Platform . This workbench supports the following profiles: Profile Description Platform origin Does the platform support the server-platform-origin API.","title":"Platform workbench"},{"location":"guides/cts/overview/#repository-workbench","text":"The open metadata conformance repository workbench is responsible for testing the ability of an open metadata repository to connect and interact with other open metadata repositories in a conformant way. It tests both the repository's repository services API and its ability to exchange events with the OMRS cohort event topic . The workbench uses the registration information that is passed when the technology under test registers with the same open metadata repository cohort as the conformance suite. It will confirm that the information received in the events matches the information returned by the technology under test's repository services. This workbench works as a pipeline processor, accumulating information from one test and using it to seed subsequent tests. A failure early on in the pipeline may prevent other tests from running. In addition, this workbench dynamically generates tests based on the types returned by the repository. So for example, the repository TypeDef test case runs for each TypeDef returned by the repository. A failure in the early set up test cases will prevent the repository workbench from generating the full suite of test cases for the repository under test. The functions expected of an open metadata repository are numerous. These functions are broken down into the profiles listed below. An open metadata repository needs to support at least one profile to be conformant: in practice, metadata sharing is required in order to support any of the other profiles, so it is mandatory. Profile Description Metadata sharing The technology under test is able to share metadata with other members of the cohort. Reference copies The technology under test is able to store reference copies of metadata from other members of the cohort. Metadata maintenance The technology under test supports requests to create, update and purge metadata instances. Effectivity dating The technology under test supports effectivity dating properties. Dynamic types The technology under test supports changes to the list of its supported types while it is running. Graph queries The technology under test supports graph-like queries that return collections of metadata instances. Historical search The technology under test supports search for the state of the metadata instances at a specific time in the past. Entity proxies The technology under test is able to store stubs for entities to use on relationships when the full entity is not available. Soft-delete and restore The technology under test allows an instance to be soft-deleted and restored. Undo an update The technology under test is able to restore an instance to its previous version (although the version number is updated). Reidentify instance The technology under test supports the command to change the unique identifier (guid) of a metadata instance. Retype instance The technology under test supports the command to change the type of a metadata instance to either its super type or a subtype. Rehome instance The technology under test supports the command to update the metadata collection id for a metadata instance.","title":"Repository workbench"},{"location":"guides/cts/overview/#performance-workbench","text":"The open metadata conformance performance workbench is responsible for testing the performance of various APIs supported by an Open Metadata and Governance ( OMAG ) server repository. Focused purely on measuring performance Note that the workbench is focused purely on measuring performance, and will not extensively test for correctness of results across a variety of edge cases, etc. For that, use the normal repository workbench . This workbench runs the following profiles, in the following order: Profile Description Entity creation tests the performance of addEntity and saveEntityReferenceCopy methods Entity search tests the performance of findEntities , findEntitiesByProperty and findEntitiesByPropertyValue methods Relationship creation tests the performance of addRelationship and saveRelationshipReferenceCopy methods Relationship search tests the performance of findRelationships , findRelationshipsByProperty and findRelationshipsByPropertyValue methods Entity classification tests the performance of classifyEntity and saveClassificationReferenceCopy methods Classification search tests the performance of findEntitiesByClassification method Entity update tests the performance of updateEntityProperties method Relationship update tests the performance of updateRelationshipProperties method Classification update tests the performance of updateEntityClassification method Entity undo tests the performance of undoEntityUpdate method Relationship undo tests the performance of undoRelationshipUpdate method Entity retrieval tests the performance of isEntityKnown , getEntitySummary and getEntityDetail methods Entity history retrieval tests the performance of getEntityDetail (with non-null asOfTime ) and getEntityDetailHistory methods Relationship retrieval tests the performance of isRelationshipKnown and getRelationship methods Relationship history retrieval tests the performance of getRelationship (with non-null asOfTime ) and getRelationshipHistory methods Entity history search tests the performance of the same search operations as Entity Search, but in each case with a non-null asOfTime Relationship history search tests the performance of the same search operations as Relationship Search, but in each case with a non-null asOfTime Graph queries tests the performance of getRelationshipsForEntity , getEntityNeighborhood , getRelatedEntities and getLinkingEntities methods Graph history queries tests the performance of the same operations as Graph Queries, but in each case with a non-null asOfTime Entity re-home tests the performance of reHomeEntity method Relationship re-home tests the performance of reHomeRelationship method Entity declassify tests the performance of declassifyEntity and purgeClassificationReferenceCopy methods Entity re-type tests the performance of reTypeEntity method Relationship re-type tests the performance of reTypeRelationship method Entity re-identify tests the performance of reIdentifyEntity method Relationship re-identify tests the performance of reIdentifyRelationship method Relationship delete tests the performance of deleteRelationship method Entity delete tests the performance of deleteEntity method Entity restore tests the performance of restoreEntity method Relationship restore tests the performance of restoreRelationship method Relationship purge tests the performance of purgeRelationship and purgeRelationshipReferenceCopy methods Entity purge tests the performance of purgeEntity and purgeEntityReferenceCopy methods Environment does not actually perform any tests, but rather gives statistics about the environment in which the tests were performed (instance counts, etc) In each profile, the methods being tested will be executed a number of times and the elapsed time of each execution captured. These elapsed times are available through the detailed profile results of the Conformance Test Suite reports, and can be extracted to calculate more detailed statistics (min, max, median, mean, etc). Configuration of the performance test can be done through the properties passed in to the admin services prior to executing it: Property Use instancesPerType controls how many instances of metadata to create (per type supported by the repository) (defaults to 50 ) maxSearchResults controls how many results to retrieve per page for any search operations (defaults to 10 ) waitBetweenScenarios controls an optional wait-point between write and read scenarios, in case you are testing a repository that has an eventually-consistent index (defaults to 0 to avoid any wait) profilesToSkip is an optional array of strings of the profile names that should be skipped during performance testing (for example, to skip very long-running profiles like the graph queries at the larger scales, where thousands or more relationships and entities could be returned by each query)","title":"Performance workbench"},{"location":"guides/cts/profiles/classification-search/","text":"Classification search profile \u00b6 The performance of programmatically searching for existing entities based on their classification(s). The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for searching for entity instances based on classification: Method Description findEntitiesByClassification searches for entity instances based on specific classifications or properties of classifications with specific values Assertions ID Description repository-classification-search-performance-findEntitiesByClassification-all Repository performs search for both of two classification property values, sorting by most recent update date, of first page of instances for a classification. repository-classification-search-performance-findEntitiesByClassification-alone Repository performs search by classification alone (no properties), sorting by most recent creation date, of first page of instances for a classification. repository-classification-search-performance-findEntitiesByClassification-any Repository performs search for either of two classification property values, sorting by oldest update date, of first page of instances for a classification. repository-classification-search-performance-findEntitiesByClassification-none Repository performs search for neither of two classification property values, sorting by entity GUID , of first page of instances for a classification. repository-classification-search-performance-findEntitiesByClassification-one Repository performs search by a single classification property value, sorting by oldest creation date, of first page of instances for a classification. repository-classification-update-performance-findEntitiesByClassification Repository performs search for unordered first instancesPerType instances with a classification. repository-entity-declassification-performance-findEntitiesByClassification Repository performs search for unordered first instancesPerType instances with a classification. repository-entity-declassification-performance-findEntitiesByClassification-rc Repository performs search for unordered first instancesPerType reference copy classifications.","title":"Classification Search"},{"location":"guides/cts/profiles/classification-search/#classification-search-profile","text":"The performance of programmatically searching for existing entities based on their classification(s). The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for searching for entity instances based on classification: Method Description findEntitiesByClassification searches for entity instances based on specific classifications or properties of classifications with specific values Assertions ID Description repository-classification-search-performance-findEntitiesByClassification-all Repository performs search for both of two classification property values, sorting by most recent update date, of first page of instances for a classification. repository-classification-search-performance-findEntitiesByClassification-alone Repository performs search by classification alone (no properties), sorting by most recent creation date, of first page of instances for a classification. repository-classification-search-performance-findEntitiesByClassification-any Repository performs search for either of two classification property values, sorting by oldest update date, of first page of instances for a classification. repository-classification-search-performance-findEntitiesByClassification-none Repository performs search for neither of two classification property values, sorting by entity GUID , of first page of instances for a classification. repository-classification-search-performance-findEntitiesByClassification-one Repository performs search by a single classification property value, sorting by oldest creation date, of first page of instances for a classification. repository-classification-update-performance-findEntitiesByClassification Repository performs search for unordered first instancesPerType instances with a classification. repository-entity-declassification-performance-findEntitiesByClassification Repository performs search for unordered first instancesPerType instances with a classification. repository-entity-declassification-performance-findEntitiesByClassification-rc Repository performs search for unordered first instancesPerType reference copy classifications.","title":"Classification search profile"},{"location":"guides/cts/profiles/classification-update/","text":"Classification update profile \u00b6 The performance of programmatically updating existing classifications. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for updating classifications: Method Description updateEntityClassification changes one or more values of the properties on an existing classification Assertions ID Description repository-classification-update-performance-updateEntityClassification See (2) in detailed logic below. For every classification type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entities with that classification. (This uses findEntitiesByClassification with a condition on the classification's name only and its performance is recorded as part of the classification search profile.) For each of these entity instances, updateEntityClassification is called to update the existing property values of the classification, using a new generated set of properties. Example So, for example, if the technology under test supports 50 classification types, and the instancesPerType parameter is set to 100, then this profile will update 50 (types) x 100 (instances per type) = 5000 classifications. (And it will run findEntitiesByClassification 50 times.) The properties of each of these instances will be fully-populated with: Any string properties containing a value representative of the property name itself (and where unique, they will be made unique through appending a unique sequence) Any non-string properties will be randomly generated, in a simple attempt to represent data that is not entirely uniform Caveats Note the following caveats: Classification type definitions that have no properties will not be updated, since there are no properties to update.","title":"Classification Update"},{"location":"guides/cts/profiles/classification-update/#classification-update-profile","text":"The performance of programmatically updating existing classifications. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for updating classifications: Method Description updateEntityClassification changes one or more values of the properties on an existing classification Assertions ID Description repository-classification-update-performance-updateEntityClassification See (2) in detailed logic below. For every classification type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entities with that classification. (This uses findEntitiesByClassification with a condition on the classification's name only and its performance is recorded as part of the classification search profile.) For each of these entity instances, updateEntityClassification is called to update the existing property values of the classification, using a new generated set of properties. Example So, for example, if the technology under test supports 50 classification types, and the instancesPerType parameter is set to 100, then this profile will update 50 (types) x 100 (instances per type) = 5000 classifications. (And it will run findEntitiesByClassification 50 times.) The properties of each of these instances will be fully-populated with: Any string properties containing a value representative of the property name itself (and where unique, they will be made unique through appending a unique sequence) Any non-string properties will be randomly generated, in a simple attempt to represent data that is not entirely uniform Caveats Note the following caveats: Classification type definitions that have no properties will not be updated, since there are no properties to update.","title":"Classification update profile"},{"location":"guides/cts/profiles/entity-classification/","text":"Entity classification profile \u00b6 The performance of programmatically classifying existing entity instances with new classifications. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for classifying entity instances: Method Description classifyEntity adds a new classification to an existing entity instance, where the technology under test is the home repository for that classification saveEntityReferenceCopy adds (or updates) an entity instance with a classification, where the technology under test is not the home repository for that classification Assertions ID Description repository-entity-classification-performance-classifyEntity See (2) in detailed logic below. repository-entity-classification-performance-saveClassificationReferenceCopy See (4) in detailed logic below. For every classification type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entities of a type that is valid for the classification. (This uses findEntities and its performance is recorded as part of the entity search profile.) For each of these entity instances, classifyEntity is called to add a new classification of this type to that instance, using a generated set of properties. Searches for instancesPerType entities of a type that is valid for the classification, and where the entity is a reference copy. (This uses findEntitiesByProperty with a condition on metadataCollectionId and its performance is recorded as part of the entity search profile.) For each of these reference copy entity instances, saveClassificationReferenceCopy is called to add reference copy classification of this type to that instance, using a generated set of properties. Example So, for example, if the technology under test supports 50 classification types, and the instancesPerType parameter is set to 100, then this profile will create 50 (types) x 100 (instances per type) x 2 (home + reference copy methods) = 10 000 classifications. (And it will run findEntities 50 times, and findEntitiesByProperty 50 times.) The properties of each of these instances will be fully-populated with: Any string properties containing a value representative of the property name itself (and where unique, they will be made unique through appending a unique sequence) Any non-string properties will be randomly generated, in a simple attempt to represent data that is not entirely uniform","title":"Entity Classification"},{"location":"guides/cts/profiles/entity-classification/#entity-classification-profile","text":"The performance of programmatically classifying existing entity instances with new classifications. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for classifying entity instances: Method Description classifyEntity adds a new classification to an existing entity instance, where the technology under test is the home repository for that classification saveEntityReferenceCopy adds (or updates) an entity instance with a classification, where the technology under test is not the home repository for that classification Assertions ID Description repository-entity-classification-performance-classifyEntity See (2) in detailed logic below. repository-entity-classification-performance-saveClassificationReferenceCopy See (4) in detailed logic below. For every classification type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entities of a type that is valid for the classification. (This uses findEntities and its performance is recorded as part of the entity search profile.) For each of these entity instances, classifyEntity is called to add a new classification of this type to that instance, using a generated set of properties. Searches for instancesPerType entities of a type that is valid for the classification, and where the entity is a reference copy. (This uses findEntitiesByProperty with a condition on metadataCollectionId and its performance is recorded as part of the entity search profile.) For each of these reference copy entity instances, saveClassificationReferenceCopy is called to add reference copy classification of this type to that instance, using a generated set of properties. Example So, for example, if the technology under test supports 50 classification types, and the instancesPerType parameter is set to 100, then this profile will create 50 (types) x 100 (instances per type) x 2 (home + reference copy methods) = 10 000 classifications. (And it will run findEntities 50 times, and findEntitiesByProperty 50 times.) The properties of each of these instances will be fully-populated with: Any string properties containing a value representative of the property name itself (and where unique, they will be made unique through appending a unique sequence) Any non-string properties will be randomly generated, in a simple attempt to represent data that is not entirely uniform","title":"Entity classification profile"},{"location":"guides/cts/profiles/entity-creation/","text":"Entity creation profile \u00b6 The performance of programmatically creating new entity instances. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for creating entity instances: Method Description addEntity adds a new entity instance, where the technology under test is the home repository for that instance saveEntityReferenceCopy adds (or updates) an existing entity instance, where the technology under test is not the home repository for that instance Assertions ID Description repository-entity-creation-performance-addEntity Invocation of the addEntity method during the profile (see detailed logic below). repository-entity-creation-performance-saveEntityReferenceCopy Invocation of the saveEntityReferenceCopy method during the profile (see detailed logic below). For every entity type supported by the technology under test, this profile invokes each of these methods instancesPerType times. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will create 50 (types) x 100 (instances per type) x 2 (home + non-home methods) = 10 000 entity instances. The properties of each of these instances will be fully-populated with: Any string properties containing a value representative of the property name itself (and where unique, such as qualifiedName , they will be made unique through appending a unique sequence) Any non-string properties will be randomly generated, in a simple attempt to represent data that is not entirely uniform In addition, these tests will record into the environment profile the totalEntitiesCreated .","title":"Entity Creation"},{"location":"guides/cts/profiles/entity-creation/#entity-creation-profile","text":"The performance of programmatically creating new entity instances. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for creating entity instances: Method Description addEntity adds a new entity instance, where the technology under test is the home repository for that instance saveEntityReferenceCopy adds (or updates) an existing entity instance, where the technology under test is not the home repository for that instance Assertions ID Description repository-entity-creation-performance-addEntity Invocation of the addEntity method during the profile (see detailed logic below). repository-entity-creation-performance-saveEntityReferenceCopy Invocation of the saveEntityReferenceCopy method during the profile (see detailed logic below). For every entity type supported by the technology under test, this profile invokes each of these methods instancesPerType times. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will create 50 (types) x 100 (instances per type) x 2 (home + non-home methods) = 10 000 entity instances. The properties of each of these instances will be fully-populated with: Any string properties containing a value representative of the property name itself (and where unique, such as qualifiedName , they will be made unique through appending a unique sequence) Any non-string properties will be randomly generated, in a simple attempt to represent data that is not entirely uniform In addition, these tests will record into the environment profile the totalEntitiesCreated .","title":"Entity creation profile"},{"location":"guides/cts/profiles/entity-declassify/","text":"Entity declassification profile \u00b6 The performance of programmatically declassifying existing entity instances from existing classifications. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for declassifying entity instances: Method Description declassifyEntity removes a classification from an existing entity instance, where the technology under test is the home repository for that classification purgeClassificationReferenceCopy removes a classification from an existing entity instance, where the technology under test is not the home repository for that classification Assertions ID Description repository-entity-declassification-performance-declassifyEntity See (2) in detailed logic below. repository-entity-declassification-performance-purgeClassificationReferenceCopy See (4) in detailed logic below. For every classification type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entities homed in the technology under test with the classification. (This uses findEntitiesByClassification with a condition on metadataCollectionId and its performance is recorded as part of the classification search profile.) For each of these entity instances, declassifyEntity is called to remove the classification of this type from that instance. Searches for instancesPerType reference copy entities with the classification. (This uses findEntitiesByClassification with a condition on metadataCollectionId and its performance is recorded as part of the classification search profile.) For each of these reference copy entity instances, purgeClassificationReferenceCopy is called to remove the reference copy classification of this type from that instance. Example So, for example, if the technology under test supports 50 classification types, and the instancesPerType parameter is set to 100, then this profile will remove 50 (types) x 100 (instances per type) x 2 (home + reference copy methods) = 10 000 classifications. (And it will run findEntitiesByClassification 100 times.)","title":"Entity Declassify"},{"location":"guides/cts/profiles/entity-declassify/#entity-declassification-profile","text":"The performance of programmatically declassifying existing entity instances from existing classifications. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for declassifying entity instances: Method Description declassifyEntity removes a classification from an existing entity instance, where the technology under test is the home repository for that classification purgeClassificationReferenceCopy removes a classification from an existing entity instance, where the technology under test is not the home repository for that classification Assertions ID Description repository-entity-declassification-performance-declassifyEntity See (2) in detailed logic below. repository-entity-declassification-performance-purgeClassificationReferenceCopy See (4) in detailed logic below. For every classification type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entities homed in the technology under test with the classification. (This uses findEntitiesByClassification with a condition on metadataCollectionId and its performance is recorded as part of the classification search profile.) For each of these entity instances, declassifyEntity is called to remove the classification of this type from that instance. Searches for instancesPerType reference copy entities with the classification. (This uses findEntitiesByClassification with a condition on metadataCollectionId and its performance is recorded as part of the classification search profile.) For each of these reference copy entity instances, purgeClassificationReferenceCopy is called to remove the reference copy classification of this type from that instance. Example So, for example, if the technology under test supports 50 classification types, and the instancesPerType parameter is set to 100, then this profile will remove 50 (types) x 100 (instances per type) x 2 (home + reference copy methods) = 10 000 classifications. (And it will run findEntitiesByClassification 100 times.)","title":"Entity declassification profile"},{"location":"guides/cts/profiles/entity-delete/","text":"Entity delete profile \u00b6 The performance of programmatically soft-deleting an existing entity instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for deleting entity instances: Method Description deleteEntity soft-deletes the current version of an entity Assertions ID Description repository-entity-delete-performance-deleteEntity See (2) in detailed logic below. repository-entity-purge-performance-deleteEntity See detailed logic of entity purge profile. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entities of that type. (This uses findEntitiesByProperty with a condition on metadataCollectionId and its performance is recorded as part of the entity search profile.) For each of these entity instances, deleteEntity is called to soft-delete it. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will soft-delete 50 (types) x 100 (instances per type) = 5000 entities. (And it will run findEntitiesByProperty 50 times.)","title":"Entity Delete"},{"location":"guides/cts/profiles/entity-delete/#entity-delete-profile","text":"The performance of programmatically soft-deleting an existing entity instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for deleting entity instances: Method Description deleteEntity soft-deletes the current version of an entity Assertions ID Description repository-entity-delete-performance-deleteEntity See (2) in detailed logic below. repository-entity-purge-performance-deleteEntity See detailed logic of entity purge profile. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entities of that type. (This uses findEntitiesByProperty with a condition on metadataCollectionId and its performance is recorded as part of the entity search profile.) For each of these entity instances, deleteEntity is called to soft-delete it. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will soft-delete 50 (types) x 100 (instances per type) = 5000 entities. (And it will run findEntitiesByProperty 50 times.)","title":"Entity delete profile"},{"location":"guides/cts/profiles/entity-history-retrieval/","text":"Entity history retrieval profile \u00b6 The performance of programmatically retrieving the history of existing entity instances based on their ID. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines methods for retrieving the history of entity instances by their ID: Method Description getEntityDetail retrieves an entity instance's details at a given point in time, if the entity was known at that point in time, or throws an exception if not getEntityDetailHistory retrieves the full history of an entity instance's details Assertions ID Description repository-entity-history-retrieval-performance-getEntityDetail See (2) in detailed logic below. repository-entity-history-retrieval-performance-getEntityDetailHistory See (3) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entity GUIDs of that type. (This uses findEntities and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, getEntityDetail is then called with an asOfTime using the timestamp captured prior to the execution of the Entity Update profile, to retrieve its historical details from that point in time (prior to any updates). For each of these entity GUIDs, getEntityDetailHistory is then called to retrieve its full history (including current version and all historical versions of the instance). Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will retrieve 50 (types) x 100 (instances per type) x 2 (operations) = 10 000 entities. (And it will run findEntities 50 times.)","title":"Entity History Retrieval"},{"location":"guides/cts/profiles/entity-history-retrieval/#entity-history-retrieval-profile","text":"The performance of programmatically retrieving the history of existing entity instances based on their ID. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines methods for retrieving the history of entity instances by their ID: Method Description getEntityDetail retrieves an entity instance's details at a given point in time, if the entity was known at that point in time, or throws an exception if not getEntityDetailHistory retrieves the full history of an entity instance's details Assertions ID Description repository-entity-history-retrieval-performance-getEntityDetail See (2) in detailed logic below. repository-entity-history-retrieval-performance-getEntityDetailHistory See (3) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entity GUIDs of that type. (This uses findEntities and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, getEntityDetail is then called with an asOfTime using the timestamp captured prior to the execution of the Entity Update profile, to retrieve its historical details from that point in time (prior to any updates). For each of these entity GUIDs, getEntityDetailHistory is then called to retrieve its full history (including current version and all historical versions of the instance). Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will retrieve 50 (types) x 100 (instances per type) x 2 (operations) = 10 000 entities. (And it will run findEntities 50 times.)","title":"Entity history retrieval profile"},{"location":"guides/cts/profiles/entity-history-search/","text":"Entity history search profile \u00b6 The performance of programmatically searching for entity instances as they existed in the past. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for searching for entity instances based on their details in the past, by passing in an asOfTime parameter (captured during the execution of the entity update profile): Method Description findEntities arbitrarily complex combinations of search criteria including ranges, nested conditions, etc findEntitiesByProperty searches for entity instances based on specific properties with specific values findEntitiesByPropertyValue searches for entity instances based on text that matches any textual property Assertions ID Description repository-entity-search-history-performance-findEntities-all-p... Interrogates the technology under test for each entity type, by calling findEntities for that type GUID with an asOfTime using the timestamp captured prior to the execution of the entity update profile. These searches are performed with basic sorting of results, and all pages of results are retrieved (the p... portion indicates a specific page number): thereby also exercising the efficiency of the technology under test to both sort and cycle through pages of results. repository-entity-search-history-performance-findEntitiesByPropertyValue-exact Repository performs historical search with an exact text value, sorting by oldest creation time, of first page of instances of a type. repository-entity-search-history-performance-findEntitiesByPropertyValue-start Repository performs historical search with a text value 'starts-with', sorting by newest creation time, of first page of instances of a type. repository-entity-search-history-performance-findEntitiesByPropertyValue-contains Repository performs historical search with a text value 'contains', sorting by oldest update time, of first page of instances of a type. repository-entity-search-history-performance-findEntitiesByPropertyValue-end Repository performs historical search with a text value 'ends-with', sorting by newest update time, of first page of instances of a type. repository-entity-search-history-performance-findEntitiesByPropertyValue-regex Repository performs historical search with a regular expression text value, sorting by GUID , of first page of instances of a type. repository-entity-search-history-performance-findEntitiesByProperty-one Repository performs historical search by a single property value, sorting by that property, of first page of instances of a type. repository-entity-search-history-performance-findEntitiesByProperty-all Repository performs historical search by two property values, sorting by the second property, of first page of instances of a type. repository-entity-search-history-performance-findEntitiesByProperty-any Repository performs historical search by two property values, sorting by the second property, of first page of instances of a type. repository-entity-search-history-performance-findEntitiesByProperty-none Repository performs historical search by two property value, sorting by the first property, of first page of instances of a type. repository-entity-history-retrieval-performance-findEntities See the detailed logic of the entity retrieval profile. repository-graph-history-query-performance-findEntities See the detailed logic of the graph history query profile. Search variations When findEntitiesByProperty is run by the assertions starting with repository-entity-search-history-performance... , the tests prefer non-string properties (if any exist for the type) given that the findEntitiesByPropertyValue searches will already heavily exercise string-based queries.","title":"Entity History Search"},{"location":"guides/cts/profiles/entity-history-search/#entity-history-search-profile","text":"The performance of programmatically searching for entity instances as they existed in the past. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for searching for entity instances based on their details in the past, by passing in an asOfTime parameter (captured during the execution of the entity update profile): Method Description findEntities arbitrarily complex combinations of search criteria including ranges, nested conditions, etc findEntitiesByProperty searches for entity instances based on specific properties with specific values findEntitiesByPropertyValue searches for entity instances based on text that matches any textual property Assertions ID Description repository-entity-search-history-performance-findEntities-all-p... Interrogates the technology under test for each entity type, by calling findEntities for that type GUID with an asOfTime using the timestamp captured prior to the execution of the entity update profile. These searches are performed with basic sorting of results, and all pages of results are retrieved (the p... portion indicates a specific page number): thereby also exercising the efficiency of the technology under test to both sort and cycle through pages of results. repository-entity-search-history-performance-findEntitiesByPropertyValue-exact Repository performs historical search with an exact text value, sorting by oldest creation time, of first page of instances of a type. repository-entity-search-history-performance-findEntitiesByPropertyValue-start Repository performs historical search with a text value 'starts-with', sorting by newest creation time, of first page of instances of a type. repository-entity-search-history-performance-findEntitiesByPropertyValue-contains Repository performs historical search with a text value 'contains', sorting by oldest update time, of first page of instances of a type. repository-entity-search-history-performance-findEntitiesByPropertyValue-end Repository performs historical search with a text value 'ends-with', sorting by newest update time, of first page of instances of a type. repository-entity-search-history-performance-findEntitiesByPropertyValue-regex Repository performs historical search with a regular expression text value, sorting by GUID , of first page of instances of a type. repository-entity-search-history-performance-findEntitiesByProperty-one Repository performs historical search by a single property value, sorting by that property, of first page of instances of a type. repository-entity-search-history-performance-findEntitiesByProperty-all Repository performs historical search by two property values, sorting by the second property, of first page of instances of a type. repository-entity-search-history-performance-findEntitiesByProperty-any Repository performs historical search by two property values, sorting by the second property, of first page of instances of a type. repository-entity-search-history-performance-findEntitiesByProperty-none Repository performs historical search by two property value, sorting by the first property, of first page of instances of a type. repository-entity-history-retrieval-performance-findEntities See the detailed logic of the entity retrieval profile. repository-graph-history-query-performance-findEntities See the detailed logic of the graph history query profile. Search variations When findEntitiesByProperty is run by the assertions starting with repository-entity-search-history-performance... , the tests prefer non-string properties (if any exist for the type) given that the findEntitiesByPropertyValue searches will already heavily exercise string-based queries.","title":"Entity history search profile"},{"location":"guides/cts/profiles/entity-purge/","text":"Entity purge profile \u00b6 The performance of programmatically hard-deleting (irreversibly) existing entity instances. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for hard-deleting entity instances: Method Description purgeEntity completely removes an entity instance homed in the technology under test (including all of its history) purgeEntityReferenceCopy completely removes an entity instance that is homed somewhere other than the technology under test (including all of its history) Assertions ID Description repository-entity-purge-performance-purgeEntity See (3) in detailed logic below. repository-entity-purge-performance-purgeEntityReferenceCopy See (5) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entity GUIDs homed in the technology under test. (This uses findEntitiesByProperty with a condition on metadataCollectionId and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, deleteEntity is called to soft-delete the instance. (This is necessary before a hard-delete can be done against the instance and its performance is recorded as part of the entity delete profile.) For each of these entity GUIDs, a purgeEntity is then called to hard-delete the instance. Searches for instancesPerType reference copy entities. (This uses findEntitiesByProperty with a condition on metadataCollectionId and its performance is recorded as part of the entity search profile.) For each of these reference copy entity instances, purgeEntityReferenceCopy is called to remove the reference copy instance. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will remove 50 (types) x 100 (instances per type) x 3 (home + reference copy methods) = 15 000 entities. (And it will run findEntitiesByProperty 100 times.)","title":"Entity Purge"},{"location":"guides/cts/profiles/entity-purge/#entity-purge-profile","text":"The performance of programmatically hard-deleting (irreversibly) existing entity instances. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for hard-deleting entity instances: Method Description purgeEntity completely removes an entity instance homed in the technology under test (including all of its history) purgeEntityReferenceCopy completely removes an entity instance that is homed somewhere other than the technology under test (including all of its history) Assertions ID Description repository-entity-purge-performance-purgeEntity See (3) in detailed logic below. repository-entity-purge-performance-purgeEntityReferenceCopy See (5) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entity GUIDs homed in the technology under test. (This uses findEntitiesByProperty with a condition on metadataCollectionId and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, deleteEntity is called to soft-delete the instance. (This is necessary before a hard-delete can be done against the instance and its performance is recorded as part of the entity delete profile.) For each of these entity GUIDs, a purgeEntity is then called to hard-delete the instance. Searches for instancesPerType reference copy entities. (This uses findEntitiesByProperty with a condition on metadataCollectionId and its performance is recorded as part of the entity search profile.) For each of these reference copy entity instances, purgeEntityReferenceCopy is called to remove the reference copy instance. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will remove 50 (types) x 100 (instances per type) x 3 (home + reference copy methods) = 15 000 entities. (And it will run findEntitiesByProperty 100 times.)","title":"Entity purge profile"},{"location":"guides/cts/profiles/entity-re-home/","text":"Entity re-home profile \u00b6 The performance of programmatically changing the home repository of an existing entity instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for changing the home repository of entity instances: Method Description reHomeEntity changes the home repository of an entity Assertions ID Description repository-entity-re-home-performance-reHomeEntity See (2) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType reference copy entity GUIDs of that type. (This uses findEntitiesByProperty with a condition on metadataCollectionId and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, reHomeEntity is called to change the home repository to the technology under test's metadataCollectionId . Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will re-home 50 (types) x 100 (instances per type) = 5000 entities. (And it will run findEntitiesByProperty 50 times.)","title":"Entity Re-home"},{"location":"guides/cts/profiles/entity-re-home/#entity-re-home-profile","text":"The performance of programmatically changing the home repository of an existing entity instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for changing the home repository of entity instances: Method Description reHomeEntity changes the home repository of an entity Assertions ID Description repository-entity-re-home-performance-reHomeEntity See (2) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType reference copy entity GUIDs of that type. (This uses findEntitiesByProperty with a condition on metadataCollectionId and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, reHomeEntity is called to change the home repository to the technology under test's metadataCollectionId . Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will re-home 50 (types) x 100 (instances per type) = 5000 entities. (And it will run findEntitiesByProperty 50 times.)","title":"Entity re-home profile"},{"location":"guides/cts/profiles/entity-re-identify/","text":"Entity re-identify profile \u00b6 The performance of programmatically changing the GUID of an existing entity instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for changing the GUID of entity instances: Method Description reIdentifyEntity changes the GUID of an existing entity Assertions ID Description repository-entity-re-identify-performance-reIdentifyEntity See (2) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType homed entity GUIDs of that type. (This uses findEntitiesByProperty with a condition on metadataCollectionId and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, reIdentifyEntity is called to change the GUID of the entity to a new random GUID . Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will re-identify 50 (types) x 100 (instances per type) = 5000 entities. (And it will run findEntitiesByProperty 50 times.)","title":"Entity Re-identify"},{"location":"guides/cts/profiles/entity-re-identify/#entity-re-identify-profile","text":"The performance of programmatically changing the GUID of an existing entity instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for changing the GUID of entity instances: Method Description reIdentifyEntity changes the GUID of an existing entity Assertions ID Description repository-entity-re-identify-performance-reIdentifyEntity See (2) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType homed entity GUIDs of that type. (This uses findEntitiesByProperty with a condition on metadataCollectionId and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, reIdentifyEntity is called to change the GUID of the entity to a new random GUID . Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will re-identify 50 (types) x 100 (instances per type) = 5000 entities. (And it will run findEntitiesByProperty 50 times.)","title":"Entity re-identify profile"},{"location":"guides/cts/profiles/entity-restore/","text":"Entity restore profile \u00b6 The performance of programmatically reversing the latest soft-delete of an existing entity instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for reverting soft-deletes on entity instances: Method Description restoreEntity reverts the last soft-delete that was made to an entity Assertions ID Description repository-entity-restore-performance-restoreEntity See (2) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entity GUIDs of that type that have been soft-deleted. (This uses findEntitiesByProperty with a condition to limit to the status DELETED only and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, restoreEntity is called to revert the soft-delete and make the entity active again. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will re-activate 50 (types) x 100 (instances per type) = 5000 entities. (And it will run findEntitiesByProperty 50 times.)","title":"Entity Restore"},{"location":"guides/cts/profiles/entity-restore/#entity-restore-profile","text":"The performance of programmatically reversing the latest soft-delete of an existing entity instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for reverting soft-deletes on entity instances: Method Description restoreEntity reverts the last soft-delete that was made to an entity Assertions ID Description repository-entity-restore-performance-restoreEntity See (2) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entity GUIDs of that type that have been soft-deleted. (This uses findEntitiesByProperty with a condition to limit to the status DELETED only and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, restoreEntity is called to revert the soft-delete and make the entity active again. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will re-activate 50 (types) x 100 (instances per type) = 5000 entities. (And it will run findEntitiesByProperty 50 times.)","title":"Entity restore profile"},{"location":"guides/cts/profiles/entity-retrieval/","text":"Entity retrieval profile \u00b6 The performance of programmatically retrieving existing entity instances based on their ID. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines methods for retrieving entity instances by their ID: Method Description isEntityKnown retrieves an entity instance's details, if available, or returns null if not getEntitySummary retrieves a summary of an entity instance, without its properties getEntityDetail retrieves an entity instance's details, if available, or throws an exception if not Assertions ID Description repository-entity-retrieval-performance-isEntityKnown See (2) in detailed logic below. repository-entity-retrieval-performance-getEntitySummary See (3) in detailed logic below. repository-entity-retrieval-performance-getEntityDetail See (4) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entity GUIDs of that type. (This uses findEntities and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, isEntityKnown is called to retrieve it. For each of these entity GUIDs, getEntitySummary is then called to retrieve its summary. For each of these entity GUIDs, getEntityDetail is then called to retrieve its details. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will retrieve 50 (types) x 100 (instances per type) x 3 (operations) = 15 000 entities. (And it will run findEntities 50 times.) Caveats Note the following caveats: The same GUIDs are used for each retrieval, but each method is run against every GUID before moving onto the next method.","title":"Entity Retrieval"},{"location":"guides/cts/profiles/entity-retrieval/#entity-retrieval-profile","text":"The performance of programmatically retrieving existing entity instances based on their ID. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines methods for retrieving entity instances by their ID: Method Description isEntityKnown retrieves an entity instance's details, if available, or returns null if not getEntitySummary retrieves a summary of an entity instance, without its properties getEntityDetail retrieves an entity instance's details, if available, or throws an exception if not Assertions ID Description repository-entity-retrieval-performance-isEntityKnown See (2) in detailed logic below. repository-entity-retrieval-performance-getEntitySummary See (3) in detailed logic below. repository-entity-retrieval-performance-getEntityDetail See (4) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entity GUIDs of that type. (This uses findEntities and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, isEntityKnown is called to retrieve it. For each of these entity GUIDs, getEntitySummary is then called to retrieve its summary. For each of these entity GUIDs, getEntityDetail is then called to retrieve its details. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will retrieve 50 (types) x 100 (instances per type) x 3 (operations) = 15 000 entities. (And it will run findEntities 50 times.) Caveats Note the following caveats: The same GUIDs are used for each retrieval, but each method is run against every GUID before moving onto the next method.","title":"Entity retrieval profile"},{"location":"guides/cts/profiles/entity-retype/","text":"Entity retype profile \u00b6 The performance of programmatically changing the type of an existing entity instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for changing the type of entity instances: Method Description reTypeEntity changes the type of an existing entity Assertions ID Description repository-entity-retype-performance-reTypeEntity-toSubtype See (3) in detailed logic below. repository-entity-retype-performance-reTypeEntity-toSupertype See (4) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType homed entity GUIDs of that type. (This uses findEntitiesByProperty with a condition on metadataCollectionId and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, updateEntityProperties is called to remove all the entity's properties (so it can be easily retyped), and its performance is recorded as part of the entity update profile. For each of these entity GUIDs, reTypeEntity is called to change the type of the entity to one of its subtypes. For each of these entity GUIDs, reTypeEntity is then called to change the type of the entity back to its original type. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will retype 50 (types) x 100 (instances per type) x 2 (operations) = 10 000 entities at most. (And it will run findEntitiesByProperty 50 times.) Caveats Note the following caveats: Instances of a given type will only be retyped if that type has any subtypes: if it has no subtypes, then all retyping operations will be skipped for that type's instances.","title":"Entity Re-type"},{"location":"guides/cts/profiles/entity-retype/#entity-retype-profile","text":"The performance of programmatically changing the type of an existing entity instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for changing the type of entity instances: Method Description reTypeEntity changes the type of an existing entity Assertions ID Description repository-entity-retype-performance-reTypeEntity-toSubtype See (3) in detailed logic below. repository-entity-retype-performance-reTypeEntity-toSupertype See (4) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType homed entity GUIDs of that type. (This uses findEntitiesByProperty with a condition on metadataCollectionId and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, updateEntityProperties is called to remove all the entity's properties (so it can be easily retyped), and its performance is recorded as part of the entity update profile. For each of these entity GUIDs, reTypeEntity is called to change the type of the entity to one of its subtypes. For each of these entity GUIDs, reTypeEntity is then called to change the type of the entity back to its original type. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will retype 50 (types) x 100 (instances per type) x 2 (operations) = 10 000 entities at most. (And it will run findEntitiesByProperty 50 times.) Caveats Note the following caveats: Instances of a given type will only be retyped if that type has any subtypes: if it has no subtypes, then all retyping operations will be skipped for that type's instances.","title":"Entity retype profile"},{"location":"guides/cts/profiles/entity-search/","text":"Entity search profile \u00b6 The performance of programmatically searching for existing entity instances. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for searching for entity instances: Method Description findEntities arbitrarily complex combinations of search criteria including ranges, nested conditions, etc findEntitiesByProperty searches for entity instances based on specific properties with specific values findEntitiesByPropertyValue searches for entity instances based on text that matches any textual property Assertions ID Description repository-entity-search-performance-findEntities-all-p... Interrogates the technology under test in its entirety, to discover every existing entity instance of every type known to Egeria. The total count of these is tallied to report later under the environment profile. In this way, even for repositories that do not support write operations, we can still calculate some metrics about read performance (including search) while being able to understand that information in light of the volumes of metadata in the repository while the test was executed. These searches are also performed with basic sorting of results, and all pages of results are retrieved (the p... portion indicates a specific page number): thereby also exercising the efficiency of the technology under test to both sort and cycle through pages of results. repository-entity-search-performance-findEntitiesByPropertyValue-exact Repository performs search with an exact text value, sorting by oldest creation time, of first page of instances of a type. repository-entity-search-performance-findEntitiesByPropertyValue-start Repository performs search with a text value 'starts-with', sorting by newest creation time, of first page of instances of a type. repository-entity-search-performance-findEntitiesByPropertyValue-contains Repository performs search with a text value 'contains', sorting by oldest update time, of first page of instances of a type. repository-entity-search-performance-findEntitiesByPropertyValue-end Repository performs search with a text value 'ends-with', sorting by newest update time, of first page of instances of a type. repository-entity-search-performance-findEntitiesByPropertyValue-regex Repository performs search with a regular expression text value, sorting by GUID , of first page of instances of a type. repository-entity-search-performance-findEntitiesByProperty-one Repository performs search by a single property value, sorting by that property, of first page of instances of a type. repository-entity-search-performance-findEntitiesByProperty-all Repository performs search by two property values, sorting by the second property, of first page of instances of a type. repository-entity-search-performance-findEntitiesByProperty-any Repository performs search by two property values, sorting by the second property, of first page of instances of a type. repository-entity-search-performance-findEntitiesByProperty-none Repository performs search by two property value, sorting by the first property, of first page of instances of a type. repository-entity-classification-performance-findEntities See the detailed logic of the entity classification profile. repository-entity-classification-performance-findEntitiesByProperty See the detailed logic of the entity classification profile. repository-entity-delete-performance-findEntitiesByProperty See the detailed logic of the entity delete profile. repository-entity-purge-performance-findEntitiesByProperty See the detailed logic of the entity purge profile. repository-entity-purge-performance-findEntitiesByProperty-rc See the detailed logic of the entity purge profile. repository-entity-re-home-performance-findEntitiesByProperty See the detailed logic of the entity re-home profile. repository-entity-re-identify-performance-findEntitiesByProperty See the detailed logic of the entity re-identify profile. repository-entity-restore-performance-findEntitiesByProperty See the detailed logic of the entity restore profile. repository-entity-retrieval-performance-findEntities See the detailed logic of the entity retrieval profile. repository-entity-retype-performance-findEntitiesByProperty See the detailed logic of the entity retype profile. repository-entity-undo-performance-findEntities See the detailed logic of the entity undo profile. repository-entity-update-performance-findEntitiesByProperty See the detailed logic of the entity update profile. repository-graph-query-performance-findEntities See the detailed logic of the graph query profile. repository-relationship-creation-performance-findEntities See the detailed logic of the relationship creation profile. In addition, these tests will record into the Environment profile the totalEntitiesFound . Search variations When findEntitiesByProperty is run by the assertions starting with repository-entity-search-performance... , the tests prefer non-string properties (if any exist for the type) given that the findEntitiesByPropertyValue searches will already heavily exercise string-based queries. Note that the various other assertions (that do not start with repository-entity-search-performance... ) will search on various other properties than those listed: in particular, including header properties like metadataCollectionId , version , and others.","title":"Entity Search"},{"location":"guides/cts/profiles/entity-search/#entity-search-profile","text":"The performance of programmatically searching for existing entity instances. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for searching for entity instances: Method Description findEntities arbitrarily complex combinations of search criteria including ranges, nested conditions, etc findEntitiesByProperty searches for entity instances based on specific properties with specific values findEntitiesByPropertyValue searches for entity instances based on text that matches any textual property Assertions ID Description repository-entity-search-performance-findEntities-all-p... Interrogates the technology under test in its entirety, to discover every existing entity instance of every type known to Egeria. The total count of these is tallied to report later under the environment profile. In this way, even for repositories that do not support write operations, we can still calculate some metrics about read performance (including search) while being able to understand that information in light of the volumes of metadata in the repository while the test was executed. These searches are also performed with basic sorting of results, and all pages of results are retrieved (the p... portion indicates a specific page number): thereby also exercising the efficiency of the technology under test to both sort and cycle through pages of results. repository-entity-search-performance-findEntitiesByPropertyValue-exact Repository performs search with an exact text value, sorting by oldest creation time, of first page of instances of a type. repository-entity-search-performance-findEntitiesByPropertyValue-start Repository performs search with a text value 'starts-with', sorting by newest creation time, of first page of instances of a type. repository-entity-search-performance-findEntitiesByPropertyValue-contains Repository performs search with a text value 'contains', sorting by oldest update time, of first page of instances of a type. repository-entity-search-performance-findEntitiesByPropertyValue-end Repository performs search with a text value 'ends-with', sorting by newest update time, of first page of instances of a type. repository-entity-search-performance-findEntitiesByPropertyValue-regex Repository performs search with a regular expression text value, sorting by GUID , of first page of instances of a type. repository-entity-search-performance-findEntitiesByProperty-one Repository performs search by a single property value, sorting by that property, of first page of instances of a type. repository-entity-search-performance-findEntitiesByProperty-all Repository performs search by two property values, sorting by the second property, of first page of instances of a type. repository-entity-search-performance-findEntitiesByProperty-any Repository performs search by two property values, sorting by the second property, of first page of instances of a type. repository-entity-search-performance-findEntitiesByProperty-none Repository performs search by two property value, sorting by the first property, of first page of instances of a type. repository-entity-classification-performance-findEntities See the detailed logic of the entity classification profile. repository-entity-classification-performance-findEntitiesByProperty See the detailed logic of the entity classification profile. repository-entity-delete-performance-findEntitiesByProperty See the detailed logic of the entity delete profile. repository-entity-purge-performance-findEntitiesByProperty See the detailed logic of the entity purge profile. repository-entity-purge-performance-findEntitiesByProperty-rc See the detailed logic of the entity purge profile. repository-entity-re-home-performance-findEntitiesByProperty See the detailed logic of the entity re-home profile. repository-entity-re-identify-performance-findEntitiesByProperty See the detailed logic of the entity re-identify profile. repository-entity-restore-performance-findEntitiesByProperty See the detailed logic of the entity restore profile. repository-entity-retrieval-performance-findEntities See the detailed logic of the entity retrieval profile. repository-entity-retype-performance-findEntitiesByProperty See the detailed logic of the entity retype profile. repository-entity-undo-performance-findEntities See the detailed logic of the entity undo profile. repository-entity-update-performance-findEntitiesByProperty See the detailed logic of the entity update profile. repository-graph-query-performance-findEntities See the detailed logic of the graph query profile. repository-relationship-creation-performance-findEntities See the detailed logic of the relationship creation profile. In addition, these tests will record into the Environment profile the totalEntitiesFound . Search variations When findEntitiesByProperty is run by the assertions starting with repository-entity-search-performance... , the tests prefer non-string properties (if any exist for the type) given that the findEntitiesByPropertyValue searches will already heavily exercise string-based queries. Note that the various other assertions (that do not start with repository-entity-search-performance... ) will search on various other properties than those listed: in particular, including header properties like metadataCollectionId , version , and others.","title":"Entity search profile"},{"location":"guides/cts/profiles/entity-undo/","text":"Entity undo profile \u00b6 The performance of programmatically reversing the latest update to an existing entity instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for reverting updates on entity instances: Method Description undoEntityUpdate reverts the last update that was made to an entity Assertions ID Description repository-entity-undo-performance-undoEntityUpdate See (2) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entities of that type that have at least one change. (This uses findEntities with a condition on both metadataCollectionId and version being greater than 1 , and its performance is recorded as part of the entity search profile.) For each of these entity instances, undoEntityUpdate is called to revert the last change. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will update 50 (types) x 100 (instances per type) = 5000 entities. (And it will run findEntities 50 times.)","title":"Entity Undo"},{"location":"guides/cts/profiles/entity-undo/#entity-undo-profile","text":"The performance of programmatically reversing the latest update to an existing entity instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for reverting updates on entity instances: Method Description undoEntityUpdate reverts the last update that was made to an entity Assertions ID Description repository-entity-undo-performance-undoEntityUpdate See (2) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entities of that type that have at least one change. (This uses findEntities with a condition on both metadataCollectionId and version being greater than 1 , and its performance is recorded as part of the entity search profile.) For each of these entity instances, undoEntityUpdate is called to revert the last change. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will update 50 (types) x 100 (instances per type) = 5000 entities. (And it will run findEntities 50 times.)","title":"Entity undo profile"},{"location":"guides/cts/profiles/entity-update/","text":"Entity update profile \u00b6 The performance of programmatically updating existing entity instances. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for updating entity instances: Method Description updateEntityProperties changes one or more values of the properties on an existing entity updateEntityStatus changes the status of an existing entity Assertions ID Description repository-entity-update-performance-updateEntityProperties See (2) in detailed logic below. repository-entity-retype-performance-updateEntityProperties-remove See the detailed logic of the entity retype profile. Captures timestamp for historical metadata profiles Prior to this profile running, a timestamp is captured by the performance workbench to denote a specific date and time prior to any updates having been made to the entities. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entities of that type. (This uses findEntitiesByProperty with a condition on metadataCollectionId and its performance is recorded as part of the entity search profile.) For each of these entity instances, updateEntityProperties is called to update the existing property values of the entity, using a new generated set of properties. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will update 50 (types) x 100 (instances per type) = 5000 entities. (And it will run findEntitiesByProperty 50 times.) The properties of each of these instances will be fully-populated with: Any string properties containing a value representative of the property name itself (and where unique, they will be made unique through appending a unique sequence) Any non-string properties will be randomly generated, in a simple attempt to represent data that is not entirely uniform Caveats Note the following caveats: updateEntityStatus is currently not tested (there are very few types that support any status that can be updated outside of a delete or restore operation). Entity type definitions that have no properties will not be updated, since there are no properties to update.","title":"Entity Update"},{"location":"guides/cts/profiles/entity-update/#entity-update-profile","text":"The performance of programmatically updating existing entity instances. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for updating entity instances: Method Description updateEntityProperties changes one or more values of the properties on an existing entity updateEntityStatus changes the status of an existing entity Assertions ID Description repository-entity-update-performance-updateEntityProperties See (2) in detailed logic below. repository-entity-retype-performance-updateEntityProperties-remove See the detailed logic of the entity retype profile. Captures timestamp for historical metadata profiles Prior to this profile running, a timestamp is captured by the performance workbench to denote a specific date and time prior to any updates having been made to the entities. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entities of that type. (This uses findEntitiesByProperty with a condition on metadataCollectionId and its performance is recorded as part of the entity search profile.) For each of these entity instances, updateEntityProperties is called to update the existing property values of the entity, using a new generated set of properties. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will update 50 (types) x 100 (instances per type) = 5000 entities. (And it will run findEntitiesByProperty 50 times.) The properties of each of these instances will be fully-populated with: Any string properties containing a value representative of the property name itself (and where unique, they will be made unique through appending a unique sequence) Any non-string properties will be randomly generated, in a simple attempt to represent data that is not entirely uniform Caveats Note the following caveats: updateEntityStatus is currently not tested (there are very few types that support any status that can be updated outside of a delete or restore operation). Entity type definitions that have no properties will not be updated, since there are no properties to update.","title":"Entity update profile"},{"location":"guides/cts/profiles/environment/","text":"Environment profile \u00b6 Does not actually run any tests, but rather gathers statistics about the environment in which the other performance tests were executed. This profile currently collects the following information about the technology under test in which the performance tests were executed: Discovered properties Test configuration Property Description instancesPerType the number of instances the test should attempt to create, per type definition maxSearchResults the number of results per page to retrieve for search queries waitBetweenScenarios the time (in seconds) to wait between write and read phases of the performance tests Egeria statistics Property Description totalEntitiesCreated the total number of entity instances that were created by the performance tests totalEntitiesFound the total number of entity instances that were found in the environment (created + pre-existing) totalRelationshipsCreated the total number of relationship instances that were created by the performance tests totalRelationshipsFound the total number of relationship instances that were found in the environment (created + pre-existing) Runtime environment information Caveat Currently these are captured only for the OMAG Server Platform that is actually running the CTS suite itself. If the technology under test is running under this same OMAG Server Platform, then these will still be useful; however, if the technology under test is running on a separate OMAG Server Platform or via a remote system, these will likely be of very limited value. Property Description operatingSystem details about the operating system (name and version) operatingSystemArchitecture the operating system's architecture operatingSystemAvailableProcessors the number of processors available, according to the operating system (likely to include hyperthreads) operatingSystemLoadAverage current load average from the operating system heapUsage current heap memory usage out of total heap memory available (in bytes) nonHeapUsage current non-heap memory usage out of total non-heap memory available (in bytes) jvmSpec details about the Java virtual machine specification jvmImplementation details about the Java virtual machine implementation","title":"Environment"},{"location":"guides/cts/profiles/environment/#environment-profile","text":"Does not actually run any tests, but rather gathers statistics about the environment in which the other performance tests were executed. This profile currently collects the following information about the technology under test in which the performance tests were executed: Discovered properties Test configuration Property Description instancesPerType the number of instances the test should attempt to create, per type definition maxSearchResults the number of results per page to retrieve for search queries waitBetweenScenarios the time (in seconds) to wait between write and read phases of the performance tests Egeria statistics Property Description totalEntitiesCreated the total number of entity instances that were created by the performance tests totalEntitiesFound the total number of entity instances that were found in the environment (created + pre-existing) totalRelationshipsCreated the total number of relationship instances that were created by the performance tests totalRelationshipsFound the total number of relationship instances that were found in the environment (created + pre-existing) Runtime environment information Caveat Currently these are captured only for the OMAG Server Platform that is actually running the CTS suite itself. If the technology under test is running under this same OMAG Server Platform, then these will still be useful; however, if the technology under test is running on a separate OMAG Server Platform or via a remote system, these will likely be of very limited value. Property Description operatingSystem details about the operating system (name and version) operatingSystemArchitecture the operating system's architecture operatingSystemAvailableProcessors the number of processors available, according to the operating system (likely to include hyperthreads) operatingSystemLoadAverage current load average from the operating system heapUsage current heap memory usage out of total heap memory available (in bytes) nonHeapUsage current non-heap memory usage out of total non-heap memory available (in bytes) jvmSpec details about the Java virtual machine specification jvmImplementation details about the Java virtual machine implementation","title":"Environment profile"},{"location":"guides/cts/profiles/graph-history-queries/","text":"Graph history queries profile \u00b6 The performance of programmatically retrieving inter-related instances across potentially multiple degrees of separation, for a given point in time. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines methods for retrieving inter-related metadata instances across several degrees at a given point in time in the past, by passing in an asOfTime parameter (captured during the execution of the entity update profile): Method Description getRelationshipsForEntity retrieves the first-degree relationships linked to an entity getEntityNeighborhood retrieves the relationships and entities linked to an entity, up to a specified number of degrees of separation getRelatedEntities retrieves the entities linked to an entity, both directly and indirectly getLinkingEntities retrieves the relationships and entities that exist between (ultimately connecting) two entities Assertions ID Description repository-graph-history-query-performance-getRelationshipsForEntity See (2) in detailed logic below. repository-graph-history-query-performance-getEntityNeighborhood-... See (3) in detailed logic below. repository-graph-history-query-performance-getRelatedEntities See (4) in detailed logic below. repository-graph-history-query-performance-getLinkingEntities See (5) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entity GUIDs of that type. (This uses findEntities and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, getRelationshipsForEntity is called with an asOfTime using the timestamp captured prior to the execution of the entity update profile, to retrieve first-degree relationships. For each of these entity GUIDs, getEntityNeighborhood is then called with an asOfTime using the timestamp captured prior to the execution of the entity update profile, 3 times to retrieve that entity's 1st, 2nd and 3rd degree relationships. For each of these entity GUIDs, getRelatedEntities is then called with an asOfTime using the timestamp captured prior to the execution of the entity update profile, to retrieve all the entities that are either directly or indirectly related to this entity. These results, per entity, are stored in a Map for the next step. For each of these entity GUIDs, getLinkingEntities is then called with an asOfTime using the timestamp captured prior to the execution of the entity update profile, up to 3 times: using the first related entity from the test above, the last related entity from the test above, and an entity from the middle of the list of the results from the test above. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will run 50 (types) x 100 (instances per type) x 8 (maximum operation variations) = 40 000 (at most) queries. (And it will run the initial findEntities 50 times.) Caveats Note the following caveats: The same GUIDs are used for each retrieval, but each method is run against every GUID before moving onto the next method.","title":"Graph History Queries"},{"location":"guides/cts/profiles/graph-history-queries/#graph-history-queries-profile","text":"The performance of programmatically retrieving inter-related instances across potentially multiple degrees of separation, for a given point in time. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines methods for retrieving inter-related metadata instances across several degrees at a given point in time in the past, by passing in an asOfTime parameter (captured during the execution of the entity update profile): Method Description getRelationshipsForEntity retrieves the first-degree relationships linked to an entity getEntityNeighborhood retrieves the relationships and entities linked to an entity, up to a specified number of degrees of separation getRelatedEntities retrieves the entities linked to an entity, both directly and indirectly getLinkingEntities retrieves the relationships and entities that exist between (ultimately connecting) two entities Assertions ID Description repository-graph-history-query-performance-getRelationshipsForEntity See (2) in detailed logic below. repository-graph-history-query-performance-getEntityNeighborhood-... See (3) in detailed logic below. repository-graph-history-query-performance-getRelatedEntities See (4) in detailed logic below. repository-graph-history-query-performance-getLinkingEntities See (5) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entity GUIDs of that type. (This uses findEntities and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, getRelationshipsForEntity is called with an asOfTime using the timestamp captured prior to the execution of the entity update profile, to retrieve first-degree relationships. For each of these entity GUIDs, getEntityNeighborhood is then called with an asOfTime using the timestamp captured prior to the execution of the entity update profile, 3 times to retrieve that entity's 1st, 2nd and 3rd degree relationships. For each of these entity GUIDs, getRelatedEntities is then called with an asOfTime using the timestamp captured prior to the execution of the entity update profile, to retrieve all the entities that are either directly or indirectly related to this entity. These results, per entity, are stored in a Map for the next step. For each of these entity GUIDs, getLinkingEntities is then called with an asOfTime using the timestamp captured prior to the execution of the entity update profile, up to 3 times: using the first related entity from the test above, the last related entity from the test above, and an entity from the middle of the list of the results from the test above. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will run 50 (types) x 100 (instances per type) x 8 (maximum operation variations) = 40 000 (at most) queries. (And it will run the initial findEntities 50 times.) Caveats Note the following caveats: The same GUIDs are used for each retrieval, but each method is run against every GUID before moving onto the next method.","title":"Graph history queries profile"},{"location":"guides/cts/profiles/graph-queries/","text":"Graph queries profile \u00b6 The performance of programmatically retrieving inter-related instances across potentially multiple degrees of separation. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines methods for retrieving inter-related metadata instances across several degrees: Method Description getRelationshipsForEntity retrieves the first-degree relationships linked to an entity getEntityNeighborhood retrieves the relationships and entities linked to an entity, up to a specified number of degrees of separation getRelatedEntities retrieves the entities linked to an entity, both directly and indirectly getLinkingEntities retrieves the relationships and entities that exist between (ultimately connecting) two entities Assertions ID Description repository-graph-query-performance-getRelationshipsForEntity See (2) in detailed logic below. repository-graph-query-performance-getEntityNeighborhood-... See (3) in detailed logic below. repository-graph-query-performance-getRelatedEntities See (4) in detailed logic below. repository-graph-query-performance-getLinkingEntities See (5) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entity GUIDs of that type. (This uses findEntities and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, getRelationshipsForEntity is called to retrieve first-degree relationships. For each of these entity GUIDs, getEntityNeighborhood is then called 3 times to retrieve that entity's 1st, 2nd and 3rd degree relationships. For each of these entity GUIDs, getRelatedEntities is then called to retrieve all the entities that are either directly or indirectly related to this entity. These results, per entity, are stored in a Map for the next step. For each of these entity GUIDs, getLinkingEntities is then called up to 3 times: using the first related entity from the test above, the last related entity from the test above, and an entity from the middle of the list of the results from the test above. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will run 50 (types) x 100 (instances per type) x 8 (maximum operation variations) = 40 000 (at most) queries. (And it will run the initial findEntities 50 times.) Caveats Note the following caveats: The same GUIDs are used for each retrieval, but each method is run against every GUID before moving onto the next method.","title":"Graph Queries"},{"location":"guides/cts/profiles/graph-queries/#graph-queries-profile","text":"The performance of programmatically retrieving inter-related instances across potentially multiple degrees of separation. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines methods for retrieving inter-related metadata instances across several degrees: Method Description getRelationshipsForEntity retrieves the first-degree relationships linked to an entity getEntityNeighborhood retrieves the relationships and entities linked to an entity, up to a specified number of degrees of separation getRelatedEntities retrieves the entities linked to an entity, both directly and indirectly getLinkingEntities retrieves the relationships and entities that exist between (ultimately connecting) two entities Assertions ID Description repository-graph-query-performance-getRelationshipsForEntity See (2) in detailed logic below. repository-graph-query-performance-getEntityNeighborhood-... See (3) in detailed logic below. repository-graph-query-performance-getRelatedEntities See (4) in detailed logic below. repository-graph-query-performance-getLinkingEntities See (5) in detailed logic below. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entity GUIDs of that type. (This uses findEntities and its performance is recorded as part of the entity search profile.) For each of these entity GUIDs, getRelationshipsForEntity is called to retrieve first-degree relationships. For each of these entity GUIDs, getEntityNeighborhood is then called 3 times to retrieve that entity's 1st, 2nd and 3rd degree relationships. For each of these entity GUIDs, getRelatedEntities is then called to retrieve all the entities that are either directly or indirectly related to this entity. These results, per entity, are stored in a Map for the next step. For each of these entity GUIDs, getLinkingEntities is then called up to 3 times: using the first related entity from the test above, the last related entity from the test above, and an entity from the middle of the list of the results from the test above. Example So, for example, if the technology under test supports 50 entity types, and the instancesPerType parameter is set to 100, then this profile will run 50 (types) x 100 (instances per type) x 8 (maximum operation variations) = 40 000 (at most) queries. (And it will run the initial findEntities 50 times.) Caveats Note the following caveats: The same GUIDs are used for each retrieval, but each method is run against every GUID before moving onto the next method.","title":"Graph queries profile"},{"location":"guides/cts/profiles/platform-origin/","text":"Platform origin profile \u00b6 The platform origin profile requires that an Open Metadata and Governance ( OMAG ) Server Platform is able to report its origin. Origin identifier \u00b6 The open metadata standards defines a getServerOrigin operation ( ../open-metadata/admin-services/users/{{adminUserId}}/server-platform-origin ). This operation returns an origin descriptor. Typically, this identifies the vendor, product name and version of the server. It is useful to administrators to be able to validate the platform that is running. This is the test case that validates whether the operation is present. Platform origin test case \u00b6 Validate the retrieval of the origin descriptor from the server platform that hosts one or more open metadata repositories and/or servers. This test uses the getServerPlatformOrigin operation ( ../open-metadata/platform-services/users/{{adminUserId}}/server-platform-origin ) operation to test that the platform knows its origin descriptor. Assertions ID Description platform-origin-01 The origin descriptor has successfully been retrieved from the server platform. If this assertion fails, check that the server platform is started and the open metadata services are activated. Discovered properties ID Description Platform origin id Descriptive name for the server platform implementation. Example output for platform origin test case { \"class\" : \"OpenMetadataTestCaseResult\" , \"testCaseId\" : \"platform-origin\" , \"testCaseName\" : \"Platform origin test case\" , \"testCaseDescriptionURL\" : \"https://egeria.odpi.org/open-metadata-conformance-suite/docs/platform-workbench/platfrom-origin-test-case.md\" , \"assertionMessage\" : \"Platform origin descriptor successfully retrieved\" , \"successfulAssertions\" : [ \"Origin descriptor retrieved from platform.\" ], \"unsuccessfulAssertions\" : [ ], \"discoveredProperties\" : { \"Repository origin id\" : \"Egeria OMAG Server Platform (version 3.1-SNAPSHOT)\" } }","title":"Platform Origin"},{"location":"guides/cts/profiles/platform-origin/#platform-origin-profile","text":"The platform origin profile requires that an Open Metadata and Governance ( OMAG ) Server Platform is able to report its origin.","title":"Platform origin profile"},{"location":"guides/cts/profiles/platform-origin/#origin-identifier","text":"The open metadata standards defines a getServerOrigin operation ( ../open-metadata/admin-services/users/{{adminUserId}}/server-platform-origin ). This operation returns an origin descriptor. Typically, this identifies the vendor, product name and version of the server. It is useful to administrators to be able to validate the platform that is running. This is the test case that validates whether the operation is present.","title":"Origin identifier"},{"location":"guides/cts/profiles/platform-origin/#platform-origin-test-case","text":"Validate the retrieval of the origin descriptor from the server platform that hosts one or more open metadata repositories and/or servers. This test uses the getServerPlatformOrigin operation ( ../open-metadata/platform-services/users/{{adminUserId}}/server-platform-origin ) operation to test that the platform knows its origin descriptor. Assertions ID Description platform-origin-01 The origin descriptor has successfully been retrieved from the server platform. If this assertion fails, check that the server platform is started and the open metadata services are activated. Discovered properties ID Description Platform origin id Descriptive name for the server platform implementation. Example output for platform origin test case { \"class\" : \"OpenMetadataTestCaseResult\" , \"testCaseId\" : \"platform-origin\" , \"testCaseName\" : \"Platform origin test case\" , \"testCaseDescriptionURL\" : \"https://egeria.odpi.org/open-metadata-conformance-suite/docs/platform-workbench/platfrom-origin-test-case.md\" , \"assertionMessage\" : \"Platform origin descriptor successfully retrieved\" , \"successfulAssertions\" : [ \"Origin descriptor retrieved from platform.\" ], \"unsuccessfulAssertions\" : [ ], \"discoveredProperties\" : { \"Repository origin id\" : \"Egeria OMAG Server Platform (version 3.1-SNAPSHOT)\" } }","title":"Platform origin test case"},{"location":"guides/cts/profiles/relationship-creation/","text":"Relationship creation profile \u00b6 The performance of programmatically creating new relationship instances. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for creating relationship instances: Method Description addRelationship adds a new relationship instance, where the technology under test is the home repository for that instance saveRelationshipReferenceCopy adds (or updates) an existing relationship instance, where the technology under test is not the home repository for that instance Assertions ID Description repository-relationship-creation-performance-addRelationship Invocation of the addRelationship method during the profile (see detailed logic below). repository-relationship-creation-performance-saveRelationshipReferenceCopy Invocation of the saveRelationshipReferenceCopy method during the profile (see detailed logic below). For every relationship type supported by the technology under test, this profile invokes each of these methods instancesPerType times. Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will create 50 (types) x 100 (instances per type) x 2 (home + non-home methods) = 10 000 relationship instances. The properties of each of these instances will be fully-populated with: Any string properties containing a value representative of the property name itself (and where unique, they will be made unique through appending a unique sequence) Any non-string properties will be randomly generated, in a simple attempt to represent data that is not entirely uniform In addition, these tests will record into the Environment profile the totalRelationshipsCreated .","title":"Relationship Creation"},{"location":"guides/cts/profiles/relationship-creation/#relationship-creation-profile","text":"The performance of programmatically creating new relationship instances. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for creating relationship instances: Method Description addRelationship adds a new relationship instance, where the technology under test is the home repository for that instance saveRelationshipReferenceCopy adds (or updates) an existing relationship instance, where the technology under test is not the home repository for that instance Assertions ID Description repository-relationship-creation-performance-addRelationship Invocation of the addRelationship method during the profile (see detailed logic below). repository-relationship-creation-performance-saveRelationshipReferenceCopy Invocation of the saveRelationshipReferenceCopy method during the profile (see detailed logic below). For every relationship type supported by the technology under test, this profile invokes each of these methods instancesPerType times. Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will create 50 (types) x 100 (instances per type) x 2 (home + non-home methods) = 10 000 relationship instances. The properties of each of these instances will be fully-populated with: Any string properties containing a value representative of the property name itself (and where unique, they will be made unique through appending a unique sequence) Any non-string properties will be randomly generated, in a simple attempt to represent data that is not entirely uniform In addition, these tests will record into the Environment profile the totalRelationshipsCreated .","title":"Relationship creation profile"},{"location":"guides/cts/profiles/relationship-delete/","text":"Relationship delete profile \u00b6 The performance of programmatically soft-deleting an existing relationship instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for deleting relationship instances: Method Description deleteRelationship soft-deletes the current version of a relationship Assertions ID Description repository-relationship-delete-performance-deleteRelationship See (2) in detailed logic below. repository-relationship-purge-performance-deleteRelationship See detailed logic of relationship purge profile. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType relationships of that type. (This uses findRelationshipsByProperty with a condition on metadataCollectionId and its performance is recorded as part of the relationship search profile.) For each of these relationship instances, deleteRelationship is called to soft-delete it. Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will soft-delete 50 (types) x 100 (instances per type) = 5000 relationships. (And it will run findRelationshipsByProperty 50 times.)","title":"Relationship Delete"},{"location":"guides/cts/profiles/relationship-delete/#relationship-delete-profile","text":"The performance of programmatically soft-deleting an existing relationship instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for deleting relationship instances: Method Description deleteRelationship soft-deletes the current version of a relationship Assertions ID Description repository-relationship-delete-performance-deleteRelationship See (2) in detailed logic below. repository-relationship-purge-performance-deleteRelationship See detailed logic of relationship purge profile. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType relationships of that type. (This uses findRelationshipsByProperty with a condition on metadataCollectionId and its performance is recorded as part of the relationship search profile.) For each of these relationship instances, deleteRelationship is called to soft-delete it. Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will soft-delete 50 (types) x 100 (instances per type) = 5000 relationships. (And it will run findRelationshipsByProperty 50 times.)","title":"Relationship delete profile"},{"location":"guides/cts/profiles/relationship-history-retrieval/","text":"Relationship history retrieval profile \u00b6 The performance of programmatically retrieving the history of existing relationship instances based on their ID. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines methods for retrieving the history of relationship instances by their ID: Method Description getRelationship retrieves a relationship instance's details at a given point in time, if the relationship was known at that point in time, or throws an exception if not getRelationshipHistory retrieves the full history of a relationship instance's details Assertions ID Description repository-relationship-history-retrieval-performance-getRelationship See (2) in detailed logic below. repository-relationship-history-retrieval-performance-getRelationshipHistory See (3) in detailed logic below. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entity GUIDs of that type. (This uses findRelationships and its performance is recorded as part of the relationship search profile.) For each of these relationship GUIDs, getRelationship is then called with an asOfTime using the timestamp captured prior to the execution of the Relationship Update profile, to retrieve its historical details from that point in time (prior to any updates). For each of these relationship GUIDs, getRelationshipHistory is then called to retrieve its full history (including current version and all historical versions of the instance). Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will retrieve 50 (types) x 100 (instances per type) x 2 (operations) = 10 000 relationships. (And it will run findRelationships 50 times.)","title":"Relationship History Retrieval"},{"location":"guides/cts/profiles/relationship-history-retrieval/#relationship-history-retrieval-profile","text":"The performance of programmatically retrieving the history of existing relationship instances based on their ID. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines methods for retrieving the history of relationship instances by their ID: Method Description getRelationship retrieves a relationship instance's details at a given point in time, if the relationship was known at that point in time, or throws an exception if not getRelationshipHistory retrieves the full history of a relationship instance's details Assertions ID Description repository-relationship-history-retrieval-performance-getRelationship See (2) in detailed logic below. repository-relationship-history-retrieval-performance-getRelationshipHistory See (3) in detailed logic below. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType entity GUIDs of that type. (This uses findRelationships and its performance is recorded as part of the relationship search profile.) For each of these relationship GUIDs, getRelationship is then called with an asOfTime using the timestamp captured prior to the execution of the Relationship Update profile, to retrieve its historical details from that point in time (prior to any updates). For each of these relationship GUIDs, getRelationshipHistory is then called to retrieve its full history (including current version and all historical versions of the instance). Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will retrieve 50 (types) x 100 (instances per type) x 2 (operations) = 10 000 relationships. (And it will run findRelationships 50 times.)","title":"Relationship history retrieval profile"},{"location":"guides/cts/profiles/relationship-history-search/","text":"Relationship history search profile \u00b6 The performance of programmatically searching for relationship instances as they existed in the past. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for searching for relationship instances based on their details in the past, by passing in an asOfTime parameter (captured during the execution of the relationship update profile): Method Description findRelationships arbitrarily complex combinations of search criteria including ranges, nested conditions, etc findRelationshipsByProperty searches for relationship instances based on specific properties with specific values findRelationshipsByPropertyValue searches for relationship instances based on text that matches any textual property Assertions ID Description repository-relationship-search-history-performance-findRelationships-all-p... Interrogates the technology under test for each relationship type, by calling findRelationships for that type GUID with an asOfTime using the timestamp captured prior to the execution of the relationship update profile. These tests are performed with basic sorting of results, and all pages of results are retrieved (the p... portion indicates a specific page number): thereby also exercising the efficiency of the technology under test to both sort and cycle through pages of results. repository-relationship-search-history-performance-findRelationshipsByPropertyValue-exact Repository performs historical search with an exact text value, sorting by oldest creation time, of first page of instances of a type. repository-relationship-search-history-performance-findRelationshipsByPropertyValue-start Repository performs historical search with a text value 'starts-with', sorting by newest creation time, of first page of instances of a type. repository-relationship-search-history-performance-findRelationshipsByPropertyValue-contains Repository performs historical search with a text value 'contains', sorting by oldest update time, of first page of instances of a type. repository-relationship-search-history-performance-findRelationshipsByPropertyValue-end Repository performs historical search with a text value 'ends-with', sorting by newest update time, of first page of instances of a type. repository-relationship-search-history-performance-findRelationshipsByPropertyValue-regex Repository performs historical search with a regular expression text value, sorting by GUID , of first page of instances of a type. repository-relationship-search-history-performance-findRelationshipsByProperty-one Repository performs historical search by a single property value, sorting by that property, of first page of instances of a type. repository-relationship-search-history-performance-findRelationshipsByProperty-all Repository performs historical search by two property values, sorting by the second property, of first page of instances of a type. repository-relationship-search-history-performance-findRelationshipsByProperty-any Repository performs historical search by two property values, sorting by the second property, of first page of instances of a type. repository-relationship-search-history-performance-findRelationshipsByProperty-none Repository performs historical search by two property value, sorting by the first property, of first page of instances of a type. repository-relationship-history-retrieval-performance-findRelationships See the detailed logic of the relationship retrieval profile. Search variations When findRelationshipsByProperty is run by the assertions starting with repository-relationship-search-history-performance... , the tests prefer non-string properties (if any exist for the type) given that the findRelationshipsByPropertyValue searches will already heavily exercise string-based queries.","title":"Relationship History Search"},{"location":"guides/cts/profiles/relationship-history-search/#relationship-history-search-profile","text":"The performance of programmatically searching for relationship instances as they existed in the past. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for searching for relationship instances based on their details in the past, by passing in an asOfTime parameter (captured during the execution of the relationship update profile): Method Description findRelationships arbitrarily complex combinations of search criteria including ranges, nested conditions, etc findRelationshipsByProperty searches for relationship instances based on specific properties with specific values findRelationshipsByPropertyValue searches for relationship instances based on text that matches any textual property Assertions ID Description repository-relationship-search-history-performance-findRelationships-all-p... Interrogates the technology under test for each relationship type, by calling findRelationships for that type GUID with an asOfTime using the timestamp captured prior to the execution of the relationship update profile. These tests are performed with basic sorting of results, and all pages of results are retrieved (the p... portion indicates a specific page number): thereby also exercising the efficiency of the technology under test to both sort and cycle through pages of results. repository-relationship-search-history-performance-findRelationshipsByPropertyValue-exact Repository performs historical search with an exact text value, sorting by oldest creation time, of first page of instances of a type. repository-relationship-search-history-performance-findRelationshipsByPropertyValue-start Repository performs historical search with a text value 'starts-with', sorting by newest creation time, of first page of instances of a type. repository-relationship-search-history-performance-findRelationshipsByPropertyValue-contains Repository performs historical search with a text value 'contains', sorting by oldest update time, of first page of instances of a type. repository-relationship-search-history-performance-findRelationshipsByPropertyValue-end Repository performs historical search with a text value 'ends-with', sorting by newest update time, of first page of instances of a type. repository-relationship-search-history-performance-findRelationshipsByPropertyValue-regex Repository performs historical search with a regular expression text value, sorting by GUID , of first page of instances of a type. repository-relationship-search-history-performance-findRelationshipsByProperty-one Repository performs historical search by a single property value, sorting by that property, of first page of instances of a type. repository-relationship-search-history-performance-findRelationshipsByProperty-all Repository performs historical search by two property values, sorting by the second property, of first page of instances of a type. repository-relationship-search-history-performance-findRelationshipsByProperty-any Repository performs historical search by two property values, sorting by the second property, of first page of instances of a type. repository-relationship-search-history-performance-findRelationshipsByProperty-none Repository performs historical search by two property value, sorting by the first property, of first page of instances of a type. repository-relationship-history-retrieval-performance-findRelationships See the detailed logic of the relationship retrieval profile. Search variations When findRelationshipsByProperty is run by the assertions starting with repository-relationship-search-history-performance... , the tests prefer non-string properties (if any exist for the type) given that the findRelationshipsByPropertyValue searches will already heavily exercise string-based queries.","title":"Relationship history search profile"},{"location":"guides/cts/profiles/relationship-purge/","text":"Relationship purge profile \u00b6 The performance of programmatically hard-deleting (irreversibly) existing relationship instances. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for hard-deleting relationship instances: Method Description purgeRelationship completely removes a relationship instance homed in the technology under test (including all of its history) purgeRelationshipReferenceCopy completely removes a relationship instance that is homed somewhere other than the technology under test (including all of its history) Assertions ID Description repository-relationship-purge-performance-purgeRelationship See (3) in detailed logic below. repository-relationship-purge-performance-purgeRelationshipReferenceCopy See (5) in detailed logic below. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType relationship GUIDs homed in the technology under test. (This uses findRelationshipsByProperty with a condition on metadataCollectionId and its performance is recorded as part of the relationship search profile.) For each of these relationship GUIDs, deleteRelationship is called to soft-delete the instance. (This is necessary before a hard-delete can be done against the instance, and is recorded as part of the relationship delete profile.) For each of these relationship GUIDs, a purgeRelationship is then called to hard-delete the instance. Searches for instancesPerType reference copy relationships. (This uses findRelationshipsByProperty with a condition on metadataCollectionId and its performance is recorded as part of the relationship search profile.) For each of these reference copy relationship instances, purgeRelationshipReferenceCopy is called to remove the reference copy instance. Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will remove 50 (types) x 100 (instances per type) x 3 (home + reference copy methods) = 15 000 relationships. (And it will run findRelationshipsByProperty 100 times.)","title":"Relationship Purge"},{"location":"guides/cts/profiles/relationship-purge/#relationship-purge-profile","text":"The performance of programmatically hard-deleting (irreversibly) existing relationship instances. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for hard-deleting relationship instances: Method Description purgeRelationship completely removes a relationship instance homed in the technology under test (including all of its history) purgeRelationshipReferenceCopy completely removes a relationship instance that is homed somewhere other than the technology under test (including all of its history) Assertions ID Description repository-relationship-purge-performance-purgeRelationship See (3) in detailed logic below. repository-relationship-purge-performance-purgeRelationshipReferenceCopy See (5) in detailed logic below. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType relationship GUIDs homed in the technology under test. (This uses findRelationshipsByProperty with a condition on metadataCollectionId and its performance is recorded as part of the relationship search profile.) For each of these relationship GUIDs, deleteRelationship is called to soft-delete the instance. (This is necessary before a hard-delete can be done against the instance, and is recorded as part of the relationship delete profile.) For each of these relationship GUIDs, a purgeRelationship is then called to hard-delete the instance. Searches for instancesPerType reference copy relationships. (This uses findRelationshipsByProperty with a condition on metadataCollectionId and its performance is recorded as part of the relationship search profile.) For each of these reference copy relationship instances, purgeRelationshipReferenceCopy is called to remove the reference copy instance. Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will remove 50 (types) x 100 (instances per type) x 3 (home + reference copy methods) = 15 000 relationships. (And it will run findRelationshipsByProperty 100 times.)","title":"Relationship purge profile"},{"location":"guides/cts/profiles/relationship-re-home/","text":"Relationship re-home profile \u00b6 The performance of programmatically changing the home repository of an existing relationship instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for changing the home repository of relationship instances: Method Description reHomeRelationship changes the home repository of a relationship Assertions ID Description repository-relationship-re-home-performance-reHomeRelationship See (2) in detailed logic below. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType reference copy relationship GUIDs of that type. (This uses findRelationshipsByProperty with a condition on metadataCollectionId and its performance is recorded as part of the relationship search profile.) For each of these relationship GUIDs, reHomeEntity is called to change the home repository to the technology under test's metadataCollectionId . Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will re-home 50 (types) x 100 (instances per type) = 5000 relationships. (And it will run findRelationshipsByProperty 50 times.)","title":"Relationship Re-home"},{"location":"guides/cts/profiles/relationship-re-home/#relationship-re-home-profile","text":"The performance of programmatically changing the home repository of an existing relationship instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for changing the home repository of relationship instances: Method Description reHomeRelationship changes the home repository of a relationship Assertions ID Description repository-relationship-re-home-performance-reHomeRelationship See (2) in detailed logic below. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType reference copy relationship GUIDs of that type. (This uses findRelationshipsByProperty with a condition on metadataCollectionId and its performance is recorded as part of the relationship search profile.) For each of these relationship GUIDs, reHomeEntity is called to change the home repository to the technology under test's metadataCollectionId . Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will re-home 50 (types) x 100 (instances per type) = 5000 relationships. (And it will run findRelationshipsByProperty 50 times.)","title":"Relationship re-home profile"},{"location":"guides/cts/profiles/relationship-re-identify/","text":"Relationship re-identify profile \u00b6 The performance of programmatically changing the GUID of an existing relationship instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for changing the GUID of relationship instances: Method Description reIdentifyRelationship changes the GUID of an existing relationship Assertions ID Description repository-relationship-re-identify-performance-reIdentifyRelationship See (2) in detailed logic below. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType homed relationship GUIDs of that type. (This uses findRelationshipsByProperty with a condition on metadataCollectionId and its performance is recorded as part of the relationship search profile.) For each of these relationship GUIDs, reIdentifyRelationship is called to change its GUID to a new random GUID . Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will re-identify 50 (types) x 100 (instances per type) = 5000 relationships. (And it will run findRelationshipsByProperty 50 times.)","title":"Relationship Re-identify"},{"location":"guides/cts/profiles/relationship-re-identify/#relationship-re-identify-profile","text":"The performance of programmatically changing the GUID of an existing relationship instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for changing the GUID of relationship instances: Method Description reIdentifyRelationship changes the GUID of an existing relationship Assertions ID Description repository-relationship-re-identify-performance-reIdentifyRelationship See (2) in detailed logic below. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType homed relationship GUIDs of that type. (This uses findRelationshipsByProperty with a condition on metadataCollectionId and its performance is recorded as part of the relationship search profile.) For each of these relationship GUIDs, reIdentifyRelationship is called to change its GUID to a new random GUID . Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will re-identify 50 (types) x 100 (instances per type) = 5000 relationships. (And it will run findRelationshipsByProperty 50 times.)","title":"Relationship re-identify profile"},{"location":"guides/cts/profiles/relationship-restore/","text":"Relationship restore profile \u00b6 The performance of programmatically reversing the latest soft-delete of an existing relationship instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for reverting soft-deletes on relationship instances: Method Description restoreRelationship reverts the last soft-delete that was made to a relationship Assertions ID Description repository-relationship-restore-performance-restoreRelationship See (2) in detailed logic below. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType relationship GUIDs of that type that have been soft-deleted. (This uses findRelationshipsByProperty with a condition to limit to the status DELETED only and its performance is recorded as part of the relationship search profile.) For each of these relationship GUIDs, restoreRelationship is called to revert the soft-delete and make the relationship active again. Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will re-activate 50 (types) x 100 (instances per type) = 5000 relationships. (And it will run findRelationshipsByProperty 50 times.)","title":"Relationship Restore"},{"location":"guides/cts/profiles/relationship-restore/#relationship-restore-profile","text":"The performance of programmatically reversing the latest soft-delete of an existing relationship instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for reverting soft-deletes on relationship instances: Method Description restoreRelationship reverts the last soft-delete that was made to a relationship Assertions ID Description repository-relationship-restore-performance-restoreRelationship See (2) in detailed logic below. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType relationship GUIDs of that type that have been soft-deleted. (This uses findRelationshipsByProperty with a condition to limit to the status DELETED only and its performance is recorded as part of the relationship search profile.) For each of these relationship GUIDs, restoreRelationship is called to revert the soft-delete and make the relationship active again. Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will re-activate 50 (types) x 100 (instances per type) = 5000 relationships. (And it will run findRelationshipsByProperty 50 times.)","title":"Relationship restore profile"},{"location":"guides/cts/profiles/relationship-retrieval/","text":"Relationship retrieval profile \u00b6 The performance of programmatically retrieving existing relationship instances based on their ID. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines methods for retrieving relationship instances by their ID: Method Description isRelationshipKnown retrieves a relationship instance's details, if available, or returns null if not getRelationship retrieves a relationship instance's details, if available, or throws an exception if not Assertions ID Description repository-relationship-retrieval-performance-isRelationshipKnown See (2) in detailed logic below. repository-relationship-retrieval-performance-getRelationship See (3) in detailed logic below. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType relationship GUIDs of that type. (This uses findRelationships and its performance is recorded as part of the relationship search profile.) For each of these relationship GUIDs, isRelationshipKnown is called to retrieve it. For each of these relationship GUIDs, getRelationship is then called to retrieve its details. Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will retrieve 50 (types) x 100 (instances per type) x 2 (operations) = 10 000 relationships. (And it will run findRelationships 50 times.) Caveats Note the following caveats: The same GUIDs are used for each retrieval, but each method is run against every GUID before moving onto the next method.","title":"Relationship Retrieval"},{"location":"guides/cts/profiles/relationship-retrieval/#relationship-retrieval-profile","text":"The performance of programmatically retrieving existing relationship instances based on their ID. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines methods for retrieving relationship instances by their ID: Method Description isRelationshipKnown retrieves a relationship instance's details, if available, or returns null if not getRelationship retrieves a relationship instance's details, if available, or throws an exception if not Assertions ID Description repository-relationship-retrieval-performance-isRelationshipKnown See (2) in detailed logic below. repository-relationship-retrieval-performance-getRelationship See (3) in detailed logic below. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType relationship GUIDs of that type. (This uses findRelationships and its performance is recorded as part of the relationship search profile.) For each of these relationship GUIDs, isRelationshipKnown is called to retrieve it. For each of these relationship GUIDs, getRelationship is then called to retrieve its details. Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will retrieve 50 (types) x 100 (instances per type) x 2 (operations) = 10 000 relationships. (And it will run findRelationships 50 times.) Caveats Note the following caveats: The same GUIDs are used for each retrieval, but each method is run against every GUID before moving onto the next method.","title":"Relationship retrieval profile"},{"location":"guides/cts/profiles/relationship-retype/","text":"Relationship retype profile \u00b6 The performance of programmatically changing the type of an existing relationship instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for changing the type of relationship instances: Method Description reTypeRelationship changes the type of an existing relationship Assertions ID Description repository-relationship-retype-performance-reTypeEntity-toSubtype See (3) in detailed logic below. repository-relationship-retype-performance-reTypeEntity-toSupertype See (4) in detailed logic below. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType homed relationship GUIDs of that type. (This uses findRelationshipsByProperty with a condition on metadataCollectionId and its performance is recorded as part of the relationship search profile.) For each of these relationship GUIDs, updateRelationshipProperties is called to remove all the relationship's properties (so it can be easily retyped), and its performance is recorded as part of the relationship update profile. For each of these relationship GUIDs, reTypeRelationship is called to change the type of the relationship to one of its subtypes. For each of these relationship GUIDs, reTypeRelationship is then called to change the type of the relationship back to its original type. Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will retype 50 (types) x 100 (instances per type) x 2 (operations) = 10 000 relationships at most. (And it will run findRelationshipsByProperty 50 times.) Caveats Note the following caveats: Instances of a given type will only be retyped if that type has any subtypes: if it has no subtypes, then all retyping operations will be skipped for that type's instances. Currently there are no open metadata relationship types that have supertypes or subtypes, so this profile will not actually have any metadata instance against which to call reTypeRelationship .","title":"Relationship Re-type"},{"location":"guides/cts/profiles/relationship-retype/#relationship-retype-profile","text":"The performance of programmatically changing the type of an existing relationship instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for changing the type of relationship instances: Method Description reTypeRelationship changes the type of an existing relationship Assertions ID Description repository-relationship-retype-performance-reTypeEntity-toSubtype See (3) in detailed logic below. repository-relationship-retype-performance-reTypeEntity-toSupertype See (4) in detailed logic below. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType homed relationship GUIDs of that type. (This uses findRelationshipsByProperty with a condition on metadataCollectionId and its performance is recorded as part of the relationship search profile.) For each of these relationship GUIDs, updateRelationshipProperties is called to remove all the relationship's properties (so it can be easily retyped), and its performance is recorded as part of the relationship update profile. For each of these relationship GUIDs, reTypeRelationship is called to change the type of the relationship to one of its subtypes. For each of these relationship GUIDs, reTypeRelationship is then called to change the type of the relationship back to its original type. Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will retype 50 (types) x 100 (instances per type) x 2 (operations) = 10 000 relationships at most. (And it will run findRelationshipsByProperty 50 times.) Caveats Note the following caveats: Instances of a given type will only be retyped if that type has any subtypes: if it has no subtypes, then all retyping operations will be skipped for that type's instances. Currently there are no open metadata relationship types that have supertypes or subtypes, so this profile will not actually have any metadata instance against which to call reTypeRelationship .","title":"Relationship retype profile"},{"location":"guides/cts/profiles/relationship-search/","text":"Relationship search profile \u00b6 The performance of programmatically searching for existing relationship instances. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for searching for relationship instances: Method Description findRelationships arbitrarily complex combinations of search criteria including ranges, nested conditions, etc findRelationshipsByProperty searches for relationship instances based on specific properties with specific values findRelationshipsByPropertyValue searches for relationship instances based on text that matches any textual property Assertions ID Description repository-relationship-search-performance-findRelationships-all-p... This profile begins by interrogating the technology under test in its entirety, to discover every existing relationship instance of every type known to Egeria. The total count of these is tallied to report later under the environment profile. In this way, even for repositories that do not support write operations, we can still calculate some metrics about read performance (including search) while being able to understand that information in light of the volumes of metadata in the repository while the test was executed. These searches are also performed with basic sorting of results, and all pages of results are retrieved (the p... portion indicates a specific page number): thereby also exercising the efficiency of the technology under test to both sort and cycle through pages of results. repository-relationship-search-performance-findRelationshipsByPropertyValue-exact Repository performs search with an exact text value, sorting by oldest creation time, of first page of instances of a type. repository-relationship-search-performance-findRelationshipsByPropertyValue-start Repository performs search with a text value 'starts-with', sorting by newest creation time, of first page of instances of a type. repository-relationship-search-performance-findRelationshipsByPropertyValue-contains Repository performs search with a text value 'contains', sorting by oldest update time, of first page of instances of a type. repository-relationship-search-performance-findRelationshipsByPropertyValue-end Repository performs search with a text value 'ends-with', sorting by newest update time, of first page of instances of a type. repository-relationship-search-performance-findRelationshipsByPropertyValue-regex Repository performs search with a regular expression text value, sorting by GUID , of first page of instances of a type. repository-relationship-search-performance-findRelationshipsByProperty-one Repository performs search by a single property value, sorting by that property, of first page of instances of a type. repository-relationship-search-performance-findRelationshipsByProperty-all Repository performs search by two property values, sorting by the second property, of first page of instances of a type. repository-relationship-search-performance-findRelationshipsByProperty-any Repository performs search by two property values, sorting by the second property, of first page of instances of a type. repository-relationship-search-performance-findRelationshipsByProperty-none Repository performs search by two property value, sorting by the first property, of first page of instances of a type. repository-relationship-delete-performance-findRelationshipsByProperty See the detailed logic of the relationship delete profile. repository-relationship-purge-performance-findRelationshipsByProperty See the detailed logic of the relationship purge profile. repository-relationship-purge-performance-findRelationshipsByProperty-rc See the detailed logic of the relationship purge profile. repository-relationship-re-home-performance-findRelationshipsByProperty See the detailed logic of the relationship re-home profile. repository-relationship-re-identify-performance-findRelationshipsByProperty See the detailed logic of the relationship re-identify profile. repository-relationship-restore-performance-findRelationshipsByProperty See the detailed logic of the relationship restore profile. repository-relationship-retrieval-performance-findRelationships See the detailed logic of the relationship retrieval profile. repository-relationship-retype-performance-findRelationshipsByProperty See the detailed logic of the relationship retype profile. repository-relationship-undo-performance-findRelationships See the detailed logic of the relationship undo profile. repository-relationship-update-performance-findRelationshipsByProperty See the detailed logic of the relationship update profile. In addition, these tests will record into the Environment profile the totalRelationshipsFound . Search variations When findRelationshipsByProperty is run by the assertions starting with repository-relationship-search-performance... , the tests prefer non-string properties (if any exist for the type) given that the findRelationshipsByPropertyValue searches will already heavily exercise string-based queries. Note that the various other assertions (that do not start with repository-relationship-search-performance... ) will search on various other properties than those listed: in particular, including header properties like metadataCollectionId , version , and others.","title":"Relationship Search"},{"location":"guides/cts/profiles/relationship-search/#relationship-search-profile","text":"The performance of programmatically searching for existing relationship instances. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for searching for relationship instances: Method Description findRelationships arbitrarily complex combinations of search criteria including ranges, nested conditions, etc findRelationshipsByProperty searches for relationship instances based on specific properties with specific values findRelationshipsByPropertyValue searches for relationship instances based on text that matches any textual property Assertions ID Description repository-relationship-search-performance-findRelationships-all-p... This profile begins by interrogating the technology under test in its entirety, to discover every existing relationship instance of every type known to Egeria. The total count of these is tallied to report later under the environment profile. In this way, even for repositories that do not support write operations, we can still calculate some metrics about read performance (including search) while being able to understand that information in light of the volumes of metadata in the repository while the test was executed. These searches are also performed with basic sorting of results, and all pages of results are retrieved (the p... portion indicates a specific page number): thereby also exercising the efficiency of the technology under test to both sort and cycle through pages of results. repository-relationship-search-performance-findRelationshipsByPropertyValue-exact Repository performs search with an exact text value, sorting by oldest creation time, of first page of instances of a type. repository-relationship-search-performance-findRelationshipsByPropertyValue-start Repository performs search with a text value 'starts-with', sorting by newest creation time, of first page of instances of a type. repository-relationship-search-performance-findRelationshipsByPropertyValue-contains Repository performs search with a text value 'contains', sorting by oldest update time, of first page of instances of a type. repository-relationship-search-performance-findRelationshipsByPropertyValue-end Repository performs search with a text value 'ends-with', sorting by newest update time, of first page of instances of a type. repository-relationship-search-performance-findRelationshipsByPropertyValue-regex Repository performs search with a regular expression text value, sorting by GUID , of first page of instances of a type. repository-relationship-search-performance-findRelationshipsByProperty-one Repository performs search by a single property value, sorting by that property, of first page of instances of a type. repository-relationship-search-performance-findRelationshipsByProperty-all Repository performs search by two property values, sorting by the second property, of first page of instances of a type. repository-relationship-search-performance-findRelationshipsByProperty-any Repository performs search by two property values, sorting by the second property, of first page of instances of a type. repository-relationship-search-performance-findRelationshipsByProperty-none Repository performs search by two property value, sorting by the first property, of first page of instances of a type. repository-relationship-delete-performance-findRelationshipsByProperty See the detailed logic of the relationship delete profile. repository-relationship-purge-performance-findRelationshipsByProperty See the detailed logic of the relationship purge profile. repository-relationship-purge-performance-findRelationshipsByProperty-rc See the detailed logic of the relationship purge profile. repository-relationship-re-home-performance-findRelationshipsByProperty See the detailed logic of the relationship re-home profile. repository-relationship-re-identify-performance-findRelationshipsByProperty See the detailed logic of the relationship re-identify profile. repository-relationship-restore-performance-findRelationshipsByProperty See the detailed logic of the relationship restore profile. repository-relationship-retrieval-performance-findRelationships See the detailed logic of the relationship retrieval profile. repository-relationship-retype-performance-findRelationshipsByProperty See the detailed logic of the relationship retype profile. repository-relationship-undo-performance-findRelationships See the detailed logic of the relationship undo profile. repository-relationship-update-performance-findRelationshipsByProperty See the detailed logic of the relationship update profile. In addition, these tests will record into the Environment profile the totalRelationshipsFound . Search variations When findRelationshipsByProperty is run by the assertions starting with repository-relationship-search-performance... , the tests prefer non-string properties (if any exist for the type) given that the findRelationshipsByPropertyValue searches will already heavily exercise string-based queries. Note that the various other assertions (that do not start with repository-relationship-search-performance... ) will search on various other properties than those listed: in particular, including header properties like metadataCollectionId , version , and others.","title":"Relationship search profile"},{"location":"guides/cts/profiles/relationship-undo/","text":"Relationship undo profile \u00b6 The performance of programmatically reversing the latest update to an existing relationship instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for reverting updates on relationship instances: Method Description undoRelationshipUpdate reverts the last update that was made to a relationship Assertions ID Description repository-relationship-undo-performance-undoRelationshipUpdate See (2) in detailed logic below. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType relationships of that type that have at least one change. (This uses findRelationships with a condition on both metadataCollectionId and version being greater than 1 , and its performance is recorded as part of the relationship search profile.) For each of these relationship instances, undoRelationshipUpdate is called to revert the last change. Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will update 50 (types) x 100 (instances per type) = 5000 relationships. (And it will run findRelationships 50 times.) Caveats Note the following caveats: Relationship type definitions that have no properties will not be reverted: since there are no properties to update, there will not have been any updated version (and thus nothing to revert).","title":"Relationship Undo"},{"location":"guides/cts/profiles/relationship-undo/#relationship-undo-profile","text":"The performance of programmatically reversing the latest update to an existing relationship instance. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines an optional method for reverting updates on relationship instances: Method Description undoRelationshipUpdate reverts the last update that was made to a relationship Assertions ID Description repository-relationship-undo-performance-undoRelationshipUpdate See (2) in detailed logic below. For every relationship type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType relationships of that type that have at least one change. (This uses findRelationships with a condition on both metadataCollectionId and version being greater than 1 , and its performance is recorded as part of the relationship search profile.) For each of these relationship instances, undoRelationshipUpdate is called to revert the last change. Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will update 50 (types) x 100 (instances per type) = 5000 relationships. (And it will run findRelationships 50 times.) Caveats Note the following caveats: Relationship type definitions that have no properties will not be reverted: since there are no properties to update, there will not have been any updated version (and thus nothing to revert).","title":"Relationship undo profile"},{"location":"guides/cts/profiles/relationship-update/","text":"Relationship update profile \u00b6 The performance of programmatically updating existing relationship instances. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for updating relationship instances: Method Description updateRelationshipProperties changes one or more values of the properties on an existing relationship updateRelationshipStatus changes the status of an existing relationship Assertions ID Description repository-relationship-update-performance-updateEntityProperties See (2) in detailed logic below. repository-relationship-retype-performance-updateEntityProperties-remove See the detailed logic of the relationship retype profile. Captures timestamp for historical metadata profiles Prior to this profile running, a timestamp is captured by the performance workbench to denote a specific date and time prior to any updates having been made to the relationships. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType relationships of that type. (This uses findRelationshipsByProperty with a condition on metadataCollectionId and its performance is recorded as part of the relationship search profile.) For each of these relationship instances, updateRelationshipProperties is called to update the existing property values of the relationship, using a new generated set of properties. Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will update 50 (types) x 100 (instances per type) = 5000 relationships. (And it will run findRelationshipsByProperty 50 times.) The properties of each of these instances will be fully-populated with: Any string properties containing a value representative of the property name itself (and where unique, they will be made unique through appending a unique sequence) Any non-string properties will be randomly generated, in a simple attempt to represent data that is not entirely uniform Caveats Note the following caveats: updateRelationshipStatus is currently not tested (there are very few types that support any status that can be updated outside of a delete or restore operation). Relationship type definitions that have no properties will not be updated, since there are no properties to update.","title":"Relationship Update"},{"location":"guides/cts/profiles/relationship-update/#relationship-update-profile","text":"The performance of programmatically updating existing relationship instances. The Open Metadata Repository Services ( OMRS ) interface for a metadata repository defines optional methods for updating relationship instances: Method Description updateRelationshipProperties changes one or more values of the properties on an existing relationship updateRelationshipStatus changes the status of an existing relationship Assertions ID Description repository-relationship-update-performance-updateEntityProperties See (2) in detailed logic below. repository-relationship-retype-performance-updateEntityProperties-remove See the detailed logic of the relationship retype profile. Captures timestamp for historical metadata profiles Prior to this profile running, a timestamp is captured by the performance workbench to denote a specific date and time prior to any updates having been made to the relationships. For every entity type supported by the technology under test, this profile does the following (in order): Searches for instancesPerType relationships of that type. (This uses findRelationshipsByProperty with a condition on metadataCollectionId and its performance is recorded as part of the relationship search profile.) For each of these relationship instances, updateRelationshipProperties is called to update the existing property values of the relationship, using a new generated set of properties. Example So, for example, if the technology under test supports 50 relationship types, and the instancesPerType parameter is set to 100, then this profile will update 50 (types) x 100 (instances per type) = 5000 relationships. (And it will run findRelationshipsByProperty 50 times.) The properties of each of these instances will be fully-populated with: Any string properties containing a value representative of the property name itself (and where unique, they will be made unique through appending a unique sequence) Any non-string properties will be randomly generated, in a simple attempt to represent data that is not entirely uniform Caveats Note the following caveats: updateRelationshipStatus is currently not tested (there are very few types that support any status that can be updated outside of a delete or restore operation). Relationship type definitions that have no properties will not be updated, since there are no properties to update.","title":"Relationship update profile"},{"location":"guides/developer/guide/","text":"Developer Guide \u00b6 Egeria is designed to simplify the effort necessary to integrate different technologies so that they can actively share and consume metadata from each other. It focuses on providing five types of integration interfaces. Connectors that translate between third party APIs and open metadata APIs. These connectors are hosted in the Egeria servers and support the active exchange of metadata with these technologies. Connectors for accessing popular type of data sources that also retrieve open metadata about the data source. This allows applications and tools to understand the structure, meaning, profile, quality and lineage of the data they are using. Java clients for applications to call the Open Metadata Access Service ( OMAS ) interfaces, each of which are crafted for particular types of technology. These interfaces support both synchronous APIs, inbound event notifications and outbound asynchronous events. REST APIs for the Egeria Services. These include the access services , admin services and platform services . Kafka topics with JSON payloads for asynchronous communication (both in and out) with the open metadata ecosystem. Learn more ... Using the clients \u00b6 The Egeria clients wrap calls to Egeria's REST APIs and topics. The aim is to provide a language-specific interface that manages the marshalling and de-marshalling of the call parameters and responses to these services. Using the REST APIs \u00b6 Egeria supports REST APIs for making synchronous (request-response) calls between OMAG Servers and between clients and OMAG Servers. REST APIs are intended for internal use The REST APIs are usable directly for calling from non-Java platforms; however, they are designed for the internal use of Egeria and are not guaranteed to be backwards compatible. The structure of the URL for an Egeria REST API varies lightly depending on whether it is a call to an OMAG Server Platform service or an OMAG Server service. What is a connector? \u00b6 Connectors are plug-in Java classes that either perform an additional service, or, more typically, enable Egeria to integrate with a third party technology. The concept of a connector comes from the Open Connector Framework ( OCF ) . The OCF provides a common framework for components that enable one technology to call another, arbitrary technology through a common interface. The implementation of the connector is dynamically loaded based on the connector's configuration. Through the OCF , we can: Plug-in different technologies to Egeria. Plug-in support for open metadata into the client libraries used by applications to access data resources and services. Many subsystems in Egeria's OMAG Server Platform and servers support the first approach. They define a specialized interface for the type of connector they support. One or more connector implementations supporting that interface are then written either by the Egeria community or other organizations. When an Egeria OMAG Server is configured, details of which connector implementation to use is specified in the server's configuration document . At start up, the OMAG Server passes the connector configuration to the OCF to instantiate the required connector instance. Connectors enable Egeria to operate in many environments and with many types of third party technologies, just by managing the configuration of the OMAG Servers. The second approach is used by organizations that want to make use of metadata directly in applications and tools - or to externalize the security and driver properties needed to call the data source or service. In this case the OCF connector typically has the same interface as the data source's client library (unless you can do better ). This minimizes the learning curve for application developers. The configuration for the connector is stored in an open metadata server and the application uses the Asset Consumer OMAS client to request a new instance of the connector . The application uses the returned connector instance to access the data source or server along with the metadata stored about it. Configuration \u00b6 The configuration for a connector is managed in a connection object. A connection contains properties about the specific use of the connector, such as user Id and password, or parameters that control the scope or resources that should be made available to the connector. It links to an optional endpoint and a mandatory connector type object. ConnectorType describes the type of the connector, its supported configuration properties and its factory object (called the connector's provider ). This information is used to create an instance of the connector at runtime. Endpoint describes the server endpoint where the third party data source or service is accessed from. Connector types and endpoints can be reused in multiple connections. Factories \u00b6 Each connector implementation has a factory object called a connector provider . The connector provider has two types of methods: Return a new instance of the connector based on the properties in a supplied Connection object. The Connection object has all the properties needed to create and configure the instance of the connector. Return additional information about the connector's behavior and usage to make it easier to consume. For example, the standard base class for a connector provider has a method to return the ConnectorType object for this connector implementation that can be added to a Connection object used to hold the properties needed to create an instance of the connector. Inside the connector \u00b6 Each connector has its own unique implementation that is structured around a simple lifecycle that is defined by the OCF . The OCF provides the interface for a connector called Connector that has three methods: initialize , start and disconnect . This connector interface supports the basic lifecycle of a connector. There are three phases: Initialization - During this phase, the connector is passed the context in which it is to operate. It should store this information. This phase is initiated by a call to the connector's initialize() method, which is called after the connector's constructor and provides the connector with a unique instance identifier (for logging) and its configuration stored in a connection . After initialize() returns, there may be other calls to pass context to the connector. For example, if the connector implements the AuditLoggingComponent , an audit log is passed to the connector. Running - The connector is completely initialized with its context, and it can start processing. This phase is initiated by a call to the connector's start() method. At this point it should create its client to any third party technology and begin processing. It may also start up threads if it needs to perform any background processing (such as listening for notifications). If the connector throws an exception during start, Egeria knows the connector has a configuration or operational issue and will report the error and move it to disconnected state. Disconnected - The connector must stop processing and release all of its resources. This phase is initiated by a call to the connector's disconnect() method. Depending on the type of connector you are writing, there may be additional initialization calls occurring between the initialize() and the start() method. The connector may also support additional methods for its normal operation that can be called between the start() and disconnect() calls. The OCF also provides the base class for a connector called ConnectorBase . The ConnectorBase base class manages the lifecycle state of the connector. For example, the default implementation of initialize() in the ConnectorBase class stores the supplied unique instance identifier and connection values in protected variables called connectorInstanceId and connectionProperties respectively. Call the base class's methods in any overrides If you override any of the initialize() , start() or disconnect() methods, be sure to call super.xxx() at the start of your implementation to call the appropriate super class method so that the state is properly maintained. Extending Egeria using connectors \u00b6 Egeria has extended the basic concept of the OCF connector and created specialized connectors for different purposes. The following types of connectors are supported by the Egeria subsystems with links to the documentation and implementation examples. Type of Connector Description Documentation Implementation Examples Integration Connector Implements metadata exchange with third party tools. Building Integration Connectors integration-connectors Open Discovery Service Implements automated metadata discovery. Open Discovery Services discovery-service-connectors Governance Action Service Implements automated governance. Governance Action Services governance-action-connectors Configuration Document Store Persists the configuration document for an OMAG Server. Configuration Document Store Connectors configuration-store-connectors Platform Security Connector Manages service authorization for the OMAG Server Platform. Metadata Security Connectors open-metadata-security-samples Server Security Connector Manages service and metadata instance authorization for an OMAG Server. Metadata Security Connectors open-metadata-security-samples Metadata Collection (repository) Store Interfaces with a metadata repository API for retrieving and storing metadata. OMRS Repository Connectors open-metadata-collection-store-connectors Metadata Collection (repository) Event Mapper Maps events from a third party metadata repository to open metadata events. OMRS Event Mappers none Open Metadata Archive Store Reads an open metadata archive from a particular type of store. OMRS Open Metadata Archive Store Connector open-metadata-archive-connectors Audit Log Store Audit logging destination OMRS Audit Log Store Connector audit-log-connectors Cohort Registry Store Local store of membership of an open metadata repository cohort. OMRS Cohort Registry Store cohort-registry-store-connectors Open Metadata Topic Connector Connects to a topic on an external event bus such as Apache Kafka. OMRS Open Metadata Topic Connectors open-metadata- topic-connectors You can write your own connectors to integrate additional types of technology or extend the capabilities of Egeria - and if you think your connector is more generally useful, you could consider contributing it to the Egeria project .","title":"Developer Guide"},{"location":"guides/developer/guide/#developer-guide","text":"Egeria is designed to simplify the effort necessary to integrate different technologies so that they can actively share and consume metadata from each other. It focuses on providing five types of integration interfaces. Connectors that translate between third party APIs and open metadata APIs. These connectors are hosted in the Egeria servers and support the active exchange of metadata with these technologies. Connectors for accessing popular type of data sources that also retrieve open metadata about the data source. This allows applications and tools to understand the structure, meaning, profile, quality and lineage of the data they are using. Java clients for applications to call the Open Metadata Access Service ( OMAS ) interfaces, each of which are crafted for particular types of technology. These interfaces support both synchronous APIs, inbound event notifications and outbound asynchronous events. REST APIs for the Egeria Services. These include the access services , admin services and platform services . Kafka topics with JSON payloads for asynchronous communication (both in and out) with the open metadata ecosystem. Learn more ...","title":"Developer Guide"},{"location":"guides/developer/guide/#using-the-clients","text":"The Egeria clients wrap calls to Egeria's REST APIs and topics. The aim is to provide a language-specific interface that manages the marshalling and de-marshalling of the call parameters and responses to these services.","title":"Using the clients"},{"location":"guides/developer/guide/#using-the-rest-apis","text":"Egeria supports REST APIs for making synchronous (request-response) calls between OMAG Servers and between clients and OMAG Servers. REST APIs are intended for internal use The REST APIs are usable directly for calling from non-Java platforms; however, they are designed for the internal use of Egeria and are not guaranteed to be backwards compatible. The structure of the URL for an Egeria REST API varies lightly depending on whether it is a call to an OMAG Server Platform service or an OMAG Server service.","title":"Using the REST APIs"},{"location":"guides/developer/guide/#what-is-a-connector","text":"Connectors are plug-in Java classes that either perform an additional service, or, more typically, enable Egeria to integrate with a third party technology. The concept of a connector comes from the Open Connector Framework ( OCF ) . The OCF provides a common framework for components that enable one technology to call another, arbitrary technology through a common interface. The implementation of the connector is dynamically loaded based on the connector's configuration. Through the OCF , we can: Plug-in different technologies to Egeria. Plug-in support for open metadata into the client libraries used by applications to access data resources and services. Many subsystems in Egeria's OMAG Server Platform and servers support the first approach. They define a specialized interface for the type of connector they support. One or more connector implementations supporting that interface are then written either by the Egeria community or other organizations. When an Egeria OMAG Server is configured, details of which connector implementation to use is specified in the server's configuration document . At start up, the OMAG Server passes the connector configuration to the OCF to instantiate the required connector instance. Connectors enable Egeria to operate in many environments and with many types of third party technologies, just by managing the configuration of the OMAG Servers. The second approach is used by organizations that want to make use of metadata directly in applications and tools - or to externalize the security and driver properties needed to call the data source or service. In this case the OCF connector typically has the same interface as the data source's client library (unless you can do better ). This minimizes the learning curve for application developers. The configuration for the connector is stored in an open metadata server and the application uses the Asset Consumer OMAS client to request a new instance of the connector . The application uses the returned connector instance to access the data source or server along with the metadata stored about it.","title":"What is a connector?"},{"location":"guides/developer/guide/#configuration","text":"The configuration for a connector is managed in a connection object. A connection contains properties about the specific use of the connector, such as user Id and password, or parameters that control the scope or resources that should be made available to the connector. It links to an optional endpoint and a mandatory connector type object. ConnectorType describes the type of the connector, its supported configuration properties and its factory object (called the connector's provider ). This information is used to create an instance of the connector at runtime. Endpoint describes the server endpoint where the third party data source or service is accessed from. Connector types and endpoints can be reused in multiple connections.","title":"Configuration"},{"location":"guides/developer/guide/#factories","text":"Each connector implementation has a factory object called a connector provider . The connector provider has two types of methods: Return a new instance of the connector based on the properties in a supplied Connection object. The Connection object has all the properties needed to create and configure the instance of the connector. Return additional information about the connector's behavior and usage to make it easier to consume. For example, the standard base class for a connector provider has a method to return the ConnectorType object for this connector implementation that can be added to a Connection object used to hold the properties needed to create an instance of the connector.","title":"Factories"},{"location":"guides/developer/guide/#inside-the-connector","text":"Each connector has its own unique implementation that is structured around a simple lifecycle that is defined by the OCF . The OCF provides the interface for a connector called Connector that has three methods: initialize , start and disconnect . This connector interface supports the basic lifecycle of a connector. There are three phases: Initialization - During this phase, the connector is passed the context in which it is to operate. It should store this information. This phase is initiated by a call to the connector's initialize() method, which is called after the connector's constructor and provides the connector with a unique instance identifier (for logging) and its configuration stored in a connection . After initialize() returns, there may be other calls to pass context to the connector. For example, if the connector implements the AuditLoggingComponent , an audit log is passed to the connector. Running - The connector is completely initialized with its context, and it can start processing. This phase is initiated by a call to the connector's start() method. At this point it should create its client to any third party technology and begin processing. It may also start up threads if it needs to perform any background processing (such as listening for notifications). If the connector throws an exception during start, Egeria knows the connector has a configuration or operational issue and will report the error and move it to disconnected state. Disconnected - The connector must stop processing and release all of its resources. This phase is initiated by a call to the connector's disconnect() method. Depending on the type of connector you are writing, there may be additional initialization calls occurring between the initialize() and the start() method. The connector may also support additional methods for its normal operation that can be called between the start() and disconnect() calls. The OCF also provides the base class for a connector called ConnectorBase . The ConnectorBase base class manages the lifecycle state of the connector. For example, the default implementation of initialize() in the ConnectorBase class stores the supplied unique instance identifier and connection values in protected variables called connectorInstanceId and connectionProperties respectively. Call the base class's methods in any overrides If you override any of the initialize() , start() or disconnect() methods, be sure to call super.xxx() at the start of your implementation to call the appropriate super class method so that the state is properly maintained.","title":"Inside the connector"},{"location":"guides/developer/guide/#extending-egeria-using-connectors","text":"Egeria has extended the basic concept of the OCF connector and created specialized connectors for different purposes. The following types of connectors are supported by the Egeria subsystems with links to the documentation and implementation examples. Type of Connector Description Documentation Implementation Examples Integration Connector Implements metadata exchange with third party tools. Building Integration Connectors integration-connectors Open Discovery Service Implements automated metadata discovery. Open Discovery Services discovery-service-connectors Governance Action Service Implements automated governance. Governance Action Services governance-action-connectors Configuration Document Store Persists the configuration document for an OMAG Server. Configuration Document Store Connectors configuration-store-connectors Platform Security Connector Manages service authorization for the OMAG Server Platform. Metadata Security Connectors open-metadata-security-samples Server Security Connector Manages service and metadata instance authorization for an OMAG Server. Metadata Security Connectors open-metadata-security-samples Metadata Collection (repository) Store Interfaces with a metadata repository API for retrieving and storing metadata. OMRS Repository Connectors open-metadata-collection-store-connectors Metadata Collection (repository) Event Mapper Maps events from a third party metadata repository to open metadata events. OMRS Event Mappers none Open Metadata Archive Store Reads an open metadata archive from a particular type of store. OMRS Open Metadata Archive Store Connector open-metadata-archive-connectors Audit Log Store Audit logging destination OMRS Audit Log Store Connector audit-log-connectors Cohort Registry Store Local store of membership of an open metadata repository cohort. OMRS Cohort Registry Store cohort-registry-store-connectors Open Metadata Topic Connector Connects to a topic on an external event bus such as Apache Kafka. OMRS Open Metadata Topic Connectors open-metadata- topic-connectors You can write your own connectors to integrate additional types of technology or extend the capabilities of Egeria - and if you think your connector is more generally useful, you could consider contributing it to the Egeria project .","title":"Extending Egeria using connectors"},{"location":"guides/developer/guidelines/","text":"Developer Guidelines \u00b6 Egeria provides technology for an open standard that seeks to improve the processing and protection of data across organizations. For its developers, this carries the benefit that their work receives high recognition, but also additional responsibilities to ensure its wide applicability and longevity. For example, Egeria seeks a broad audience - from developers to adopting vendors to consuming users. Building this audience and allowing the community to scale requires clarity in the way the software is written, documented, packaged and used. Many of the guidelines seek to make it easier for someone new to pick up the software, at the expense of maybe a little more work, or a little less freedom of action for the original developer. As such, these guidelines exist to remind us of these broader responsibilities. Build environment \u00b6 The core of Egeria is written primarily in Java , and the minimum level required to build and run it is 11. Most developers use MacOS, while our official builds use Linux (Ubuntu/Centos/RHEL should all be fine). Windows is unsupported The traditional Windows environment is not directly supported. It is recommended to use WSL2 which offers a full Linux environment. Apache Maven is used to control the builds, and 3.5 or higher is required to build Egeria (3.6.x or above is recommended). Gradle is not currently supported but is being developed. IDEs can make navigating the Egeria code easier. Each IDE can vary a lot. Many of our team use JetBrains IntelliJ . In the case of problems the first problem determination step is to check you can build Egeria normally at the command line i.e. mvn clean install from the source root. That will prove at least Java and Maven are correct. Set JAVA_HOME We have also noticed that you need to ensure JAVA_HOME is set or the build will fail running Javadoc. Eliminate any build warnings \u00b6 Build output should be checked for any warnings, i.e. [WARNING] , and these should be eliminated. The Java compiler is set to use -Xlint:all and may report warnings about deprecated function, unsafe casts, unchecked conversions, and so on -- all of which should be addressed. Other tools used in the build may also result in warnings which should also be addressed, whilst test cases should ensure output is captured to avoid such warnings appear in the build logs. Include license in every file \u00b6 All files for Egeria should have a license included. We use the SPDX encoding to keep the headers simple. License header for documentation ( .md ) 1 2 <!-- SPDX-License-Identifier: CC-BY-4.0 --> <!-- Copyright Contributors to the Egeria project. --> Note that we no longer need to include an explicit footer in documentation files, as this is already included in the overall documentation site as the footer of every page. License header for XML files ( .xml ) 1 2 3 <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!-- SPDX-License-Identifier: Apache-2.0 --> <!-- Copyright Contributors to the Egeria project. --> License header for Java code ( .java ) 1 2 /* SPDX-License-Identifier: Apache-2.0 */ /* Copyright Contributors to the Egeria project. */ Document \u00b6 Although all code for Egeria should be clear and easy to read, the code itself can only describe what it is doing: it can rarely describe why it is doing it. Also, the Egeria codebase is quite large and hard to digest in one go. Having summaries of its behavior and philosophy helps people to understand its capability faster. README.md \u00b6 Each directory (apart from Java packages) should have a README.md file that describes the content of the directory. These files are displayed automatically by GitHub when the directory is accessed and this helps someone to understand the structure while navigating through the directories. The exception is that directories representing Java packages do not need README files because they are covered by Javadoc. Javadoc \u00b6 Javadoc is used to build a code reference for our public site. It is generated as part of the build. There are three places where Javadoc should be provided by the developer of Java code: Every Java source file should begin with a header Javadoc tag just before the start of the class/interface/enum, which explains the purpose and responsibilities of the code. All public methods should have a clear Javadoc header describing the purpose, parameters and results (including exceptions). This includes test cases. Each Java package should include a package-info.java file describing the purpose of the package and its content. Java code files may have additional comments, particularly where the processing is complex. The most useful comments are those that describe the purpose, or intent of the code, rather than a description of what each line of code is doing. The output from a build should be checked to ensure there are no Javadoc warnings: for example about undocumented parameters or exceptions. Log through ALF \u00b6 Egeria will typically be embedded in complex deployment environments. This means that we cannot rely on standard developer logging provided by components such as SLF4J. Instead, we use First Failure Data Capture (FFDC) through the Audit Log Framework ( ALF ) . Be consistent with style and layout \u00b6 There are many coding and layout styles that provide clear and readable code. Developers can choose the layout they prefer but with the following restrictions / suggestions: Try to use full words rather than abbreviations or shortened versions of a word for names such as class names, method names and variable names. Cryptic names create more effort for the reader to follow the code. Use the same style throughout a file. If changing an existing file, use the same style and layout as the original developer. Do not impose your own style in the middle of the code since the inconsistency that you introduce makes the whole file harder to read. It should not be possible to see where you have made the changes once the code is committed into git. For Java unit tests use /src/test/java folder of the module (standard Maven location), and postfix Java file names for tests with the word Test . Dates and times \u00b6 In Egeria, date / time instants are always represented as Unix Epoch time with millisecond precision (milliseconds elapsed since January 1, 1970). The Egeria OMRS layer handles date / time as either java.lang.Long or as java.util.Date objects. It does not store localised versions of the date / time. In other Egeria APIs that might be developed, it is strongly recommended to store dates and times as a Long or Date. In addition, it is possible to expose localised date representations if required. Write tests \u00b6 Egeria is an integration technology which means that it uses a comprehensive multi-level approach to testing. Modules include unit tests. These unit tests should focus on simple validation of Java beans, utilities and code that can easily be tested in isolation. The unit tests run as part of the build and a pull request cannot be incorporated into master if any unit tests are failing. They should not significantly extend the time of the build since this impacts all the contributors' productivity. Our preferred Java frameworks for unit testing are TestNG and Mockito . External APIs (typically they include both a client and a server component) are tested using functional verification tests (FVTs). These are located in the open-metadata-test/open-metadata-fvt module. The aim of these tests is to check that the APIs validate all of their parameters and function correctly in a single server environment. These tests also operate as part of the build but are not run as part of the PR process. Modules should ensure they include some FVTs as they move from development to technical preview . By the time the module is moving to released function, the FVTs should be able to validate that this function is stable and correct. Some connectors are tested via the Conformance Test Suite . If you deliver a connector that is covered by this test suite, you should run the tests before merging changes into master. The conformance test suite is also run as part of the release process. Egeria's hands on labs provide a complex multi-server environment and are typically used by contributors to verify that their changes have not regressed any of the basic function. We are also interested in building out a comprehensive integration test to allow automated complex multi-server scenarios that can be running continuously. Sign commits to accept DCO \u00b6 We have tried to make it as easy as possible to make contributions. This applies to how we handle the legal aspects of contribution. We simply ask that when submitting a patch for review, the developer must include a sign-off statement in the commit message. This is the same approach that the Linux\u00ae Kernel community uses to manage code contributions. Here is an example Signed-off-by line, which indicates that the submitter accepts the DCO: Signed-off-by: John Doe <john.doe@hisdomain.com> You can include this automatically when you commit a change to your local git repository using: Include DCO automatically when committing changes $ git commit -s By signing your work, you are confirming that the origin of the content makes it suitable to add to this project. See Developer Certificate of Origin (DCO) . Review code changes \u00b6 If you are asked to review a code change it will be located in a pull request ( PR ) on one of Egeria's git repositories . Within the pull requests are a number of commits that describe the changes to particular files that will be made when the pull request is merged into the repository. As a reviewer, you need to look at the code changes and satisfy yourself that: The code change is neat and readable and follows the code style of the rest of the module. The logic is clear and there are comments if the logic is complex. The code does not have any obvious defects - such as likely to cause a NullPointerException . There are no uses of log.error() for logging errors that are not accompanied by an equivalent message to the audit log. If new dependencies have been added, these are documented in the developer resources. If changes to the types have been made, these changes are: only made to the current release's types (that is, in OpenMetadataTypesArchive.java ). It is permissible to correct typos in the other files but not change the shape of the types in the types created in previous releases (in files called OpenMetadataTypesArchiveX_X.java ). documented in UML diagrams in the drawio files and the diagram has been exported as an image. If you are also the code owner of the changed code then you also need to be sure that the changes are consistent with the current and intended future design of the module. Create samples \u00b6 Postman artifacts for APIs \u00b6 We tend to use Postman to test the various API endpoints we develop in Egeria. As such, there are a number of samples we make available for anyone to use for testing or otherwise becoming familiar with the Egeria APIs. Disable SSL verification in Postman Egeria by default uses https:// requests with a self-signed certificate. Any Postman users therefore will need to go into Settings -> General and turn off SSL certificate verification or requests will fail. When developing a new API in Egeria, you may want to make similar samples available to both provide examples of using the API and for basic testing purposes. These should be developed as follows: Wherever possible, re-use the environment variables that are already defined in Egeria.postman_environment.json . If you need another variable that is not already defined, add it to this environment definition. This way we have a single environment definition that covers all possible sample configurations. Create a Postman Collection that includes REST samples for your API. Name it using the convention Egeria-<area>-<operations> where <area> represents the unique area of your API (for example the name of an OMAS ) and <operations> can optionally be used to distinguish between multiple collections that may be useful for different purposes (e.g. read vs. write operations). Consider adding test scripts to your collection to check expected values, if you intend to use them for testing purposes. Once ready for sharing, export the collection into a file and commit your collection into GitHub wherever is most appropriate for the anticipated users of the samples. Create a descriptive entry in postman-rest-samples/README.md under a sub-section of the Sample Collections heading, linking to your new collection within GitHub. Use the existing samples defined there for guidance: provide a limited introductory description to any pre-requisites for your collection, if it needs to be run after some other collection define these in a sequence, etc. If your description for use requires more than 1-2 simple sentences, consider linking to more detailed instructions rather than putting these all into the general README . (See samples where we link out to more information on loading Coco Pharmaceuticals samples rather than embedding all of this detail directly in the one README .) Within your descriptive entry, link to your collection. Following the other examples, provide a link to the raw file so that the link itself can be copy / pasted into Postman (without needing to download the file and then import it). In this way, anyone wanting access to the REST samples of Egeria has a single place from which to find them, while those working in a particular area of Egeria can still find the appropriate samples for that area directly within the area of interest. Manage dependencies \u00b6 New dependencies must only be introduced with the agreement of the broader community. These include frameworks, utility classes, annotations and external packages. This may seem annoying but there are good reasons for this: The Egeria code needs to be embeddable in different vendor products. This is made easier by keeping the code libraries we are dependent on to the minimum in order to avoid conflicts with libraries a consuming vendor may have already chosen, or where it needs to be embedded in an environment where certain dependencies may not be available. As developers, we have legal obligations to ensure we only use appropriately licensed software in our work and part of the discussion related a new dependency is to understand its license. Some projects may provide useful functionality but are only supported by one person who may get bored with it, or no longer have the time to support it. We should aim to build on dependent libraries that are backed by a strong community or vendor. Each library function, or set of annotations, adds to the learning curve of new people joining the team. By only bringing in the really beneficial libraries we ensure that the complexity they see relates only to the complexity of the problem space, rather than the additional complexity we have introduced in pursuit of playing with new functions. Each additional library extends the code footprint on which Egeria is based, and this inevitably extends the potential security exposure footprint. Limiting the libraries we use allows us to more quickly focus on resolving any potential security concerns (CVEs) any particular library may introduce. If a developer wishes to introduce a new dependency to the Egeria project, they should prepare a short guide (in a markdown file) that explains the value of the new library, how it is to be used and links to more information. They should then present their recommendation to the community and if agreed by the community, store the guide in the developer resources. Once in place, the dependency should be maintained across the smallest appropriate number of modules, and should be consistent throughout: particularly when it may impact consuming technologies. General rules \u00b6 Calls to third party technology that Egeria is integrating must be isolated into connectors so that they are optional. Try to use standard Java and Egeria's existing dependencies where possible - consider carefully if a new dependency is needed. Always define the dependency at the lowest-level pom.xml where it's needed. Use a current non-beta version of a dependency. Check build output carefully for any dependency warnings and errors. Do not add any exceptions to the existing rules without discussion with other maintainers. slf4j and bindings \u00b6 Any utility, sample, tool or other applications (like the server chassis) that have an entry point (typically main() ) should include a binding for slf4j . Use logback when possible (for example, ch.qos.logback:logback-classic ). Do not provide a configuration file: default formatting will be used and can be overriden by logback configuration at deployment time. Test code automatically includes slf4j-simple - a simple logging implementation Other code that forms libraries (most of our code) must not include a slf4j logging implementation. Otherwise, the application loses control of the logging implementation, hidden config files can change behavior, and a multiple_bindings issue will be raised by slf4j . Understanding dependencies \u00b6 Running mvn dependency:tree is a useful way to understand what dependencies (direct and transitive) a module has. Adding a new dependency \u00b6 Check if the dependency is already listed in the top-level pom.xml . If not, add a section such as the following within the <dependencyManagement> section of the top-level pom.xml : Example dependency entry in top-level pom.xml 1 2 3 4 5 6 <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-clients </artifactId> <scope> compile </scope> <version> ${kafka.version} </version> </dependency> This declaration only means that if a dependency is used, these are the defaults to use -- most critically including version, though scope is a useful default to add, too: for example if the dependency is only for tests. Add the dependency to the <dependency> section of your module's pom.xml : Example dependency entry in module's pom.xml 1 2 3 4 <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-clients </artifactId> </dependency> Note that the version is not included - it will be picked up from <dependencyManagement> . Now build to include some checks for correct usage of dependencies (see below): Build Egeria mvn clean install More on scopes Most dependencies will be of scope compile (used for build and runtime), or test (for test tools). There are other scopes available that you may want to use in specific circumstances . Build time checks \u00b6 The top-level pom.xml defines checks that are run in reference to dependencies: the maven dependency plugin analyze-only goal is used to check that any dependencies referred to in the object code are declared as dependencies, and that any not used are not. Any discrepancies will be reported as part of the build. Occasionally exceptions may be required, generally for dependencies that are only needed at runtime. the maven dependency plugin analyze-dep-mgt goal is used to check all dependencies declared are of the same version as that in dependencyManagement in the top-level pom.xml the maven enforcer plugin enforce goal is used with the following rules: reactorModuleConvergence checks for correct parent/child relationships and inconsistency requireUpperBoundDeps checks that minimum versions are satisfied for all transitive dependencies. If any of these checks fail an appropriate message will be displayed and the build will fail. Incompatible versions In some cases where incompatible versions are reported, it may be due to transitive dependencies: for example a component the Egeria code does not depend on directly, but only indirectly. The path to resolve the version could result in different versions being used, or at least attempted, then failing. To resolve this a reference can be added in <dependencyManagement> to specify the version to use. Maintain security \u00b6 Egeria's dependencies are scanned for potential CVEs automatically in two main ways: GitHub scans dependencies for known CVEs. A weekly Nexus CLM scan is run. The maintainers will review these regularly and action any required changes through issues and pull requests. Egeria code itself is also scanned for vulnerabilities using Sonar . Any developer can perform similar checks by running: Perform security scans mvn clean install -DfindBugs This will run (and create a file for each module): Goal(s) Output file(s) spotBugs including findsecbugs spotBugsXml.xml pmd pmd.xml OWASP dependency checker dependency-check-report.html May take more than an hour Note that the scan may take a long time - an hour or more for all checks. Handling memory requirements If running against all components (i.e. from the root) an invocation like the following may be needed due to the memory requirements of a security scan: Run with additional memory MAVEN_OPTS=\"-Xmx5000M -Xss512M -XX:MaxPermSize=2048M -XX:+CMSClassUnloadingEnabled -XX:+UseConcMarkSweepGC\" mvn clean install -DfindBugs For more information on how potential security issues are handled, see security hardening .","title":"Developer Guidelines"},{"location":"guides/developer/guidelines/#developer-guidelines","text":"Egeria provides technology for an open standard that seeks to improve the processing and protection of data across organizations. For its developers, this carries the benefit that their work receives high recognition, but also additional responsibilities to ensure its wide applicability and longevity. For example, Egeria seeks a broad audience - from developers to adopting vendors to consuming users. Building this audience and allowing the community to scale requires clarity in the way the software is written, documented, packaged and used. Many of the guidelines seek to make it easier for someone new to pick up the software, at the expense of maybe a little more work, or a little less freedom of action for the original developer. As such, these guidelines exist to remind us of these broader responsibilities.","title":"Developer Guidelines"},{"location":"guides/developer/guidelines/#build-environment","text":"The core of Egeria is written primarily in Java , and the minimum level required to build and run it is 11. Most developers use MacOS, while our official builds use Linux (Ubuntu/Centos/RHEL should all be fine). Windows is unsupported The traditional Windows environment is not directly supported. It is recommended to use WSL2 which offers a full Linux environment. Apache Maven is used to control the builds, and 3.5 or higher is required to build Egeria (3.6.x or above is recommended). Gradle is not currently supported but is being developed. IDEs can make navigating the Egeria code easier. Each IDE can vary a lot. Many of our team use JetBrains IntelliJ . In the case of problems the first problem determination step is to check you can build Egeria normally at the command line i.e. mvn clean install from the source root. That will prove at least Java and Maven are correct. Set JAVA_HOME We have also noticed that you need to ensure JAVA_HOME is set or the build will fail running Javadoc.","title":"Build environment"},{"location":"guides/developer/guidelines/#eliminate-any-build-warnings","text":"Build output should be checked for any warnings, i.e. [WARNING] , and these should be eliminated. The Java compiler is set to use -Xlint:all and may report warnings about deprecated function, unsafe casts, unchecked conversions, and so on -- all of which should be addressed. Other tools used in the build may also result in warnings which should also be addressed, whilst test cases should ensure output is captured to avoid such warnings appear in the build logs.","title":"Eliminate any build warnings"},{"location":"guides/developer/guidelines/#include-license-in-every-file","text":"All files for Egeria should have a license included. We use the SPDX encoding to keep the headers simple. License header for documentation ( .md ) 1 2 <!-- SPDX-License-Identifier: CC-BY-4.0 --> <!-- Copyright Contributors to the Egeria project. --> Note that we no longer need to include an explicit footer in documentation files, as this is already included in the overall documentation site as the footer of every page. License header for XML files ( .xml ) 1 2 3 <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!-- SPDX-License-Identifier: Apache-2.0 --> <!-- Copyright Contributors to the Egeria project. --> License header for Java code ( .java ) 1 2 /* SPDX-License-Identifier: Apache-2.0 */ /* Copyright Contributors to the Egeria project. */","title":"Include license in every file"},{"location":"guides/developer/guidelines/#document","text":"Although all code for Egeria should be clear and easy to read, the code itself can only describe what it is doing: it can rarely describe why it is doing it. Also, the Egeria codebase is quite large and hard to digest in one go. Having summaries of its behavior and philosophy helps people to understand its capability faster.","title":"Document"},{"location":"guides/developer/guidelines/#readmemd","text":"Each directory (apart from Java packages) should have a README.md file that describes the content of the directory. These files are displayed automatically by GitHub when the directory is accessed and this helps someone to understand the structure while navigating through the directories. The exception is that directories representing Java packages do not need README files because they are covered by Javadoc.","title":"README.md"},{"location":"guides/developer/guidelines/#javadoc","text":"Javadoc is used to build a code reference for our public site. It is generated as part of the build. There are three places where Javadoc should be provided by the developer of Java code: Every Java source file should begin with a header Javadoc tag just before the start of the class/interface/enum, which explains the purpose and responsibilities of the code. All public methods should have a clear Javadoc header describing the purpose, parameters and results (including exceptions). This includes test cases. Each Java package should include a package-info.java file describing the purpose of the package and its content. Java code files may have additional comments, particularly where the processing is complex. The most useful comments are those that describe the purpose, or intent of the code, rather than a description of what each line of code is doing. The output from a build should be checked to ensure there are no Javadoc warnings: for example about undocumented parameters or exceptions.","title":"Javadoc"},{"location":"guides/developer/guidelines/#log-through-alf","text":"Egeria will typically be embedded in complex deployment environments. This means that we cannot rely on standard developer logging provided by components such as SLF4J. Instead, we use First Failure Data Capture (FFDC) through the Audit Log Framework ( ALF ) .","title":"Log through ALF"},{"location":"guides/developer/guidelines/#be-consistent-with-style-and-layout","text":"There are many coding and layout styles that provide clear and readable code. Developers can choose the layout they prefer but with the following restrictions / suggestions: Try to use full words rather than abbreviations or shortened versions of a word for names such as class names, method names and variable names. Cryptic names create more effort for the reader to follow the code. Use the same style throughout a file. If changing an existing file, use the same style and layout as the original developer. Do not impose your own style in the middle of the code since the inconsistency that you introduce makes the whole file harder to read. It should not be possible to see where you have made the changes once the code is committed into git. For Java unit tests use /src/test/java folder of the module (standard Maven location), and postfix Java file names for tests with the word Test .","title":"Be consistent with style and layout"},{"location":"guides/developer/guidelines/#dates-and-times","text":"In Egeria, date / time instants are always represented as Unix Epoch time with millisecond precision (milliseconds elapsed since January 1, 1970). The Egeria OMRS layer handles date / time as either java.lang.Long or as java.util.Date objects. It does not store localised versions of the date / time. In other Egeria APIs that might be developed, it is strongly recommended to store dates and times as a Long or Date. In addition, it is possible to expose localised date representations if required.","title":"Dates and times"},{"location":"guides/developer/guidelines/#write-tests","text":"Egeria is an integration technology which means that it uses a comprehensive multi-level approach to testing. Modules include unit tests. These unit tests should focus on simple validation of Java beans, utilities and code that can easily be tested in isolation. The unit tests run as part of the build and a pull request cannot be incorporated into master if any unit tests are failing. They should not significantly extend the time of the build since this impacts all the contributors' productivity. Our preferred Java frameworks for unit testing are TestNG and Mockito . External APIs (typically they include both a client and a server component) are tested using functional verification tests (FVTs). These are located in the open-metadata-test/open-metadata-fvt module. The aim of these tests is to check that the APIs validate all of their parameters and function correctly in a single server environment. These tests also operate as part of the build but are not run as part of the PR process. Modules should ensure they include some FVTs as they move from development to technical preview . By the time the module is moving to released function, the FVTs should be able to validate that this function is stable and correct. Some connectors are tested via the Conformance Test Suite . If you deliver a connector that is covered by this test suite, you should run the tests before merging changes into master. The conformance test suite is also run as part of the release process. Egeria's hands on labs provide a complex multi-server environment and are typically used by contributors to verify that their changes have not regressed any of the basic function. We are also interested in building out a comprehensive integration test to allow automated complex multi-server scenarios that can be running continuously.","title":"Write tests"},{"location":"guides/developer/guidelines/#sign-commits-to-accept-dco","text":"We have tried to make it as easy as possible to make contributions. This applies to how we handle the legal aspects of contribution. We simply ask that when submitting a patch for review, the developer must include a sign-off statement in the commit message. This is the same approach that the Linux\u00ae Kernel community uses to manage code contributions. Here is an example Signed-off-by line, which indicates that the submitter accepts the DCO: Signed-off-by: John Doe <john.doe@hisdomain.com> You can include this automatically when you commit a change to your local git repository using: Include DCO automatically when committing changes $ git commit -s By signing your work, you are confirming that the origin of the content makes it suitable to add to this project. See Developer Certificate of Origin (DCO) .","title":"Sign commits to accept DCO"},{"location":"guides/developer/guidelines/#review-code-changes","text":"If you are asked to review a code change it will be located in a pull request ( PR ) on one of Egeria's git repositories . Within the pull requests are a number of commits that describe the changes to particular files that will be made when the pull request is merged into the repository. As a reviewer, you need to look at the code changes and satisfy yourself that: The code change is neat and readable and follows the code style of the rest of the module. The logic is clear and there are comments if the logic is complex. The code does not have any obvious defects - such as likely to cause a NullPointerException . There are no uses of log.error() for logging errors that are not accompanied by an equivalent message to the audit log. If new dependencies have been added, these are documented in the developer resources. If changes to the types have been made, these changes are: only made to the current release's types (that is, in OpenMetadataTypesArchive.java ). It is permissible to correct typos in the other files but not change the shape of the types in the types created in previous releases (in files called OpenMetadataTypesArchiveX_X.java ). documented in UML diagrams in the drawio files and the diagram has been exported as an image. If you are also the code owner of the changed code then you also need to be sure that the changes are consistent with the current and intended future design of the module.","title":"Review code changes"},{"location":"guides/developer/guidelines/#create-samples","text":"","title":"Create samples"},{"location":"guides/developer/guidelines/#postman-artifacts-for-apis","text":"We tend to use Postman to test the various API endpoints we develop in Egeria. As such, there are a number of samples we make available for anyone to use for testing or otherwise becoming familiar with the Egeria APIs. Disable SSL verification in Postman Egeria by default uses https:// requests with a self-signed certificate. Any Postman users therefore will need to go into Settings -> General and turn off SSL certificate verification or requests will fail. When developing a new API in Egeria, you may want to make similar samples available to both provide examples of using the API and for basic testing purposes. These should be developed as follows: Wherever possible, re-use the environment variables that are already defined in Egeria.postman_environment.json . If you need another variable that is not already defined, add it to this environment definition. This way we have a single environment definition that covers all possible sample configurations. Create a Postman Collection that includes REST samples for your API. Name it using the convention Egeria-<area>-<operations> where <area> represents the unique area of your API (for example the name of an OMAS ) and <operations> can optionally be used to distinguish between multiple collections that may be useful for different purposes (e.g. read vs. write operations). Consider adding test scripts to your collection to check expected values, if you intend to use them for testing purposes. Once ready for sharing, export the collection into a file and commit your collection into GitHub wherever is most appropriate for the anticipated users of the samples. Create a descriptive entry in postman-rest-samples/README.md under a sub-section of the Sample Collections heading, linking to your new collection within GitHub. Use the existing samples defined there for guidance: provide a limited introductory description to any pre-requisites for your collection, if it needs to be run after some other collection define these in a sequence, etc. If your description for use requires more than 1-2 simple sentences, consider linking to more detailed instructions rather than putting these all into the general README . (See samples where we link out to more information on loading Coco Pharmaceuticals samples rather than embedding all of this detail directly in the one README .) Within your descriptive entry, link to your collection. Following the other examples, provide a link to the raw file so that the link itself can be copy / pasted into Postman (without needing to download the file and then import it). In this way, anyone wanting access to the REST samples of Egeria has a single place from which to find them, while those working in a particular area of Egeria can still find the appropriate samples for that area directly within the area of interest.","title":"Postman artifacts for APIs"},{"location":"guides/developer/guidelines/#manage-dependencies","text":"New dependencies must only be introduced with the agreement of the broader community. These include frameworks, utility classes, annotations and external packages. This may seem annoying but there are good reasons for this: The Egeria code needs to be embeddable in different vendor products. This is made easier by keeping the code libraries we are dependent on to the minimum in order to avoid conflicts with libraries a consuming vendor may have already chosen, or where it needs to be embedded in an environment where certain dependencies may not be available. As developers, we have legal obligations to ensure we only use appropriately licensed software in our work and part of the discussion related a new dependency is to understand its license. Some projects may provide useful functionality but are only supported by one person who may get bored with it, or no longer have the time to support it. We should aim to build on dependent libraries that are backed by a strong community or vendor. Each library function, or set of annotations, adds to the learning curve of new people joining the team. By only bringing in the really beneficial libraries we ensure that the complexity they see relates only to the complexity of the problem space, rather than the additional complexity we have introduced in pursuit of playing with new functions. Each additional library extends the code footprint on which Egeria is based, and this inevitably extends the potential security exposure footprint. Limiting the libraries we use allows us to more quickly focus on resolving any potential security concerns (CVEs) any particular library may introduce. If a developer wishes to introduce a new dependency to the Egeria project, they should prepare a short guide (in a markdown file) that explains the value of the new library, how it is to be used and links to more information. They should then present their recommendation to the community and if agreed by the community, store the guide in the developer resources. Once in place, the dependency should be maintained across the smallest appropriate number of modules, and should be consistent throughout: particularly when it may impact consuming technologies.","title":"Manage dependencies"},{"location":"guides/developer/guidelines/#general-rules","text":"Calls to third party technology that Egeria is integrating must be isolated into connectors so that they are optional. Try to use standard Java and Egeria's existing dependencies where possible - consider carefully if a new dependency is needed. Always define the dependency at the lowest-level pom.xml where it's needed. Use a current non-beta version of a dependency. Check build output carefully for any dependency warnings and errors. Do not add any exceptions to the existing rules without discussion with other maintainers.","title":"General rules"},{"location":"guides/developer/guidelines/#slf4j-and-bindings","text":"Any utility, sample, tool or other applications (like the server chassis) that have an entry point (typically main() ) should include a binding for slf4j . Use logback when possible (for example, ch.qos.logback:logback-classic ). Do not provide a configuration file: default formatting will be used and can be overriden by logback configuration at deployment time. Test code automatically includes slf4j-simple - a simple logging implementation Other code that forms libraries (most of our code) must not include a slf4j logging implementation. Otherwise, the application loses control of the logging implementation, hidden config files can change behavior, and a multiple_bindings issue will be raised by slf4j .","title":"slf4j and bindings"},{"location":"guides/developer/guidelines/#understanding-dependencies","text":"Running mvn dependency:tree is a useful way to understand what dependencies (direct and transitive) a module has.","title":"Understanding dependencies"},{"location":"guides/developer/guidelines/#adding-a-new-dependency","text":"Check if the dependency is already listed in the top-level pom.xml . If not, add a section such as the following within the <dependencyManagement> section of the top-level pom.xml : Example dependency entry in top-level pom.xml 1 2 3 4 5 6 <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-clients </artifactId> <scope> compile </scope> <version> ${kafka.version} </version> </dependency> This declaration only means that if a dependency is used, these are the defaults to use -- most critically including version, though scope is a useful default to add, too: for example if the dependency is only for tests. Add the dependency to the <dependency> section of your module's pom.xml : Example dependency entry in module's pom.xml 1 2 3 4 <dependency> <groupId> org.apache.kafka </groupId> <artifactId> kafka-clients </artifactId> </dependency> Note that the version is not included - it will be picked up from <dependencyManagement> . Now build to include some checks for correct usage of dependencies (see below): Build Egeria mvn clean install More on scopes Most dependencies will be of scope compile (used for build and runtime), or test (for test tools). There are other scopes available that you may want to use in specific circumstances .","title":"Adding a new dependency"},{"location":"guides/developer/guidelines/#build-time-checks","text":"The top-level pom.xml defines checks that are run in reference to dependencies: the maven dependency plugin analyze-only goal is used to check that any dependencies referred to in the object code are declared as dependencies, and that any not used are not. Any discrepancies will be reported as part of the build. Occasionally exceptions may be required, generally for dependencies that are only needed at runtime. the maven dependency plugin analyze-dep-mgt goal is used to check all dependencies declared are of the same version as that in dependencyManagement in the top-level pom.xml the maven enforcer plugin enforce goal is used with the following rules: reactorModuleConvergence checks for correct parent/child relationships and inconsistency requireUpperBoundDeps checks that minimum versions are satisfied for all transitive dependencies. If any of these checks fail an appropriate message will be displayed and the build will fail. Incompatible versions In some cases where incompatible versions are reported, it may be due to transitive dependencies: for example a component the Egeria code does not depend on directly, but only indirectly. The path to resolve the version could result in different versions being used, or at least attempted, then failing. To resolve this a reference can be added in <dependencyManagement> to specify the version to use.","title":"Build time checks"},{"location":"guides/developer/guidelines/#maintain-security","text":"Egeria's dependencies are scanned for potential CVEs automatically in two main ways: GitHub scans dependencies for known CVEs. A weekly Nexus CLM scan is run. The maintainers will review these regularly and action any required changes through issues and pull requests. Egeria code itself is also scanned for vulnerabilities using Sonar . Any developer can perform similar checks by running: Perform security scans mvn clean install -DfindBugs This will run (and create a file for each module): Goal(s) Output file(s) spotBugs including findsecbugs spotBugsXml.xml pmd pmd.xml OWASP dependency checker dependency-check-report.html May take more than an hour Note that the scan may take a long time - an hour or more for all checks. Handling memory requirements If running against all components (i.e. from the root) an invocation like the following may be needed due to the memory requirements of a security scan: Run with additional memory MAVEN_OPTS=\"-Xmx5000M -Xss512M -XX:MaxPermSize=2048M -XX:+CMSClassUnloadingEnabled -XX:+UseConcMarkSweepGC\" mvn clean install -DfindBugs For more information on how potential security issues are handled, see security hardening .","title":"Maintain security"},{"location":"guides/developer/implement-a-connector/","text":"Develop a Connector \u00b6 When you want to connect to a tool or system from an existing service, you need to create an open connector for that tool or system. For example, you might want to connect a new metadata repository into Egeria, or connect Egeria with a new data processing engine. To write an open connector you need to complete four steps: Identify the properties for the connection . Write the connector provider . Understand the interface the connector needs to implement. Write the connector itself. All the code you write to implement these should exist in its own module, and as illustrated by the examples could even be in its own independent code repository. Their implementation will have dependencies on Egeria's: Open Connector Framework ( OCF ) Audit Log Framework ( ALF ) Specific interfaces used by the type of connector No dependency on Egeria's OMAG Server Platform Note that there is no dependency on Egeria's OMAG Server Platform for these specific connector implementations: they could run in another runtime that supported the connector APIs. Identify connection properties \u00b6 Begin by identifying and designing the properties needed to connect to your tool or system. These will commonly include a network address, protocol, and user credentials, but could also include other information. Code the connector provider \u00b6 The connector provider is a simple Java factory that implements the creation of the connector type it can instantiate using: a GUID for the connector type a name for the connector type a description of what the connector is for and how to configure it the connector class it instantiates a list of the additional properties, configuration properties and secured properties needed to configure instances of the connector Example: connector provider for IBM DataStage For example, the DataStageConnectorProvider is used to instantiate connectors to IBM DataStage data processing engines. Therefore, its name and description refer to DataStage, and the connectors it instantiates are DataStageConnector s . Example: connector provider for IBM Information Governance Catalog Similarly, the IGCOMRSRepositoryConnectorProvider is used to instantiate connectors to IBM Information Governance Catalog ( IGC ) metadata repositories. In contrast to the DataStageConnectorProvider , the IGCOMRSRepositoryConnectorProvider 's name and description refer to IGC , and the connectors it instantiates are IGCOMRSRepositoryConnector s . Connectors implement Egeria interfaces, not vice versa Note that the code of all of these connector implementations exists outside Egeria itself (in separate code repositories), and there are no direct dependencies within Egeria on these external repositories or connectors. All connectors can be configured with the network address and credential information needed to access the underlying tool or system. Therefore, you do not need to explicitly list properties for such basic details. However, the names of any additional configuration properties that may be useful to a specific type of connector can be described through the recognizedConfigurationProperties of the connector type. Implementation pattern \u00b6 From the two examples ( DataStageConnectorProvider and IGCOMRSRepositoryConnectorProvider ), you will see that writing a connector provider follows a simple pattern: Extend a connector provider base class specific to your connector's interface. Define static final class members for the GUID , name, description and the names of any additional configuration properties. Write a single public constructor, with no parameters, that: Calls super.setConnectorClassName() with the name of your connector class. Creates a new ConnectorType object, sets it characteristics to the static final class members, and uses .setConnectorProviderClassName() to set the name of the connector provider class itself. (Optional) Creates a list of additional configuration properties from the static final class members, and uses .setRecognizedConfigurationProperties() to add these to the connector type. Sets super.connectorTypeBean = connectorType . Understand the connector interface \u00b6 Now that you have the connector provider to instantiate your connector , you need to understand what your connector actually needs to do. For a service to use your connector, the connector must provide a set of methods that are relevant to that service. Example: data engine proxy connector interface For example, the data engine proxy services integrate metadata from data engines with Egeria. To integrate DataStage with Egeria, we want our DataStageConnector to be used by the data engine proxy services. Therefore, the connector needs to extend DataEngineConnectorBase , because this defines the methods needed by the data engine proxy services. Example: OMRS repository connector interface Likewise, we want our IGCOMRSRepositoryConnector to integrate IGC with Egeria as a metadata repository. Therefore, the connector needs to extend OMRSRepositoryConnector , because this defines the methods needed to integrate with Open Metadata Repository Services ( OMRS ) . How would you know to extend these base classes? The connector provider implementations in the previous step each extended a base class specific to the type of connector they provide ( DataEngineConnectorProviderBase and OMRSRepositoryConnectorProviderBase ). These connector base classes ( DataEngineConnectorBase and OMRSRepositoryConnector ) are in the same package structure as those connector provider base classes. In both cases, by extending the abstract classes ( DataEngineConnectorBase and OMRSRepositoryConnector ) your connector must implement the methods these abstract classes define. These general methods implement your services (data engine proxy services and OMRS ), without needing to know anything about the underlying technology. Therefore, you can simply \"plug-in\" the underlying technology: any technology with a connector that implements these methods can run your service. Furthermore, each technology-specific connector can decide how best to implement those methods for itself. Code the connector itself \u00b6 Which brings you to writing the connector itself. Now that you understand the interface your connector must provide, you need to implement the methods defined by that interface. Implement the connector by: Retrieving connection information provided by the configuration. The default method for initialize() saves the connection object used to create the connector. If your connector needs to override the initialize() method, it should call super.initialize() to capture the connection properties for the base classes. Implementing the start() method, where the main logic for your connector runs. Use the configuration details from the connection object to connect to your underlying technology. If the connector is long-running, this may be the time to start up a separate thread. However, this has to conform the rules laid down for the category of connector you are implementing. Using pre-existing, technology-specific clients and APIs to talk to your underlying technology. Translating the underlying technology's representation of information into the open metadata representation used by the connector interface itself. For the first point, you can retrieve general connection information like: the server address and protocol, by first retrieving the embedded EndpointProperties with getEndpoint() : retrieving the protocol by calling getProtocol() on the EndpointProperties retrieving the address by calling getAddress() on the EndpointProperties the user Id, by calling getUserId() on the ConnectionProperties the password, by calling either getClearPassword() or getEncryptedPassword() on the ConnectionProperties , depending on what your underlying technology can handle Use these details to connect to and authenticate against your underlying technology, even when it is running on a different system from the connector itself. Of course, check for null objects (like the EndpointProperties ) as well before blindly operating on them. Retrieve additional properties by: calling getConfigurationProperties() on the ConnectionProperties , which returns a Map<String, Object> calling get(name) against that Map<> with the name of each additional property of interest Implementation of the remaining points (2-3) will vary widely depending on the specific technology being used. See the examples previously linked to delve deeper.","title":"Develop a Connector"},{"location":"guides/developer/implement-a-connector/#develop-a-connector","text":"When you want to connect to a tool or system from an existing service, you need to create an open connector for that tool or system. For example, you might want to connect a new metadata repository into Egeria, or connect Egeria with a new data processing engine. To write an open connector you need to complete four steps: Identify the properties for the connection . Write the connector provider . Understand the interface the connector needs to implement. Write the connector itself. All the code you write to implement these should exist in its own module, and as illustrated by the examples could even be in its own independent code repository. Their implementation will have dependencies on Egeria's: Open Connector Framework ( OCF ) Audit Log Framework ( ALF ) Specific interfaces used by the type of connector No dependency on Egeria's OMAG Server Platform Note that there is no dependency on Egeria's OMAG Server Platform for these specific connector implementations: they could run in another runtime that supported the connector APIs.","title":"Develop a Connector"},{"location":"guides/developer/implement-a-connector/#identify-connection-properties","text":"Begin by identifying and designing the properties needed to connect to your tool or system. These will commonly include a network address, protocol, and user credentials, but could also include other information.","title":"Identify connection properties"},{"location":"guides/developer/implement-a-connector/#code-the-connector-provider","text":"The connector provider is a simple Java factory that implements the creation of the connector type it can instantiate using: a GUID for the connector type a name for the connector type a description of what the connector is for and how to configure it the connector class it instantiates a list of the additional properties, configuration properties and secured properties needed to configure instances of the connector Example: connector provider for IBM DataStage For example, the DataStageConnectorProvider is used to instantiate connectors to IBM DataStage data processing engines. Therefore, its name and description refer to DataStage, and the connectors it instantiates are DataStageConnector s . Example: connector provider for IBM Information Governance Catalog Similarly, the IGCOMRSRepositoryConnectorProvider is used to instantiate connectors to IBM Information Governance Catalog ( IGC ) metadata repositories. In contrast to the DataStageConnectorProvider , the IGCOMRSRepositoryConnectorProvider 's name and description refer to IGC , and the connectors it instantiates are IGCOMRSRepositoryConnector s . Connectors implement Egeria interfaces, not vice versa Note that the code of all of these connector implementations exists outside Egeria itself (in separate code repositories), and there are no direct dependencies within Egeria on these external repositories or connectors. All connectors can be configured with the network address and credential information needed to access the underlying tool or system. Therefore, you do not need to explicitly list properties for such basic details. However, the names of any additional configuration properties that may be useful to a specific type of connector can be described through the recognizedConfigurationProperties of the connector type.","title":"Code the connector provider"},{"location":"guides/developer/implement-a-connector/#implementation-pattern","text":"From the two examples ( DataStageConnectorProvider and IGCOMRSRepositoryConnectorProvider ), you will see that writing a connector provider follows a simple pattern: Extend a connector provider base class specific to your connector's interface. Define static final class members for the GUID , name, description and the names of any additional configuration properties. Write a single public constructor, with no parameters, that: Calls super.setConnectorClassName() with the name of your connector class. Creates a new ConnectorType object, sets it characteristics to the static final class members, and uses .setConnectorProviderClassName() to set the name of the connector provider class itself. (Optional) Creates a list of additional configuration properties from the static final class members, and uses .setRecognizedConfigurationProperties() to add these to the connector type. Sets super.connectorTypeBean = connectorType .","title":"Implementation pattern"},{"location":"guides/developer/implement-a-connector/#understand-the-connector-interface","text":"Now that you have the connector provider to instantiate your connector , you need to understand what your connector actually needs to do. For a service to use your connector, the connector must provide a set of methods that are relevant to that service. Example: data engine proxy connector interface For example, the data engine proxy services integrate metadata from data engines with Egeria. To integrate DataStage with Egeria, we want our DataStageConnector to be used by the data engine proxy services. Therefore, the connector needs to extend DataEngineConnectorBase , because this defines the methods needed by the data engine proxy services. Example: OMRS repository connector interface Likewise, we want our IGCOMRSRepositoryConnector to integrate IGC with Egeria as a metadata repository. Therefore, the connector needs to extend OMRSRepositoryConnector , because this defines the methods needed to integrate with Open Metadata Repository Services ( OMRS ) . How would you know to extend these base classes? The connector provider implementations in the previous step each extended a base class specific to the type of connector they provide ( DataEngineConnectorProviderBase and OMRSRepositoryConnectorProviderBase ). These connector base classes ( DataEngineConnectorBase and OMRSRepositoryConnector ) are in the same package structure as those connector provider base classes. In both cases, by extending the abstract classes ( DataEngineConnectorBase and OMRSRepositoryConnector ) your connector must implement the methods these abstract classes define. These general methods implement your services (data engine proxy services and OMRS ), without needing to know anything about the underlying technology. Therefore, you can simply \"plug-in\" the underlying technology: any technology with a connector that implements these methods can run your service. Furthermore, each technology-specific connector can decide how best to implement those methods for itself.","title":"Understand the connector interface"},{"location":"guides/developer/implement-a-connector/#code-the-connector-itself","text":"Which brings you to writing the connector itself. Now that you understand the interface your connector must provide, you need to implement the methods defined by that interface. Implement the connector by: Retrieving connection information provided by the configuration. The default method for initialize() saves the connection object used to create the connector. If your connector needs to override the initialize() method, it should call super.initialize() to capture the connection properties for the base classes. Implementing the start() method, where the main logic for your connector runs. Use the configuration details from the connection object to connect to your underlying technology. If the connector is long-running, this may be the time to start up a separate thread. However, this has to conform the rules laid down for the category of connector you are implementing. Using pre-existing, technology-specific clients and APIs to talk to your underlying technology. Translating the underlying technology's representation of information into the open metadata representation used by the connector interface itself. For the first point, you can retrieve general connection information like: the server address and protocol, by first retrieving the embedded EndpointProperties with getEndpoint() : retrieving the protocol by calling getProtocol() on the EndpointProperties retrieving the address by calling getAddress() on the EndpointProperties the user Id, by calling getUserId() on the ConnectionProperties the password, by calling either getClearPassword() or getEncryptedPassword() on the ConnectionProperties , depending on what your underlying technology can handle Use these details to connect to and authenticate against your underlying technology, even when it is running on a different system from the connector itself. Of course, check for null objects (like the EndpointProperties ) as well before blindly operating on them. Retrieve additional properties by: calling getConfigurationProperties() on the ConnectionProperties , which returns a Map<String, Object> calling get(name) against that Map<> with the name of each additional property of interest Implementation of the remaining points (2-3) will vary widely depending on the specific technology being used. See the examples previously linked to delve deeper.","title":"Code the connector itself"},{"location":"guides/developer/implement-repository-connector/","text":"A common tripping point for conformance The routing behavior described for homed metadata instances can only be enforced when the requests go through OMRS itself. For third party tools that provide their own services / user interface through which updates can be made, a common tripping point becomes the fact that these services / user interfaces need to adhere to the same protocol principles outlined above -- specifically, ensuring reference copies are also immutable through these product-native interfaces -- in order to conform to the Egeria protocol. For cases where the tool is unable to do so, we are actively investigating other mitigation measures like providing a Smart Repository Proxy to ensure that any changes to metadata that violate the protocol remain isolated in that third party technology and are not inadvertently propagated elsewhere in the cohort.","title":"Develop Repository Connector"},{"location":"guides/developer/languages/","text":"Programming Languages \u00b6 Java \u00b6 Egeria's runtime and clients are written in Java. Java is a strongly-typed, compiled language. The resulting object code runs in a virtual machine called the Java Virtual Machine ( JVM ). The JVM is supported on most operating systems and so Java programs can run with the same behavior on almost any machine. This portability of code is why Java is used for the Egeria runtime (the OMAG Server Platform ) and the clients. If you want to run Egeria you need to install the Java Runtime Environment ( JRE ). To build and test Egeria, you need the Java Development Kit ( JDK ) installed. The JDK also contains the runtime environment ( JRE ). There are various JDK 's available, and you may even have one pre-installed on your system. Check for a pre-installed Java java -version Egeria requires Java 11 as a minimum level. Language constructs up to Java 11 are permitted, but not above. We use the Adoptium (formerly AdoptOpenJDK) distribution. Official images and maven artifacts are built with this level. Additionally, code must compile and run on the current latest Java release. This is validated before any code can be merged. Java can be installed by: Downloading the OpenJDK 11 (LTS) HotSpot JVM from Adoptium . Running the installer that is downloaded. Alternatively, JDK 's may be found on your operating system install repositories or via third party tools like HomeBrew on MacOS. Also, you must ensure JAVA_HOME is set, and pointing to a JDK . If this is not done, an error such as Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.1.1:jar (attach-javadocs) on project open-connector-framework: MavenReportException: Error while generating Javadoc: Unable to find javadoc command: The environment variable JAVA_HOME is not correctly set. will be seen as the Javadoc Maven plugin depends on this value to work correctly. Python \u00b6 Python is used in much of the educational material . Python is an interpreted programming language. It is favored by data scientists and script writers because it supports writing in snippets where variables can be created on the fly and are typically global. Egeria uses Python in the hands-on labs since it is the native language of the Jupyter Notebooks environment we use for the labs. Markdown \u00b6 Markdown is a simple tagging language that generates HTML webpages. We use it for documentation (this page is written in Markdown for example), in GitHub comments and in the Jupyter Notebooks that form the teaching material for the hand-on labs . GitHub provides a useful summary for Markdown and our own documentation guide provides Egeria-specific formatting and stylistic pointers, as well as further information on the system we use to translate such .md files into the website you are currently reading.","title":"Programming Languages"},{"location":"guides/developer/languages/#programming-languages","text":"","title":"Programming Languages"},{"location":"guides/developer/languages/#java","text":"Egeria's runtime and clients are written in Java. Java is a strongly-typed, compiled language. The resulting object code runs in a virtual machine called the Java Virtual Machine ( JVM ). The JVM is supported on most operating systems and so Java programs can run with the same behavior on almost any machine. This portability of code is why Java is used for the Egeria runtime (the OMAG Server Platform ) and the clients. If you want to run Egeria you need to install the Java Runtime Environment ( JRE ). To build and test Egeria, you need the Java Development Kit ( JDK ) installed. The JDK also contains the runtime environment ( JRE ). There are various JDK 's available, and you may even have one pre-installed on your system. Check for a pre-installed Java java -version Egeria requires Java 11 as a minimum level. Language constructs up to Java 11 are permitted, but not above. We use the Adoptium (formerly AdoptOpenJDK) distribution. Official images and maven artifacts are built with this level. Additionally, code must compile and run on the current latest Java release. This is validated before any code can be merged. Java can be installed by: Downloading the OpenJDK 11 (LTS) HotSpot JVM from Adoptium . Running the installer that is downloaded. Alternatively, JDK 's may be found on your operating system install repositories or via third party tools like HomeBrew on MacOS. Also, you must ensure JAVA_HOME is set, and pointing to a JDK . If this is not done, an error such as Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.1.1:jar (attach-javadocs) on project open-connector-framework: MavenReportException: Error while generating Javadoc: Unable to find javadoc command: The environment variable JAVA_HOME is not correctly set. will be seen as the Javadoc Maven plugin depends on this value to work correctly.","title":"Java"},{"location":"guides/developer/languages/#python","text":"Python is used in much of the educational material . Python is an interpreted programming language. It is favored by data scientists and script writers because it supports writing in snippets where variables can be created on the fly and are typically global. Egeria uses Python in the hands-on labs since it is the native language of the Jupyter Notebooks environment we use for the labs.","title":"Python"},{"location":"guides/developer/languages/#markdown","text":"Markdown is a simple tagging language that generates HTML webpages. We use it for documentation (this page is written in Markdown for example), in GitHub comments and in the Jupyter Notebooks that form the teaching material for the hand-on labs . GitHub provides a useful summary for Markdown and our own documentation guide provides Egeria-specific formatting and stylistic pointers, as well as further information on the system we use to translate such .md files into the website you are currently reading.","title":"Markdown"},{"location":"guides/developer/process/","text":"Way of Working \u00b6 Issues \u00b6 Egeria uses GitHub issues . Opening issues \u00b6 Issues can be opened by any GitHub user, and are used for a variety of purposes: Problems getting Egeria working Proposed new features Identification of a bug Suggested process change Or really anything that affects Egeria All PRs should have an associated issue to facilitate discussion. You should include a helpful abstract and as many notes as possible about what you see, what you've tried, your environment, any logs, what you expected to happen, etc. Triaging new issues \u00b6 New issues are triaged by maintainers , who will: Assign the issue to someone who can take care of what is reported - even if not the final owner Assign a milestone if it is immediately obvious that the issue relates to capability set out in a release plan, or is needed very soon, otherwise leave blank Assign relevant tags to the issue Working on issues \u00b6 The issue owner (assignee) will then: Update the issue as soon as possible with an initial response When raising a PR , refer to the issue number (e.g. #1234 ) so that the discussion is clearly linked with the proposed code change. Use Fixes #1234 if this PR will completely address the issue, so that GitHub will automatically close the issue when the PR is merged. Keep the milestone realistic, if set Regularly review outstanding issues and update, reassign, close as needed Closing issues \u00b6 Issues with PRs marked as fixes #1234 will close automatically when the PR is merged Other issues fixed in other ways should be closed manually Any issues open after 60 days with no activity (including assignments) will have a comment added saying they will be closed 20 days later the issue will be closed If an issue is closed accidentally or prematurely, reopen and add appropriate comments Release process \u00b6 New releases can be created by Egeria maintainers that have the appropriate access on each GitHub repository. Releases are published to Maven Central . Overall release policy \u00b6 Aim to release approximately every month Typically, target end of month for external availability Will only release an update between releases in exceptional circumstances Preserves backwards compatibility as much as possible Try and maintain a regular heartbeat: even if completion of some features continues in a subsequent release master kept open for new code features Obtaining releases / artifacts \u00b6 Location Usage Maven Central typically used by other developers integrating with our code Github Release source code in zip and tar.gz formats git git checkout Vx.y to get version as-shipped (each release is tagged at the point it is shipped) Release notes are available as part of the online documentation . Release process \u00b6 1. Agree schedule Agree on appropriate dates for branching given expected duration for testing, vacation / public holidays Typically, allow 1-2 weeks between branching and availability Communicate with team on regular calls, and via #egeria-github on Slack In the last week before branching discuss holding off on any big changes in master that could destabilize the codebase 2. Track remaining issues and PRs Ensure any required issues / PRs for the release have the correct milestone set Move any issues / PRs not expected to make / not required for the release to a future milestone Aim to branch when most issues / PRs are complete to minimize back-porting from master, but not at the expense of impacting ongoing master development Agree final branch date / criteria 3. Create branch Checkout master git checkout master Ensure local update git pull upstream master Create branch git branch egeria-release-x.y Push to upstream git push upstream egeria-release-x.y 4. Update master from x.y-SNAPSHOT to x.z-SNAPSHOT git checkout master git pull upstream master Edit all files (command line or IDE) to replace x.y-SNAPSHOT with the next version, e.g. change 1.3-SNAPSHOT to 3.1-SNAPSHOT . Most of the changes are in pom.xml files, however some code and documentation also has references to our versions and all need modifying. If using an IDE like IntelliJ, make sure you have all hits by searching again as by default only a limited number of hits are shown . Commit Now remove all the release notes from the release-notes directory other than README.md - so users will always get directed to the latest in master Commit Create a PR , have reviewed / approved and merged as usual - aim to do this as quickly as reasonable so that there is no potential for version clash 5. Test, merge any remaining required changes into branch Run appropriate tests for the release. For example, in addition to automated tests: check notebooks, run the CTS and check for compliance, check the user interface. Raise issues for any changes required as usual Note that approval is required for changes going into a release branch PR builds are run as usual; however, merge builds, Sonar, etc do not run To backport changes from master , first wait until the PR is merged into master , then use git cherrypick -s <commithash> to apply to egeria-release-x.y , then push as usual. In some cases a merge commit will need to be made using git cherrypick -s -m 1 <commithash> If code has diverged significantly a manual recode may be easiest 6. Update branch's release version from x.y-SNAPSHOT to x.y Aim to make this change when the code appears to be ready to ship apart from final tests in order to avoid version confusion git checkout egeria-release-x.y git pull upstream egeria-release-x.y Edit all files (command line or IDE) to replace x.y-SNAPSHOT with x.y , i.e. removing the -SNAPSHOT designation. Most of the changes are in pom.xml files; however, some code and documentation also has references to our versions and all need modifying. Commit, and do not make any other changes. Create a PR , have reviewed / approved and merged as usual 7. Create a release in GitHub Create the GitHub release . Use Vx.y as the tag, and ensure the correct branch is set for the target, i.e. egeria-release-x.y Fill in the release notes using a title of Release x.y and copy the notes from the appropriate release notes Artifacts will be available on Maven Central within around half a day. Source archives will be added to the release on GitHub within an hour or so. Security hardening \u00b6 As part of developing Egeria, we will inevitably come across areas identified by various code analysis tools as potential security vulnerabilities. The following guidelines define the way we will work with these (identifying, reporting, tracking, etc) as well as some common techniques we can apply to address them. The maintainers have a weekly call to triage identified vulnerabilities from various sources: Sonar scans Nexus IQ scans Any third party inputs (i.e. from consumers) -- which can be sent to us at egeria-security@lists.lfaidata.foundation Work can then begin on resolving them, with two potential options (depending on complexity): Quick to resolve: create an issue when we believe we have a fix, and link the PR with the fix to the issue For any we cannot quickly resolve, we will use GitHub's security advisories to capture the details and notify publicly about the potential vulnerability In general, any vulnerabilities will typically be addressed through one of the following techniques: Code changes \u00b6 When the code identified as having a potential vulnerability is our own, we should naturally investigate how to change our code in order to reduce or remove the impact or likelihood of that exposure. This could be through applying input or output validation of data we receive, or ensuring that we use features built-in to any external components to do such processing. Dependency exclusions \u00b6 External modules on which we depend often have their own set of embedded dependencies. Some of these transitive dependencies may have vulnerabilities, and we may not actually use any of the functionality they provide. In these cases, we can (and should) safely exclude these transitive dependencies as part of the POM dependency management. Example: excluding a transitive dependency from a dependent library For example, testng has a dependency on the snakeyaml library, but this is only used when configuring testng with YAML documents (which we do not do). We can therefore safely exclude the transitive snakeyaml dependency of testng using the following in the root-level pom.xml : 1 2 3 4 5 6 7 8 9 10 11 12 13 <dependency> <groupId> org.testng </groupId> <artifactId> testng </artifactId> <scope> test </scope> <version> 7.1.0 </version> <exclusions> <!-- Exclude snakeyaml, which has open CVEs and is unused --> <exclusion> <groupId> org.yaml </groupId> <artifactId> snakeyaml </artifactId> </exclusion> </exclusions> </dependency> Forced dependency version updates \u00b6 In other cases we actually do rely on the functionality provided by these transitive dependencies, so we cannot simply exclude them. However, it may be possible to force the version of these dependencies to be updated so that a vulnerable older version of the dependency (the minimal version on which the library depends) is not used by default. Example: forcing an updated version of a transitive dependency Take for example janusgraph -- it has a transitive dependency on the sleepycat library, and by default quite an old version which has some known problems. By adding an explicit dependency for a newer version of the sleepycat module we can force this newer version to be used by janusgraph as well. <dependency> <groupId> com.sleepycat </groupId> <artifactId> je </artifactId> <version> 18.3.12 </version> </dependency> Of course making this change requires testing, to ensure that the newer version of the transitive library is still compatible with the base dependency. Feature branches \u00b6 The standard development approach for Egeria is to: make code changes on a branch on one's own fork create a PR to push from this branch to master Most of the time these are coded by a single developer, with additional review / testing from peers as part of the PR process. On occasion a few developers may directly collaborate on the code changes and can pull / push to / from each other's branches, or share updates in other ways. master therefore always represents the \"best so far\" code, ideally in a \"ready to release\" state, through build automation, testing and peer pressure. Everyone benefits from the latest code changes and any divergence between a developer's environment and master is minimized. Sometimes, however, there is a need to coordinate a larger piece of work in a team of developers who need the ability to: Reduce the impact of changes on master - i.e. for everyone else Reduce the impact of constant updates from master, in order to have a stable environment for feature-oriented testing In these cases a feature branch may be proposed. A GitHub issue should be created, and the proposal discussed in one of the regular Egeria calls to build consensus around the need for such a branch. Feature branches add overhead They can lead to code divergence and complexity, and they will only be created in compelling circumstances for long-running feature work. Once agreed, one of the maintainers / admins will make the required setup. See the last section of this document for some more information on this. Working in a feature branch \u00b6 Any work specifically and solely for the feature should be done on the agreed branch, but it's important that normal defect fixes and enhancements to unrelated features should continue to be worked on via master : i.e. working on a dev's own fork for a short period (hours/days) and merged back to master . This helps other developers working on the project, and reduces the complexity of subsequent merges from the feature branch. The team working on the feature will need to arrange / agree their own builds for testing / deployment. Merging to master and releasing \u00b6 We do not release from a feature branch. All release branches are made from master . It is the feature team's responsibility to: Merge the latest code from master Merge feature branch back to master There's no set schedule for this. Longer intervals offers the feature more stability, but can rapidly build up a much more complex merge scenario which the feature team will need to resolve. It is the feature team's responsibility to respond to any issues in master , and to validate that the feature is \"good\". Administrative tasks \u00b6 These tasks should only be performed by someone familiar with the process and with appropriate authority after establishing team agreement. As such, exact commands are not given below: Creating a branch Create a feature branch named feature-XXX where XXX is a descriptive name for the feature. (With issues, using the issue number can be helpful, but since we expect a small number of feature branches, this seems clearer.) Ensure branch protections are set to the same as master , to ensure all changes follow the same process as for master: for example, must go via PRs. Builds It's expected that all Feature branches should have PR verification to ensure submitted code changes in a PR do not break the main build. This is purely a compilation test to check against breakage. Build artifacts are not distributed or saved. Features could benefit from a 'merge' build which ensures the latest code in the branch works well together. This build also typically generates: - Maven artifacts (to a snapshot repository) - Docker images (to Docker Hub) In the future, it's expected these will get used for automatic tests, and used by other deployment approaches such as Docker Compose and Kubernetes. However, currently our repository and naming / versioning setup is not able to do this since branch names are not taken into account. In Egeria we also may perform: Scans for code quality Scans for licensing Security-related scans Builds for Docker images other than core Egeria These will also not be done for a feature branch. Closing a feature branch \u00b6 When the feature branch is no longer required, it can be deleted by an admin. Similarly to requesting a feature branch, an issue should be raised, and team agreement sought beforehand.","title":"Way of Working"},{"location":"guides/developer/process/#way-of-working","text":"","title":"Way of Working"},{"location":"guides/developer/process/#issues","text":"Egeria uses GitHub issues .","title":"Issues"},{"location":"guides/developer/process/#opening-issues","text":"Issues can be opened by any GitHub user, and are used for a variety of purposes: Problems getting Egeria working Proposed new features Identification of a bug Suggested process change Or really anything that affects Egeria All PRs should have an associated issue to facilitate discussion. You should include a helpful abstract and as many notes as possible about what you see, what you've tried, your environment, any logs, what you expected to happen, etc.","title":"Opening issues"},{"location":"guides/developer/process/#triaging-new-issues","text":"New issues are triaged by maintainers , who will: Assign the issue to someone who can take care of what is reported - even if not the final owner Assign a milestone if it is immediately obvious that the issue relates to capability set out in a release plan, or is needed very soon, otherwise leave blank Assign relevant tags to the issue","title":"Triaging new issues"},{"location":"guides/developer/process/#working-on-issues","text":"The issue owner (assignee) will then: Update the issue as soon as possible with an initial response When raising a PR , refer to the issue number (e.g. #1234 ) so that the discussion is clearly linked with the proposed code change. Use Fixes #1234 if this PR will completely address the issue, so that GitHub will automatically close the issue when the PR is merged. Keep the milestone realistic, if set Regularly review outstanding issues and update, reassign, close as needed","title":"Working on issues"},{"location":"guides/developer/process/#closing-issues","text":"Issues with PRs marked as fixes #1234 will close automatically when the PR is merged Other issues fixed in other ways should be closed manually Any issues open after 60 days with no activity (including assignments) will have a comment added saying they will be closed 20 days later the issue will be closed If an issue is closed accidentally or prematurely, reopen and add appropriate comments","title":"Closing issues"},{"location":"guides/developer/process/#release-process","text":"New releases can be created by Egeria maintainers that have the appropriate access on each GitHub repository. Releases are published to Maven Central .","title":"Release process"},{"location":"guides/developer/process/#overall-release-policy","text":"Aim to release approximately every month Typically, target end of month for external availability Will only release an update between releases in exceptional circumstances Preserves backwards compatibility as much as possible Try and maintain a regular heartbeat: even if completion of some features continues in a subsequent release master kept open for new code features","title":"Overall release policy"},{"location":"guides/developer/process/#obtaining-releases-artifacts","text":"Location Usage Maven Central typically used by other developers integrating with our code Github Release source code in zip and tar.gz formats git git checkout Vx.y to get version as-shipped (each release is tagged at the point it is shipped) Release notes are available as part of the online documentation .","title":"Obtaining releases / artifacts"},{"location":"guides/developer/process/#release-process_1","text":"1. Agree schedule Agree on appropriate dates for branching given expected duration for testing, vacation / public holidays Typically, allow 1-2 weeks between branching and availability Communicate with team on regular calls, and via #egeria-github on Slack In the last week before branching discuss holding off on any big changes in master that could destabilize the codebase 2. Track remaining issues and PRs Ensure any required issues / PRs for the release have the correct milestone set Move any issues / PRs not expected to make / not required for the release to a future milestone Aim to branch when most issues / PRs are complete to minimize back-porting from master, but not at the expense of impacting ongoing master development Agree final branch date / criteria 3. Create branch Checkout master git checkout master Ensure local update git pull upstream master Create branch git branch egeria-release-x.y Push to upstream git push upstream egeria-release-x.y 4. Update master from x.y-SNAPSHOT to x.z-SNAPSHOT git checkout master git pull upstream master Edit all files (command line or IDE) to replace x.y-SNAPSHOT with the next version, e.g. change 1.3-SNAPSHOT to 3.1-SNAPSHOT . Most of the changes are in pom.xml files, however some code and documentation also has references to our versions and all need modifying. If using an IDE like IntelliJ, make sure you have all hits by searching again as by default only a limited number of hits are shown . Commit Now remove all the release notes from the release-notes directory other than README.md - so users will always get directed to the latest in master Commit Create a PR , have reviewed / approved and merged as usual - aim to do this as quickly as reasonable so that there is no potential for version clash 5. Test, merge any remaining required changes into branch Run appropriate tests for the release. For example, in addition to automated tests: check notebooks, run the CTS and check for compliance, check the user interface. Raise issues for any changes required as usual Note that approval is required for changes going into a release branch PR builds are run as usual; however, merge builds, Sonar, etc do not run To backport changes from master , first wait until the PR is merged into master , then use git cherrypick -s <commithash> to apply to egeria-release-x.y , then push as usual. In some cases a merge commit will need to be made using git cherrypick -s -m 1 <commithash> If code has diverged significantly a manual recode may be easiest 6. Update branch's release version from x.y-SNAPSHOT to x.y Aim to make this change when the code appears to be ready to ship apart from final tests in order to avoid version confusion git checkout egeria-release-x.y git pull upstream egeria-release-x.y Edit all files (command line or IDE) to replace x.y-SNAPSHOT with x.y , i.e. removing the -SNAPSHOT designation. Most of the changes are in pom.xml files; however, some code and documentation also has references to our versions and all need modifying. Commit, and do not make any other changes. Create a PR , have reviewed / approved and merged as usual 7. Create a release in GitHub Create the GitHub release . Use Vx.y as the tag, and ensure the correct branch is set for the target, i.e. egeria-release-x.y Fill in the release notes using a title of Release x.y and copy the notes from the appropriate release notes Artifacts will be available on Maven Central within around half a day. Source archives will be added to the release on GitHub within an hour or so.","title":"Release process"},{"location":"guides/developer/process/#security-hardening","text":"As part of developing Egeria, we will inevitably come across areas identified by various code analysis tools as potential security vulnerabilities. The following guidelines define the way we will work with these (identifying, reporting, tracking, etc) as well as some common techniques we can apply to address them. The maintainers have a weekly call to triage identified vulnerabilities from various sources: Sonar scans Nexus IQ scans Any third party inputs (i.e. from consumers) -- which can be sent to us at egeria-security@lists.lfaidata.foundation Work can then begin on resolving them, with two potential options (depending on complexity): Quick to resolve: create an issue when we believe we have a fix, and link the PR with the fix to the issue For any we cannot quickly resolve, we will use GitHub's security advisories to capture the details and notify publicly about the potential vulnerability In general, any vulnerabilities will typically be addressed through one of the following techniques:","title":"Security hardening"},{"location":"guides/developer/process/#code-changes","text":"When the code identified as having a potential vulnerability is our own, we should naturally investigate how to change our code in order to reduce or remove the impact or likelihood of that exposure. This could be through applying input or output validation of data we receive, or ensuring that we use features built-in to any external components to do such processing.","title":"Code changes"},{"location":"guides/developer/process/#dependency-exclusions","text":"External modules on which we depend often have their own set of embedded dependencies. Some of these transitive dependencies may have vulnerabilities, and we may not actually use any of the functionality they provide. In these cases, we can (and should) safely exclude these transitive dependencies as part of the POM dependency management. Example: excluding a transitive dependency from a dependent library For example, testng has a dependency on the snakeyaml library, but this is only used when configuring testng with YAML documents (which we do not do). We can therefore safely exclude the transitive snakeyaml dependency of testng using the following in the root-level pom.xml : 1 2 3 4 5 6 7 8 9 10 11 12 13 <dependency> <groupId> org.testng </groupId> <artifactId> testng </artifactId> <scope> test </scope> <version> 7.1.0 </version> <exclusions> <!-- Exclude snakeyaml, which has open CVEs and is unused --> <exclusion> <groupId> org.yaml </groupId> <artifactId> snakeyaml </artifactId> </exclusion> </exclusions> </dependency>","title":"Dependency exclusions"},{"location":"guides/developer/process/#forced-dependency-version-updates","text":"In other cases we actually do rely on the functionality provided by these transitive dependencies, so we cannot simply exclude them. However, it may be possible to force the version of these dependencies to be updated so that a vulnerable older version of the dependency (the minimal version on which the library depends) is not used by default. Example: forcing an updated version of a transitive dependency Take for example janusgraph -- it has a transitive dependency on the sleepycat library, and by default quite an old version which has some known problems. By adding an explicit dependency for a newer version of the sleepycat module we can force this newer version to be used by janusgraph as well. <dependency> <groupId> com.sleepycat </groupId> <artifactId> je </artifactId> <version> 18.3.12 </version> </dependency> Of course making this change requires testing, to ensure that the newer version of the transitive library is still compatible with the base dependency.","title":"Forced dependency version updates"},{"location":"guides/developer/process/#feature-branches","text":"The standard development approach for Egeria is to: make code changes on a branch on one's own fork create a PR to push from this branch to master Most of the time these are coded by a single developer, with additional review / testing from peers as part of the PR process. On occasion a few developers may directly collaborate on the code changes and can pull / push to / from each other's branches, or share updates in other ways. master therefore always represents the \"best so far\" code, ideally in a \"ready to release\" state, through build automation, testing and peer pressure. Everyone benefits from the latest code changes and any divergence between a developer's environment and master is minimized. Sometimes, however, there is a need to coordinate a larger piece of work in a team of developers who need the ability to: Reduce the impact of changes on master - i.e. for everyone else Reduce the impact of constant updates from master, in order to have a stable environment for feature-oriented testing In these cases a feature branch may be proposed. A GitHub issue should be created, and the proposal discussed in one of the regular Egeria calls to build consensus around the need for such a branch. Feature branches add overhead They can lead to code divergence and complexity, and they will only be created in compelling circumstances for long-running feature work. Once agreed, one of the maintainers / admins will make the required setup. See the last section of this document for some more information on this.","title":"Feature branches"},{"location":"guides/developer/process/#working-in-a-feature-branch","text":"Any work specifically and solely for the feature should be done on the agreed branch, but it's important that normal defect fixes and enhancements to unrelated features should continue to be worked on via master : i.e. working on a dev's own fork for a short period (hours/days) and merged back to master . This helps other developers working on the project, and reduces the complexity of subsequent merges from the feature branch. The team working on the feature will need to arrange / agree their own builds for testing / deployment.","title":"Working in a feature branch"},{"location":"guides/developer/process/#merging-to-master-and-releasing","text":"We do not release from a feature branch. All release branches are made from master . It is the feature team's responsibility to: Merge the latest code from master Merge feature branch back to master There's no set schedule for this. Longer intervals offers the feature more stability, but can rapidly build up a much more complex merge scenario which the feature team will need to resolve. It is the feature team's responsibility to respond to any issues in master , and to validate that the feature is \"good\".","title":"Merging to master and releasing"},{"location":"guides/developer/process/#administrative-tasks","text":"These tasks should only be performed by someone familiar with the process and with appropriate authority after establishing team agreement. As such, exact commands are not given below: Creating a branch Create a feature branch named feature-XXX where XXX is a descriptive name for the feature. (With issues, using the issue number can be helpful, but since we expect a small number of feature branches, this seems clearer.) Ensure branch protections are set to the same as master , to ensure all changes follow the same process as for master: for example, must go via PRs. Builds It's expected that all Feature branches should have PR verification to ensure submitted code changes in a PR do not break the main build. This is purely a compilation test to check against breakage. Build artifacts are not distributed or saved. Features could benefit from a 'merge' build which ensures the latest code in the branch works well together. This build also typically generates: - Maven artifacts (to a snapshot repository) - Docker images (to Docker Hub) In the future, it's expected these will get used for automatic tests, and used by other deployment approaches such as Docker Compose and Kubernetes. However, currently our repository and naming / versioning setup is not able to do this since branch names are not taken into account. In Egeria we also may perform: Scans for code quality Scans for licensing Security-related scans Builds for Docker images other than core Egeria These will also not be done for a feature branch.","title":"Administrative tasks"},{"location":"guides/developer/process/#closing-a-feature-branch","text":"When the feature branch is no longer required, it can be deleted by an admin. Similarly to requesting a feature branch, an issue should be raised, and team agreement sought beforehand.","title":"Closing a feature branch"},{"location":"guides/developer/using-connectors/","text":"Using Connectors \u00b6 Connectors can be created through the following clients: Asset Consumer OMAS Asset Owner OMAS Example: connecting to CSV files using Asset Consumer OMAS The code sample below uses the Asset Consumer OMAS client to retrieve a list of assets from a metadata server and then create a connector to each one using the getConnectorToAsset() method. This method assumes that there is a connection object with a connector type and endpoint linked to the requested asset in the metadata repository. An exception is thrown if an asset does not have a connection. In the sample, the connector returned by the Asset Consumer OMAS client is then cast to the CSVFileConnector . Assets that are not CSV files will have a different connector implementation and so the casting to CSVFileConnector also results in an exception. Assets that do not have a CSVFileConnector are ignored. The result is that the sample method returns a connector for the first CSV file asset retrieved from the metadata repository. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 /** * This method uses Asset Consumer OMAS to locate and create an Open Connector Framework (OCF) connector * instance. * * @return connector to first CSVFile located in the catalog */ private CSVFileStoreConnector getConnectorUsingMetadata () { try { /* * The Asset Consumer OMAS supports a REST API to extract metadata from the open metadata repositories * linked to the same open metadata cohort as the Asset Consumer OMAS. It also has a Java client that * provides an equivalent interface to the REST API plus connector factory methods supported by an * embedded Connector Broker. The Connector Broker is an Open Connector Framework (OCF) component * that is able to create and configure instances of compliant connectors. It is passed a Connection * object which has all of the properties needed to create the connector. The Asset Consumer OMAS * extracts the Connection object from the open metadata repositories and then calls the Connector Broker. */ AssetConsumer client = new AssetConsumer ( serverName , serverURLRoot ); /* * This call extracts the list of assets stored in the open metadata repositories that have a name * that matches the requested filename. */ List < String > knownAssets = client . findAssets ( clientUserId , \".*\" , 0 , 4 ); if ( knownAssets != null ) { System . out . println ( \"The open metadata repositories have returned \" + knownAssets . size () + \" asset definitions for the requested file name \" + fileName ); for ( String assetGUID : knownAssets ) { if ( assetGUID != null ) { try { /* * The aim is to return a connector for the first matching asset. If an asset of a different * type is returned, on one where it is not possible to create a connector for, then an * exception is thrown and the code moves on to process the next asset. */ return ( CSVFileStoreConnector ) client . getConnectorForAsset ( clientUserId , assetGUID ); } catch ( Exception error ) { System . out . println ( \"Unable to create connector for asset: \" + assetGUID ); } } } } else { System . out . println ( \"The open metadata repositories do not have an asset definition for the requested file name \" + fileName ); } } catch ( Exception error ) { System . out . println ( \"The connector can not be created from metadata. Error message is: \" + error . getMessage ()); } return null ; } Connecting to assets with different levels of security \u00b6 It is possible that an asset can have multiple connections, each with different levels of security access encoded. Egeria is able to determine which one to use by calling the validateUserForAssetConnectionList() method of the Server Security Metadata Connector . Other links to the connection \u00b6 Open metadata is a connected network (graph) of information. The connector type and endpoint that a connection object links to are typically shared with many connections. This creates some interesting insight. For example, there is typically one connector type for each connector implementation. By retrieving the relationships from the connector type to the connections, it is possible to see the extent to which the connector is used. Connector Types \u00b6 The connector types for Egeria's data store connectors are available in an open metadata archive called DataStoreConnectorTypes.json that can be loaded into the server. This approach can be used for all of your connector implementations to create the connector type objects in our metadata repository. See the open-connector-archives for more detail. Endpoints \u00b6 The endpoints are typically linked to the software server that is called by the connector. By navigating from the Endpoint to the linked connections it is possible to trace the callers to the software server. Software servers and endpoints are set up through the IT Infrastructure OMAS . Further information The connector catalog lists the connectors available to digital resources.","title":"Using Connectors"},{"location":"guides/developer/using-connectors/#using-connectors","text":"Connectors can be created through the following clients: Asset Consumer OMAS Asset Owner OMAS Example: connecting to CSV files using Asset Consumer OMAS The code sample below uses the Asset Consumer OMAS client to retrieve a list of assets from a metadata server and then create a connector to each one using the getConnectorToAsset() method. This method assumes that there is a connection object with a connector type and endpoint linked to the requested asset in the metadata repository. An exception is thrown if an asset does not have a connection. In the sample, the connector returned by the Asset Consumer OMAS client is then cast to the CSVFileConnector . Assets that are not CSV files will have a different connector implementation and so the casting to CSVFileConnector also results in an exception. Assets that do not have a CSVFileConnector are ignored. The result is that the sample method returns a connector for the first CSV file asset retrieved from the metadata repository. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 /** * This method uses Asset Consumer OMAS to locate and create an Open Connector Framework (OCF) connector * instance. * * @return connector to first CSVFile located in the catalog */ private CSVFileStoreConnector getConnectorUsingMetadata () { try { /* * The Asset Consumer OMAS supports a REST API to extract metadata from the open metadata repositories * linked to the same open metadata cohort as the Asset Consumer OMAS. It also has a Java client that * provides an equivalent interface to the REST API plus connector factory methods supported by an * embedded Connector Broker. The Connector Broker is an Open Connector Framework (OCF) component * that is able to create and configure instances of compliant connectors. It is passed a Connection * object which has all of the properties needed to create the connector. The Asset Consumer OMAS * extracts the Connection object from the open metadata repositories and then calls the Connector Broker. */ AssetConsumer client = new AssetConsumer ( serverName , serverURLRoot ); /* * This call extracts the list of assets stored in the open metadata repositories that have a name * that matches the requested filename. */ List < String > knownAssets = client . findAssets ( clientUserId , \".*\" , 0 , 4 ); if ( knownAssets != null ) { System . out . println ( \"The open metadata repositories have returned \" + knownAssets . size () + \" asset definitions for the requested file name \" + fileName ); for ( String assetGUID : knownAssets ) { if ( assetGUID != null ) { try { /* * The aim is to return a connector for the first matching asset. If an asset of a different * type is returned, on one where it is not possible to create a connector for, then an * exception is thrown and the code moves on to process the next asset. */ return ( CSVFileStoreConnector ) client . getConnectorForAsset ( clientUserId , assetGUID ); } catch ( Exception error ) { System . out . println ( \"Unable to create connector for asset: \" + assetGUID ); } } } } else { System . out . println ( \"The open metadata repositories do not have an asset definition for the requested file name \" + fileName ); } } catch ( Exception error ) { System . out . println ( \"The connector can not be created from metadata. Error message is: \" + error . getMessage ()); } return null ; }","title":"Using Connectors"},{"location":"guides/developer/using-connectors/#connecting-to-assets-with-different-levels-of-security","text":"It is possible that an asset can have multiple connections, each with different levels of security access encoded. Egeria is able to determine which one to use by calling the validateUserForAssetConnectionList() method of the Server Security Metadata Connector .","title":"Connecting to assets with different levels of security"},{"location":"guides/developer/using-connectors/#other-links-to-the-connection","text":"Open metadata is a connected network (graph) of information. The connector type and endpoint that a connection object links to are typically shared with many connections. This creates some interesting insight. For example, there is typically one connector type for each connector implementation. By retrieving the relationships from the connector type to the connections, it is possible to see the extent to which the connector is used.","title":"Other links to the connection"},{"location":"guides/developer/using-connectors/#connector-types","text":"The connector types for Egeria's data store connectors are available in an open metadata archive called DataStoreConnectorTypes.json that can be loaded into the server. This approach can be used for all of your connector implementations to create the connector type objects in our metadata repository. See the open-connector-archives for more detail.","title":"Connector Types"},{"location":"guides/developer/using-connectors/#endpoints","text":"The endpoints are typically linked to the software server that is called by the connector. By navigating from the Endpoint to the linked connections it is possible to trace the callers to the software server. Software servers and endpoints are set up through the IT Infrastructure OMAS . Further information The connector catalog lists the connectors available to digital resources.","title":"Endpoints"},{"location":"guides/developer/tools/development/","text":"Development Tools \u00b6 Git and GitHub \u00b6 git is an open source version control system. It is what we use to: Store all of our source code, documentation and other file-based resources. Track changes to the underlying Egeria code as the project evolves. Track issues and enhancements, and link these back to the code changes that resolve them. Collaborate on and review the issues, enhancements and code changes. As a result, it gives us a definitive source for the latest and greatest source code for Egeria itself, its history, and the rationale behind various decisions that are made over time. The Egeria project's git repositories are located on GitHub . GitHub is a free, public git service for sharing code and related files. It has a web interface to make it easier for the Egeria community to monitor the activity in the project and process new content. Repositories \u00b6 Repository Purpose egeria contains the core Egeria function along with samples, tutorials and documentation. egeria-connector-xtdb contains a plugin repository connector to use XTDB as a pluggable back-end for an Egeria metadata server . egeria-connector-hadoop-ecosystem contains connectors to integrate technologies from the Hadoop ecosystem into the open metadata ecosystem. egeria-connector-ibm-information-server contains connectors to integrate IBM Information Server into the open metadata ecosystem. data-governance contains Egeria's Guidance on Governance (GoG) as well as large media files such as presentations and movies. egeria-dev-projects contains fun projects for developers to help them learn about the Egeria technology. egeria-palisade contains content from the collaboration between the Egeria project and the Palisade project. All of these repositories are publicly visible; however, if you want to contribute new content then you need to create a GitHub account. This can be done from the top of the GitHub home page . Further information Interested to learn more? GitHub provides some great introductory guides to git . Egeria provides specific tutorials for working with Egeria's git repositories . IntelliJ IDEA \u00b6 IntelliJ IDEA by JetBrains is the Interactive Development Environment (IDE) used by most of the Egeria developers. The community edition is free to use and covers all the function needed by an Egeria developer. We provide our own tutorial for IntelliJ . Lombok Plugin \u00b6 Egeria makes use of Project Lombok . If using JetBrains IntelliJ IDEA ensure it has the required plugin configured :material-dock-window . Don't detect generated sources Also, before running a Maven build please choose Don't detect from the Generated sources folders dropdown in Preferences -> Build, Execution, Deployment -> Build Tools -> Maven -> Importing . This will avoid triggering a duplicate classes build error caused by the delombok ed sources folders being added as source folders for the Maven module. If this wasn't set when your project was initially setup, you may find that delombok directories are already present in IntelliJ's source path for some modules, leading to errors with duplicate classes. To check for any modules still refering to delombok you can run this at the command line, from your top-level source tree: Find any existing delombok source entries in IntelliJ find . -name '*.iml' | xargs -n50 grep -y delombok If you find any hits such as: ./open-metadata-implementation/access-services/data-engine/data-engine-api/data-engine-api.iml: <sourceFolder url=\"file://$MODULE_DIR$/target/delombok\" isTestSource=\"false\" /> then either remove those lines without IntelliJ running, or go into File -> Project Structure -> Modules , and remove target/delombok from the Source Folders list Explanation: in addition to importing module defintions from the Maven pom.xml , IntelliJ also tries to look for any generated source. It finds the delombok directory, causing duplicates: in fact we only use this directory for generating Javadoc of lombok-enabled modules. Switching the setting / removing these source folders prevents these duplicate classes. Apache Maven \u00b6 Apache Maven is the tool that supports our project build. This includes the code compilation, running unit tests, validating dependencies and Javadoc as well as build our distribution archive. Maven 3.5 or higher is required to build Egeria. 3.6.x or above is recommended. The Maven processing organizes the modules into a hierarchy. Each module has a pom.xml file (called the pom file ) that defines the artifact, its parent / children, dependencies and any special processing that the module builds. The top-level pom file is the pom.xml file at the root of the repository's source code directory structure. When the Maven command is run, it passes through the hierarchy of modules multiple times. Each pass processes a particular lifecycle phase of the build (to ensure, for example, Java source files are compiled before the resulting object files are packaged into a jar file). Maven repositories This processing includes locating and downloading external libraries and dependencies, typically from an online open source repository called Maven Central. The directory where these external dependencies is stored locally is called .m2 . Rebuild the project with Maven mvn clean install The building Egeria tutorial covers more details on the build process. Check if Maven is installed mvn --version Install Maven using: MacOS Install Maven through HomeBrew brew install maven RedHat Install through yum yum install maven Debian Install through apt-get apt-get install maven Windows On Windows, you should use Windows Subsystem for Linux Version 2 or above, install an appropriate Linux distribution, and follow the instructions for that Linux distribution. Ensure you are using version 3.5.0 or higher in order to build Egeria. Gradle \u00b6 Gradle is an alternative build tool to Maven and offers: better support for parallel builds more flexibility for build tasks breaking the link between directory structure and maven artifacts extremely fast incremental builds Our direction is for a Gradle build to replace Maven; however, that work is still underway . As such, our supported build environment remains Maven As of release 3.0, most components are building with gradle, but artifacts are not being created, and verification has not been done. Contributions to this work are welcome, as are issue reports! No gradle installation is required, as we use the 'gradle wrapper' which will automatically install gradle if needed. This reduces the setup steps, and ensure everyone runs the same version of gradle (currently 7.02 in Release 3.0). Rebuild the project with Gradle ./gradlew build","title":"Development Tools"},{"location":"guides/developer/tools/development/#development-tools","text":"","title":"Development Tools"},{"location":"guides/developer/tools/development/#git-and-github","text":"git is an open source version control system. It is what we use to: Store all of our source code, documentation and other file-based resources. Track changes to the underlying Egeria code as the project evolves. Track issues and enhancements, and link these back to the code changes that resolve them. Collaborate on and review the issues, enhancements and code changes. As a result, it gives us a definitive source for the latest and greatest source code for Egeria itself, its history, and the rationale behind various decisions that are made over time. The Egeria project's git repositories are located on GitHub . GitHub is a free, public git service for sharing code and related files. It has a web interface to make it easier for the Egeria community to monitor the activity in the project and process new content.","title":"Git and GitHub"},{"location":"guides/developer/tools/development/#repositories","text":"Repository Purpose egeria contains the core Egeria function along with samples, tutorials and documentation. egeria-connector-xtdb contains a plugin repository connector to use XTDB as a pluggable back-end for an Egeria metadata server . egeria-connector-hadoop-ecosystem contains connectors to integrate technologies from the Hadoop ecosystem into the open metadata ecosystem. egeria-connector-ibm-information-server contains connectors to integrate IBM Information Server into the open metadata ecosystem. data-governance contains Egeria's Guidance on Governance (GoG) as well as large media files such as presentations and movies. egeria-dev-projects contains fun projects for developers to help them learn about the Egeria technology. egeria-palisade contains content from the collaboration between the Egeria project and the Palisade project. All of these repositories are publicly visible; however, if you want to contribute new content then you need to create a GitHub account. This can be done from the top of the GitHub home page . Further information Interested to learn more? GitHub provides some great introductory guides to git . Egeria provides specific tutorials for working with Egeria's git repositories .","title":"Repositories"},{"location":"guides/developer/tools/development/#intellij-idea","text":"IntelliJ IDEA by JetBrains is the Interactive Development Environment (IDE) used by most of the Egeria developers. The community edition is free to use and covers all the function needed by an Egeria developer. We provide our own tutorial for IntelliJ .","title":"IntelliJ IDEA"},{"location":"guides/developer/tools/development/#lombok-plugin","text":"Egeria makes use of Project Lombok . If using JetBrains IntelliJ IDEA ensure it has the required plugin configured :material-dock-window . Don't detect generated sources Also, before running a Maven build please choose Don't detect from the Generated sources folders dropdown in Preferences -> Build, Execution, Deployment -> Build Tools -> Maven -> Importing . This will avoid triggering a duplicate classes build error caused by the delombok ed sources folders being added as source folders for the Maven module. If this wasn't set when your project was initially setup, you may find that delombok directories are already present in IntelliJ's source path for some modules, leading to errors with duplicate classes. To check for any modules still refering to delombok you can run this at the command line, from your top-level source tree: Find any existing delombok source entries in IntelliJ find . -name '*.iml' | xargs -n50 grep -y delombok If you find any hits such as: ./open-metadata-implementation/access-services/data-engine/data-engine-api/data-engine-api.iml: <sourceFolder url=\"file://$MODULE_DIR$/target/delombok\" isTestSource=\"false\" /> then either remove those lines without IntelliJ running, or go into File -> Project Structure -> Modules , and remove target/delombok from the Source Folders list Explanation: in addition to importing module defintions from the Maven pom.xml , IntelliJ also tries to look for any generated source. It finds the delombok directory, causing duplicates: in fact we only use this directory for generating Javadoc of lombok-enabled modules. Switching the setting / removing these source folders prevents these duplicate classes.","title":"Lombok Plugin"},{"location":"guides/developer/tools/development/#apache-maven","text":"Apache Maven is the tool that supports our project build. This includes the code compilation, running unit tests, validating dependencies and Javadoc as well as build our distribution archive. Maven 3.5 or higher is required to build Egeria. 3.6.x or above is recommended. The Maven processing organizes the modules into a hierarchy. Each module has a pom.xml file (called the pom file ) that defines the artifact, its parent / children, dependencies and any special processing that the module builds. The top-level pom file is the pom.xml file at the root of the repository's source code directory structure. When the Maven command is run, it passes through the hierarchy of modules multiple times. Each pass processes a particular lifecycle phase of the build (to ensure, for example, Java source files are compiled before the resulting object files are packaged into a jar file). Maven repositories This processing includes locating and downloading external libraries and dependencies, typically from an online open source repository called Maven Central. The directory where these external dependencies is stored locally is called .m2 . Rebuild the project with Maven mvn clean install The building Egeria tutorial covers more details on the build process. Check if Maven is installed mvn --version Install Maven using: MacOS Install Maven through HomeBrew brew install maven RedHat Install through yum yum install maven Debian Install through apt-get apt-get install maven Windows On Windows, you should use Windows Subsystem for Linux Version 2 or above, install an appropriate Linux distribution, and follow the instructions for that Linux distribution. Ensure you are using version 3.5.0 or higher in order to build Egeria.","title":"Apache Maven"},{"location":"guides/developer/tools/development/#gradle","text":"Gradle is an alternative build tool to Maven and offers: better support for parallel builds more flexibility for build tasks breaking the link between directory structure and maven artifacts extremely fast incremental builds Our direction is for a Gradle build to replace Maven; however, that work is still underway . As such, our supported build environment remains Maven As of release 3.0, most components are building with gradle, but artifacts are not being created, and verification has not been done. Contributions to this work are welcome, as are issue reports! No gradle installation is required, as we use the 'gradle wrapper' which will automatically install gradle if needed. This reduces the setup steps, and ensure everyone runs the same version of gradle (currently 7.02 in Release 3.0). Rebuild the project with Gradle ./gradlew build","title":"Gradle"},{"location":"guides/developer/tools/documentation/","text":"Documentation Tools \u00b6 draw.io \u00b6 We use the free draw.io tool to produce all the diagrams for our website and presentations. Following the process outlined below, the draw.io files are stored as XML, which means it is easy to manage them in git. The tool can be run from the browser, or as a desktop tool. We recommend the desktop tool if you are doing editing of the diagrams because it supports auto-save. Opening multiple diagrams Whenever you start draw.io, it offers the choice to either open a new diagram or an existing file. On the desktop, New... means new window, and you can use it to have multiple files open at once. New diagrams \u00b6 Export new diagrams as an SVG, including a copy of the diagram The following instructions must be followed to maximize diagram maintainability. To save a new diagram, choose File -> Export as -> SVG... Ensure the Include a copy of my diagram box is ticked, and tick the Transparent Background box as well: This will ensure that: the output renders nicely at all sizes in any modern web browser (no image artifacts, especially on text) the diagram is entirely self-contained, and can continue to be edited and evolved in draw.io each diagram is version-controlled in git as its own independent XML file subsequent changes to the diagram can just directly edit this SVG file (no further Export as needed) Existing diagrams \u00b6 As indicated above, to edit existing SVG diagrams, you should be able to simply: open them directly in draw.io make any changes directly to the diagram save the file (no Export as needed) Swagger \u00b6 Swagger automatically generates a website that documents the REST APIs supported by the OMAG Server Platform . It is based on the Open API Specification (V3) . The website is found at <serverURLroot>/swagger-ui.html , where <serverURLroot> is the location of the OMAG Server Platform (for example, https://localhost:9443/swagger-ui.html ). The top of the page gives a general description of the OMAG Server Platform plus a link to more documentation. The content for this header is located in the OMAGServerPlatform.java file that provides the main() method for the OMAG Server Platform. Swagger annotations in OMAGServerPlatform.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @OpenAPIDefinition ( info = @Info ( title = \"Egeria's Open Metadata and Governance (OMAG) Server Platform\" , version = \"3.1-SNAPSHOT\" , description = \"The OMAG Server Platform provides a runtime process and platform for Open Metadata and Governance (OMAG) Services.\\n\" + \"\\n\" + \"The OMAG services are configured and activated in OMAG Servers using the Administration Services.\\n\" + \"The configuration operations of the admin services create configuration documents, one for each OMAG Server. \" + \"Inside a configuration document is the definition of which OMAG services to activate in the server. \" + \"These include the repository services (any type of server), the access services (for metadata access points \" + \"and metadata servers), governance services (for governance servers) and view services (for view servers). \" + \"Once a configuration document is defined, the OMAG Server can be started and stopped multiple times by \" + \"the admin services server instance operations. \\n\" + \"\\n\" + \"The OMAG Server Platform also supports platform services to query details of the servers running on the platform.\\n\" + \"\\n\" + \"The OMAG Server Platform can host multiple OMAG servers at any one time. \" + \"Each OMAG server is isolated within the server platform and so the OMAG server platform can be used to support multi-tenant \" + \"operation for a cloud service, \" + \"or host a variety of different OMAG Servers needed at a particular location.\\n\" + \"\\n\" + \"Click on the documentation link to find out more ...\" , license = @License ( name = \"Apache 2.0\" , url = \"https://www.apache.org/licenses/LICENSE-2.0\" ), contact = @Contact ( url = \"https://egeria.odpi.org\" , name = \"Egeria Project\" , email = \"egeria-technical-discuss@lists.lfaidata.foundation\" ) ), externalDocs = @ExternalDocumentation ( description = \"OMAG Server Platform documentation\" , url = \"https://egeria.odpi.org/open-metadata-implementation/admin-services/docs/user\" ) ) Beneath the header is a list of the platform's REST APIs. This is the definition for the operational services that are part of the administration services : The REST API operations are grouped into services by the following @Tag annotation that appears in each Spring resource bean that is part of the service. If the name of the @Tag matches then the operations in the resource beans are all part of the same service. Swagger annotations in OperationalServicesResource.java 1 2 3 4 5 @Tag ( name = \"Administration Services - Operational\" , description = \"The operational administration services support the management \" + \"of OMAG Server instances. This includes starting and stopping the servers as well as querying and changing their operational state.\" , externalDocs = @ExternalDocumentation ( description = \"Further information\" , url = \"https://egeria.odpi.org/open-metadata-implementation/admin-services/docs/user/operating-omag-server.html\" )) Further information can be provided for each operation. This is one of the operational services operations: This is added to the spring resource bean using the @Operation annotation: Swagger annotations 1 2 3 4 @Operation ( summary = \"Activate server with stored configuration document\" , description = \"Activate the named OMAG server using the appropriate configuration document found in the configuration store.\" , externalDocs = @ExternalDocumentation ( description = \"Configuration Documents\" , url = \"https://egeria.odpi.org/open-metadata-implementation/admin-services/docs/concepts/configuration-document.html\" ))","title":"Documentation Tools"},{"location":"guides/developer/tools/documentation/#documentation-tools","text":"","title":"Documentation Tools"},{"location":"guides/developer/tools/documentation/#drawio","text":"We use the free draw.io tool to produce all the diagrams for our website and presentations. Following the process outlined below, the draw.io files are stored as XML, which means it is easy to manage them in git. The tool can be run from the browser, or as a desktop tool. We recommend the desktop tool if you are doing editing of the diagrams because it supports auto-save. Opening multiple diagrams Whenever you start draw.io, it offers the choice to either open a new diagram or an existing file. On the desktop, New... means new window, and you can use it to have multiple files open at once.","title":"draw.io"},{"location":"guides/developer/tools/documentation/#new-diagrams","text":"Export new diagrams as an SVG, including a copy of the diagram The following instructions must be followed to maximize diagram maintainability. To save a new diagram, choose File -> Export as -> SVG... Ensure the Include a copy of my diagram box is ticked, and tick the Transparent Background box as well: This will ensure that: the output renders nicely at all sizes in any modern web browser (no image artifacts, especially on text) the diagram is entirely self-contained, and can continue to be edited and evolved in draw.io each diagram is version-controlled in git as its own independent XML file subsequent changes to the diagram can just directly edit this SVG file (no further Export as needed)","title":"New diagrams"},{"location":"guides/developer/tools/documentation/#existing-diagrams","text":"As indicated above, to edit existing SVG diagrams, you should be able to simply: open them directly in draw.io make any changes directly to the diagram save the file (no Export as needed)","title":"Existing diagrams"},{"location":"guides/developer/tools/documentation/#swagger","text":"Swagger automatically generates a website that documents the REST APIs supported by the OMAG Server Platform . It is based on the Open API Specification (V3) . The website is found at <serverURLroot>/swagger-ui.html , where <serverURLroot> is the location of the OMAG Server Platform (for example, https://localhost:9443/swagger-ui.html ). The top of the page gives a general description of the OMAG Server Platform plus a link to more documentation. The content for this header is located in the OMAGServerPlatform.java file that provides the main() method for the OMAG Server Platform. Swagger annotations in OMAGServerPlatform.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @OpenAPIDefinition ( info = @Info ( title = \"Egeria's Open Metadata and Governance (OMAG) Server Platform\" , version = \"3.1-SNAPSHOT\" , description = \"The OMAG Server Platform provides a runtime process and platform for Open Metadata and Governance (OMAG) Services.\\n\" + \"\\n\" + \"The OMAG services are configured and activated in OMAG Servers using the Administration Services.\\n\" + \"The configuration operations of the admin services create configuration documents, one for each OMAG Server. \" + \"Inside a configuration document is the definition of which OMAG services to activate in the server. \" + \"These include the repository services (any type of server), the access services (for metadata access points \" + \"and metadata servers), governance services (for governance servers) and view services (for view servers). \" + \"Once a configuration document is defined, the OMAG Server can be started and stopped multiple times by \" + \"the admin services server instance operations. \\n\" + \"\\n\" + \"The OMAG Server Platform also supports platform services to query details of the servers running on the platform.\\n\" + \"\\n\" + \"The OMAG Server Platform can host multiple OMAG servers at any one time. \" + \"Each OMAG server is isolated within the server platform and so the OMAG server platform can be used to support multi-tenant \" + \"operation for a cloud service, \" + \"or host a variety of different OMAG Servers needed at a particular location.\\n\" + \"\\n\" + \"Click on the documentation link to find out more ...\" , license = @License ( name = \"Apache 2.0\" , url = \"https://www.apache.org/licenses/LICENSE-2.0\" ), contact = @Contact ( url = \"https://egeria.odpi.org\" , name = \"Egeria Project\" , email = \"egeria-technical-discuss@lists.lfaidata.foundation\" ) ), externalDocs = @ExternalDocumentation ( description = \"OMAG Server Platform documentation\" , url = \"https://egeria.odpi.org/open-metadata-implementation/admin-services/docs/user\" ) ) Beneath the header is a list of the platform's REST APIs. This is the definition for the operational services that are part of the administration services : The REST API operations are grouped into services by the following @Tag annotation that appears in each Spring resource bean that is part of the service. If the name of the @Tag matches then the operations in the resource beans are all part of the same service. Swagger annotations in OperationalServicesResource.java 1 2 3 4 5 @Tag ( name = \"Administration Services - Operational\" , description = \"The operational administration services support the management \" + \"of OMAG Server instances. This includes starting and stopping the servers as well as querying and changing their operational state.\" , externalDocs = @ExternalDocumentation ( description = \"Further information\" , url = \"https://egeria.odpi.org/open-metadata-implementation/admin-services/docs/user/operating-omag-server.html\" )) Further information can be provided for each operation. This is one of the operational services operations: This is added to the spring resource bean using the @Operation annotation: Swagger annotations 1 2 3 4 @Operation ( summary = \"Activate server with stored configuration document\" , description = \"Activate the named OMAG server using the appropriate configuration document found in the configuration store.\" , externalDocs = @ExternalDocumentation ( description = \"Configuration Documents\" , url = \"https://egeria.odpi.org/open-metadata-implementation/admin-services/docs/concepts/configuration-document.html\" ))","title":"Swagger"},{"location":"guides/developer/tools/runtime/","text":"Runtime Tools \u00b6 Apache Kafka \u00b6 Apache Kafka is an event bus that is used to pass events between different Egeria servers. Kafka's own QuickStart Guide covers installation and basic usage. You may alternatively wish to install Kafka using a package manager such as HomeBrew on MacOS. For Egeria, the Kafka server needs to be running a PLAINTEXT listener. From the directory where Kafka is installed, check the config/server.properties file so that the listeners and advertised.listeners are setup as follows: Example: Kafka configuration listeners=PLAINTEXT://localhost:9092 advertised.listeners=PLAINTEXT://localhost:5092 The example above uses localhost:9092 for simplicity, assuming you are running Kafka locally on the same machine where you are doing your development. If running elsewhere, replace this with a name that is fully network resolveable (i.e. by both the host running Kafka and the client machines that will connect to Kafka from other hosts). Starting Kafka: MacOS When installed via HomeBrew brew services start zookeeper brew services start kafka Linux From within the bin folder of Apache Kafka ./zookeeper-server-start.sh ../config/zookeeper.properties & rm -rf /tmp/kafka-logs/* ./kafka-server-start.sh ../config/server.properties Shutting down Kafka: MacOS When installed via HomeBrew brew services stop kafka brew services stop zookeeper Linux From within the bin folder of Apache Kafka ./kafka-server-stop.sh ./zookeeper-server-stop.sh Docker \u00b6 Docker is a simple container runtime and standard . Every day, the Egeria build processing creates a Docker image of Egeria and pushes it to the public Docker catalog called Docker Hub . This docker image provides a simple way to bring a runnable version of Egeria onto your machine without any additional dependencies aside from Docker itself. It also provides the basis for a Kubernetes deployment of Egeria. The Overview tab describes the docker container. The Tags tab shows the different releases that are available. The docker image needs a runtime to execute. It can run in the cloud using various platforms or on your machine using Docker Desktop . Docker Desktop supports running a docker image as a standalone container, or as part of group of containers started with docker-compose . Follow the instructions for you operating system. For MacOS, Docker Desktop is installed like a standard application. Once it is installed, it can be launched like any other application, such as through the launchpad / start menu. Further information If you are working through the Egeria Dojo, you can return to the guide for Day 1 of the Egeria Dojo Otherwise, use the Docker tutorial to get the image up and running in Docker. It describes how to install, set up Docker and make use of Egeria's Docker image. Kubernetes \u00b6 Kubernetes orchestrates (starts / stops / connects) different containers, such as Docker containers together so that they can be managed as a complete solution. Kubernetes is an open source project managed by the Cloud Native Computing Foundation . Egeria uses Kubernetes to run all the components in the Coco Pharmaceuticals hands-on labs . Helm \u00b6 Helm is a package manager for Kubernetes . Through Helm, a chart can be used to deploy multiple containers and other related components as a single deployable unit. This makes it one simple step to deploy what may otherwise be a complex solution composed of multiple runtimes (like Egeria's OMAG Server Platform , Apache Kafka , JupyterHub , and so on) -- without needing to obtain or download all the various runtimes, know how to operate them, and so on. The Egeria team maintains an odpi-egeria-lab Helm chart that can be used to automatically deploy all the components necessary to run through the hands-on labs . Spring \u00b6 Spring is a framework and set of annotations for building REST APIs. Spring Boot provides the server chassis (or main() method) for hosting RESTful services in a server. It is used in the OMAG Server Platform to provide the server chassis that searches for all REST API definitions to start them in a server. Spring is used in our client libraries to call REST APIs. Specifically it provides the org.springframework.web.client.RestTemplate class for formatting REST calls and parsing the responses. On the server-side, Spring provides the annotations that define how a Java method is exposed as a REST API. This includes the URL of the call, and how the parameters and responses are mapped. A REST API is typically implemented as a single Java class where each method is a different operation on the REST API. At the top of the Java class is a declaration of the URI that is common for all methods in the class. Example: declaration used for the OMRS REST APIs 1 2 @RestController @RequestMapping ( \"/open-metadata/repository-services\" ) Such a URI follows the root URL of the server: so if the server was using https://localhost:9443 , the methods are called using: https://localhost:9443/open-metadata/repository-services ... For each method / operation, the rest of the URL is defined and mapped through additional annotations. Example: operation defined through annotation 1 2 3 4 5 6 7 @GetMapping ( path = \"/metadata-collection-id\" ) public MetadataCollectionIdResponse getMetadataCollectionId () { /* * ... implementation here */ } Jupyter Notebooks \u00b6 Project Jupyter provides tools for interactive computing. In particular, we use Jupyter notebooks to provide an interactive environment for running snippets of Python code, interspersed with Markdown documentation, for our hands-on labs . A free version of the latest Jupyter Notebook support (called JupyterHub ) can be installed in various ways. Installing JupyterHub on MacOS Using HomeBrew you can simply run: brew install jupyterlab","title":"Runtime Tools"},{"location":"guides/developer/tools/runtime/#runtime-tools","text":"","title":"Runtime Tools"},{"location":"guides/developer/tools/runtime/#apache-kafka","text":"Apache Kafka is an event bus that is used to pass events between different Egeria servers. Kafka's own QuickStart Guide covers installation and basic usage. You may alternatively wish to install Kafka using a package manager such as HomeBrew on MacOS. For Egeria, the Kafka server needs to be running a PLAINTEXT listener. From the directory where Kafka is installed, check the config/server.properties file so that the listeners and advertised.listeners are setup as follows: Example: Kafka configuration listeners=PLAINTEXT://localhost:9092 advertised.listeners=PLAINTEXT://localhost:5092 The example above uses localhost:9092 for simplicity, assuming you are running Kafka locally on the same machine where you are doing your development. If running elsewhere, replace this with a name that is fully network resolveable (i.e. by both the host running Kafka and the client machines that will connect to Kafka from other hosts). Starting Kafka: MacOS When installed via HomeBrew brew services start zookeeper brew services start kafka Linux From within the bin folder of Apache Kafka ./zookeeper-server-start.sh ../config/zookeeper.properties & rm -rf /tmp/kafka-logs/* ./kafka-server-start.sh ../config/server.properties Shutting down Kafka: MacOS When installed via HomeBrew brew services stop kafka brew services stop zookeeper Linux From within the bin folder of Apache Kafka ./kafka-server-stop.sh ./zookeeper-server-stop.sh","title":"Apache Kafka"},{"location":"guides/developer/tools/runtime/#docker","text":"Docker is a simple container runtime and standard . Every day, the Egeria build processing creates a Docker image of Egeria and pushes it to the public Docker catalog called Docker Hub . This docker image provides a simple way to bring a runnable version of Egeria onto your machine without any additional dependencies aside from Docker itself. It also provides the basis for a Kubernetes deployment of Egeria. The Overview tab describes the docker container. The Tags tab shows the different releases that are available. The docker image needs a runtime to execute. It can run in the cloud using various platforms or on your machine using Docker Desktop . Docker Desktop supports running a docker image as a standalone container, or as part of group of containers started with docker-compose . Follow the instructions for you operating system. For MacOS, Docker Desktop is installed like a standard application. Once it is installed, it can be launched like any other application, such as through the launchpad / start menu. Further information If you are working through the Egeria Dojo, you can return to the guide for Day 1 of the Egeria Dojo Otherwise, use the Docker tutorial to get the image up and running in Docker. It describes how to install, set up Docker and make use of Egeria's Docker image.","title":"Docker"},{"location":"guides/developer/tools/runtime/#kubernetes","text":"Kubernetes orchestrates (starts / stops / connects) different containers, such as Docker containers together so that they can be managed as a complete solution. Kubernetes is an open source project managed by the Cloud Native Computing Foundation . Egeria uses Kubernetes to run all the components in the Coco Pharmaceuticals hands-on labs .","title":"Kubernetes"},{"location":"guides/developer/tools/runtime/#helm","text":"Helm is a package manager for Kubernetes . Through Helm, a chart can be used to deploy multiple containers and other related components as a single deployable unit. This makes it one simple step to deploy what may otherwise be a complex solution composed of multiple runtimes (like Egeria's OMAG Server Platform , Apache Kafka , JupyterHub , and so on) -- without needing to obtain or download all the various runtimes, know how to operate them, and so on. The Egeria team maintains an odpi-egeria-lab Helm chart that can be used to automatically deploy all the components necessary to run through the hands-on labs .","title":"Helm"},{"location":"guides/developer/tools/runtime/#spring","text":"Spring is a framework and set of annotations for building REST APIs. Spring Boot provides the server chassis (or main() method) for hosting RESTful services in a server. It is used in the OMAG Server Platform to provide the server chassis that searches for all REST API definitions to start them in a server. Spring is used in our client libraries to call REST APIs. Specifically it provides the org.springframework.web.client.RestTemplate class for formatting REST calls and parsing the responses. On the server-side, Spring provides the annotations that define how a Java method is exposed as a REST API. This includes the URL of the call, and how the parameters and responses are mapped. A REST API is typically implemented as a single Java class where each method is a different operation on the REST API. At the top of the Java class is a declaration of the URI that is common for all methods in the class. Example: declaration used for the OMRS REST APIs 1 2 @RestController @RequestMapping ( \"/open-metadata/repository-services\" ) Such a URI follows the root URL of the server: so if the server was using https://localhost:9443 , the methods are called using: https://localhost:9443/open-metadata/repository-services ... For each method / operation, the rest of the URL is defined and mapped through additional annotations. Example: operation defined through annotation 1 2 3 4 5 6 7 @GetMapping ( path = \"/metadata-collection-id\" ) public MetadataCollectionIdResponse getMetadataCollectionId () { /* * ... implementation here */ }","title":"Spring"},{"location":"guides/developer/tools/runtime/#jupyter-notebooks","text":"Project Jupyter provides tools for interactive computing. In particular, we use Jupyter notebooks to provide an interactive environment for running snippets of Python code, interspersed with Markdown documentation, for our hands-on labs . A free version of the latest Jupyter Notebook support (called JupyterHub ) can be installed in various ways. Installing JupyterHub on MacOS Using HomeBrew you can simply run: brew install jupyterlab","title":"Jupyter Notebooks"},{"location":"guides/developer/tools/testing/","text":"Testing Tools \u00b6 Postman \u00b6 Postman is an interactive tool for calling REST APIs. The Egeria community uses Postman for demos and education as well as testing APIs during development. Disable SSL certificate verification Egeria by default uses secure HTTP requests ( https:// ) with a self-signed certificate. By default, Postman does not allow self-signed certificates. Any Postman users will therefore need to go into Preferences -> Settings and on the General tab, turn off SSL certificate verification or requests will fail. Further information Egeria-specific Postman tutorial . Adding Postman samples Command-line request tools \u00b6 In addition to Postman there are command line tools for calling REST APIs. curl \u00b6 The command that is most commonly available is curl . Example curl command curl --insecure -X GET https://localhost:9443/open-metadata/platform-services/users/test/server-platform/origin Disable SSL certificate verification Note that Egeria is using https:// , so if you have not replaced the provided self-signed certificate, ensure you include --insecure on any requests to skip certificate validation. HTTPie \u00b6 As an alternative to curl you might like to try HTTPie , which has more advanced functions. Disable SSL certificate verification Note that Egeria is using https:// , so if you have not replaced the provided self-signed certificate, ensure you include --verify no to any requests to skip certificate validation.","title":"Testing Tools"},{"location":"guides/developer/tools/testing/#testing-tools","text":"","title":"Testing Tools"},{"location":"guides/developer/tools/testing/#postman","text":"Postman is an interactive tool for calling REST APIs. The Egeria community uses Postman for demos and education as well as testing APIs during development. Disable SSL certificate verification Egeria by default uses secure HTTP requests ( https:// ) with a self-signed certificate. By default, Postman does not allow self-signed certificates. Any Postman users will therefore need to go into Preferences -> Settings and on the General tab, turn off SSL certificate verification or requests will fail. Further information Egeria-specific Postman tutorial . Adding Postman samples","title":"Postman"},{"location":"guides/developer/tools/testing/#command-line-request-tools","text":"In addition to Postman there are command line tools for calling REST APIs.","title":"Command-line request tools"},{"location":"guides/developer/tools/testing/#curl","text":"The command that is most commonly available is curl . Example curl command curl --insecure -X GET https://localhost:9443/open-metadata/platform-services/users/test/server-platform/origin Disable SSL certificate verification Note that Egeria is using https:// , so if you have not replaced the provided self-signed certificate, ensure you include --insecure on any requests to skip certificate validation.","title":"curl"},{"location":"guides/developer/tools/testing/#httpie","text":"As an alternative to curl you might like to try HTTPie , which has more advanced functions. Disable SSL certificate verification Note that Egeria is using https:// , so if you have not replaced the provided self-signed certificate, ensure you include --verify no to any requests to skip certificate validation.","title":"HTTPie"},{"location":"guides/documentation/formatting/","text":"Formatting Standards \u00b6 These formatting standards exist to keep the content of the documentation consistent. Links \u00b6 Link within docs using absolute links \u00b6 When linking between pages in the documentation use the regular Markdown linking syntax, but with the absolute path to the Markdown document or area you wish to link to. For example: ... is a type of [ OMAG Server ]( /egeria-docs/concepts/omag-server ) that ... Note that we do not need to point at a specific Markdown file Note from the example above that we do not need to point at a specific Markdown file. In the example above, the actual file is omag-server.md ; however, MkDocs will automatically generate an omag-server/index.html output for this Markdown which our URL above refers to. This syntax is useful because it means that if we start with a simple single file for some content (like omag-server.md ) but later decide it needs additional explanation across various other files, we can simply turn this area of documentation into a directory ( omag-server ) with many files within it. The link above does not need to change and will simply point to the omag-server/index.md that should exist after moving from single-file to a directory-based documentation for that area. This ensures certain contexts in the documentation hierarchy are retained, even if deeply-nested documentation itself happens to move around, and takes advantage of the way MkDocs works to give us additional flexibility in extending the documentation later on without needing to retroactively go back and fix links everywhere that refers to that content. Link to code with a GitHub icon \u00b6 When linking to one of the code repositories, add both a :material-github: icon to the end of the link name, and a target to the link: [ `Foo` :material-github:](https://github.com/.../Foo.java){ target=gh } This ensures that a reader clicking the link ( Foo ) will know that they will be directed to code, and that it will open in a new tab / window. Link to other sites with a window icon \u00b6 When linking to some other non-Egeria site, add both a :material-dock-window: icon to the end of the link name, and a target to the link: [ some example :material-dock-window: ]( https://example.com ){ target=example } This ensures that a reader clicking the link will know that they will be going to some non-Egeria site, and it will open in a new tab / window. Prefer SVG format for diagrams \u00b6 Use regular Markdown syntax for images. For example: ![ Description of what the image depicts ]( image-filename.svg ) To make localization easier and enhance accessibility, the preferred image format is SVG. Use draw.io for creating images and diagrams. To save, follow these steps: Select everything you want to include in the diagram (e.g. Ctrl + A / Cmd + A ). Use File , Export as , SVG... to save your image in SVG format. Check the Selection Only box, and ensure that the Size drop-down changes to Selection Only . Check the Transparent Background box. Keep the Include a copy of my diagram option checked to allow later loading the SVG in draw.io. If your diagram contains any embedded images (rare), be sure to check Embed Images as well. Click the Export button to save the file. If your diagram depicts a process, try to avoid adding the descriptions of the steps to the diagram. Instead, only add the numbers of the steps to the diagram and add the descriptions of the steps as a numbered list in the document. Ensure that the numbers on the list match the numbers on your diagram. The SVG format allows the diagram to be dynamically scaled in the browser without introducing various image artifacts that create noise or otherwise impair visibility of the image: making the diagrams easier to understand. Providing a description of the image is required for accessibility purposes, as it will act as the alternative text for e.g. screen-readers for anyone who is unable to see the image itself. Avoiding text descriptions and explanations embedded in the diagrams means that the content is also more accessible (again for screen-readers), but also more broadly in that it is then indexed by the documentation site and can be searched. (Text embedded inside a diagram or image is not searchable.) Do not wrap lines \u00b6 Avoid wrapping lines after a fixed number of characters or in a middle of a sentence. Instead, configure your editor to soft-wrap when editing documentation. Do Don't This is a long line. This is a long line. This enhances the maintainability of the documentation by a broader audience: different people will inevitably have different preferences for screen sizes and widths (we do not need to worry about constantly reformatting or scrolling what one or another contributor has decided is their own optimal line-break size) reviewing a diff of changes will be easier to identify actual content vs spacing / newline positioning changes changes in indentation that may be needed for e.g. bullet lists, inclusion within an admonition, etc will only require indenting the single wrapped line including content into a table in Markdown (which does not allow newlines within it) will be easier Use angle brackets for placeholders \u00b6 Use angle brackets for placeholders in commands or code samples. Tell the reader what the placeholder represents. For example: Display information about a pod: $ kubectl describe pod <pod-name> Where <pod-name> is the name of one of your pods. We may revise this approach in the future We may revise this approach if / when we start to make use of the theme's extensive code annotation capabilities that allow such descriptions of the placeholders to be embedded more directly within the code block itself. Use bold to emphasize user interface elements \u00b6 Use Markdown's double-asterisk to indicate bold : Click **Fork** . Do Don't Click Fork . Click \"Fork\". Select Other . Select 'Other'. This enhances the readability of the key actions that a user is expected to take through a user interface. Why use double-asterisk ( ** ) and not double-underscore ( __ )? Markdown applications in general do not agree on how to handle underscores in the middle of words. While we would expect this need to be rare, the double-asterisk form allows bold to be used even within the middle of a word. Use bold to emphasize important text \u00b6 Use bold to emphasize text that is particularly important. Avoid overusing bold as it reduces its impact and readability. Do Don't Examples of bad configurations: Examples of bad configurations : The maximum length of the name field is 256 characters . The maximum length of the name field is 256 characters. Do not use capitalization for emphasis \u00b6 Only use the original capitalization found in the code or configuration files when referencing those values directly. Use back-ticks ` ` around the referenced value to make the connection explicit. For example, use InstanceHeader , not Instance Header or instance header . If you are not referencing values or code directly, use normal sentence capitalization, for example, \"The instance header captures key information about the metadata instance like its GUID .\" For code, the back-tick form is intended to represent exactly what you're referring to, so you should specify it exactly as it is defined: with precisely the same spacing, capitalization, etc. For non-code values, mixing capitalization makes the text harder to scan and comprehend, as well as more difficult and therefore stressful to read . Therefore, using normal sentence capitalization greatly enhances the readability of the content. The only exceptions to this should be as follows: Proper nouns (i.e. Egeria) Any phrase that is prefixed or suffixed with Open Metadata (or an OMxx abbreviation) Any phrase that we commonly abbreviate using an acronym (i.e. frameworks like Open Discovery Framework): check the snippets/abbr.md for a list of such common abbreviations. Do Don't Egeria egeria Open Metadata Repository Services ( OMRS ) open metadata repository services repository services Repository Services Asset Consumer OMAS asset consumer OMAS OMAG Server Platform OMAG server platform OMAG Server OMAG server metadata access point Metadata Access Point Metadata Access Point OMAG Server metadata access point OMAG Server Open Discovery Framework ( ODF ) open discovery framework Audit Log Framework ( ALF ) audit log framework Use italics to emphasize new terms \u00b6 Use Markdown's single-asterisk to indicate italics : Do Don't A cluster is a set of nodes ... A \"cluster\" is a set of nodes ... These components form the control plane . These components form the control plane . Why use single-asterisk ( * ) and not single-underscore ( _ )? Markdown applications in general do not agree on how to handle underscores in the middle of words. While we would expect this need to be rare, the single-asterisk form allows italics to be used even within the middle of a word. Use back-ticks around file names, directories, and paths \u00b6 Do Don't Open the foo.yaml file. Open the foo.yaml file. Go to the /content/docs/architecture directory. Go to the /content/docs/architecture directory. Open the /data/args.yaml file. Open the /data/args.yaml file. Go to the /docs directory. Go to the \"/docs\" directory. Use back-ticks around inline code, commands, objects and field names \u00b6 Do Don't The foo run command creates a Deployment . The \"foo run\" command creates a Deployment . For declarative management, use foo apply . For declarative management, use foo apply . Set the value of the ports field in the configuration file. Set the value of the \"ports\" field in the configuration file. The value of the rule field is a Rule object. The value of the rule field is a Rule object. Only use inline code and commands to mention specific labels, flags, values, functions, objects, variables, modules, or commands. Code blocks \u00b6 Use the provided custom admonitions for code and configuration blocks, within which the standard Markdown triple back-tick approach can be used to give the code block itself. Using the triple back-tick approach ensures that we can include syntax highlighting for the specific language being shown within the code block, and provides a simple single-click link mechanism for the reader to copy the entire content of that code block for further revision or pasting elsewhere. Each admonition wraps these to give a distinct visual cue as to its content: allowing us to avoid other cues that we might otherwise need to embed within the code block itself (for example, the $ or # command-line prompts). Avoiding the inclusion of this visual cue information in the text means that readers who copy the text do not need to manually remove this from what's been copied before making use of it. Finally, by using admonitions we not only have the visual cue but can easily make use of the distinct capabilities of the !!! , ??? and ???+ syntax to always display the content, start with the content collapsed but allow it to be expanded, or start with it expanded but allow it to be collapsed (respectively). Use cli admonition for commands \u00b6 Use the cli admonition to wrap the code block in a visual cue that indicates a command: ???+ cli \"Brief description of command\" ```shell kubectl get pods ``` Leave out any command-line prompt ( $ , # , etc). Brief description of command kubectl get pods Use post , get , put , delete admonitions for APIs \u00b6 For APIs, use the distinct admonition that represents the type of operation: post , get , put , delete . ???+ get \"GET - brief description of API call\" ``` {{platformURLRoot}}/open-metadata/... ``` While the colors have been chosen to align with the Swagger documentation colors for each operation, for accessibility purposes always start the description of the API call with the operation type. For parameters in the URL, always do the following: Specify the platform URL variable at the beginning, always named {{platformURLRoot}} Specify the administrative user variable as {{adminUserId}} Specify the server's name variable as {{serverName}} Specify any other parameters or variables in the URL using the double-curly-brace formatting {{...}} . This ensures that the endpoint can be quickly copied into tools like Postman, and by using the same variable names consistently someone can setup their own environment definition once and make use of all the API calls they decide to copy into the tool without needing to manually make each parameter consistent. GET - brief description of API call {{platformURLRoot}}/open-metadata/... POST - brief description of API call {{platformURLRoot}}/open-metadata/... PUT - brief description of API call {{platformURLRoot}}/open-metadata/... DELETE - brief description of API call {{platformURLRoot}}/open-metadata/... Use example admonition for file content \u00b6 Use the example admonition to wrap a code block that represents a file's contents, and include both the type of content ( json , etc) and the linenums=\"1\" directives next to the opening triple back-tick . Use two spaces for indentation of lines. ???+ example \"Name or brief description of file's purpose\" ```json linenums=\"1\" { \"foo\": \"bar\", \"baz\": { } } ``` This ensures that the syntax of the file is appropriately highlighted, that line numbers are printed for ease of reference, and that the indentation is not overly-intrusive (causing horizontal scrolling within the code block to become necessary). Name or brief description of file's purpose 1 2 3 4 5 { \"foo\" : \"bar\" , \"baz\" : { } } Use hyphen ( - ) for unordered lists \u00b6 Use a single hyphen ( - ) for unordered lists rather than a single asterisk ( * ). This should avoid the potential for misinterpretation by the generator that a bulleted list is text that we intended (or not) to be either bolded or italicized. Include license header \u00b6 Every Markdown document should include a license header with the CC-BY-4.0 attribute license: License header for documentation files 1 2 <!-- SPDX-License-Identifier: CC-BY-4.0 --> <!-- Copyright Contributors to the Egeria project. --> Remove license footer \u00b6 The MkDocs generator automatically includes a footer at the bottom of every page on the site, which includes displaying overall copyright information and the CC-BY-4.0 license (and link). Therefore, these footers should be removed from the Markdown files themselves. Include abbreviations snippet \u00b6 A list of abbreviations is being maintained under snippets/abbr.md . This snippet should therefore be included in every Markdown document to automatically highlight and provide a hover-over expansion for acronyms. Add this line, on its own, to the end of every (non-snippet) Markdown document: --8<-- \"snippets/abbr.md\"","title":"Formatting Standards"},{"location":"guides/documentation/formatting/#formatting-standards","text":"These formatting standards exist to keep the content of the documentation consistent.","title":"Formatting Standards"},{"location":"guides/documentation/formatting/#links","text":"","title":"Links"},{"location":"guides/documentation/formatting/#link-within-docs-using-absolute-links","text":"When linking between pages in the documentation use the regular Markdown linking syntax, but with the absolute path to the Markdown document or area you wish to link to. For example: ... is a type of [ OMAG Server ]( /egeria-docs/concepts/omag-server ) that ... Note that we do not need to point at a specific Markdown file Note from the example above that we do not need to point at a specific Markdown file. In the example above, the actual file is omag-server.md ; however, MkDocs will automatically generate an omag-server/index.html output for this Markdown which our URL above refers to. This syntax is useful because it means that if we start with a simple single file for some content (like omag-server.md ) but later decide it needs additional explanation across various other files, we can simply turn this area of documentation into a directory ( omag-server ) with many files within it. The link above does not need to change and will simply point to the omag-server/index.md that should exist after moving from single-file to a directory-based documentation for that area. This ensures certain contexts in the documentation hierarchy are retained, even if deeply-nested documentation itself happens to move around, and takes advantage of the way MkDocs works to give us additional flexibility in extending the documentation later on without needing to retroactively go back and fix links everywhere that refers to that content.","title":"Link within docs using absolute links"},{"location":"guides/documentation/formatting/#link-to-code-with-a-github-icon","text":"When linking to one of the code repositories, add both a :material-github: icon to the end of the link name, and a target to the link: [ `Foo` :material-github:](https://github.com/.../Foo.java){ target=gh } This ensures that a reader clicking the link ( Foo ) will know that they will be directed to code, and that it will open in a new tab / window.","title":"Link to code with a GitHub icon"},{"location":"guides/documentation/formatting/#link-to-other-sites-with-a-window-icon","text":"When linking to some other non-Egeria site, add both a :material-dock-window: icon to the end of the link name, and a target to the link: [ some example :material-dock-window: ]( https://example.com ){ target=example } This ensures that a reader clicking the link will know that they will be going to some non-Egeria site, and it will open in a new tab / window.","title":"Link to other sites with a window icon"},{"location":"guides/documentation/formatting/#prefer-svg-format-for-diagrams","text":"Use regular Markdown syntax for images. For example: ![ Description of what the image depicts ]( image-filename.svg ) To make localization easier and enhance accessibility, the preferred image format is SVG. Use draw.io for creating images and diagrams. To save, follow these steps: Select everything you want to include in the diagram (e.g. Ctrl + A / Cmd + A ). Use File , Export as , SVG... to save your image in SVG format. Check the Selection Only box, and ensure that the Size drop-down changes to Selection Only . Check the Transparent Background box. Keep the Include a copy of my diagram option checked to allow later loading the SVG in draw.io. If your diagram contains any embedded images (rare), be sure to check Embed Images as well. Click the Export button to save the file. If your diagram depicts a process, try to avoid adding the descriptions of the steps to the diagram. Instead, only add the numbers of the steps to the diagram and add the descriptions of the steps as a numbered list in the document. Ensure that the numbers on the list match the numbers on your diagram. The SVG format allows the diagram to be dynamically scaled in the browser without introducing various image artifacts that create noise or otherwise impair visibility of the image: making the diagrams easier to understand. Providing a description of the image is required for accessibility purposes, as it will act as the alternative text for e.g. screen-readers for anyone who is unable to see the image itself. Avoiding text descriptions and explanations embedded in the diagrams means that the content is also more accessible (again for screen-readers), but also more broadly in that it is then indexed by the documentation site and can be searched. (Text embedded inside a diagram or image is not searchable.)","title":"Prefer SVG format for diagrams"},{"location":"guides/documentation/formatting/#do-not-wrap-lines","text":"Avoid wrapping lines after a fixed number of characters or in a middle of a sentence. Instead, configure your editor to soft-wrap when editing documentation. Do Don't This is a long line. This is a long line. This enhances the maintainability of the documentation by a broader audience: different people will inevitably have different preferences for screen sizes and widths (we do not need to worry about constantly reformatting or scrolling what one or another contributor has decided is their own optimal line-break size) reviewing a diff of changes will be easier to identify actual content vs spacing / newline positioning changes changes in indentation that may be needed for e.g. bullet lists, inclusion within an admonition, etc will only require indenting the single wrapped line including content into a table in Markdown (which does not allow newlines within it) will be easier","title":"Do not wrap lines"},{"location":"guides/documentation/formatting/#use-angle-brackets-for-placeholders","text":"Use angle brackets for placeholders in commands or code samples. Tell the reader what the placeholder represents. For example: Display information about a pod: $ kubectl describe pod <pod-name> Where <pod-name> is the name of one of your pods. We may revise this approach in the future We may revise this approach if / when we start to make use of the theme's extensive code annotation capabilities that allow such descriptions of the placeholders to be embedded more directly within the code block itself.","title":"Use angle brackets for placeholders"},{"location":"guides/documentation/formatting/#use-bold-to-emphasize-user-interface-elements","text":"Use Markdown's double-asterisk to indicate bold : Click **Fork** . Do Don't Click Fork . Click \"Fork\". Select Other . Select 'Other'. This enhances the readability of the key actions that a user is expected to take through a user interface. Why use double-asterisk ( ** ) and not double-underscore ( __ )? Markdown applications in general do not agree on how to handle underscores in the middle of words. While we would expect this need to be rare, the double-asterisk form allows bold to be used even within the middle of a word.","title":"Use bold to emphasize user interface elements"},{"location":"guides/documentation/formatting/#use-bold-to-emphasize-important-text","text":"Use bold to emphasize text that is particularly important. Avoid overusing bold as it reduces its impact and readability. Do Don't Examples of bad configurations: Examples of bad configurations : The maximum length of the name field is 256 characters . The maximum length of the name field is 256 characters.","title":"Use bold to emphasize important text"},{"location":"guides/documentation/formatting/#do-not-use-capitalization-for-emphasis","text":"Only use the original capitalization found in the code or configuration files when referencing those values directly. Use back-ticks ` ` around the referenced value to make the connection explicit. For example, use InstanceHeader , not Instance Header or instance header . If you are not referencing values or code directly, use normal sentence capitalization, for example, \"The instance header captures key information about the metadata instance like its GUID .\" For code, the back-tick form is intended to represent exactly what you're referring to, so you should specify it exactly as it is defined: with precisely the same spacing, capitalization, etc. For non-code values, mixing capitalization makes the text harder to scan and comprehend, as well as more difficult and therefore stressful to read . Therefore, using normal sentence capitalization greatly enhances the readability of the content. The only exceptions to this should be as follows: Proper nouns (i.e. Egeria) Any phrase that is prefixed or suffixed with Open Metadata (or an OMxx abbreviation) Any phrase that we commonly abbreviate using an acronym (i.e. frameworks like Open Discovery Framework): check the snippets/abbr.md for a list of such common abbreviations. Do Don't Egeria egeria Open Metadata Repository Services ( OMRS ) open metadata repository services repository services Repository Services Asset Consumer OMAS asset consumer OMAS OMAG Server Platform OMAG server platform OMAG Server OMAG server metadata access point Metadata Access Point Metadata Access Point OMAG Server metadata access point OMAG Server Open Discovery Framework ( ODF ) open discovery framework Audit Log Framework ( ALF ) audit log framework","title":"Do not use capitalization for emphasis"},{"location":"guides/documentation/formatting/#use-italics-to-emphasize-new-terms","text":"Use Markdown's single-asterisk to indicate italics : Do Don't A cluster is a set of nodes ... A \"cluster\" is a set of nodes ... These components form the control plane . These components form the control plane . Why use single-asterisk ( * ) and not single-underscore ( _ )? Markdown applications in general do not agree on how to handle underscores in the middle of words. While we would expect this need to be rare, the single-asterisk form allows italics to be used even within the middle of a word.","title":"Use italics to emphasize new terms"},{"location":"guides/documentation/formatting/#use-back-ticks-around-file-names-directories-and-paths","text":"Do Don't Open the foo.yaml file. Open the foo.yaml file. Go to the /content/docs/architecture directory. Go to the /content/docs/architecture directory. Open the /data/args.yaml file. Open the /data/args.yaml file. Go to the /docs directory. Go to the \"/docs\" directory.","title":"Use back-ticks around file names, directories, and paths"},{"location":"guides/documentation/formatting/#use-back-ticks-around-inline-code-commands-objects-and-field-names","text":"Do Don't The foo run command creates a Deployment . The \"foo run\" command creates a Deployment . For declarative management, use foo apply . For declarative management, use foo apply . Set the value of the ports field in the configuration file. Set the value of the \"ports\" field in the configuration file. The value of the rule field is a Rule object. The value of the rule field is a Rule object. Only use inline code and commands to mention specific labels, flags, values, functions, objects, variables, modules, or commands.","title":"Use back-ticks around inline code, commands, objects and field names"},{"location":"guides/documentation/formatting/#code-blocks","text":"Use the provided custom admonitions for code and configuration blocks, within which the standard Markdown triple back-tick approach can be used to give the code block itself. Using the triple back-tick approach ensures that we can include syntax highlighting for the specific language being shown within the code block, and provides a simple single-click link mechanism for the reader to copy the entire content of that code block for further revision or pasting elsewhere. Each admonition wraps these to give a distinct visual cue as to its content: allowing us to avoid other cues that we might otherwise need to embed within the code block itself (for example, the $ or # command-line prompts). Avoiding the inclusion of this visual cue information in the text means that readers who copy the text do not need to manually remove this from what's been copied before making use of it. Finally, by using admonitions we not only have the visual cue but can easily make use of the distinct capabilities of the !!! , ??? and ???+ syntax to always display the content, start with the content collapsed but allow it to be expanded, or start with it expanded but allow it to be collapsed (respectively).","title":"Code blocks"},{"location":"guides/documentation/formatting/#use-cli-admonition-for-commands","text":"Use the cli admonition to wrap the code block in a visual cue that indicates a command: ???+ cli \"Brief description of command\" ```shell kubectl get pods ``` Leave out any command-line prompt ( $ , # , etc). Brief description of command kubectl get pods","title":"Use cli admonition for commands"},{"location":"guides/documentation/formatting/#use-post-get-put-delete-admonitions-for-apis","text":"For APIs, use the distinct admonition that represents the type of operation: post , get , put , delete . ???+ get \"GET - brief description of API call\" ``` {{platformURLRoot}}/open-metadata/... ``` While the colors have been chosen to align with the Swagger documentation colors for each operation, for accessibility purposes always start the description of the API call with the operation type. For parameters in the URL, always do the following: Specify the platform URL variable at the beginning, always named {{platformURLRoot}} Specify the administrative user variable as {{adminUserId}} Specify the server's name variable as {{serverName}} Specify any other parameters or variables in the URL using the double-curly-brace formatting {{...}} . This ensures that the endpoint can be quickly copied into tools like Postman, and by using the same variable names consistently someone can setup their own environment definition once and make use of all the API calls they decide to copy into the tool without needing to manually make each parameter consistent. GET - brief description of API call {{platformURLRoot}}/open-metadata/... POST - brief description of API call {{platformURLRoot}}/open-metadata/... PUT - brief description of API call {{platformURLRoot}}/open-metadata/... DELETE - brief description of API call {{platformURLRoot}}/open-metadata/...","title":"Use post, get, put, delete admonitions for APIs"},{"location":"guides/documentation/formatting/#use-example-admonition-for-file-content","text":"Use the example admonition to wrap a code block that represents a file's contents, and include both the type of content ( json , etc) and the linenums=\"1\" directives next to the opening triple back-tick . Use two spaces for indentation of lines. ???+ example \"Name or brief description of file's purpose\" ```json linenums=\"1\" { \"foo\": \"bar\", \"baz\": { } } ``` This ensures that the syntax of the file is appropriately highlighted, that line numbers are printed for ease of reference, and that the indentation is not overly-intrusive (causing horizontal scrolling within the code block to become necessary). Name or brief description of file's purpose 1 2 3 4 5 { \"foo\" : \"bar\" , \"baz\" : { } }","title":"Use example admonition for file content"},{"location":"guides/documentation/formatting/#use-hyphen-for-unordered-lists","text":"Use a single hyphen ( - ) for unordered lists rather than a single asterisk ( * ). This should avoid the potential for misinterpretation by the generator that a bulleted list is text that we intended (or not) to be either bolded or italicized.","title":"Use hyphen (-) for unordered lists"},{"location":"guides/documentation/formatting/#include-license-header","text":"Every Markdown document should include a license header with the CC-BY-4.0 attribute license: License header for documentation files 1 2 <!-- SPDX-License-Identifier: CC-BY-4.0 --> <!-- Copyright Contributors to the Egeria project. -->","title":"Include license header"},{"location":"guides/documentation/formatting/#remove-license-footer","text":"The MkDocs generator automatically includes a footer at the bottom of every page on the site, which includes displaying overall copyright information and the CC-BY-4.0 license (and link). Therefore, these footers should be removed from the Markdown files themselves.","title":"Remove license footer"},{"location":"guides/documentation/formatting/#include-abbreviations-snippet","text":"A list of abbreviations is being maintained under snippets/abbr.md . This snippet should therefore be included in every Markdown document to automatically highlight and provide a hover-over expansion for acronyms. Add this line, on its own, to the end of every (non-snippet) Markdown document: --8<-- \"snippets/abbr.md\"","title":"Include abbreviations snippet"},{"location":"guides/documentation/guide/","text":"Documentation Guide \u00b6 We strive to provide complete, concise and maintainable documentation for the project. To achieve these goals, all contributed documentation should adhere to the guidance provided in this documentation guide , split into the following sub-guides: Formatting standards to keep the content of the documentation consistent. Style guide to keep the documentation clear and understandable. Technical context \u00b6 The repository hosting the documentation for Egeria is separate from the individual code repositories, and is maintained at odpi/egeria-docs . The documentation itself is all managed as Markdown files for ease of maintenance and readability. We use the MkDocs generator along with the Material for MkDocs theme to generate a static website from these Markdown files. This static website is deployed to the gh-pages branch of the documentation repository, from which GitHub pages then hosts the documentation website itself. Adding to the documentation \u00b6 When contributing to the documentation, you can easily use this to test documentation changes out on your own machine locally before committing them in a PR : On a Mac, just do a brew install mkdocs (requires HomeBrew ). Then do a pip3 install mkdocs-material [--upgrade] (the last bit you might want to add and re-run periodically to keep things up-to-date). From the site directory of your local clone of this repository, you can then just run: $ mkdocs serve ... and you'll shortly have a local documentation site running on your local machine, complete with any changes you make (each change you make will auto-refresh the content and your browser). Look for warnings As part of running its generator, MkDocs will automatically warn you if it finds any broken links within your documentation. Pay close attention to these, as the PR merge mechanism for documentation will strictly enforce that no broken links are found in order to merge any changes into the main documentation. Special handling of index.md Be aware that the specific theme we use will treat every index.md as an index page for a section: these behave in special ways depending on the level of the navigation in which they appear. Unless you mean to create an overview of some section of the navigation, do not name your Markdown file index.md .","title":"Documentation Guide"},{"location":"guides/documentation/guide/#documentation-guide","text":"We strive to provide complete, concise and maintainable documentation for the project. To achieve these goals, all contributed documentation should adhere to the guidance provided in this documentation guide , split into the following sub-guides: Formatting standards to keep the content of the documentation consistent. Style guide to keep the documentation clear and understandable.","title":"Documentation Guide"},{"location":"guides/documentation/guide/#technical-context","text":"The repository hosting the documentation for Egeria is separate from the individual code repositories, and is maintained at odpi/egeria-docs . The documentation itself is all managed as Markdown files for ease of maintenance and readability. We use the MkDocs generator along with the Material for MkDocs theme to generate a static website from these Markdown files. This static website is deployed to the gh-pages branch of the documentation repository, from which GitHub pages then hosts the documentation website itself.","title":"Technical context"},{"location":"guides/documentation/guide/#adding-to-the-documentation","text":"When contributing to the documentation, you can easily use this to test documentation changes out on your own machine locally before committing them in a PR : On a Mac, just do a brew install mkdocs (requires HomeBrew ). Then do a pip3 install mkdocs-material [--upgrade] (the last bit you might want to add and re-run periodically to keep things up-to-date). From the site directory of your local clone of this repository, you can then just run: $ mkdocs serve ... and you'll shortly have a local documentation site running on your local machine, complete with any changes you make (each change you make will auto-refresh the content and your browser). Look for warnings As part of running its generator, MkDocs will automatically warn you if it finds any broken links within your documentation. Pay close attention to these, as the PR merge mechanism for documentation will strictly enforce that no broken links are found in order to merge any changes into the main documentation. Special handling of index.md Be aware that the specific theme we use will treat every index.md as an index page for a section: these behave in special ways depending on the level of the navigation in which they appear. Unless you mean to create an overview of some section of the navigation, do not name your Markdown file index.md .","title":"Adding to the documentation"},{"location":"guides/documentation/style/","text":"Style Guide \u00b6 The style guidelines exist to keep the documentation clear and understandable. Choose the right title \u00b6 Use a short, keyword-rich title that captures the intent of the document and draws the reader in. Ensure that the title clearly and concisely conveys the content or subject matter and is meaningful to a global audience. The text for the title of the document must use title case. Capitalize the first letter of every word except conjunctions and prepositions. Do Don't # Security Architecture # Security architecture # Code of Conduct # Code Of Conduct Use sentence case for headings \u00b6 Use sentence case for the headings in your document. Only capitalize the first word of the heading, except for proper nouns or acronyms. Do Don't Configuring rate limits Configuring Rate Limits Using Envoy for ingress Using envoy for ingress Using HTTPS Using https Use present tense \u00b6 Do Don't This command starts a proxy. This command will start a proxy. Exception Use future or past tense if it is required to convey the correct meaning. This exception is extremely rare and should be avoided. Use active voice \u00b6 Do Don't You can explore the API using a browser. The API can be explored using a browser. The YAML file specifies the replica count. The replica count is specified in the YAML file. Use simple and direct language \u00b6 Use simple and direct language. Avoid using unnecessary phrases, such as saying \"please.\" Do Don't To create a ReplicaSet , ... In order to create a ReplicaSet , ... See the configuration file. Please see the configuration file. View the Pods. With this next command, we'll view the Pods. Prefer shorter words over longer alternatives \u00b6 Do Don't This tool helps scaling up pods. This tool facilitates scaling up pods. Pilot uses the purpose field to ... Pilot utilizes the purpose field to ... Address the reader as \"you\" \u00b6 Do Don't You can create a Deployment by ... We'll create a Deployment by ... In the preceding output, you can see... In the preceding output, we can see ... Avoid using \"we\" \u00b6 Using \"we\" in a sentence can be confusing, because the reader might not know whether they're part of the \"we\" you're describing. Do Don't Version 1.4 includes ... In version 1.4, we have added ... Egeria provides a new feature for ... We provide a new feature ... This page teaches you how to use pods. In this page, we are going to learn about pods. Avoid jargon and idioms \u00b6 Some readers speak English as a second language. Avoid jargon and idioms to help make their understanding easier. Do Don't Internally, ... Under the hood, ... Create a new cluster. Turn up a new cluster. Initially, ... Out of the box, ... Avoid statements that will soon be out of date \u00b6 Avoid using wording that becomes outdated quickly like \"currently\" and \"new\". A feature that is new today is not new for long. Do Don't In version 1.4, ... In the current version, ... The Federation feature provides ... The new Federation feature provides ... Create useful links \u00b6 Don't use here , click here or URLs as the text for a hyperlink. Do Don't ... in a metadata server you can ... ... in a metadata server, which you can read more about here . The metadata access point ... Read more about metadata access points ... The FooBar class ... The FooBar class ( https://example.com/somewhere/something/FooBar.java ) ... There are numerous articles explaining what makes a good hyperlink , but fundamentally the \"Don't\" examples above: Decrease the overall usability of links. Decrease the overall accessibility of the links. Decrease search engine performance and content find-ability.","title":"Style Guide"},{"location":"guides/documentation/style/#style-guide","text":"The style guidelines exist to keep the documentation clear and understandable.","title":"Style Guide"},{"location":"guides/documentation/style/#choose-the-right-title","text":"Use a short, keyword-rich title that captures the intent of the document and draws the reader in. Ensure that the title clearly and concisely conveys the content or subject matter and is meaningful to a global audience. The text for the title of the document must use title case. Capitalize the first letter of every word except conjunctions and prepositions. Do Don't # Security Architecture # Security architecture # Code of Conduct # Code Of Conduct","title":"Choose the right title"},{"location":"guides/documentation/style/#use-sentence-case-for-headings","text":"Use sentence case for the headings in your document. Only capitalize the first word of the heading, except for proper nouns or acronyms. Do Don't Configuring rate limits Configuring Rate Limits Using Envoy for ingress Using envoy for ingress Using HTTPS Using https","title":"Use sentence case for headings"},{"location":"guides/documentation/style/#use-present-tense","text":"Do Don't This command starts a proxy. This command will start a proxy. Exception Use future or past tense if it is required to convey the correct meaning. This exception is extremely rare and should be avoided.","title":"Use present tense"},{"location":"guides/documentation/style/#use-active-voice","text":"Do Don't You can explore the API using a browser. The API can be explored using a browser. The YAML file specifies the replica count. The replica count is specified in the YAML file.","title":"Use active voice"},{"location":"guides/documentation/style/#use-simple-and-direct-language","text":"Use simple and direct language. Avoid using unnecessary phrases, such as saying \"please.\" Do Don't To create a ReplicaSet , ... In order to create a ReplicaSet , ... See the configuration file. Please see the configuration file. View the Pods. With this next command, we'll view the Pods.","title":"Use simple and direct language"},{"location":"guides/documentation/style/#prefer-shorter-words-over-longer-alternatives","text":"Do Don't This tool helps scaling up pods. This tool facilitates scaling up pods. Pilot uses the purpose field to ... Pilot utilizes the purpose field to ...","title":"Prefer shorter words over longer alternatives"},{"location":"guides/documentation/style/#address-the-reader-as-you","text":"Do Don't You can create a Deployment by ... We'll create a Deployment by ... In the preceding output, you can see... In the preceding output, we can see ...","title":"Address the reader as \"you\""},{"location":"guides/documentation/style/#avoid-using-we","text":"Using \"we\" in a sentence can be confusing, because the reader might not know whether they're part of the \"we\" you're describing. Do Don't Version 1.4 includes ... In version 1.4, we have added ... Egeria provides a new feature for ... We provide a new feature ... This page teaches you how to use pods. In this page, we are going to learn about pods.","title":"Avoid using \"we\""},{"location":"guides/documentation/style/#avoid-jargon-and-idioms","text":"Some readers speak English as a second language. Avoid jargon and idioms to help make their understanding easier. Do Don't Internally, ... Under the hood, ... Create a new cluster. Turn up a new cluster. Initially, ... Out of the box, ...","title":"Avoid jargon and idioms"},{"location":"guides/documentation/style/#avoid-statements-that-will-soon-be-out-of-date","text":"Avoid using wording that becomes outdated quickly like \"currently\" and \"new\". A feature that is new today is not new for long. Do Don't In version 1.4, ... In the current version, ... The Federation feature provides ... The new Federation feature provides ...","title":"Avoid statements that will soon be out of date"},{"location":"guides/documentation/style/#create-useful-links","text":"Don't use here , click here or URLs as the text for a hyperlink. Do Don't ... in a metadata server you can ... ... in a metadata server, which you can read more about here . The metadata access point ... Read more about metadata access points ... The FooBar class ... The FooBar class ( https://example.com/somewhere/something/FooBar.java ) ... There are numerous articles explaining what makes a good hyperlink , but fundamentally the \"Don't\" examples above: Decrease the overall usability of links. Decrease the overall accessibility of the links. Decrease search engine performance and content find-ability.","title":"Create useful links"},{"location":"guides/planning/guide/","text":"Planning Guide \u00b6 The planning guide provides information to help you plan the deployment of Egeria in your organization. Egeria is highly flexible and configurable and so the first piece of advice is to start small and simple and then expand as your experience and understanding of your workloads grows. Platforms and servers \u00b6 Egeria exchanges metadata between many types of tools distributed across different data centers and cloud platforms. The green clouds represent each of the deployment locations. The different technologies are shown as grey boxes and Egeria itself is shown in blue and orange. The blue rounded boxes are Egeria's Open Metadata and Governance ( OMAG ) Server Platform . This platform is the heart of Egeria's implementation. Typically you would expect to have at least one OMAG Server Platform deployed in each location. However, when you are experimenting with Egeria, it is often sufficient to start with one OMAG Server Platform and expand the number of platforms as you expand the technologies being integrated. The OMAG Server Platform is capable of hosting one or more Open Metadata and Governance ( OMAG ) Servers . The OMAG Servers are the orange circles in the illustration above. They manage the connectors to third party technology as well as the frameworks and intelligence that Egeria brings to distributed metadata management. It is a simple command to move an OMAG Server from one platform instance to another. This means you can start experimenting with a single platform and then add more as your deployment grows. The platform can also run as a node in container technologies such as Docker and Kubernetes. Different types of technology need different types of integration and Egeria has OMAG Servers to match. Each type of OMAG Server is focused on the integration of a specific type of tool, engine or platform: The way to understand the diagram is that the arrows should be read as is a . For example, the repository proxy is a cohort member and the cohort member is a OMAG Server . This means that everything documented about a particular type of server is also true for all server types that point to it through the is a arrow, all the way down the hierarchy. Object-oriented software engineers would know of this type of relationship as behavior inheritance . OMAG Server interactions \u00b6 The cohort members communicate with one another via an open metadata repository cohort . This means that they exchange metadata through a low level, fine-grained API supported by the Open Metadata Repository Services ( OMRS ) . The Open Metadata Access Services ( OMAS ) are built on top of the repository services. They live in the metadata access point / metadata servers . They offer more course-grained interfaces, specialized for particular types of technology. The governance servers are again specialized for particular types of metadata integration or additional governance activity. They connect to a metadata access point / metadata server. Finally, the view servers support the services for the solution user interfaces. They also connect to a metadata access point / metadata server. Suggested incremental deployment approach \u00b6 The servers' integration can be viewed as a series of nested spheres. The inner sphere involves the cohort members and provides metadata exchange between metadata repositories (and conformance test the integrations). The next level out adds the governance servers to automate the exchange of metadata between the metadata repositories and third party tools, engines and platforms. Finally, adding the view server and user interfaces delivers a governance solution to an organization. This architecture means that you can incrementally add function to your deployment. Here is a suggested approach: Start with creating an Egeria graph metadata server . This will provide a metadata repository that can store any type of open metadata. Decide on a name for an open metadata repository cohort and configure your graph metadata repository to join it. If you want to have other third party metadata repositories that you want to share metadata with, configure repository proxies for each including registering them to the same cohort as the metadata server. If you then want to add in metadata synchronization with other types of technology beyond metadata repositories, work out which integration daemons you need and configure them to connect to the metadata server . Make sure the appropriate access services for these integration daemons are enabled in the metadata server. If you want to use the discovery or governance action services then these run in an engine host server and connect to the metadata server via the Discovery Engine OMAS and Governance Engine OMAS respectively. Finally, if you want to deploy the user interfaces, make sure you have at least one view server for the presentation server that hosts the UI application. Working through this exercise gives you an understanding of the Egeria technology that you need for your deployment and how it connects together. The Solutions Guide describes different solutions that you can build with Egeria, how they work and the configuration that you will need. Building connectors \u00b6 If you discover that there is a third party technology that is not currently supported by Egeria then you need to build a connector to translate between the APIs and events of that technology and the open metadata APIs and events. Identify the scope of the metadata integration \u00b6 Another useful planning exercise is to identify the community of users and the tools that they use that need to share metadata. This gives you a view of the technology that needs to be integrated. If each community is fairly self-contained with their own tools deployment then you may want to consider deploying an OMAG Server platform for this community and deploying the servers they need onto it. This means they can control their access to open metadata along with the delivery of open metadata to the rest of the organization. More importantly, it helps with the definition of the organization's governance zones . Deployment checklist \u00b6 This is a checklist of planning tasks for the deployment of your OMAG Server Platforms and OMAG Servers: Set up unique certificates for your OMAG Server Platforms. Use an encrypted configuration document store for your platforms since configuration documents can have certificates and passwords in them. Implement the metadata security connectors for your organization to ensure only authorized users access metadata. Choose and configure the audit log destinations for your OMAG Servers. Ensure you have at least one Egeria metadata server in each of your open metadata repository cohorts . Assign a separate user id for each of your servers and ensure they are defined in your user directory and are authorized users according to the metadata security connectors. Consider where you need to have multiple instances of the same server running to give continuous availability . Plan your use of the event bus : which technology to use (Apache Kafka is the default) and the names of the topics that your OMAG Servers will use. Design the governance zones that you want to use to control the visibility of assets to different communities of users - or processes. More detail to follow... \u00b6 The text above is a very high level overview of the planning process. More detail will be added to this guide as time permits.","title":"Planning Guide"},{"location":"guides/planning/guide/#planning-guide","text":"The planning guide provides information to help you plan the deployment of Egeria in your organization. Egeria is highly flexible and configurable and so the first piece of advice is to start small and simple and then expand as your experience and understanding of your workloads grows.","title":"Planning Guide"},{"location":"guides/planning/guide/#platforms-and-servers","text":"Egeria exchanges metadata between many types of tools distributed across different data centers and cloud platforms. The green clouds represent each of the deployment locations. The different technologies are shown as grey boxes and Egeria itself is shown in blue and orange. The blue rounded boxes are Egeria's Open Metadata and Governance ( OMAG ) Server Platform . This platform is the heart of Egeria's implementation. Typically you would expect to have at least one OMAG Server Platform deployed in each location. However, when you are experimenting with Egeria, it is often sufficient to start with one OMAG Server Platform and expand the number of platforms as you expand the technologies being integrated. The OMAG Server Platform is capable of hosting one or more Open Metadata and Governance ( OMAG ) Servers . The OMAG Servers are the orange circles in the illustration above. They manage the connectors to third party technology as well as the frameworks and intelligence that Egeria brings to distributed metadata management. It is a simple command to move an OMAG Server from one platform instance to another. This means you can start experimenting with a single platform and then add more as your deployment grows. The platform can also run as a node in container technologies such as Docker and Kubernetes. Different types of technology need different types of integration and Egeria has OMAG Servers to match. Each type of OMAG Server is focused on the integration of a specific type of tool, engine or platform: The way to understand the diagram is that the arrows should be read as is a . For example, the repository proxy is a cohort member and the cohort member is a OMAG Server . This means that everything documented about a particular type of server is also true for all server types that point to it through the is a arrow, all the way down the hierarchy. Object-oriented software engineers would know of this type of relationship as behavior inheritance .","title":"Platforms and servers"},{"location":"guides/planning/guide/#omag-server-interactions","text":"The cohort members communicate with one another via an open metadata repository cohort . This means that they exchange metadata through a low level, fine-grained API supported by the Open Metadata Repository Services ( OMRS ) . The Open Metadata Access Services ( OMAS ) are built on top of the repository services. They live in the metadata access point / metadata servers . They offer more course-grained interfaces, specialized for particular types of technology. The governance servers are again specialized for particular types of metadata integration or additional governance activity. They connect to a metadata access point / metadata server. Finally, the view servers support the services for the solution user interfaces. They also connect to a metadata access point / metadata server.","title":"OMAG Server interactions"},{"location":"guides/planning/guide/#suggested-incremental-deployment-approach","text":"The servers' integration can be viewed as a series of nested spheres. The inner sphere involves the cohort members and provides metadata exchange between metadata repositories (and conformance test the integrations). The next level out adds the governance servers to automate the exchange of metadata between the metadata repositories and third party tools, engines and platforms. Finally, adding the view server and user interfaces delivers a governance solution to an organization. This architecture means that you can incrementally add function to your deployment. Here is a suggested approach: Start with creating an Egeria graph metadata server . This will provide a metadata repository that can store any type of open metadata. Decide on a name for an open metadata repository cohort and configure your graph metadata repository to join it. If you want to have other third party metadata repositories that you want to share metadata with, configure repository proxies for each including registering them to the same cohort as the metadata server. If you then want to add in metadata synchronization with other types of technology beyond metadata repositories, work out which integration daemons you need and configure them to connect to the metadata server . Make sure the appropriate access services for these integration daemons are enabled in the metadata server. If you want to use the discovery or governance action services then these run in an engine host server and connect to the metadata server via the Discovery Engine OMAS and Governance Engine OMAS respectively. Finally, if you want to deploy the user interfaces, make sure you have at least one view server for the presentation server that hosts the UI application. Working through this exercise gives you an understanding of the Egeria technology that you need for your deployment and how it connects together. The Solutions Guide describes different solutions that you can build with Egeria, how they work and the configuration that you will need.","title":"Suggested incremental deployment approach"},{"location":"guides/planning/guide/#building-connectors","text":"If you discover that there is a third party technology that is not currently supported by Egeria then you need to build a connector to translate between the APIs and events of that technology and the open metadata APIs and events.","title":"Building connectors"},{"location":"guides/planning/guide/#identify-the-scope-of-the-metadata-integration","text":"Another useful planning exercise is to identify the community of users and the tools that they use that need to share metadata. This gives you a view of the technology that needs to be integrated. If each community is fairly self-contained with their own tools deployment then you may want to consider deploying an OMAG Server platform for this community and deploying the servers they need onto it. This means they can control their access to open metadata along with the delivery of open metadata to the rest of the organization. More importantly, it helps with the definition of the organization's governance zones .","title":"Identify the scope of the metadata integration"},{"location":"guides/planning/guide/#deployment-checklist","text":"This is a checklist of planning tasks for the deployment of your OMAG Server Platforms and OMAG Servers: Set up unique certificates for your OMAG Server Platforms. Use an encrypted configuration document store for your platforms since configuration documents can have certificates and passwords in them. Implement the metadata security connectors for your organization to ensure only authorized users access metadata. Choose and configure the audit log destinations for your OMAG Servers. Ensure you have at least one Egeria metadata server in each of your open metadata repository cohorts . Assign a separate user id for each of your servers and ensure they are defined in your user directory and are authorized users according to the metadata security connectors. Consider where you need to have multiple instances of the same server running to give continuous availability . Plan your use of the event bus : which technology to use (Apache Kafka is the default) and the names of the topics that your OMAG Servers will use. Design the governance zones that you want to use to control the visibility of assets to different communities of users - or processes.","title":"Deployment checklist"},{"location":"guides/planning/guide/#more-detail-to-follow","text":"The text above is a very high level overview of the planning process. More detail will be added to this guide as time permits.","title":"More detail to follow..."},{"location":"guides/using/external-identifiers/","text":"Managing External Identifiers \u00b6 Every open metadata instance has a unique identifier called its GUID . This provides a means to locate and retrieve the instance from the metadata repository. However, often the GUID is not known in the systems and tools that integrate with open metadata. Metadata instances that inherit from the Referenceable type have a property called qualifiedName . This is a unique name for the Referenceable instance. When such an instance is created, the qualified name can be set up to a unique identifier that is a natural unique name for the resource that it represents (such as the full path name of a file) or a unique identifier from an external tool. The element can then be retrieved using the qualifiedName . Now consider the situation where each external tool that uses the instance has a different identifier for the instance. There is only one qualified name property in the instance which will not be able to cover all the identifiers from the external systems/tools. In this situation it is possible to set up multiple external identifiers for an open metadata instance. Each external identifier is linked to the open metadata instance it represents and the software server capability of the external system/tool that uses it. You can think of this link to the software server capability as providing a scope in which the external identifier is valid. The external identifiers can support both one-to-many, many-to-one and many-to-many between metadata elements from external systems/tools and open metadata instances. Many-to-one \u00b6 Imagine situation where an external tool called myCatalog uses two metadata elements: one for a type it refers to as BusinessTerm and the other of type Example , to represent all the properties that are stored in one open metadata GlossaryTerm . To represent this in open metadata, the unique identifiers for the business term and example metadata elements ( gt1 and ex6 respectively) are each stored in their own external identifier that is linked to both the myCatalog 's software server capability and the corresponding open metadata glossary term. This means the glossary term can be located in an open metadata repository either using the identifier gt1 or ex6 . Similarly, it is possible to locate a glossary term's properties in myCatalog by looking up both the gt1 and ex6 elements. One-to-many \u00b6 Now imagine the opposite situation: where it takes multiple open metadata instances to represent a single metadata element in an external system/tool. In this example the external system/tool directly links its Database elements to its Table elements. Whereas in the open metadata types, there is a SchemaType (specifically RelationalDBSchemaType ) between a DeployedDatabaseSchema instance and the RelationalTable instance. Again, an external identifier is created for each of the external metadata elements and this is linked to the software server capability for myCatalog . Each external identifier is then linked to each of the open metadata instances that have properties that map to its equivalent metadata element in the external system/tool. The use of external identifiers is particularly important to the integration connectors running in the Open Metadata Integration Services ( OMIS ) , where the ability to maintain consistent metadata stores in both open metadata and third party systems and tools is important. Open metadata representation \u00b6 The open metadata types for external identifier are in model 0017 . The ExternalIdLink relationship is between the external identifier and the open metadata instance it represents. The ExternalIdScope is the relationship between the external identifier and the software server capability that represents the external system/tool. Implementations \u00b6 The Asset Manager OMAS provides support for external identifier mapping on its APIs. This capability is visible through the Catalog Integrator OMIS and the Lineage Integrator OMIS that are based on the Asset Manager OMAS client. The Open Connector Framework ( OCF ) provides the ability to query the external identifiers attached to an asset through the connected asset properties . This is also visible through the AssetUniverse interfaces of the: Asset Consumer OMAS Asset Owner OMAS Discovery Engine OMAS","title":"Managing External Identifiers"},{"location":"guides/using/external-identifiers/#managing-external-identifiers","text":"Every open metadata instance has a unique identifier called its GUID . This provides a means to locate and retrieve the instance from the metadata repository. However, often the GUID is not known in the systems and tools that integrate with open metadata. Metadata instances that inherit from the Referenceable type have a property called qualifiedName . This is a unique name for the Referenceable instance. When such an instance is created, the qualified name can be set up to a unique identifier that is a natural unique name for the resource that it represents (such as the full path name of a file) or a unique identifier from an external tool. The element can then be retrieved using the qualifiedName . Now consider the situation where each external tool that uses the instance has a different identifier for the instance. There is only one qualified name property in the instance which will not be able to cover all the identifiers from the external systems/tools. In this situation it is possible to set up multiple external identifiers for an open metadata instance. Each external identifier is linked to the open metadata instance it represents and the software server capability of the external system/tool that uses it. You can think of this link to the software server capability as providing a scope in which the external identifier is valid. The external identifiers can support both one-to-many, many-to-one and many-to-many between metadata elements from external systems/tools and open metadata instances.","title":"Managing External Identifiers"},{"location":"guides/using/external-identifiers/#many-to-one","text":"Imagine situation where an external tool called myCatalog uses two metadata elements: one for a type it refers to as BusinessTerm and the other of type Example , to represent all the properties that are stored in one open metadata GlossaryTerm . To represent this in open metadata, the unique identifiers for the business term and example metadata elements ( gt1 and ex6 respectively) are each stored in their own external identifier that is linked to both the myCatalog 's software server capability and the corresponding open metadata glossary term. This means the glossary term can be located in an open metadata repository either using the identifier gt1 or ex6 . Similarly, it is possible to locate a glossary term's properties in myCatalog by looking up both the gt1 and ex6 elements.","title":"Many-to-one"},{"location":"guides/using/external-identifiers/#one-to-many","text":"Now imagine the opposite situation: where it takes multiple open metadata instances to represent a single metadata element in an external system/tool. In this example the external system/tool directly links its Database elements to its Table elements. Whereas in the open metadata types, there is a SchemaType (specifically RelationalDBSchemaType ) between a DeployedDatabaseSchema instance and the RelationalTable instance. Again, an external identifier is created for each of the external metadata elements and this is linked to the software server capability for myCatalog . Each external identifier is then linked to each of the open metadata instances that have properties that map to its equivalent metadata element in the external system/tool. The use of external identifiers is particularly important to the integration connectors running in the Open Metadata Integration Services ( OMIS ) , where the ability to maintain consistent metadata stores in both open metadata and third party systems and tools is important.","title":"One-to-many"},{"location":"guides/using/external-identifiers/#open-metadata-representation","text":"The open metadata types for external identifier are in model 0017 . The ExternalIdLink relationship is between the external identifier and the open metadata instance it represents. The ExternalIdScope is the relationship between the external identifier and the software server capability that represents the external system/tool.","title":"Open metadata representation"},{"location":"guides/using/external-identifiers/#implementations","text":"The Asset Manager OMAS provides support for external identifier mapping on its APIs. This capability is visible through the Catalog Integrator OMIS and the Lineage Integrator OMIS that are based on the Asset Manager OMAS client. The Open Connector Framework ( OCF ) provides the ability to query the external identifiers attached to an asset through the connected asset properties . This is also visible through the AssetUniverse interfaces of the: Asset Consumer OMAS Asset Owner OMAS Discovery Engine OMAS","title":"Implementations"},{"location":"introduction/challenge/","text":"The challenge \u00b6 Every week we hear of new tools, data platforms and opportunities for organizations to embrace advanced digital technologies such as artificial intelligence. Yet despite investment and the focus of smart people, few organizations succeed in making wide and systematic use of their data. Today's IT is at the heart of the problem. Many tools and data platforms recognize the value of metadata, but manage it in a siloed, proprietary way that assumes they are the only technology employed by the organization. The result is that knowledge is not shared between people that use different tool sets. Egeria is an open source project dedicated to making metadata open and automatically exchanged between tools and platforms, no matter which vendor they come from. Open metadata and governance manifesto \u00b6 Our guiding beliefs: The maintenance of metadata must be automated to scale to the sheer volumes and variety of data involved in modern business. Similarly, metadata should be used to drive the governance of data and create a business-friendly logical interface to the data landscape. The availability of metadata management must become ubiquitous in cloud platforms and large data platforms, such as Apache Hadoop, so that the processing engines on these platforms can rely on its availability and build capability around it. Metadata access must become open and remotely accessible so that tools from different vendors can work with metadata located on different platforms. This implies unique identifiers for metadata elements, some level of standardization in the types and formats for metadata and standard interfaces for manipulating metadata. Wherever possible, discovery and maintenance of metadata has to be an integral part of all tools that access, change and move information. Code talks Egeria provides an Apache 2.0 licensed platform to support vendors that sign up to the open metadata and governance manifesto.","title":"The Challenge"},{"location":"introduction/challenge/#the-challenge","text":"Every week we hear of new tools, data platforms and opportunities for organizations to embrace advanced digital technologies such as artificial intelligence. Yet despite investment and the focus of smart people, few organizations succeed in making wide and systematic use of their data. Today's IT is at the heart of the problem. Many tools and data platforms recognize the value of metadata, but manage it in a siloed, proprietary way that assumes they are the only technology employed by the organization. The result is that knowledge is not shared between people that use different tool sets. Egeria is an open source project dedicated to making metadata open and automatically exchanged between tools and platforms, no matter which vendor they come from.","title":"The challenge"},{"location":"introduction/challenge/#open-metadata-and-governance-manifesto","text":"Our guiding beliefs: The maintenance of metadata must be automated to scale to the sheer volumes and variety of data involved in modern business. Similarly, metadata should be used to drive the governance of data and create a business-friendly logical interface to the data landscape. The availability of metadata management must become ubiquitous in cloud platforms and large data platforms, such as Apache Hadoop, so that the processing engines on these platforms can rely on its availability and build capability around it. Metadata access must become open and remotely accessible so that tools from different vendors can work with metadata located on different platforms. This implies unique identifiers for metadata elements, some level of standardization in the types and formats for metadata and standard interfaces for manipulating metadata. Wherever possible, discovery and maintenance of metadata has to be an integral part of all tools that access, change and move information. Code talks Egeria provides an Apache 2.0 licensed platform to support vendors that sign up to the open metadata and governance manifesto.","title":"Open metadata and governance manifesto"},{"location":"introduction/key-concepts/","text":"Key concepts \u00b6 To further explain some key concepts of Egeria, let us delve deeper into an example: The inner ring, titled Integrated Metadata , illustrates the exchange of metadata between metadata servers. The servers are connected together through an Open Metadata Repository Cohort or just cohort for short. Cohorts \u00b6 A cohort 1 can support the exchange of many metadata servers: both internal to Egeria and third party. A cohort is a group of OMAG Servers that are exchanging metadata using a peer-to-peer replication protocol and federated queries. The cohort is self-configuring. At the core it is between one and four shared topics. Each member publishes a registration request on the appropriate topic when they want to join. This is picked up by the existing members who add this new server to their registry of members and re-send their registration through the same topic to allow the new member to build up its own registry. There is no central cohort controller Note that there is no central cohort control or coordination logic: the registration and so on are all handled in a peer-to-peer manner with each participant communicating with all other participants. When an OMAG Server permanently leaves the cohort, it sends an unregistration request. This enables the other members to remove the parting member from their registries. Federation \u00b6 The purpose of the registry in each member is to configure its federated query capability supported by the enterprise repository services . The registration information includes the URL root and server name of the member. The federation capability in each OMAG Server allows it to issue metadata create, update, delete and search requests to each and every member of the cohort. This is the primary mechanism for accessing metadata. Replication \u00b6 In addition, any change to metadata made by a member is replicated to the other members of the cohort through the relevant cohort topic. This gives the other members an opportunity to maintain cached copies of the metadata for performance / availability reasons. Refresh requests A member may also request that metadata is \"refreshed\" across the cohort. The originator of the requested metadata then sends the latest version of this metadata to the rest of the cohort through the cohort topic. This mechanism is useful to seed the cache in a new member of the cohort and is invoked as a result of a federated query issued from the new member. (A federated query occurs automatically whenever an access service makes a request for metadata.) Exchange protocol \u00b6 The exchange of metadata uses the Open Metadata Repository Services ( OMRS ) interfaces, which gives fine-grained metadata notifications and updates 2 . The server's metadata security connector provides fine-grained control on which metadata is sent, received and/or stored by the server. This level of control is necessary for metadata repositories that are managing specific collections of valuable objects such as assets . Members \u00b6 A third party metadata server can embed the Egeria libraries in its own runtime or, more commonly, use a special OMAG Server called the repository proxy to host connectors that map between the events and APIs of the third party metadata server and the Open Metadata events and APIs. The repository proxy manages all the interaction with the other members of the cohort. The cohort protocols are peer-to-peer and the membership is dynamic. When a third party metadata server connects to the cohort, either directly or through its repository proxy, it automatically begins receiving metadata from all the other members. When it shares metadata, it is shared with all the other members. Each member is free to choose what to share and what to process from the other members of the cohort. Other types of OMAG Servers that can be members of the cohort: The conformance test server is used to verify that a member of the cohort is operating correctly. It is typically only used in test environments because it sends out a lot of test metadata on the cohort and validates the responses from the cohort member it is testing. The metadata server provides a metadata repository that supports any type of open metadata. It is a valuable member of the cohort because it is a metadata gap-filler. By that we mean that is can store relationships between metadata from different third party repositories along with additional types of metadata not supported by any of the third party metadata repositories. It may optionally have the access services enabled so it can also act as a metadata access point. The metadata access point supports Egeria's Open Metadata Access Services ( OMAS ) , or access services, for short. These access services provide specialized APIs and events for different types of technologies. Integrating metadata into solutions \u00b6 The metadata access point is the bridge to the governance servers (the middle ring in the initial illustration). As mentioned above, the metadata access point hosts the access services ( OMAS 's) for consumption and integration of metadata by various tools. The addition of the governance servers provides active metadata exchange and governance of any type of third party technology, not just metadata servers. We call this integrated governance . For the most part, Egeria is a background technology. However, once metadata is being exchanged and linked, new governance solutions may emerge that bring value directly to individuals working in an organization. Therefore, we have added servers to support browser-based user interfaces: The view server provides REST APIs specifically for user interfaces. They are consumed by the Egeria UIs but can also be used by other UIs and tools. The presentation server hosts the JavaScript applications that provide an interactive browser-based user interface for Egeria. Metadata objects \u00b6 When referring to metadata, we distinguish between the level of granularity at which you may be thinking about that metadata. For Egeria, that level of granularity broadly splits between: The granular repository services level, used for the cohort's underlying metadata federation, replication and exchange The more coarse-grained access services level, where most tool- and user-oriented consumption of and integration with metadata would occur Metadata elements \u00b6 At the more coarse-grained level of the access services metadata objects are simply referred to as metadata elements . Each access service describes its own model for the metadata elements it handles, and the access service itself determines how these coarse-grained representations are transformed to and from the more granular representations described below. Metadata instances \u00b6 At the granular level of the repository services, we refer to specific metadata objects as metadata instances , which can only be one of the following: An entity is capable of describing a metadata object on its own: for example, a business vocabulary term, database, field in a schema, and so on. If you think about metadata as a graph, these are the nodes (vertices) in the graph. They typically describe concepts, people, places and things. A relationship describes a (typically) directional association between two entities: for example, the semantic meaning of a relational database column by relating a business vocabulary term with the relational database column. In a graph sense, these are the links (edges) that show how entities are related. While not strictly speaking a kind of metadata instance, a classification provides a means to extend an entity with additional facets of information: for example, describing the level of confidentiality with which a particular relational database column should be treated. Classifications describe additional attributes of an entity and can be used to identify entities that are similar in a specific aspect. Metadata types \u00b6 Every metadata instance is linked to an open metadata type definition (sometimes referred to as a TypeDef ) that describes what it represents and the properties that further describe and differentiate it from other instances of that same type. TypeDef inheritance TypeDefs can inherit from other TypeDefs from the same category: open metadata supports single inheritance. An example: GlossaryTerm, RelationalColumn and SemanticAssignment For example, GlossaryTerm is a type of entity that can be used to describe a specific term of business vocabulary. As a type of entity, GlossaryTerm is defined using an EntityDef (a subtype of TypeDef specific to entities). It has a number of properties like a displayName , itself defined as a PrimitiveDef of type string. And GlossaryTerm as a type extends a base entity type called Referenceable which defines common characteristics that many entities possess such as a qualifiedName (another PrimitiveDef of type string). RelationalColumn is another example of an entity, in this case one that can be used to describe relational database columns. Once again it is defined using an EntityDef , has a number of properties, and also extends the base type called Referenceable and therefore also gains common properties like the qualifiedName . Finally, let's consider a different type: SemanticAssignment is a type of relationship that can be used to describe the meaning of something. Because it is a type of relationship, it is defined using a RelationshipDef (another subtype of EntityDef , this time specific to relationships). As a relationship, the RelationshipDef defines the entities that it can inter-relate: in this example a GlossaryTerm and any other Referenceable (for example, a RelationalColumn ). Where are the types modeled? The TypeDef s themselves are described in detail under the types reference area, and the canonical definitions ultimately live in the code itself . Homed metadata \u00b6 The metadata repository where a metadata instance is created is called its home repository . Metadata in its home repository is mutable : it can be updated and deleted. Each instance of metadata can be independently homed Note that each instance of metadata -- whether an entity, relationship or classification -- can be homed independently from any other instance of metadata. For example: if we have a business vocabulary term Address that is related to a relational database column ADDR and given a Confidentiality classification, each of these could be homed in a different repository. That is, Address (entity) could be homed in repository A, ADDR (another entity) in repository B, the relationship between them in repository C, and the Confidentiality classification in repository D. As such, not only can a query for metadata be federated, but indeed even the holistic representation of a given piece of metadata (its instance and directly-related instances) is federated across the cohort. The Open Metadata Repository Services ( OMRS ) is responsible for sharing this metadata with other metadata repositories who are members of the same cohort. Reference copies \u00b6 The shared copies are called reference copies and are read-only ( immutable ). Update requests to a reference copy are automatically redirected to the home repository by the OMRS , without the caller being aware. Distinguishing homed metadata from reference copies Every metadata repository in a cohort has a unique identifier called the local metadata collection id . This identifier is set up in the server configuration and shared when this server connects to a cohort. When metadata is shared by OMRS , each instance is tagged with the metadata collection id of its home repository. OMRS is able to route update requests to the right server by comparing the metadata collection id in the metadata instance with the cohort registration information passed between members of the cohort when they connect. Unique identifiers (GUIDs) \u00b6 Every open metadata instance has a unique identifier called the GUID . GUIDs must be unique This identifier needs to be globally unique -- so even if two metadata repositories simultaneously created a metadata instance about the same thing, the GUIDs of these two instances should be different. For example, in Egeria new GUIDs are created using the UUID.randomUUID().toString() method to produce something like: 87b06ffe-9db2-4ef5-ba6e-8127480cf30d Egeria does not mandate the use this or any other particular algorithm for generating GUIDs, only that the principle of uniqueness is adhered to. There should be, at most, a tiny chance 3 that two servers will generate the same GUID . Egeria expects this to be exceedingly rare, but not impossible, and therefore if it does happen it is detected by the repository services and at a minimum messages are output on the detecting server's audit log. The repository services also have APIs for re-identifying (changing the GUID ) for a metadata instance to automate the resolution of such conflicts. We can expect that such an operation could be resource-intensive; however, this is balanced against the exceeding rareness with which it should need to be used. Conformance \u00b6 Adhering to these concepts and the principles by which they behave is the subject of conformance . Egeria provides an automated testing suite to validate that a given repository or third party integration behaves according to these expectations , the successful completion of which is a necessary input to a tool being granted the use of an Egeria conformance mark. You may want to see the cohort interactions walkthrough for more details on how cohort participants interact. \u21a9 You may want to see the OMRS metamodel for more details on the granularity of metadata exchange. \u21a9 The rarity will depend on the specific algorithm used, but as an example the algorithm used within Egeria generates type 4 UUIDs, for which the probability of a collision is so small that it can almost be ignored . But as it is not impossible , Egeria does still provide the mechanisms to detect and resolve such conflicts. \u21a9","title":"Key Concepts"},{"location":"introduction/key-concepts/#key-concepts","text":"To further explain some key concepts of Egeria, let us delve deeper into an example: The inner ring, titled Integrated Metadata , illustrates the exchange of metadata between metadata servers. The servers are connected together through an Open Metadata Repository Cohort or just cohort for short.","title":"Key concepts"},{"location":"introduction/key-concepts/#cohorts","text":"A cohort 1 can support the exchange of many metadata servers: both internal to Egeria and third party. A cohort is a group of OMAG Servers that are exchanging metadata using a peer-to-peer replication protocol and federated queries. The cohort is self-configuring. At the core it is between one and four shared topics. Each member publishes a registration request on the appropriate topic when they want to join. This is picked up by the existing members who add this new server to their registry of members and re-send their registration through the same topic to allow the new member to build up its own registry. There is no central cohort controller Note that there is no central cohort control or coordination logic: the registration and so on are all handled in a peer-to-peer manner with each participant communicating with all other participants. When an OMAG Server permanently leaves the cohort, it sends an unregistration request. This enables the other members to remove the parting member from their registries.","title":"Cohorts"},{"location":"introduction/key-concepts/#federation","text":"The purpose of the registry in each member is to configure its federated query capability supported by the enterprise repository services . The registration information includes the URL root and server name of the member. The federation capability in each OMAG Server allows it to issue metadata create, update, delete and search requests to each and every member of the cohort. This is the primary mechanism for accessing metadata.","title":"Federation"},{"location":"introduction/key-concepts/#replication","text":"In addition, any change to metadata made by a member is replicated to the other members of the cohort through the relevant cohort topic. This gives the other members an opportunity to maintain cached copies of the metadata for performance / availability reasons. Refresh requests A member may also request that metadata is \"refreshed\" across the cohort. The originator of the requested metadata then sends the latest version of this metadata to the rest of the cohort through the cohort topic. This mechanism is useful to seed the cache in a new member of the cohort and is invoked as a result of a federated query issued from the new member. (A federated query occurs automatically whenever an access service makes a request for metadata.)","title":"Replication"},{"location":"introduction/key-concepts/#exchange-protocol","text":"The exchange of metadata uses the Open Metadata Repository Services ( OMRS ) interfaces, which gives fine-grained metadata notifications and updates 2 . The server's metadata security connector provides fine-grained control on which metadata is sent, received and/or stored by the server. This level of control is necessary for metadata repositories that are managing specific collections of valuable objects such as assets .","title":"Exchange protocol"},{"location":"introduction/key-concepts/#members","text":"A third party metadata server can embed the Egeria libraries in its own runtime or, more commonly, use a special OMAG Server called the repository proxy to host connectors that map between the events and APIs of the third party metadata server and the Open Metadata events and APIs. The repository proxy manages all the interaction with the other members of the cohort. The cohort protocols are peer-to-peer and the membership is dynamic. When a third party metadata server connects to the cohort, either directly or through its repository proxy, it automatically begins receiving metadata from all the other members. When it shares metadata, it is shared with all the other members. Each member is free to choose what to share and what to process from the other members of the cohort. Other types of OMAG Servers that can be members of the cohort: The conformance test server is used to verify that a member of the cohort is operating correctly. It is typically only used in test environments because it sends out a lot of test metadata on the cohort and validates the responses from the cohort member it is testing. The metadata server provides a metadata repository that supports any type of open metadata. It is a valuable member of the cohort because it is a metadata gap-filler. By that we mean that is can store relationships between metadata from different third party repositories along with additional types of metadata not supported by any of the third party metadata repositories. It may optionally have the access services enabled so it can also act as a metadata access point. The metadata access point supports Egeria's Open Metadata Access Services ( OMAS ) , or access services, for short. These access services provide specialized APIs and events for different types of technologies.","title":"Members"},{"location":"introduction/key-concepts/#integrating-metadata-into-solutions","text":"The metadata access point is the bridge to the governance servers (the middle ring in the initial illustration). As mentioned above, the metadata access point hosts the access services ( OMAS 's) for consumption and integration of metadata by various tools. The addition of the governance servers provides active metadata exchange and governance of any type of third party technology, not just metadata servers. We call this integrated governance . For the most part, Egeria is a background technology. However, once metadata is being exchanged and linked, new governance solutions may emerge that bring value directly to individuals working in an organization. Therefore, we have added servers to support browser-based user interfaces: The view server provides REST APIs specifically for user interfaces. They are consumed by the Egeria UIs but can also be used by other UIs and tools. The presentation server hosts the JavaScript applications that provide an interactive browser-based user interface for Egeria.","title":"Integrating metadata into solutions"},{"location":"introduction/key-concepts/#metadata-objects","text":"When referring to metadata, we distinguish between the level of granularity at which you may be thinking about that metadata. For Egeria, that level of granularity broadly splits between: The granular repository services level, used for the cohort's underlying metadata federation, replication and exchange The more coarse-grained access services level, where most tool- and user-oriented consumption of and integration with metadata would occur","title":"Metadata objects"},{"location":"introduction/key-concepts/#metadata-elements","text":"At the more coarse-grained level of the access services metadata objects are simply referred to as metadata elements . Each access service describes its own model for the metadata elements it handles, and the access service itself determines how these coarse-grained representations are transformed to and from the more granular representations described below.","title":"Metadata elements"},{"location":"introduction/key-concepts/#metadata-instances","text":"At the granular level of the repository services, we refer to specific metadata objects as metadata instances , which can only be one of the following: An entity is capable of describing a metadata object on its own: for example, a business vocabulary term, database, field in a schema, and so on. If you think about metadata as a graph, these are the nodes (vertices) in the graph. They typically describe concepts, people, places and things. A relationship describes a (typically) directional association between two entities: for example, the semantic meaning of a relational database column by relating a business vocabulary term with the relational database column. In a graph sense, these are the links (edges) that show how entities are related. While not strictly speaking a kind of metadata instance, a classification provides a means to extend an entity with additional facets of information: for example, describing the level of confidentiality with which a particular relational database column should be treated. Classifications describe additional attributes of an entity and can be used to identify entities that are similar in a specific aspect.","title":"Metadata instances"},{"location":"introduction/key-concepts/#metadata-types","text":"Every metadata instance is linked to an open metadata type definition (sometimes referred to as a TypeDef ) that describes what it represents and the properties that further describe and differentiate it from other instances of that same type. TypeDef inheritance TypeDefs can inherit from other TypeDefs from the same category: open metadata supports single inheritance. An example: GlossaryTerm, RelationalColumn and SemanticAssignment For example, GlossaryTerm is a type of entity that can be used to describe a specific term of business vocabulary. As a type of entity, GlossaryTerm is defined using an EntityDef (a subtype of TypeDef specific to entities). It has a number of properties like a displayName , itself defined as a PrimitiveDef of type string. And GlossaryTerm as a type extends a base entity type called Referenceable which defines common characteristics that many entities possess such as a qualifiedName (another PrimitiveDef of type string). RelationalColumn is another example of an entity, in this case one that can be used to describe relational database columns. Once again it is defined using an EntityDef , has a number of properties, and also extends the base type called Referenceable and therefore also gains common properties like the qualifiedName . Finally, let's consider a different type: SemanticAssignment is a type of relationship that can be used to describe the meaning of something. Because it is a type of relationship, it is defined using a RelationshipDef (another subtype of EntityDef , this time specific to relationships). As a relationship, the RelationshipDef defines the entities that it can inter-relate: in this example a GlossaryTerm and any other Referenceable (for example, a RelationalColumn ). Where are the types modeled? The TypeDef s themselves are described in detail under the types reference area, and the canonical definitions ultimately live in the code itself .","title":"Metadata types"},{"location":"introduction/key-concepts/#homed-metadata","text":"The metadata repository where a metadata instance is created is called its home repository . Metadata in its home repository is mutable : it can be updated and deleted. Each instance of metadata can be independently homed Note that each instance of metadata -- whether an entity, relationship or classification -- can be homed independently from any other instance of metadata. For example: if we have a business vocabulary term Address that is related to a relational database column ADDR and given a Confidentiality classification, each of these could be homed in a different repository. That is, Address (entity) could be homed in repository A, ADDR (another entity) in repository B, the relationship between them in repository C, and the Confidentiality classification in repository D. As such, not only can a query for metadata be federated, but indeed even the holistic representation of a given piece of metadata (its instance and directly-related instances) is federated across the cohort. The Open Metadata Repository Services ( OMRS ) is responsible for sharing this metadata with other metadata repositories who are members of the same cohort.","title":"Homed metadata"},{"location":"introduction/key-concepts/#reference-copies","text":"The shared copies are called reference copies and are read-only ( immutable ). Update requests to a reference copy are automatically redirected to the home repository by the OMRS , without the caller being aware. Distinguishing homed metadata from reference copies Every metadata repository in a cohort has a unique identifier called the local metadata collection id . This identifier is set up in the server configuration and shared when this server connects to a cohort. When metadata is shared by OMRS , each instance is tagged with the metadata collection id of its home repository. OMRS is able to route update requests to the right server by comparing the metadata collection id in the metadata instance with the cohort registration information passed between members of the cohort when they connect.","title":"Reference copies"},{"location":"introduction/key-concepts/#unique-identifiers-guids","text":"Every open metadata instance has a unique identifier called the GUID . GUIDs must be unique This identifier needs to be globally unique -- so even if two metadata repositories simultaneously created a metadata instance about the same thing, the GUIDs of these two instances should be different. For example, in Egeria new GUIDs are created using the UUID.randomUUID().toString() method to produce something like: 87b06ffe-9db2-4ef5-ba6e-8127480cf30d Egeria does not mandate the use this or any other particular algorithm for generating GUIDs, only that the principle of uniqueness is adhered to. There should be, at most, a tiny chance 3 that two servers will generate the same GUID . Egeria expects this to be exceedingly rare, but not impossible, and therefore if it does happen it is detected by the repository services and at a minimum messages are output on the detecting server's audit log. The repository services also have APIs for re-identifying (changing the GUID ) for a metadata instance to automate the resolution of such conflicts. We can expect that such an operation could be resource-intensive; however, this is balanced against the exceeding rareness with which it should need to be used.","title":"Unique identifiers (GUIDs)"},{"location":"introduction/key-concepts/#conformance","text":"Adhering to these concepts and the principles by which they behave is the subject of conformance . Egeria provides an automated testing suite to validate that a given repository or third party integration behaves according to these expectations , the successful completion of which is a necessary input to a tool being granted the use of an Egeria conformance mark. You may want to see the cohort interactions walkthrough for more details on how cohort participants interact. \u21a9 You may want to see the OMRS metamodel for more details on the granularity of metadata exchange. \u21a9 The rarity will depend on the specific algorithm used, but as an example the algorithm used within Egeria generates type 4 UUIDs, for which the probability of a collision is so small that it can almost be ignored . But as it is not impossible , Egeria does still provide the mechanisms to detect and resolve such conflicts. \u21a9","title":"Conformance"},{"location":"introduction/overview/","text":"Our solution (overview) \u00b6 Today's organizations have their tools and technologies distributed across multiple data centres and cloud providers (green clouds). Within each location, we can run a platform (blue boxes) that provides integration services tailored to specific types of tools (orange circles). Peer-to-peer network of metadata The resulting exchange of metadata (blue arrows) creates a peer-to-peer network that spans both locations and vendor tools. Egeria implements a set of open metadata types , frameworks , connectors , APIs , event payloads and interchange protocols to allow all types of tools and metadata repositories to share and exchange metadata. It also provides this platform and pre-built integration services that can run within it to provide a comprehensive toolkit for integrating and distributing metadata between different tools and technologies. OMAG Server Platform \u00b6 Egeria provides the platform (the blue boxes), called the Open Metadata and Governance Server Platform (or OMAG Server Platform ) . The OMAG Server Platform is a multi-tenant platform that supports horizontal scale-out in Kubernetes and yet is light enough to run as an edge server on a Raspberry Pi. This platform is used to host the actual metadata integration and automation capabilities. OMAG Servers \u00b6 Within an instance of the OMAG Server Platform , one or more OMAG Servers can be configured (the orange circles). These servers are collections of activated services that host connectors to the different technologies with which Egeria exchanges metadata. Summary Combined, the OMAG Server Platforms and servers running within them provide an enterprise catalog of data and IT resources that are transparently assessed, governed and consumed through many types of tools and technologies. The enterprise catalog is not a physically-centralized one, but a logical one composed of federated metadata from across this peer-to-peer network. Why open source? Delivering this capability as open source is a critical part of the project, since multiple vendors must buy into this ecosystem. They are not going to do this if one organization dominates the technology base. Thus, the open metadata and governance technology must be freely available with an open source governance model that allows a community of organizations and practitioners to develop and evolve the base, and then use it in their offerings and deployments.","title":"Our Solution"},{"location":"introduction/overview/#our-solution-overview","text":"Today's organizations have their tools and technologies distributed across multiple data centres and cloud providers (green clouds). Within each location, we can run a platform (blue boxes) that provides integration services tailored to specific types of tools (orange circles). Peer-to-peer network of metadata The resulting exchange of metadata (blue arrows) creates a peer-to-peer network that spans both locations and vendor tools. Egeria implements a set of open metadata types , frameworks , connectors , APIs , event payloads and interchange protocols to allow all types of tools and metadata repositories to share and exchange metadata. It also provides this platform and pre-built integration services that can run within it to provide a comprehensive toolkit for integrating and distributing metadata between different tools and technologies.","title":"Our solution (overview)"},{"location":"introduction/overview/#omag-server-platform","text":"Egeria provides the platform (the blue boxes), called the Open Metadata and Governance Server Platform (or OMAG Server Platform ) . The OMAG Server Platform is a multi-tenant platform that supports horizontal scale-out in Kubernetes and yet is light enough to run as an edge server on a Raspberry Pi. This platform is used to host the actual metadata integration and automation capabilities.","title":"OMAG Server Platform"},{"location":"introduction/overview/#omag-servers","text":"Within an instance of the OMAG Server Platform , one or more OMAG Servers can be configured (the orange circles). These servers are collections of activated services that host connectors to the different technologies with which Egeria exchanges metadata. Summary Combined, the OMAG Server Platforms and servers running within them provide an enterprise catalog of data and IT resources that are transparently assessed, governed and consumed through many types of tools and technologies. The enterprise catalog is not a physically-centralized one, but a logical one composed of federated metadata from across this peer-to-peer network. Why open source? Delivering this capability as open source is a critical part of the project, since multiple vendors must buy into this ecosystem. They are not going to do this if one organization dominates the technology base. Thus, the open metadata and governance technology must be freely available with an open source governance model that allows a community of organizations and practitioners to develop and evolve the base, and then use it in their offerings and deployments.","title":"OMAG Servers"},{"location":"release-notes/1-0/","text":"Release 1.0 (February 2019) \u00b6 Release 1.0 provides the open metadata integration and exchange between metadata repositories: The Open Connector Framework ( OCF ) provides standard interfaces for implementing connectors. These are used to access the data stored in Assets and to plug in platform capabilities into the Open Metadata Repository Services ( OMRS ). The implementation of the Open Metadata Repository Services ( OMRS ) . The repository services provide support for the open metadata types , the interfaces used by a repository technology the wants to support the open metadata protocols, the event management for open metadata replication and the cohort registration and management. The repository services can be used as embeddable libraries in technologies that wish to participate in an open metadata repository cohort . Egeria Implementation Status at Release 1.0 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":1.0},{"location":"release-notes/1-0/#release-10-february-2019","text":"Release 1.0 provides the open metadata integration and exchange between metadata repositories: The Open Connector Framework ( OCF ) provides standard interfaces for implementing connectors. These are used to access the data stored in Assets and to plug in platform capabilities into the Open Metadata Repository Services ( OMRS ). The implementation of the Open Metadata Repository Services ( OMRS ) . The repository services provide support for the open metadata types , the interfaces used by a repository technology the wants to support the open metadata protocols, the event management for open metadata replication and the cohort registration and management. The repository services can be used as embeddable libraries in technologies that wish to participate in an open metadata repository cohort .","title":"Release 1.0 (February 2019)"},{"location":"release-notes/1-0/#egeria-implementation-status-at-release-10","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 1.0"},{"location":"release-notes/1-1/","text":"Release 1.1 (November 2019) \u00b6 Release 1.1 focuses on establishing a secure, multi-tenant platform for metadata servers and governance servers. There is also a new JanusGraph based metadata repository. Below are the highlights: A persistent metadata repository based on JanusGraph. A multi-tenant OMAG Server Platform that is able to host one to many metadata servers and/or governance servers. This platform supports APIs to administer servers, start and stop servers and query their status. The platform offers a connector to change the store used for configuration documents and to control who can issue platform API calls. There are now tutorials and hands-on labs demonstrating the configuring and start up of servers on the OMAG Server Platform. The aim is to help people to get up and running with the Egeria technology. In addition, there are both docker scripts and Kubernetes helm charts to deploy the platforms and related technology used in the tutorials, samples and labs. Note that currently we do not push release specific docker containers to dockerhub. If you are using the docker/kubernetes environments it is recommended to work from the 'master' branch instead of this release. This will be addressed in a future release. The Open Metadata Repository Services ( OMRS ) shipped in the first release have been enhanced with REST APIs to query the cohorts that a server is connected to. There are also REST APIs to issue federated queries across the cohorts that a metadata server is connected to. There is a new user interface to explore the open metadata types. It is called the Type Explorer . Code fixes and changes to released function \u00b6 Release 1.1 fixes a problem in the open metadata cohort registration when there was a metadata server without a local repository. The wrong registration message was being used which meant that the registration details were not being properly distributed around the cohort. Release 1.1 changed the type of the AdditionalProperties from a map of String to Object to a map of String to String in order to match the open metadata types. There is a new property called ConfigurationProperties which is a map from String to Object. Release 1.1 changed the implementation of the data primitive type from a Java Date object to a Java Long object. This was to overcome a problem deserialization dates from JSON to Java. Egeria Implementation Status at Release 1.1 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":1.1},{"location":"release-notes/1-1/#release-11-november-2019","text":"Release 1.1 focuses on establishing a secure, multi-tenant platform for metadata servers and governance servers. There is also a new JanusGraph based metadata repository. Below are the highlights: A persistent metadata repository based on JanusGraph. A multi-tenant OMAG Server Platform that is able to host one to many metadata servers and/or governance servers. This platform supports APIs to administer servers, start and stop servers and query their status. The platform offers a connector to change the store used for configuration documents and to control who can issue platform API calls. There are now tutorials and hands-on labs demonstrating the configuring and start up of servers on the OMAG Server Platform. The aim is to help people to get up and running with the Egeria technology. In addition, there are both docker scripts and Kubernetes helm charts to deploy the platforms and related technology used in the tutorials, samples and labs. Note that currently we do not push release specific docker containers to dockerhub. If you are using the docker/kubernetes environments it is recommended to work from the 'master' branch instead of this release. This will be addressed in a future release. The Open Metadata Repository Services ( OMRS ) shipped in the first release have been enhanced with REST APIs to query the cohorts that a server is connected to. There are also REST APIs to issue federated queries across the cohorts that a metadata server is connected to. There is a new user interface to explore the open metadata types. It is called the Type Explorer .","title":"Release 1.1 (November 2019)"},{"location":"release-notes/1-1/#code-fixes-and-changes-to-released-function","text":"Release 1.1 fixes a problem in the open metadata cohort registration when there was a metadata server without a local repository. The wrong registration message was being used which meant that the registration details were not being properly distributed around the cohort. Release 1.1 changed the type of the AdditionalProperties from a map of String to Object to a map of String to String in order to match the open metadata types. There is a new property called ConfigurationProperties which is a map from String to Object. Release 1.1 changed the implementation of the data primitive type from a Java Date object to a Java Long object. This was to overcome a problem deserialization dates from JSON to Java.","title":"Code fixes and changes to released function"},{"location":"release-notes/1-1/#egeria-implementation-status-at-release-11","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 1.1"},{"location":"release-notes/1-2/","text":"Release 1.2 (December 2019) \u00b6 Release 1.2 provides the ability to build an asset catalog, search for assets and also access the data and function provided by these assets. It is also possible to group Assets into governance zones to control the discoverability and visibility of the Assets. In addition, we release the Egeria conformance test suite and four conformant repositories. Below are the highlights: A conformance test suite that validates implementations of open metadata repository connectors. In addition we are highlighting the repositories that are conformant. These are: The JanusGraph metadata repository . The In-memory metadata repository used for testing and demos. The Apache Atlas repository proxy . The IBM Information Governance Catalog repository proxy . There are new access services to support the cataloging of assets: The Asset Consumer OMAS supports the access to both the data and metadata associated with an asset. The Asset Owner OMAS supports the manual cataloging of new Assets. The assets cataloged by these access services can be scoped by governance zones. The metadata servers and future governance servers running on the OMAG Server Platform support metadata-centric security that is controlled by a connector. This connector can validate access to individual servers, services, and assets based on the identity of the caller and the service or asset they wish to access. The Open Metadata Repository Services ( OMRS ) have been enhanced to support support dynamic types and type patching. There is also function to load archives of metadata instances. There are tutorials , hands-on labs and samples demonstrating the new Asset cataloging capabilities. Egeria Implementation Status at Release 1.2 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":1.2},{"location":"release-notes/1-2/#release-12-december-2019","text":"Release 1.2 provides the ability to build an asset catalog, search for assets and also access the data and function provided by these assets. It is also possible to group Assets into governance zones to control the discoverability and visibility of the Assets. In addition, we release the Egeria conformance test suite and four conformant repositories. Below are the highlights: A conformance test suite that validates implementations of open metadata repository connectors. In addition we are highlighting the repositories that are conformant. These are: The JanusGraph metadata repository . The In-memory metadata repository used for testing and demos. The Apache Atlas repository proxy . The IBM Information Governance Catalog repository proxy . There are new access services to support the cataloging of assets: The Asset Consumer OMAS supports the access to both the data and metadata associated with an asset. The Asset Owner OMAS supports the manual cataloging of new Assets. The assets cataloged by these access services can be scoped by governance zones. The metadata servers and future governance servers running on the OMAG Server Platform support metadata-centric security that is controlled by a connector. This connector can validate access to individual servers, services, and assets based on the identity of the caller and the service or asset they wish to access. The Open Metadata Repository Services ( OMRS ) have been enhanced to support support dynamic types and type patching. There is also function to load archives of metadata instances. There are tutorials , hands-on labs and samples demonstrating the new Asset cataloging capabilities.","title":"Release 1.2 (December 2019)"},{"location":"release-notes/1-2/#egeria-implementation-status-at-release-12","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 1.2"},{"location":"release-notes/1-3/","text":"Release 1.3 (January 2020) \u00b6 Release 1.3 focuses on the support for open metadata archives, and formal versioning of open metadata types. It includes much of the ground-work for supporting design lineage and the detection and management of duplicate assets but that function is officially released in 1.4 . Below are the highlights of the 1.3 release: The hands-on labs have been updated to provide reusable Python functions for working with Egeria. The management of open metadata types includes formal versioning and patching of types. This makes it clearer where additions and updates are being made to the open metadata types. See open metadata types archive . There are the following changes to the open metadata types: The EmbeddedConnection has a new property called position . The OpenDiscoveryAnalysisReport has a new property called discoveryRequestStep . There is a new collection of Annotations for recording suspected duplicates and divergent values in acknowledged duplicates. This is to support the asset deduplication work scheduled for the next release. There are new open metadata archive utilities for creating your own open metadata archives. See the open connector archives and design model archives . The Conformance Suite Repository Workbench is now at Version 1.1, with the following enhancements: Tests for relationship searches have moved into a separate, optional RELATIONSHIP_SEARCH profile. A repository connector can be fully conformant with the (mandatory) METADATA_SHARING profile despite not supporting the findRelationshipsByProperty or findRelationshipsByPropertyValue methods. The ADVANCED_SEARCH profile is now divided into two profiles: ENTITY_ADVANCED _SEARCH and RELATIONSHIP_ADVANCED_SEARCH. They are both optional profiles. A repository connector can be fully conformant with the ENTITY_ADVANCED _SEARCH profile, despite not supporting either of the RELATIONSHIP_SEARCH or RELATIONSHIP_ADVANCED_SEARCH profiles. New test verify the correct handling of mappingProperties in the InstanceAuditHeader. The tests for 're-home' of a reference copy now use an instance mastered by a third (virtual) repository rather than the CTS Server's repository. Type verification of relationship end types now cater for connectors that do not support all the supertypes of an entity type. Search result checking is improved. The CTS notebook (under open-metadata-resources/open-metadata-labs) has been enhanced: The Conformance Profile Results cell there is additional reporting for the new profiles A new cell \"Polling for Status\" shows how to determine whether the repository-workbench has completed its synchronous tests. The new API it demonstrates could be used to support automated testing A new cell \"Monitoring Progress\" show how progress can be monitored based on workbench results retrieved during a test run. Improvements to HTTP response checking and reporting of errors Egeria Implementation Status at Release 1.3 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":1.3},{"location":"release-notes/1-3/#release-13-january-2020","text":"Release 1.3 focuses on the support for open metadata archives, and formal versioning of open metadata types. It includes much of the ground-work for supporting design lineage and the detection and management of duplicate assets but that function is officially released in 1.4 . Below are the highlights of the 1.3 release: The hands-on labs have been updated to provide reusable Python functions for working with Egeria. The management of open metadata types includes formal versioning and patching of types. This makes it clearer where additions and updates are being made to the open metadata types. See open metadata types archive . There are the following changes to the open metadata types: The EmbeddedConnection has a new property called position . The OpenDiscoveryAnalysisReport has a new property called discoveryRequestStep . There is a new collection of Annotations for recording suspected duplicates and divergent values in acknowledged duplicates. This is to support the asset deduplication work scheduled for the next release. There are new open metadata archive utilities for creating your own open metadata archives. See the open connector archives and design model archives . The Conformance Suite Repository Workbench is now at Version 1.1, with the following enhancements: Tests for relationship searches have moved into a separate, optional RELATIONSHIP_SEARCH profile. A repository connector can be fully conformant with the (mandatory) METADATA_SHARING profile despite not supporting the findRelationshipsByProperty or findRelationshipsByPropertyValue methods. The ADVANCED_SEARCH profile is now divided into two profiles: ENTITY_ADVANCED _SEARCH and RELATIONSHIP_ADVANCED_SEARCH. They are both optional profiles. A repository connector can be fully conformant with the ENTITY_ADVANCED _SEARCH profile, despite not supporting either of the RELATIONSHIP_SEARCH or RELATIONSHIP_ADVANCED_SEARCH profiles. New test verify the correct handling of mappingProperties in the InstanceAuditHeader. The tests for 're-home' of a reference copy now use an instance mastered by a third (virtual) repository rather than the CTS Server's repository. Type verification of relationship end types now cater for connectors that do not support all the supertypes of an entity type. Search result checking is improved. The CTS notebook (under open-metadata-resources/open-metadata-labs) has been enhanced: The Conformance Profile Results cell there is additional reporting for the new profiles A new cell \"Polling for Status\" shows how to determine whether the repository-workbench has completed its synchronous tests. The new API it demonstrates could be used to support automated testing A new cell \"Monitoring Progress\" show how progress can be monitored based on workbench results retrieved during a test run. Improvements to HTTP response checking and reporting of errors","title":"Release 1.3 (January 2020)"},{"location":"release-notes/1-3/#egeria-implementation-status-at-release-13","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 1.3"},{"location":"release-notes/1-4/","text":"Release 1.4 (February 2020) \u00b6 Release 1.4 focused on bug fixes and documentation. Egeria Implementation Status at Release 1.4 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":1.4},{"location":"release-notes/1-4/#release-14-february-2020","text":"Release 1.4 focused on bug fixes and documentation.","title":"Release 1.4 (February 2020)"},{"location":"release-notes/1-4/#egeria-implementation-status-at-release-14","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 1.4"},{"location":"release-notes/1-5/","text":"Release 1.5 (March 2020) \u00b6 Release 1.5 delivers the automatic metadata discovery of duplicate assets. Additional, in Tech Preview, Data Engine OMAS , and a data engine proxy server. Released Components \u00b6 The first governance server is released: The Discovery Server supports the scanning of assets and the notification when duplicate suspects are detected. This server is supported by: * The Discovery Engine OMAS supports the detection, recording and notification of exceptions and duplicate suspects. The Open Discovery Framework ( ODF ) is now defined and implemented to support the interfaces for automated discovery services. It complements the Open Connector Framework ( OCF ) delivered in release 1.0. There are new tutorials , hands-on labs and samples demonstrating the new de-duplication detection features. Technical Previews \u00b6 The Data Engine OMAS supports the processing of notifications from data engines such as ETL platforms in order to catalog information about the data movement, transformation and copying they are engaged in. The Data Engine Proxy Server is also included in the technical preview. It supports the polling of data engines such as ETL platforms in order to catalog information about the data movement, transformation and copying they are engaged in. It calls the Data Engine OMAS . Egeria Implementation Status at Release 1.5 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":1.5},{"location":"release-notes/1-5/#release-15-march-2020","text":"Release 1.5 delivers the automatic metadata discovery of duplicate assets. Additional, in Tech Preview, Data Engine OMAS , and a data engine proxy server.","title":"Release 1.5 (March 2020)"},{"location":"release-notes/1-5/#released-components","text":"The first governance server is released: The Discovery Server supports the scanning of assets and the notification when duplicate suspects are detected. This server is supported by: * The Discovery Engine OMAS supports the detection, recording and notification of exceptions and duplicate suspects. The Open Discovery Framework ( ODF ) is now defined and implemented to support the interfaces for automated discovery services. It complements the Open Connector Framework ( OCF ) delivered in release 1.0. There are new tutorials , hands-on labs and samples demonstrating the new de-duplication detection features.","title":"Released Components"},{"location":"release-notes/1-5/#technical-previews","text":"The Data Engine OMAS supports the processing of notifications from data engines such as ETL platforms in order to catalog information about the data movement, transformation and copying they are engaged in. The Data Engine Proxy Server is also included in the technical preview. It supports the polling of data engines such as ETL platforms in order to catalog information about the data movement, transformation and copying they are engaged in. It calls the Data Engine OMAS .","title":"Technical Previews"},{"location":"release-notes/1-5/#egeria-implementation-status-at-release-15","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 1.5"},{"location":"release-notes/1-6/","text":"Release 1.6 (April 2020) \u00b6 Release 1.6 adds support for: * Audit Log Framework ( ALF ) technical preview * Repository Explorer (REX) Below are the highlights: There is a new framework: The Audit Log Framework ( ALF ) provides interface definitions and classes to enable connectors to support natural language enabled diagnostics such as exception messages and audit log messages. There is a new user interface module: The Repository Explorer (Rex) can help you explore and visualize the metadata in a repository. It retrieves entities and relationships from the repository and displays them. A details panel also shows the properties and other information about an object. Each entity or relationship is added to a network diagram, which shows how they are connected. The Swagger-based API documentation for the Egeria server chassis has been reorganized to align with our modules structure & to provide links into our other documentation which also will clarify if the module is released, in Tech Preview, or still in development. The docs can be found at https://<server>:<port>/swagger-ui.htm . Further enhancements will follow in future releases. Many dependencies have been updated including: Kafka client upgraded to 2.4.1 Spring updated to 5.2.4, spring boot to 2.2.5 & other spring components accordingly. For a full list refer to the git commit logs. Egeria Implementation Status at Release 1.6 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":1.6},{"location":"release-notes/1-6/#release-16-april-2020","text":"Release 1.6 adds support for: * Audit Log Framework ( ALF ) technical preview * Repository Explorer (REX) Below are the highlights: There is a new framework: The Audit Log Framework ( ALF ) provides interface definitions and classes to enable connectors to support natural language enabled diagnostics such as exception messages and audit log messages. There is a new user interface module: The Repository Explorer (Rex) can help you explore and visualize the metadata in a repository. It retrieves entities and relationships from the repository and displays them. A details panel also shows the properties and other information about an object. Each entity or relationship is added to a network diagram, which shows how they are connected. The Swagger-based API documentation for the Egeria server chassis has been reorganized to align with our modules structure & to provide links into our other documentation which also will clarify if the module is released, in Tech Preview, or still in development. The docs can be found at https://<server>:<port>/swagger-ui.htm . Further enhancements will follow in future releases. Many dependencies have been updated including: Kafka client upgraded to 2.4.1 Spring updated to 5.2.4, spring boot to 2.2.5 & other spring components accordingly. For a full list refer to the git commit logs.","title":"Release 1.6 (April 2020)"},{"location":"release-notes/1-6/#egeria-implementation-status-at-release-16","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 1.6"},{"location":"release-notes/1-7/","text":"Release 1.7 (May 2020) \u00b6 Release 1.7 contains many bug fixes & preparatory development work for future new features. Below are the highlights: There is support for loading standard glossaries and design models coded in OWL/JSON-LD into the open metadata ecosystem. The input file is converted to an Open Metadata Archive which can be loaded directly into a metadata server. Many dependencies have been updated including: Kafka client upgraded to 2.5 Spring security updated to 5.3.1, spring boot,data to 2.2.6, spring to 5.2.5 For a full list refer to the git commit logs. Known Issues \u00b6 (https://github.com/odpi/egeria/issues/2935)[2935] - Governance Engine OMAS reports exception when entities added (https://github.com/odpi/egeria/issues/3005)[3005] - Occasional failure in 'Building a Data Catalog' notebook Egeria Implementation Status at Release 1.7 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":1.7},{"location":"release-notes/1-7/#release-17-may-2020","text":"Release 1.7 contains many bug fixes & preparatory development work for future new features. Below are the highlights: There is support for loading standard glossaries and design models coded in OWL/JSON-LD into the open metadata ecosystem. The input file is converted to an Open Metadata Archive which can be loaded directly into a metadata server. Many dependencies have been updated including: Kafka client upgraded to 2.5 Spring security updated to 5.3.1, spring boot,data to 2.2.6, spring to 5.2.5 For a full list refer to the git commit logs.","title":"Release 1.7 (May 2020)"},{"location":"release-notes/1-7/#known-issues","text":"(https://github.com/odpi/egeria/issues/2935)[2935] - Governance Engine OMAS reports exception when entities added (https://github.com/odpi/egeria/issues/3005)[3005] - Occasional failure in 'Building a Data Catalog' notebook","title":"Known Issues"},{"location":"release-notes/1-7/#egeria-implementation-status-at-release-17","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 1.7"},{"location":"release-notes/1-8/","text":"Release 1.8 (June 2020) \u00b6 Below are the highlights of Release 1.8: New tutorial information has been added in the form of the Egeria Dojo Usability & Capability improvements to Repository Explorer Samples & utilities are now also packaged into jars with dependencies to make them easier to use (java -jar) Connections to kafka will now retry to improve availability. See 'Bring up Issues' in the connector documentation \\ New dependencies has been included: Spring Boot Actuator - Provides features to help you monitor and manage your application when you push it to production micrometer-registry-prometheus - Exposes metrics in a format that can be scraped by a Prometheus server Many dependencies have been updated. The most relevant include: Spring has been updated to 5.2.6 Spring Boot, Spring Security, Spring Security, Spring Data have been updated to 2.3.0 Egeria Implementation Status at Release 1.8 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":1.8},{"location":"release-notes/1-8/#release-18-june-2020","text":"Below are the highlights of Release 1.8: New tutorial information has been added in the form of the Egeria Dojo Usability & Capability improvements to Repository Explorer Samples & utilities are now also packaged into jars with dependencies to make them easier to use (java -jar) Connections to kafka will now retry to improve availability. See 'Bring up Issues' in the connector documentation \\ New dependencies has been included: Spring Boot Actuator - Provides features to help you monitor and manage your application when you push it to production micrometer-registry-prometheus - Exposes metrics in a format that can be scraped by a Prometheus server Many dependencies have been updated. The most relevant include: Spring has been updated to 5.2.6 Spring Boot, Spring Security, Spring Security, Spring Data have been updated to 2.3.0","title":"Release 1.8 (June 2020)"},{"location":"release-notes/1-8/#egeria-implementation-status-at-release-18","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 1.8"},{"location":"release-notes/2-0/","text":"Release 2.0 (July 2020) \u00b6 Release 2.0 adds support for: Encryption by default (HTTPS/SSL, encrypted configuration file) bug fixes dependency updates Below are the highlights: The Egeria server chassis default URL is now https://localhost:9443 - the server now listens on port 9443 and supports https only. All clients have been updated accordingly. At this point SSL certificate validation is disabled. This will be enabled in a future release. Docker containers, docker-compose scripts, kubernetes deployments have all been updated to use https accordingly. The Encrypted Configuration File Store Connector is now used by default to ensure security of sensitive configuration details like credentials. Egeria Implementation Status at Release 2.0 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":2.0},{"location":"release-notes/2-0/#release-20-july-2020","text":"Release 2.0 adds support for: Encryption by default (HTTPS/SSL, encrypted configuration file) bug fixes dependency updates Below are the highlights: The Egeria server chassis default URL is now https://localhost:9443 - the server now listens on port 9443 and supports https only. All clients have been updated accordingly. At this point SSL certificate validation is disabled. This will be enabled in a future release. Docker containers, docker-compose scripts, kubernetes deployments have all been updated to use https accordingly. The Encrypted Configuration File Store Connector is now used by default to ensure security of sensitive configuration details like credentials.","title":"Release 2.0 (July 2020)"},{"location":"release-notes/2-0/#egeria-implementation-status-at-release-20","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 2.0"},{"location":"release-notes/2-1/","text":"Release 2.1 (August 2020) \u00b6 Release 2.1 primarily contains bug fixes and preparatory work for future capabilities. A full list of changes can be seen by comparing on github . The highlights include: Bug fixes Subject Area OMAS has added testing including automated FVTs, code cleanup & bug fixes User Interface fixes & usability improvements Dependency Updates Spring updated to 5.2.8 additional dependencies to remain current Egeria Implementation Status at Release 2.1 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":2.1},{"location":"release-notes/2-1/#release-21-august-2020","text":"Release 2.1 primarily contains bug fixes and preparatory work for future capabilities. A full list of changes can be seen by comparing on github . The highlights include: Bug fixes Subject Area OMAS has added testing including automated FVTs, code cleanup & bug fixes User Interface fixes & usability improvements Dependency Updates Spring updated to 5.2.8 additional dependencies to remain current","title":"Release 2.1 (August 2020)"},{"location":"release-notes/2-1/#egeria-implementation-status-at-release-21","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 2.1"},{"location":"release-notes/2-10/","text":"Release 2.10 (June 2021) \u00b6 Release 2.10 adds: * New and improved open metadata types for governance * New API for Governance Program OMAS See also an important notice about removal of Java 8 support in a future release. Details of these and other changes are in the sections that follow. Description of Changes \u00b6 Metadata Types \u00b6 Correction to the Campaign classification. The Campaign classification is supposed to connect to a Project entity to indicate the project is a grouping of a series of projects designed to achieve a large goal. Unfortunately it was coded to connect to the Collection entity. In 2.10, the Campaign classification has been changed to attach to Referenceable to allow it to be connected to Project without affecting backward compatibility. See new type description in model 0130 . Extension of the UsedInContext relationship. The UsedInContext relationship linked two GlossaryTerm entities together to show that one glossary term described the context in which the other was valid. This relationship has been updated to allow the end that describes the context to be a Referenceable . This means that the context where glossary terms are valid can be expressed as elements such as projects, business capabilities, parts of an organization, subject areas, zones and many more. See new type description in model 0360 . Allowing ownership to be expressed as a PersonRole . Issue #5104 described the problem that the governance open metadata types allowed owners to be expressed as PersonalProfile or UserIdentity entities. The properties did not identify if the owner was identified as a GUID or as a qualifiedName. In 2.10, a new classification for expressing ownership called, not surprisingly, Ownership , was introduced which allows and type and property name for the owner to be specified with the owner's identifier. The types for entities such as GovernanceActionType and IncidentReport that include ownership properties in them have been updated to deprecate these properties in favor of using the Ownership classification. The AssetOwnership classification is also deprecated. See new type description in model 0445 . Update to Certification relationship. The Certification relationship supports the identification of the people involved in the certification of an element. 2.10 adds new attributes to specify the type and property names of the identifier used to identify these people. This is consistent with the new Ownership classification. See new type descriptions in model 0482 . Update to License relationship. The License relationship supports the identification of the people involved in the licencing of an element. 2.10 adds new attributes to specify the type and property names of the identifier used to identify these people. This is consistent with the new Ownership classification. See new type descriptions in model 0481 . Governance role updates. The GovernanceOfficer entity now inherits from GovernanceRole rather than PersonRole . This means it is just another governance role that can be linked with GovernanceResponsibility definitions. In addition there are new types for different types of owners. These are ComponentOwner and DataItemOwner . See new type descriptions in model 0445 . Extend the GovernanceResponsibilityAssignment The GovernanceResponsibilityAssignment relationship identifies the responsibilities for a particular role. It used to link to a GovernanceRole and in 2.10, it has been updated to link to a PersonRole . This means that governance responsibilities can be added to any role, such as ProjectManager and CommunityMember , rather than just specialized governance roles. See new type descriptions in model 0445 . Deprecating the domain attribute in various governance types. The domain attribute is typed by the GovernanceDomain enum. This provides a fixed list of governance domains. An extensible mechanism for expressing the governance domain was added in release 2.4 using the GovernanceDomainDescription entity and the domainIdentifier attribute. In this release, the use of domain is deprecated in GovernanceDefinition , GovernanceZone , SubjectAreaDefinition , GovernanceMetric , GovernanceRole and GovernanceOfficer . See new type descriptions in model 0401 . Extensions to governance drivers. There are now new subtypes of the GovernanceDriver entity called RegulationArticle and BusinessImperative . It is also possible to link governance drivers using the new GovernanceDriverLink relationship. See new type descriptions in model 0405 . Update to AssetOrigin classification. The AssetOrigin classification supports the identification of the origin of an asset. This can be in terms of the organization, business capability and other values. 2.10 adds new attributes to identify if the organization or business capability is identified by its GUID or qualifiedName. See new type descriptions in model 0440 . Improve IncidentClassifiers. If is possible to define multiple sets of IncidentClassifier values that can be used on IncidentReport entities to help to group and prioritize them. The IncidentClassifier entity now inherits from Referencable and there is a new classification called IncidentClassifierSet to mark a Collection entity as containing IncidentClassifier definitions. See new type descriptions in model 0470 . Deprecation of ResponsibilityStaffContact . The ResponsibilityStaffContact relationship has been deprecated in favor of the GovernanceResponsibilityAssignment relationship. New Services for Governance Program OMAS \u00b6 The APIs defined for Governance Program OMAS have been updated to reflect the changes to the open metadata types described above. The APIs and client implementations are in place. The server-side is coming in a future release. Bug fixes and other updates \u00b6 Additional Bug Fixes Cascaded deletes for entities grouped using the Anchors classification are now deleting the correct entities. Prior to this release, some entities were missed and others were deleted incorrectly. Dependency Updates For details on both see the commit history in GitHub. Known Issues \u00b6 It is recommended to use a chromium-based browser such as Google Chrome or Microsoft Edge, or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . When running the 'Understanding Platform Services' lab, ensure you run the 'egeria-service-config' notebook first and do not restart the python kernel before running this lab. See #4842 . When logging in to the react UI for the coco pharma lab demo, ensure to use http://myhost.mydomain/coco/login as otherwise the login will not work. see odpi/egeria#41 A few further bugs are noted at https://github.com/odpi/egeria/issues/5211#issuecomment-850321243 including for samples and UI. Removal of Java 8 Support \u00b6 Egeria will drop support for Java 8 in a forthcoming release within the next few months. We have been building and testing with Java 11 for over a year and will move to build all packages with Java 11, and require Java 11. At this point Java 8 will no longer be supported for new releases. Egeria Implementation Status at Release 2.10 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content. Further Help and Support \u00b6 See the Community Guide .","title":"2.10"},{"location":"release-notes/2-10/#release-210-june-2021","text":"Release 2.10 adds: * New and improved open metadata types for governance * New API for Governance Program OMAS See also an important notice about removal of Java 8 support in a future release. Details of these and other changes are in the sections that follow.","title":"Release 2.10 (June 2021)"},{"location":"release-notes/2-10/#description-of-changes","text":"","title":"Description of Changes"},{"location":"release-notes/2-10/#metadata-types","text":"Correction to the Campaign classification. The Campaign classification is supposed to connect to a Project entity to indicate the project is a grouping of a series of projects designed to achieve a large goal. Unfortunately it was coded to connect to the Collection entity. In 2.10, the Campaign classification has been changed to attach to Referenceable to allow it to be connected to Project without affecting backward compatibility. See new type description in model 0130 . Extension of the UsedInContext relationship. The UsedInContext relationship linked two GlossaryTerm entities together to show that one glossary term described the context in which the other was valid. This relationship has been updated to allow the end that describes the context to be a Referenceable . This means that the context where glossary terms are valid can be expressed as elements such as projects, business capabilities, parts of an organization, subject areas, zones and many more. See new type description in model 0360 . Allowing ownership to be expressed as a PersonRole . Issue #5104 described the problem that the governance open metadata types allowed owners to be expressed as PersonalProfile or UserIdentity entities. The properties did not identify if the owner was identified as a GUID or as a qualifiedName. In 2.10, a new classification for expressing ownership called, not surprisingly, Ownership , was introduced which allows and type and property name for the owner to be specified with the owner's identifier. The types for entities such as GovernanceActionType and IncidentReport that include ownership properties in them have been updated to deprecate these properties in favor of using the Ownership classification. The AssetOwnership classification is also deprecated. See new type description in model 0445 . Update to Certification relationship. The Certification relationship supports the identification of the people involved in the certification of an element. 2.10 adds new attributes to specify the type and property names of the identifier used to identify these people. This is consistent with the new Ownership classification. See new type descriptions in model 0482 . Update to License relationship. The License relationship supports the identification of the people involved in the licencing of an element. 2.10 adds new attributes to specify the type and property names of the identifier used to identify these people. This is consistent with the new Ownership classification. See new type descriptions in model 0481 . Governance role updates. The GovernanceOfficer entity now inherits from GovernanceRole rather than PersonRole . This means it is just another governance role that can be linked with GovernanceResponsibility definitions. In addition there are new types for different types of owners. These are ComponentOwner and DataItemOwner . See new type descriptions in model 0445 . Extend the GovernanceResponsibilityAssignment The GovernanceResponsibilityAssignment relationship identifies the responsibilities for a particular role. It used to link to a GovernanceRole and in 2.10, it has been updated to link to a PersonRole . This means that governance responsibilities can be added to any role, such as ProjectManager and CommunityMember , rather than just specialized governance roles. See new type descriptions in model 0445 . Deprecating the domain attribute in various governance types. The domain attribute is typed by the GovernanceDomain enum. This provides a fixed list of governance domains. An extensible mechanism for expressing the governance domain was added in release 2.4 using the GovernanceDomainDescription entity and the domainIdentifier attribute. In this release, the use of domain is deprecated in GovernanceDefinition , GovernanceZone , SubjectAreaDefinition , GovernanceMetric , GovernanceRole and GovernanceOfficer . See new type descriptions in model 0401 . Extensions to governance drivers. There are now new subtypes of the GovernanceDriver entity called RegulationArticle and BusinessImperative . It is also possible to link governance drivers using the new GovernanceDriverLink relationship. See new type descriptions in model 0405 . Update to AssetOrigin classification. The AssetOrigin classification supports the identification of the origin of an asset. This can be in terms of the organization, business capability and other values. 2.10 adds new attributes to identify if the organization or business capability is identified by its GUID or qualifiedName. See new type descriptions in model 0440 . Improve IncidentClassifiers. If is possible to define multiple sets of IncidentClassifier values that can be used on IncidentReport entities to help to group and prioritize them. The IncidentClassifier entity now inherits from Referencable and there is a new classification called IncidentClassifierSet to mark a Collection entity as containing IncidentClassifier definitions. See new type descriptions in model 0470 . Deprecation of ResponsibilityStaffContact . The ResponsibilityStaffContact relationship has been deprecated in favor of the GovernanceResponsibilityAssignment relationship.","title":"Metadata Types"},{"location":"release-notes/2-10/#new-services-for-governance-program-omas","text":"The APIs defined for Governance Program OMAS have been updated to reflect the changes to the open metadata types described above. The APIs and client implementations are in place. The server-side is coming in a future release.","title":"New Services for Governance Program OMAS"},{"location":"release-notes/2-10/#bug-fixes-and-other-updates","text":"Additional Bug Fixes Cascaded deletes for entities grouped using the Anchors classification are now deleting the correct entities. Prior to this release, some entities were missed and others were deleted incorrectly. Dependency Updates For details on both see the commit history in GitHub.","title":"Bug fixes and other updates"},{"location":"release-notes/2-10/#known-issues","text":"It is recommended to use a chromium-based browser such as Google Chrome or Microsoft Edge, or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . When running the 'Understanding Platform Services' lab, ensure you run the 'egeria-service-config' notebook first and do not restart the python kernel before running this lab. See #4842 . When logging in to the react UI for the coco pharma lab demo, ensure to use http://myhost.mydomain/coco/login as otherwise the login will not work. see odpi/egeria#41 A few further bugs are noted at https://github.com/odpi/egeria/issues/5211#issuecomment-850321243 including for samples and UI.","title":"Known Issues"},{"location":"release-notes/2-10/#removal-of-java-8-support","text":"Egeria will drop support for Java 8 in a forthcoming release within the next few months. We have been building and testing with Java 11 for over a year and will move to build all packages with Java 11, and require Java 11. At this point Java 8 will no longer be supported for new releases.","title":"Removal of Java 8 Support"},{"location":"release-notes/2-10/#egeria-implementation-status-at-release-210","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 2.10"},{"location":"release-notes/2-10/#further-help-and-support","text":"See the Community Guide .","title":"Further Help and Support"},{"location":"release-notes/2-11/","text":"2.11 (July 2021) \u00b6 Special note on Java support This is expected to be the last release supporting Java 8. The next release will commence the 3.x series of releases and will require Java 11. Known issue: use chromium-based browser for UIs It is recommended to use a chromium-based browser such as Google Chrome, Microsoft Edge or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . Functional changes Multiple topics for a cohort An open metadata repository cohort uses three types of events to enable peer-to-peer sharing of metadata. In previous releases, these events have all been exchanged through a single event topic. Release 2.11 provides the option for a cohort to use a dedicated topic for each type of event. This improves the time to register a new member in the cohort and allows multiple instances of a metadata server or a metadata access point to access the cohort. Details of these new options can be found in the administration guide . Data Manager OMAS extension to support capture of event metadata The Data Manager OMAS has been extended to support the capture of metadata from Event Brokers such as Apache Kafka and API Managers such as an API Gateway. There are two new associated Open Metadata Integration Services (OMISs) to support integration connectors that extract metadata from these types of data managers: Topic Integrator OMIS supports integration connectors extracting metadata from Event Brokers. API Integrator OMIS supports integration connectors extracting API information from API managers. For more information on the use of the Data Manager OMAS with these integration services, see the Data Manager Integration solution. Information View OMAS , Virtualization Services, Security Officer Services, Gaian connector The following capabilities have now been removed (they were previously deprecated and/or in not in 'Released' status): Information View OMAS Virtualization Services and associated connectors Security Officer Services, Security sync services and associated connectors including for Apache Ranger Gaian database connector and additional authentication/impersonation support Much of the above capability can be implemented via Integration Services . Hadoop specifics may be developed in the future within the Egeria Hadoop GitHub repository. For more details of this change see #5314 . Data Platform Services, Data Platform OMAS , and Cassandra connectors Similarly, the following services have been deleted: Data Platform Services and Data Platform OMAS . For more details see #5344 . Data Platform capabilities are already available in Database Integrator and Files Integrator already part of Integration Services. Cassandra connectors: cassandra-data-store-connector and cassandra-metadata-extractor-connector will be introduced back in the Data Connectors GitHub repository. For more information see #2671 . Type changes: added, modified, deprecated APIManager, EventBroker New types for APIManager and EventBroker . These types inherit from SoftwareServerCapability . These are used in the new Data Manager OMAS APIs. See new type descriptions in model 0050 . Threat And a relationship to connect a GovernanceDefinition with a metadata element that defines the scope where it is applicable. See new type descriptions in models 0401 and 0405 . TabularFileColumn A new subtype for TabularColumn called TabularFileColumn is added to be able to distinguish between tabular columns from files and RelationalColumn (which also inherits from TabularColumn ). See type descriptions in model 0530 . EventTypeList New type called EventTypeList to allow a list of event types to be associated with a topic and a specific subtype of SchemaAttribute for an attribute in an event type to make it easier to search for data fields that are exclusively found in events. See new type descriptions in model 0535 . APIParameter, APIOperation New types for APIParameter to allow the capture of properties related to the API's treatment of the parameters. There are also properties for APIOperation . See new type descriptions in model 0536 . Types for display of data to end users New types for DisplayDataSchemaType , DisplayDataContainer , DisplayDataField , QuerySchemaType , QueryDataContainer and QueryDataField to allow the capture of properties related to the display of data to end users. See new type descriptions in model 0537 . RelationalTableType Updated supertype of RelationalTableType to inherit from ComplexSchemaType rather than TabularColumnType since TabularColumnType is now deprecated. See type descriptions in model 0534 . Asset properties A number of properties that where originally defined in Asset were moved to classifications to allow them to be managed independently of the original asset. This occurred before the TypeDefPatch support was in place and so these properties were not marked as deprecated at that time. In 2.11, this deprecation has now been officially recorded in the Asset TypeDef. The properties are: owner - now captured in the Ownership classification ownerType - also captured in the Ownership classification zoneMembership - now captured in the AssetZoneMembership classification latestChange - now captures]d in the LatestChange classification See new type description in model 0010 . TabularColumnType Deprecated type called TabularColumnType because it restricts tabular columns to primitive types when it could be a literal for example. See type descriptions in model 0530 . SimpleDocumentType, StructDocumentType, MapDocumentType Deprecated types called SimpleDocumentType , StructDocumentType and MapDocumentType because they offer little value since the type is typically stored in the TypeEmbeddedAttribute classification. This change makes the document schemas consistent with other types of schema. See type descriptions in model 0531 . TermISATypeOFRelationship Deprecated TermISATypeOFRelationship because the ends are defined the wrong way round. When visualizing end1 should point to end2. This relationship incorrectly has the super type pointing to the subtype. Use the new IsATypeOfRelationship instead to represent an is-a-type-of relationship between two spine objects. See new type descriptions in model 0380 . Bug fixes and other updates For details, see the commit history in GitHub . Implementation status","title":2.11},{"location":"release-notes/2-11/#211-july-2021","text":"Special note on Java support This is expected to be the last release supporting Java 8. The next release will commence the 3.x series of releases and will require Java 11. Known issue: use chromium-based browser for UIs It is recommended to use a chromium-based browser such as Google Chrome, Microsoft Edge or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . Functional changes Multiple topics for a cohort An open metadata repository cohort uses three types of events to enable peer-to-peer sharing of metadata. In previous releases, these events have all been exchanged through a single event topic. Release 2.11 provides the option for a cohort to use a dedicated topic for each type of event. This improves the time to register a new member in the cohort and allows multiple instances of a metadata server or a metadata access point to access the cohort. Details of these new options can be found in the administration guide . Data Manager OMAS extension to support capture of event metadata The Data Manager OMAS has been extended to support the capture of metadata from Event Brokers such as Apache Kafka and API Managers such as an API Gateway. There are two new associated Open Metadata Integration Services (OMISs) to support integration connectors that extract metadata from these types of data managers: Topic Integrator OMIS supports integration connectors extracting metadata from Event Brokers. API Integrator OMIS supports integration connectors extracting API information from API managers. For more information on the use of the Data Manager OMAS with these integration services, see the Data Manager Integration solution. Information View OMAS , Virtualization Services, Security Officer Services, Gaian connector The following capabilities have now been removed (they were previously deprecated and/or in not in 'Released' status): Information View OMAS Virtualization Services and associated connectors Security Officer Services, Security sync services and associated connectors including for Apache Ranger Gaian database connector and additional authentication/impersonation support Much of the above capability can be implemented via Integration Services . Hadoop specifics may be developed in the future within the Egeria Hadoop GitHub repository. For more details of this change see #5314 . Data Platform Services, Data Platform OMAS , and Cassandra connectors Similarly, the following services have been deleted: Data Platform Services and Data Platform OMAS . For more details see #5344 . Data Platform capabilities are already available in Database Integrator and Files Integrator already part of Integration Services. Cassandra connectors: cassandra-data-store-connector and cassandra-metadata-extractor-connector will be introduced back in the Data Connectors GitHub repository. For more information see #2671 . Type changes: added, modified, deprecated APIManager, EventBroker New types for APIManager and EventBroker . These types inherit from SoftwareServerCapability . These are used in the new Data Manager OMAS APIs. See new type descriptions in model 0050 . Threat And a relationship to connect a GovernanceDefinition with a metadata element that defines the scope where it is applicable. See new type descriptions in models 0401 and 0405 . TabularFileColumn A new subtype for TabularColumn called TabularFileColumn is added to be able to distinguish between tabular columns from files and RelationalColumn (which also inherits from TabularColumn ). See type descriptions in model 0530 . EventTypeList New type called EventTypeList to allow a list of event types to be associated with a topic and a specific subtype of SchemaAttribute for an attribute in an event type to make it easier to search for data fields that are exclusively found in events. See new type descriptions in model 0535 . APIParameter, APIOperation New types for APIParameter to allow the capture of properties related to the API's treatment of the parameters. There are also properties for APIOperation . See new type descriptions in model 0536 . Types for display of data to end users New types for DisplayDataSchemaType , DisplayDataContainer , DisplayDataField , QuerySchemaType , QueryDataContainer and QueryDataField to allow the capture of properties related to the display of data to end users. See new type descriptions in model 0537 . RelationalTableType Updated supertype of RelationalTableType to inherit from ComplexSchemaType rather than TabularColumnType since TabularColumnType is now deprecated. See type descriptions in model 0534 . Asset properties A number of properties that where originally defined in Asset were moved to classifications to allow them to be managed independently of the original asset. This occurred before the TypeDefPatch support was in place and so these properties were not marked as deprecated at that time. In 2.11, this deprecation has now been officially recorded in the Asset TypeDef. The properties are: owner - now captured in the Ownership classification ownerType - also captured in the Ownership classification zoneMembership - now captured in the AssetZoneMembership classification latestChange - now captures]d in the LatestChange classification See new type description in model 0010 . TabularColumnType Deprecated type called TabularColumnType because it restricts tabular columns to primitive types when it could be a literal for example. See type descriptions in model 0530 . SimpleDocumentType, StructDocumentType, MapDocumentType Deprecated types called SimpleDocumentType , StructDocumentType and MapDocumentType because they offer little value since the type is typically stored in the TypeEmbeddedAttribute classification. This change makes the document schemas consistent with other types of schema. See type descriptions in model 0531 . TermISATypeOFRelationship Deprecated TermISATypeOFRelationship because the ends are defined the wrong way round. When visualizing end1 should point to end2. This relationship incorrectly has the super type pointing to the subtype. Use the new IsATypeOfRelationship instead to represent an is-a-type-of relationship between two spine objects. See new type descriptions in model 0380 . Bug fixes and other updates For details, see the commit history in GitHub . Implementation status","title":"2.11 (July 2021)"},{"location":"release-notes/2-2/","text":"Release 2.2 (September 2020) \u00b6 Below are the highlights of this release: Additional connectors are now placed in our assembly under server/lib without dependencies. If you need to use a connector that requires additional dependencies that are not already part of the server chassis, you will need to add those libraries here. The docker image has been updated to use a later openjdk alpine base image due to incompatibilities with our JanusGraph support in the old images for java 8. In the VDC chart, 2 new values have been added, 'ibmigc.connectorversion' and 'atlas.connectorversion'. In this release these are set to use the 2.1 connectors, since the connectors run to a different release cycle than the main Egeria code. Once new connectors are released you can update these values to get the latest connectors Further code to support lineage has been added, but in this release it remains in development and is not yet ready for use in production. User interface improvements. Ongoing bug fixes and refactoring especially in subject-area omas. Known Issues \u00b6 In the VDC helm chart, the Apache Atlas initialization job fails to complete. This is due to a problem with the Apache Atlas server and Apache SOLR. See https://github.com/odpi/egeria/issues/3587 for more information. Dependencies \u00b6 Spring Boot is updated to 2.3.3. Egeria Implementation Status at Release 2.2 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":2.2},{"location":"release-notes/2-2/#release-22-september-2020","text":"Below are the highlights of this release: Additional connectors are now placed in our assembly under server/lib without dependencies. If you need to use a connector that requires additional dependencies that are not already part of the server chassis, you will need to add those libraries here. The docker image has been updated to use a later openjdk alpine base image due to incompatibilities with our JanusGraph support in the old images for java 8. In the VDC chart, 2 new values have been added, 'ibmigc.connectorversion' and 'atlas.connectorversion'. In this release these are set to use the 2.1 connectors, since the connectors run to a different release cycle than the main Egeria code. Once new connectors are released you can update these values to get the latest connectors Further code to support lineage has been added, but in this release it remains in development and is not yet ready for use in production. User interface improvements. Ongoing bug fixes and refactoring especially in subject-area omas.","title":"Release 2.2 (September 2020)"},{"location":"release-notes/2-2/#known-issues","text":"In the VDC helm chart, the Apache Atlas initialization job fails to complete. This is due to a problem with the Apache Atlas server and Apache SOLR. See https://github.com/odpi/egeria/issues/3587 for more information.","title":"Known Issues"},{"location":"release-notes/2-2/#dependencies","text":"Spring Boot is updated to 2.3.3.","title":"Dependencies"},{"location":"release-notes/2-2/#egeria-implementation-status-at-release-22","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 2.2"},{"location":"release-notes/2-3/","text":"Release 2.3 (October 2020) \u00b6 New capabilities & Major changes \u00b6 A new Presentation Server User interface has been added, making use of React & Carbon Presentation Server is still in development For developers not contributing to Presentation Server, running in a container under Kubernetes or docker-compose is the easiest way to get started See the last section of the Presentation Server README for instructions on running Presentation Server For contributors, The Presentation Server README also documents 'Configuring the Presentation Server' - this is done automatically in our k8s/compose environment. However if doing this manually note (4th point) that the environment variable is called EGERIA_PRESENTATIONSERVER_SERVER_<ui server name> where the <ui server name> is the tenant's serverName.. The examples in the document are correct. The Dino User Interface for presentation server now allows an Egeria operations user to display a graph and details of Egeria resources including platforms, servers, services and cohort memberships. Type Explorer & Repository Explorer, previously found in the Polymer based UI, are now available in Presentation Server. Raise a github issue or Contact the Egeria team via slack at slack.odpi.com if you experience issues or have questions. The Egeria Docker image is now based on Redhat's UBI-8 openjdk-11 image, to improve security & operational support. See issue #3580 Bug Fixes & ongoing feature work Known Issues \u00b6 Several maven artifacts have not been published to maven central/JCenter. See issue #3675 They can be retrieved from JFrog Artifactory at 'https://odpi.jfrog.io/odpi/egeria-release-local' if needed and it is not possible to build locally. org.odpi.egeria:presentation-server org.odpi.egeria:subject-area-fvt org.odpi.egeria:dev-ops-api org.odpi.egeria:digital-service-spring Dependencies \u00b6 Spring has been updated to 2.3.9 Spring Security has been updated to 5.4.0 For a full list run 'mvn dependency:tree' against top level directory and/or review the top level pom.xml ## Egeria Implementation Status at Release 2.3 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":2.3},{"location":"release-notes/2-3/#release-23-october-2020","text":"","title":"Release 2.3 (October 2020)"},{"location":"release-notes/2-3/#new-capabilities-major-changes","text":"A new Presentation Server User interface has been added, making use of React & Carbon Presentation Server is still in development For developers not contributing to Presentation Server, running in a container under Kubernetes or docker-compose is the easiest way to get started See the last section of the Presentation Server README for instructions on running Presentation Server For contributors, The Presentation Server README also documents 'Configuring the Presentation Server' - this is done automatically in our k8s/compose environment. However if doing this manually note (4th point) that the environment variable is called EGERIA_PRESENTATIONSERVER_SERVER_<ui server name> where the <ui server name> is the tenant's serverName.. The examples in the document are correct. The Dino User Interface for presentation server now allows an Egeria operations user to display a graph and details of Egeria resources including platforms, servers, services and cohort memberships. Type Explorer & Repository Explorer, previously found in the Polymer based UI, are now available in Presentation Server. Raise a github issue or Contact the Egeria team via slack at slack.odpi.com if you experience issues or have questions. The Egeria Docker image is now based on Redhat's UBI-8 openjdk-11 image, to improve security & operational support. See issue #3580 Bug Fixes & ongoing feature work","title":"New capabilities &amp; Major changes"},{"location":"release-notes/2-3/#known-issues","text":"Several maven artifacts have not been published to maven central/JCenter. See issue #3675 They can be retrieved from JFrog Artifactory at 'https://odpi.jfrog.io/odpi/egeria-release-local' if needed and it is not possible to build locally. org.odpi.egeria:presentation-server org.odpi.egeria:subject-area-fvt org.odpi.egeria:dev-ops-api org.odpi.egeria:digital-service-spring","title":"Known Issues"},{"location":"release-notes/2-3/#dependencies","text":"Spring has been updated to 2.3.9 Spring Security has been updated to 5.4.0 For a full list run 'mvn dependency:tree' against top level directory and/or review the top level pom.xml ## Egeria Implementation Status at Release 2.3 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Dependencies"},{"location":"release-notes/2-4/","text":"Release 2.4 (November 2020) \u00b6 The Integration Daemon now makes it simpler to exchange metadata with third party technology such as catalogs and databases. The Repository Explorer User Interface ('REX') must now be configured with a list of available platforms and servers to improve security and usability. The latest version of Repository Explorer, Type Explorer and Dino, is found in the 'Presentation Server' UI. The Repository Explorer User Interface ('REX') must now be configured with a list of available platforms and servers to improve security and usability. See Presentation Server component documentation and Configuring the Presentation Server . The UI Server Chassis no longer includes static content. It is now required to deploy the egeria-ui project in addition to the spring application. A docker image is available, and the docker-compose & Kubernetes lab environments include this pre-configured. Additional Access Services Functional Verification tests have been added to improve code quality Bug fixes Dependency updates Spring has been updated to 5.2.9 Spring Security has been updated to 5.4.1 Spring Boot has been updated to 2.3.3 For a full list run 'mvn dependency:tree' against top level directory and/or review the top level pom.xml Known Issues \u00b6 The docker-compose based lab environment is incorrectly pulling docker images from the wrong repository. To correct this change open-metadata-resources/open-metadata-deployment/compose/tutorials/.env to egeria_repo=odpi . The original repository has however been updated to include 2.4 images, so will now work even if unchanged. Egeria Implementation Status at Release 2.4 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":2.4},{"location":"release-notes/2-4/#release-24-november-2020","text":"The Integration Daemon now makes it simpler to exchange metadata with third party technology such as catalogs and databases. The Repository Explorer User Interface ('REX') must now be configured with a list of available platforms and servers to improve security and usability. The latest version of Repository Explorer, Type Explorer and Dino, is found in the 'Presentation Server' UI. The Repository Explorer User Interface ('REX') must now be configured with a list of available platforms and servers to improve security and usability. See Presentation Server component documentation and Configuring the Presentation Server . The UI Server Chassis no longer includes static content. It is now required to deploy the egeria-ui project in addition to the spring application. A docker image is available, and the docker-compose & Kubernetes lab environments include this pre-configured. Additional Access Services Functional Verification tests have been added to improve code quality Bug fixes Dependency updates Spring has been updated to 5.2.9 Spring Security has been updated to 5.4.1 Spring Boot has been updated to 2.3.3 For a full list run 'mvn dependency:tree' against top level directory and/or review the top level pom.xml","title":"Release 2.4 (November 2020)"},{"location":"release-notes/2-4/#known-issues","text":"The docker-compose based lab environment is incorrectly pulling docker images from the wrong repository. To correct this change open-metadata-resources/open-metadata-deployment/compose/tutorials/.env to egeria_repo=odpi . The original repository has however been updated to include 2.4 images, so will now work even if unchanged.","title":"Known Issues"},{"location":"release-notes/2-4/#egeria-implementation-status-at-release-24","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 2.4"},{"location":"release-notes/2-5/","text":"Release 2.5 (December 2020) \u00b6 Below are the highlights of the 2.5 release: The following improvements to the presentation-server user interface: The Type Explorer UI supports options to show/hide deprecated types and/or deprecated attributes. Please refer to the Type Explorer help.md for details. preserves the user-selected focus type across reloads of type information from the repository server. The Repository Explorer UI has the Enterprise option enabled by default. It can be disabled to perform more specific, localized queries. now indicates whether an instance was returned by an enterprise or local scope operation against its home repository or is a reference copy or proxy. has a user-settable limit on the number of search results (and a warning to the user if it is exceeded) now colors nodes based on their home metadata collection's ID. This previously used metadata collection's name but a metadata collection's name can be changed, whereas the metadata collection's ID is permanent. has improved help information covering search The Dino UI displays a server's status history in a separate dialog instead of inline in the server details view. The following improvements to the repositories: The Graph Repository find methods have reinstated support for core properties, previously temporarily disabled due to property name clashes that are now resolved A new type OpenMetadataRoot has been added as the root type for all Open Metadata Types. See the base model The admin services guide has some additional information on configuring TLS security Improvements to the gradle build scripts, but at this point it remains incomplete and build of egeria still requires maven Bug Fixes Dependency Updates Egeria Implementation Status at Release 2.5 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":2.5},{"location":"release-notes/2-5/#release-25-december-2020","text":"Below are the highlights of the 2.5 release: The following improvements to the presentation-server user interface: The Type Explorer UI supports options to show/hide deprecated types and/or deprecated attributes. Please refer to the Type Explorer help.md for details. preserves the user-selected focus type across reloads of type information from the repository server. The Repository Explorer UI has the Enterprise option enabled by default. It can be disabled to perform more specific, localized queries. now indicates whether an instance was returned by an enterprise or local scope operation against its home repository or is a reference copy or proxy. has a user-settable limit on the number of search results (and a warning to the user if it is exceeded) now colors nodes based on their home metadata collection's ID. This previously used metadata collection's name but a metadata collection's name can be changed, whereas the metadata collection's ID is permanent. has improved help information covering search The Dino UI displays a server's status history in a separate dialog instead of inline in the server details view. The following improvements to the repositories: The Graph Repository find methods have reinstated support for core properties, previously temporarily disabled due to property name clashes that are now resolved A new type OpenMetadataRoot has been added as the root type for all Open Metadata Types. See the base model The admin services guide has some additional information on configuring TLS security Improvements to the gradle build scripts, but at this point it remains incomplete and build of egeria still requires maven Bug Fixes Dependency Updates","title":"Release 2.5 (December 2020)"},{"location":"release-notes/2-5/#egeria-implementation-status-at-release-25","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 2.5"},{"location":"release-notes/2-6/","text":"Release 2.6 (February 2021) \u00b6 Release 2.6 adds support for: * New Governance Server called Engine Host with associated Open Metadata Engine Services ( OMES ) . This Governance Server replaces the Discovery Server, Stewardship Server, Virtualizer and Security Officer Server. Extensions to Open Metadata Types for lineage, duplicate processing, governance actions, the software development lifecycle and analytics models. The release also changes the default location of some important files in order to facilitate deployment and seperate program files from writeable data. Details of these and other changes are in the sections that follow. Description of Changes \u00b6 Changes to data files created/used by Egeria \u00b6 Up to and including release 2.5, various data files were created in the current working directory when Egeria was run. This included configuration files, cohort information, graph repositories etc. This made it difficult to manage Egeria in a container environment where we want to manage persistent data explicitly - for example via a docker volume, or a kubernetes persistent volume (claim). Because of this the default locations of a number of files have changed, so when deploying release 2.6 make sure you copy any existing files you need to preserve over to their new locations. If this is not done server configs and repository data may not be found, or configuration files may not be decrytable. usage old new variables Server configuration omag.server.{0}.config data/servers/{0}/config/{0}.config 0 = server Name File based audit log omag.server.{0}.auditlog/ data/servers/{0}/logs/auditlog/ 0 = server Name cohort registry {0}.{1}.registrystore data/servers/{0}/cohorts/{1}.registrystore 0 = server Name, 1 = cohort name Graph repository {0}-graph-repository/ data/servers/{0}/repository/graph/ 0 = server Name Encrypted config key keystore_* data/platform/keys/keystore_* The result of this is that all the dynamic data created by egeria locally in the filesystem is restricted to the 'data' directory so this can be mapped to a volume easily. If you have already explicitly configured the relevant connector yourself there will be no change. this updates the defaults only. Removal of the Discovery Server and Stewardship Server \u00b6 The Discovery Server, Stewardship Server, Virtualizer and Security Officer Server have been consolidated into a new type of server called the Engine Host OMAG Server . The Engine Host runs one-to-many Open Metadata Engine Services ( OMES ) . Each engine services hosts a specific type of governance engine. The first engine service called Asset Analysis OMES will be for discovery engines and others for the different types of governance action engines . from the Governance Action Framework ( GAF ) . The reason for this change is that there is a lot of duplicated code in the original servers and this change simplifies the Governance Server Services and Server Administration . With this change it will also be easier for Egeria to host other types of governance engines such as Palisade and Gaian. New open metadata types for Governance Actions \u00b6 The following types have been added to support the governance action engines: 0461 Governance Action Engines 0462 Governance Action Types 0463 Governance Actions Updates to open metadata types for Lineage Mapping \u00b6 The LineageMapping open metadata relationship type has been updated to link Referenceables rather than SchemaElements . This is to capture lineage between components at different levels of detail since the data field mappings may not always be available. Lineage mapping is described in more detail here . New open metadata types for Duplicate Processing \u00b6 Since Egeria is integrating and distributing metadata from many different sources, it is inevitable that there will be multiple metadata instances that represent the same real-world \"thing\". The 0465 Duplicate Processing types allow these elements to be linked together. Presentation Server / React UI \u00b6 The node based User Interface component known as 'Presentation Server' has now fully moved to it's own GitHub Repository . The docker image has been renamed to egeria-react-ui Dino - Adds display of integration servers\u2019 integration services and engine hosts\u2019 engine services, including display of a dependency on a partnerOMAS. Rex - Improved error reporting and geometry management plus more consistent handling of focus objects. Enterprise queries are now the default, but can be over-ridden to perform a local operation. At this time 'Server Author' and 'Glossary Author' are still in development. New Helm Chart \u00b6 In addition to our 'lab' helm chart to support the Coco Pharmaceuticals environment, we have now added an additional helm chart which provides a simpler environment with just a single platform, and a single server, but configured with persistence and auto start. This offers an example of a simple Kubernetes deployment. Graph Repository \u00b6 Now implements the findEntities and fnidRelationships methods of the OMRS MetadataCollection API. Added detailed documentation for the graph repository Conformance Test Suite \u00b6 CTS now has tests for findEntities and findRelationships methods and search tests have been realigned into profiles so that all search operations are in optional profiles, with basic and advanced profiles for each of entities and relationships. Other changes \u00b6 Release 2.6 also contains many bug fixes and minor improvements & dependency updates Removals and Deprecations \u00b6 Discovery Server, Stewardship Server, Virtualizer and Security Officer Server have been replaced with more extensive capability - see above. Information View OMAS has now been removed following earlier deprecation. Egeria Implementation Status at Release 2.6 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content. Further Help and Support \u00b6 As part of the Linux AI & Data Foundation, our slack channels have moved to the LF AI & Data Slack workspace , and our mailing lists can now be found at https://lists.lfaidata.foundation/groups Continue to use these resources, along with GitHub to report bugs or ask questions.","title":2.6},{"location":"release-notes/2-6/#release-26-february-2021","text":"Release 2.6 adds support for: * New Governance Server called Engine Host with associated Open Metadata Engine Services ( OMES ) . This Governance Server replaces the Discovery Server, Stewardship Server, Virtualizer and Security Officer Server. Extensions to Open Metadata Types for lineage, duplicate processing, governance actions, the software development lifecycle and analytics models. The release also changes the default location of some important files in order to facilitate deployment and seperate program files from writeable data. Details of these and other changes are in the sections that follow.","title":"Release 2.6 (February 2021)"},{"location":"release-notes/2-6/#description-of-changes","text":"","title":"Description of Changes"},{"location":"release-notes/2-6/#changes-to-data-files-createdused-by-egeria","text":"Up to and including release 2.5, various data files were created in the current working directory when Egeria was run. This included configuration files, cohort information, graph repositories etc. This made it difficult to manage Egeria in a container environment where we want to manage persistent data explicitly - for example via a docker volume, or a kubernetes persistent volume (claim). Because of this the default locations of a number of files have changed, so when deploying release 2.6 make sure you copy any existing files you need to preserve over to their new locations. If this is not done server configs and repository data may not be found, or configuration files may not be decrytable. usage old new variables Server configuration omag.server.{0}.config data/servers/{0}/config/{0}.config 0 = server Name File based audit log omag.server.{0}.auditlog/ data/servers/{0}/logs/auditlog/ 0 = server Name cohort registry {0}.{1}.registrystore data/servers/{0}/cohorts/{1}.registrystore 0 = server Name, 1 = cohort name Graph repository {0}-graph-repository/ data/servers/{0}/repository/graph/ 0 = server Name Encrypted config key keystore_* data/platform/keys/keystore_* The result of this is that all the dynamic data created by egeria locally in the filesystem is restricted to the 'data' directory so this can be mapped to a volume easily. If you have already explicitly configured the relevant connector yourself there will be no change. this updates the defaults only.","title":"Changes to data files created/used by Egeria"},{"location":"release-notes/2-6/#removal-of-the-discovery-server-and-stewardship-server","text":"The Discovery Server, Stewardship Server, Virtualizer and Security Officer Server have been consolidated into a new type of server called the Engine Host OMAG Server . The Engine Host runs one-to-many Open Metadata Engine Services ( OMES ) . Each engine services hosts a specific type of governance engine. The first engine service called Asset Analysis OMES will be for discovery engines and others for the different types of governance action engines . from the Governance Action Framework ( GAF ) . The reason for this change is that there is a lot of duplicated code in the original servers and this change simplifies the Governance Server Services and Server Administration . With this change it will also be easier for Egeria to host other types of governance engines such as Palisade and Gaian.","title":"Removal of the Discovery Server and Stewardship Server"},{"location":"release-notes/2-6/#new-open-metadata-types-for-governance-actions","text":"The following types have been added to support the governance action engines: 0461 Governance Action Engines 0462 Governance Action Types 0463 Governance Actions","title":"New open metadata types for Governance Actions"},{"location":"release-notes/2-6/#updates-to-open-metadata-types-for-lineage-mapping","text":"The LineageMapping open metadata relationship type has been updated to link Referenceables rather than SchemaElements . This is to capture lineage between components at different levels of detail since the data field mappings may not always be available. Lineage mapping is described in more detail here .","title":"Updates to open metadata types for Lineage Mapping"},{"location":"release-notes/2-6/#new-open-metadata-types-for-duplicate-processing","text":"Since Egeria is integrating and distributing metadata from many different sources, it is inevitable that there will be multiple metadata instances that represent the same real-world \"thing\". The 0465 Duplicate Processing types allow these elements to be linked together.","title":"New open metadata types for Duplicate Processing"},{"location":"release-notes/2-6/#presentation-server-react-ui","text":"The node based User Interface component known as 'Presentation Server' has now fully moved to it's own GitHub Repository . The docker image has been renamed to egeria-react-ui Dino - Adds display of integration servers\u2019 integration services and engine hosts\u2019 engine services, including display of a dependency on a partnerOMAS. Rex - Improved error reporting and geometry management plus more consistent handling of focus objects. Enterprise queries are now the default, but can be over-ridden to perform a local operation. At this time 'Server Author' and 'Glossary Author' are still in development.","title":"Presentation Server / React UI"},{"location":"release-notes/2-6/#new-helm-chart","text":"In addition to our 'lab' helm chart to support the Coco Pharmaceuticals environment, we have now added an additional helm chart which provides a simpler environment with just a single platform, and a single server, but configured with persistence and auto start. This offers an example of a simple Kubernetes deployment.","title":"New Helm Chart"},{"location":"release-notes/2-6/#graph-repository","text":"Now implements the findEntities and fnidRelationships methods of the OMRS MetadataCollection API. Added detailed documentation for the graph repository","title":"Graph Repository"},{"location":"release-notes/2-6/#conformance-test-suite","text":"CTS now has tests for findEntities and findRelationships methods and search tests have been realigned into profiles so that all search operations are in optional profiles, with basic and advanced profiles for each of entities and relationships.","title":"Conformance Test Suite"},{"location":"release-notes/2-6/#other-changes","text":"Release 2.6 also contains many bug fixes and minor improvements & dependency updates","title":"Other changes"},{"location":"release-notes/2-6/#removals-and-deprecations","text":"Discovery Server, Stewardship Server, Virtualizer and Security Officer Server have been replaced with more extensive capability - see above. Information View OMAS has now been removed following earlier deprecation.","title":"Removals and Deprecations"},{"location":"release-notes/2-6/#egeria-implementation-status-at-release-26","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 2.6"},{"location":"release-notes/2-6/#further-help-and-support","text":"As part of the Linux AI & Data Foundation, our slack channels have moved to the LF AI & Data Slack workspace , and our mailing lists can now be found at https://lists.lfaidata.foundation/groups Continue to use these resources, along with GitHub to report bugs or ask questions.","title":"Further Help and Support"},{"location":"release-notes/2-7/","text":"Release 2.7 (March 2021) \u00b6 Release 2.7 adds: * Performance improvements in the graph repo * Changes to metadata types * Changes to the distribution process Details of these and other changes are in the sections that follow. Description of Changes \u00b6 Build & Release changes \u00b6 Previously release maven artifacts have been pushed to JCenter (in addition to Maven Central), and snapshots have been pushed to odpi.jfrog.io/odpi/egeria-snapshot. As of 2.7 this no longer occurs and release and snapshots are pushed only to Maven Central. Due to the above, maven artifacts may be signed by a different user to previously (and this may change in a future release). Performance Improvements \u00b6 Release 2.7 includes performance improvements to the graph repository's search methods, which select an efficient query strategy based on the properties and type filters supplied to the search. Metadata Types \u00b6 We have removed overloaded properties that existed at multiple levels in a type definition: for example, length was defined both on SchemaAttribute and again on RelationalColumn (which is a subtype of SchemaAttribute). This change removes them from being defined again the lower level (RelationalColumn in this example); however, the property itself will still be available at the lower level due to inheriting it from the supertype. Bug fixes and other updates \u00b6 Additional Bug Fixes Dependency Updates For details on both see the commit history in GitHub. Known Issues \u00b6 It is recommended to use a chromium-based browser such as Google Chrome or Microsoft Edge, or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . When running the 'Understanding Platform Services' lab, ensure you run the 'egeria-service-config' notebook first and do not restart the python kernel before running this lab. See #4842 . Egeria Implementation Status at Release 2.7 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content. Further Help and Support \u00b6 As part of the Linux AI & Data Foundation, our slack channels have moved to the LF AI & Data Slack workspace , and our mailing lists can now be found at https://lists.lfaidata.foundation/groups Continue to use these resources, along with GitHub to report bugs or ask questions.","title":2.7},{"location":"release-notes/2-7/#release-27-march-2021","text":"Release 2.7 adds: * Performance improvements in the graph repo * Changes to metadata types * Changes to the distribution process Details of these and other changes are in the sections that follow.","title":"Release 2.7 (March 2021)"},{"location":"release-notes/2-7/#description-of-changes","text":"","title":"Description of Changes"},{"location":"release-notes/2-7/#build-release-changes","text":"Previously release maven artifacts have been pushed to JCenter (in addition to Maven Central), and snapshots have been pushed to odpi.jfrog.io/odpi/egeria-snapshot. As of 2.7 this no longer occurs and release and snapshots are pushed only to Maven Central. Due to the above, maven artifacts may be signed by a different user to previously (and this may change in a future release).","title":"Build &amp; Release changes"},{"location":"release-notes/2-7/#performance-improvements","text":"Release 2.7 includes performance improvements to the graph repository's search methods, which select an efficient query strategy based on the properties and type filters supplied to the search.","title":"Performance Improvements"},{"location":"release-notes/2-7/#metadata-types","text":"We have removed overloaded properties that existed at multiple levels in a type definition: for example, length was defined both on SchemaAttribute and again on RelationalColumn (which is a subtype of SchemaAttribute). This change removes them from being defined again the lower level (RelationalColumn in this example); however, the property itself will still be available at the lower level due to inheriting it from the supertype.","title":"Metadata Types"},{"location":"release-notes/2-7/#bug-fixes-and-other-updates","text":"Additional Bug Fixes Dependency Updates For details on both see the commit history in GitHub.","title":"Bug fixes and other updates"},{"location":"release-notes/2-7/#known-issues","text":"It is recommended to use a chromium-based browser such as Google Chrome or Microsoft Edge, or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . When running the 'Understanding Platform Services' lab, ensure you run the 'egeria-service-config' notebook first and do not restart the python kernel before running this lab. See #4842 .","title":"Known Issues"},{"location":"release-notes/2-7/#egeria-implementation-status-at-release-27","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 2.7"},{"location":"release-notes/2-7/#further-help-and-support","text":"As part of the Linux AI & Data Foundation, our slack channels have moved to the LF AI & Data Slack workspace , and our mailing lists can now be found at https://lists.lfaidata.foundation/groups Continue to use these resources, along with GitHub to report bugs or ask questions.","title":"Further Help and Support"},{"location":"release-notes/2-8/","text":"Release 2.8 (April 2021) \u00b6 Release 2.8 adds: * New support for event and property filtering for the open metadata server security connector * Changes to metadata types * New performance workbench for the CTS (technical preview) * New interface for retrieving the complete history of a single metadata instance * Splitting of CTS results into multiple smaller files Details of these and other changes are in the sections that follow. Description of Changes \u00b6 Updates to the Open Metadata Security Connector \u00b6 Before this release, the repository services support 3 filtering points for managing events for the OMRS Cohort Topic. Should an event be sent to the cohort Should an event be retrieved from the cohort Should a received event be stored in the local repository These filtering points are set up in the configuration document of the server. It is possible to specify rules to determine which types of events and which types of metadata elements are filtered out. However this configuration provides no control to allow filtering of events for specific instances. This release extends the metadata server security connector so it can be called at these same filter points. This will be through optional interfaces that the security connector can choose to implement. If the current rules are set up, they will still be executed. This change complements the existing filtering. The server security connector also implements the repository security interface called when metadata is being added/updated/deleted/retrieved through the APIs. Extending the security connector for event filtering means that it can make consistent decisions on the sharing of metadata through the cohorts and through the APIs. Configuring the server security connector \u00b6 Configuring the server security connector will not change with this feature. If the connector needs custom attributes to select rule sets etc, these can be specified in the configuration properties. See https://egeria.odpi.org/open-metadata-implementation/admin-services/docs/user/configuring-the-server-security-connector.html . Implementing the server security connector \u00b6 The security server connector will have two new interfaces that it can implement: one for the cohort events and one for saving events to the local repository. The event interface will have two methods, one for sending and one for receiving. The parameters will include the cohort name and the event contents. It can return the event unchanged, return a modified event (eg with sensitive content removed) or return null to say that the event is filtered out. The event saving interface will receive the instance header and can return a boolean to indicate if the local repository should store it. If true is returned, the refresh event sequence is initiated. The repository connector then has the ultimate choice when the refreshed instance is returned from the home repository as to whether to store it or not. There is a single instance of the connector in the server so it is able to maintain counts and cache rules etc. It can also be implemented as a facade to a proprietary service. More information on the security connector can be found on this page: https://egeria.odpi.org/open-metadata-implementation/common-services/metadata-security/metadata-security-apis Metadata Types \u00b6 Updates to the location types in model 0025 : Add the mapProjection property to the FixedLocation classification Change the address property to networkAddress in the CyberLocation classification Deprecated HostLocation in favor of the AssetLocation relationship Deprecate the RuntimeForProcess relationship since it is superfluous - use ServerAssetUse since Application is a SoftwareServerCapability . See model 0045 . Replace the deployedImplementationType property with the businessCapabilityType in the BusinessCapability since it is a more descriptive name. See model 0440 . Performance workbench \u00b6 The performance workbench intends to test the response time of all repository (metadata collection) methods for the technology under test. The volume of the test can be easily configured to also test scalability. More information is available in the workbench's documentation . Instance history interface \u00b6 Two new (optional) methods have been introduced to the metadata collection interface: getEntityDetailHistory getRelationshipHistory Both methods take the GUID of the instance for which to retrieve history, an optional range of times between which to retrieve the historical versions (or if both are null to retrieve all historical versions), and a set of paging parameters. If not implemented by a repository, these will simply throw FunctionNotSupported exceptions by default to indicate that they are not implemented. CTS results output \u00b6 Up to this release, the detailed results of a CTS run could only be be retrieved by pulling a huge (100's of MB) file across the REST interface for the CTS . Aside from not typically working with most REST clients (like Postman), this had the additional impact of a sudden huge hit on the JVM heap to serialize such a large JSON structure (immediately grabbing ~1GB of the heap). While this old interface still exists for backwards compatibility, the new default interface provided in this release allows users to pull down just an overall summary of the results separately from the full detailed results, and the detailed results are now broken down into separate files by profile and test case: each of which can therefore be retrieved individually. (So, for example, if you see from the summary that only 1-2 profiles are not conformant, you can retrieve just the details for those profiles rather than all details.) Changes to deployment of the Polymer based UI \u00b6 In previous releases, a zuul router component was used within the UI server chassis to route requests for static content to a separate server. In this release any routing needs to be setup externally, for example by placing a nginx proxy in front of both the ui chassis and static content server. This is now done by our docker-compose environment & helm charts so to access the UI you need to go to the nginx proxy. Further summary information can be found in the documentation for those assets. Bug fixes and other updates \u00b6 Additional Bug Fixes Dependency Updates For details on both see the commit history in GitHub. Known Issues \u00b6 It is recommended to use a chromium-based browser such as Google Chrome or Microsoft Edge, or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . Egeria source code currently fails to build on Windows natively. Please use Linux, MacOS, or compile under WSL/WSL2 on Windows. See #4917 Several Java samples fail (governance, admin) - #4656 , #4662 , #4056 The React UI used by the helm charts and compose is based on react UI release 2.7.0 due to layout issues found with 2.8.0. See #5022 The platform services notebook may fail to query servers correctly. See #5023 The building a data catalog notebook may fail if run quickly. See #2688 The data curation notebook is incomplete and still being developed. The final steps may fail to work in a container environment. See #5021 Egeria Implementation Status at Release 2.8 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content. Further Help and Support \u00b6 As part of the Linux AI & Data Foundation, our slack channels have moved to the LF AI & Data Slack workspace , and our mailing lists can now be found at https://lists.lfaidata.foundation/groups Continue to use these resources, along with GitHub to report bugs or ask questions.","title":2.8},{"location":"release-notes/2-8/#release-28-april-2021","text":"Release 2.8 adds: * New support for event and property filtering for the open metadata server security connector * Changes to metadata types * New performance workbench for the CTS (technical preview) * New interface for retrieving the complete history of a single metadata instance * Splitting of CTS results into multiple smaller files Details of these and other changes are in the sections that follow.","title":"Release 2.8 (April 2021)"},{"location":"release-notes/2-8/#description-of-changes","text":"","title":"Description of Changes"},{"location":"release-notes/2-8/#updates-to-the-open-metadata-security-connector","text":"Before this release, the repository services support 3 filtering points for managing events for the OMRS Cohort Topic. Should an event be sent to the cohort Should an event be retrieved from the cohort Should a received event be stored in the local repository These filtering points are set up in the configuration document of the server. It is possible to specify rules to determine which types of events and which types of metadata elements are filtered out. However this configuration provides no control to allow filtering of events for specific instances. This release extends the metadata server security connector so it can be called at these same filter points. This will be through optional interfaces that the security connector can choose to implement. If the current rules are set up, they will still be executed. This change complements the existing filtering. The server security connector also implements the repository security interface called when metadata is being added/updated/deleted/retrieved through the APIs. Extending the security connector for event filtering means that it can make consistent decisions on the sharing of metadata through the cohorts and through the APIs.","title":"Updates to the Open Metadata Security Connector"},{"location":"release-notes/2-8/#configuring-the-server-security-connector","text":"Configuring the server security connector will not change with this feature. If the connector needs custom attributes to select rule sets etc, these can be specified in the configuration properties. See https://egeria.odpi.org/open-metadata-implementation/admin-services/docs/user/configuring-the-server-security-connector.html .","title":"Configuring the server security connector"},{"location":"release-notes/2-8/#implementing-the-server-security-connector","text":"The security server connector will have two new interfaces that it can implement: one for the cohort events and one for saving events to the local repository. The event interface will have two methods, one for sending and one for receiving. The parameters will include the cohort name and the event contents. It can return the event unchanged, return a modified event (eg with sensitive content removed) or return null to say that the event is filtered out. The event saving interface will receive the instance header and can return a boolean to indicate if the local repository should store it. If true is returned, the refresh event sequence is initiated. The repository connector then has the ultimate choice when the refreshed instance is returned from the home repository as to whether to store it or not. There is a single instance of the connector in the server so it is able to maintain counts and cache rules etc. It can also be implemented as a facade to a proprietary service. More information on the security connector can be found on this page: https://egeria.odpi.org/open-metadata-implementation/common-services/metadata-security/metadata-security-apis","title":"Implementing the server security connector"},{"location":"release-notes/2-8/#metadata-types","text":"Updates to the location types in model 0025 : Add the mapProjection property to the FixedLocation classification Change the address property to networkAddress in the CyberLocation classification Deprecated HostLocation in favor of the AssetLocation relationship Deprecate the RuntimeForProcess relationship since it is superfluous - use ServerAssetUse since Application is a SoftwareServerCapability . See model 0045 . Replace the deployedImplementationType property with the businessCapabilityType in the BusinessCapability since it is a more descriptive name. See model 0440 .","title":"Metadata Types"},{"location":"release-notes/2-8/#performance-workbench","text":"The performance workbench intends to test the response time of all repository (metadata collection) methods for the technology under test. The volume of the test can be easily configured to also test scalability. More information is available in the workbench's documentation .","title":"Performance workbench"},{"location":"release-notes/2-8/#instance-history-interface","text":"Two new (optional) methods have been introduced to the metadata collection interface: getEntityDetailHistory getRelationshipHistory Both methods take the GUID of the instance for which to retrieve history, an optional range of times between which to retrieve the historical versions (or if both are null to retrieve all historical versions), and a set of paging parameters. If not implemented by a repository, these will simply throw FunctionNotSupported exceptions by default to indicate that they are not implemented.","title":"Instance history interface"},{"location":"release-notes/2-8/#cts-results-output","text":"Up to this release, the detailed results of a CTS run could only be be retrieved by pulling a huge (100's of MB) file across the REST interface for the CTS . Aside from not typically working with most REST clients (like Postman), this had the additional impact of a sudden huge hit on the JVM heap to serialize such a large JSON structure (immediately grabbing ~1GB of the heap). While this old interface still exists for backwards compatibility, the new default interface provided in this release allows users to pull down just an overall summary of the results separately from the full detailed results, and the detailed results are now broken down into separate files by profile and test case: each of which can therefore be retrieved individually. (So, for example, if you see from the summary that only 1-2 profiles are not conformant, you can retrieve just the details for those profiles rather than all details.)","title":"CTS results output"},{"location":"release-notes/2-8/#changes-to-deployment-of-the-polymer-based-ui","text":"In previous releases, a zuul router component was used within the UI server chassis to route requests for static content to a separate server. In this release any routing needs to be setup externally, for example by placing a nginx proxy in front of both the ui chassis and static content server. This is now done by our docker-compose environment & helm charts so to access the UI you need to go to the nginx proxy. Further summary information can be found in the documentation for those assets.","title":"Changes to deployment of the Polymer based UI"},{"location":"release-notes/2-8/#bug-fixes-and-other-updates","text":"Additional Bug Fixes Dependency Updates For details on both see the commit history in GitHub.","title":"Bug fixes and other updates"},{"location":"release-notes/2-8/#known-issues","text":"It is recommended to use a chromium-based browser such as Google Chrome or Microsoft Edge, or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . Egeria source code currently fails to build on Windows natively. Please use Linux, MacOS, or compile under WSL/WSL2 on Windows. See #4917 Several Java samples fail (governance, admin) - #4656 , #4662 , #4056 The React UI used by the helm charts and compose is based on react UI release 2.7.0 due to layout issues found with 2.8.0. See #5022 The platform services notebook may fail to query servers correctly. See #5023 The building a data catalog notebook may fail if run quickly. See #2688 The data curation notebook is incomplete and still being developed. The final steps may fail to work in a container environment. See #5021","title":"Known Issues"},{"location":"release-notes/2-8/#egeria-implementation-status-at-release-28","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 2.8"},{"location":"release-notes/2-8/#further-help-and-support","text":"As part of the Linux AI & Data Foundation, our slack channels have moved to the LF AI & Data Slack workspace , and our mailing lists can now be found at https://lists.lfaidata.foundation/groups Continue to use these resources, along with GitHub to report bugs or ask questions.","title":"Further Help and Support"},{"location":"release-notes/2-9/","text":"Release 2.9 (May 2021) \u00b6 Release 2.9 adds: * Changes to metadata types * Changes to building Egeria on Windows Details of these and other changes are in the sections that follow. Description of Changes \u00b6 Metadata Types \u00b6 The UserProfileManager , UserAccessDirectory and MasterDataManager classification for Referenceables has been added to model 0056 Asset Managers . A new relationship called ActionTarget to link a To Do to the elements that need work has been added to model 0137 Actions for People . A new attribute called filterExpression has been added to the Port entity in model 0217 Ports . A new classification called PrimaryCategory has been added to model 0335 Primary Category . A new classification called GovernanceMeasurements has been added to model 0450 Governance Rollout . The RelationalColumnType entity in model 0534 Relational Schema . only allows for a column to be primitive. It could be a literal, enum or external and so this type has been deprecated and the appropriate schema types should be used directly. Building Egeria on Windows \u00b6 To build Egeria on Windows you should use Windows Subsystem for Linux Version 2 or above & install an appropriate Linux distribution. Egeria should then be built & run within this environment. IDEs such as IntelliJ and VSCode support editing and code management within the Windows GUI alongside build and execution in Linux. See PR #5084 for more information. Bug fixes and other updates \u00b6 Additional Bug Fixes Dependency Updates For details on both see the commit history in GitHub. Known Issues \u00b6 It is recommended to use a chromium-based browser such as Google Chrome or Microsoft Edge, or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . When running the 'Understanding Platform Services' lab, ensure you run the 'egeria-service-config' notebook first and do not restart the python kernel before running this lab. See #4842 . Egeria Implementation Status at Release 2.9 \u00b6 Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content. Further Help and Support \u00b6 See the Community Guide .","title":2.9},{"location":"release-notes/2-9/#release-29-may-2021","text":"Release 2.9 adds: * Changes to metadata types * Changes to building Egeria on Windows Details of these and other changes are in the sections that follow.","title":"Release 2.9 (May 2021)"},{"location":"release-notes/2-9/#description-of-changes","text":"","title":"Description of Changes"},{"location":"release-notes/2-9/#metadata-types","text":"The UserProfileManager , UserAccessDirectory and MasterDataManager classification for Referenceables has been added to model 0056 Asset Managers . A new relationship called ActionTarget to link a To Do to the elements that need work has been added to model 0137 Actions for People . A new attribute called filterExpression has been added to the Port entity in model 0217 Ports . A new classification called PrimaryCategory has been added to model 0335 Primary Category . A new classification called GovernanceMeasurements has been added to model 0450 Governance Rollout . The RelationalColumnType entity in model 0534 Relational Schema . only allows for a column to be primitive. It could be a literal, enum or external and so this type has been deprecated and the appropriate schema types should be used directly.","title":"Metadata Types"},{"location":"release-notes/2-9/#building-egeria-on-windows","text":"To build Egeria on Windows you should use Windows Subsystem for Linux Version 2 or above & install an appropriate Linux distribution. Egeria should then be built & run within this environment. IDEs such as IntelliJ and VSCode support editing and code management within the Windows GUI alongside build and execution in Linux. See PR #5084 for more information.","title":"Building Egeria on Windows"},{"location":"release-notes/2-9/#bug-fixes-and-other-updates","text":"Additional Bug Fixes Dependency Updates For details on both see the commit history in GitHub.","title":"Bug fixes and other updates"},{"location":"release-notes/2-9/#known-issues","text":"It is recommended to use a chromium-based browser such as Google Chrome or Microsoft Edge, or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . When running the 'Understanding Platform Services' lab, ensure you run the 'egeria-service-config' notebook first and do not restart the python kernel before running this lab. See #4842 .","title":"Known Issues"},{"location":"release-notes/2-9/#egeria-implementation-status-at-release-29","text":"Link to Egeria's Roadmap for more details about the Open Metadata and Governance vision, strategy and content.","title":"Egeria Implementation Status at Release 2.9"},{"location":"release-notes/2-9/#further-help-and-support","text":"See the Community Guide .","title":"Further Help and Support"},{"location":"release-notes/3-0/","text":"3.0 (August 2021) \u00b6 Special note on Java support Java 11 is now required to build and run Egeria. Known issue: use chromium-based browser for UIs It is recommended to use a chromium-based browser such as Google Chrome, Microsoft Edge or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . Functional changes Java 11 required As of Release 3.0 of Egeria, Java 11 is required to build and run Egeria. Egeria will not build / run / be supported on Java 8. Developers are now able to use Java 11 only functionality. Java releases beyond Java 11 up to the current release have some informal testing, and we do build verification on the current release (currently 16). See Java for further information. Defaults to multiple topics for cohorts The option added in release 2.11 to allow multiple topics per cohort now becomes the default in release 3.0. Changed passwords Passwords for the sample Coco Pharmaceuticals users were changed to secret . Bug fixes and other updates For details, see the commit history in GitHub . Implementation status","title":"3 0"},{"location":"release-notes/3-0/#30-august-2021","text":"Special note on Java support Java 11 is now required to build and run Egeria. Known issue: use chromium-based browser for UIs It is recommended to use a chromium-based browser such as Google Chrome, Microsoft Edge or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . Functional changes Java 11 required As of Release 3.0 of Egeria, Java 11 is required to build and run Egeria. Egeria will not build / run / be supported on Java 8. Developers are now able to use Java 11 only functionality. Java releases beyond Java 11 up to the current release have some informal testing, and we do build verification on the current release (currently 16). See Java for further information. Defaults to multiple topics for cohorts The option added in release 2.11 to allow multiple topics per cohort now becomes the default in release 3.0. Changed passwords Passwords for the sample Coco Pharmaceuticals users were changed to secret . Bug fixes and other updates For details, see the commit history in GitHub . Implementation status","title":"3.0 (August 2021)"},{"location":"release-notes/3-1/","text":"3.1 (expected September 2021) \u00b6 Special note on Java support Java 11 is now required to build and run Egeria. Known issue: use chromium-based browser for UIs It is recommended to use a chromium-based browser such as Google Chrome, Microsoft Edge or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . Functional changes docker-compose The docker-compose environment for running our Coco Pharmaceuticals lab demo/tutorial is now deprecated. The configuration is still available, but will not be further developed or tested, and will be removed in a future release. Our Kubernetes Helm charts are now recommended to quickly setup the same lab environment, and the documentation for these has been improved to cover a Kubernetes introduction, and example based on 'microk8s' which are suited to an end-user desktop environment (and can also be run in enterprise/cloud environments). Type changes: added, modified, deprecated Data processing purposes See new type descriptions in model 0485 . Distinguishing between virtual machines, containers and bare metal hardware There are now types for distinguishing between virtual machines and virtual containers as well as bare metal hardware. There are also new types for specific technologies such as HadoopCluster , KubernetesCluster and DockerContainer to provide concrete examples of different types of hosts using popular technologies. Storage volumes There are new types for defining a storage volume that has been attached to a host. See description in model 0036 . ApplicationService A new subtype of software server for reusable business functions (such as microservices) has been added called ApplicationService . See description in model 0057 . ServerEndpoint The ServerEndpoint relationship can now connect to any ITInfrastructure elements, not just SoftwareServers . See description in model 0040 . OperatingPlatform The OperatingPlatform entity can now record the patch level of the operating system. There are also new types for describing the contents of an operating platform. See description in model 0030 . DeployedVirtualContainer The DeployedVirtualContainer relationship has been deprecated in favor of a more generic HostedHost relationship. See description in model 0035 . BoundedSchemaType, BoundedSchemaElementType, ArraySchemaType, SetSchemaType See description in model 0507 . Bug fixes and other updates For details, see the commit history in GitHub . Implementation status","title":"3 1"},{"location":"release-notes/3-1/#31-expected-september-2021","text":"Special note on Java support Java 11 is now required to build and run Egeria. Known issue: use chromium-based browser for UIs It is recommended to use a chromium-based browser such as Google Chrome, Microsoft Edge or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . Functional changes docker-compose The docker-compose environment for running our Coco Pharmaceuticals lab demo/tutorial is now deprecated. The configuration is still available, but will not be further developed or tested, and will be removed in a future release. Our Kubernetes Helm charts are now recommended to quickly setup the same lab environment, and the documentation for these has been improved to cover a Kubernetes introduction, and example based on 'microk8s' which are suited to an end-user desktop environment (and can also be run in enterprise/cloud environments). Type changes: added, modified, deprecated Data processing purposes See new type descriptions in model 0485 . Distinguishing between virtual machines, containers and bare metal hardware There are now types for distinguishing between virtual machines and virtual containers as well as bare metal hardware. There are also new types for specific technologies such as HadoopCluster , KubernetesCluster and DockerContainer to provide concrete examples of different types of hosts using popular technologies. Storage volumes There are new types for defining a storage volume that has been attached to a host. See description in model 0036 . ApplicationService A new subtype of software server for reusable business functions (such as microservices) has been added called ApplicationService . See description in model 0057 . ServerEndpoint The ServerEndpoint relationship can now connect to any ITInfrastructure elements, not just SoftwareServers . See description in model 0040 . OperatingPlatform The OperatingPlatform entity can now record the patch level of the operating system. There are also new types for describing the contents of an operating platform. See description in model 0030 . DeployedVirtualContainer The DeployedVirtualContainer relationship has been deprecated in favor of a more generic HostedHost relationship. See description in model 0035 . BoundedSchemaType, BoundedSchemaElementType, ArraySchemaType, SetSchemaType See description in model 0507 . Bug fixes and other updates For details, see the commit history in GitHub . Implementation status","title":"3.1 (expected September 2021)"},{"location":"release-notes/content-status/","text":"Content status \u00b6 The Egeria Community is constantly innovating in the field of metadata integration and governance. The code is developed using an agile process. As such, new code is continuously introduced. The benefit is that there is plenty of opportunity to feedback and influence the development process. The downside is that the master branch contains code at different stages of development. The aim of this page is to document how we label the different modules, so you can choose what to consume. These labels are found in the README.md files at the top level of each module. Basically, a module will go through the phases in this order: In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Technical preview Technical preview function is in a state that it can be tried. The development is complete, there is documentation and there are samples, tutorials and hands-on labs as appropriate. The community is looking for feedback on the function before releasing it. This feedback may result in changes to the external interfaces. Released This function is complete and can be used. The interfaces will be supported until the function is removed from the project via the deprecation process. There will be ongoing extensions to this function, but it will be done to ensure backward compatibility as far as possible. If there is a need to break backward compatibility, this will be discussed and reviewed in the community, with a documented timeline. Deprecated This function has been previously released. However, the maintainers believe there is no one interested in using, maintaining and developing this function. It may be removed in the future if there is consensus in the community. The README for this module will describe the timeline for this process. If you see deprecated function that you need, please contact the community and vote for its continued support. The current phase is shown at the top of the page. The history of the phases that the module has gone through is shown towards the end of the page.","title":"Content Status"},{"location":"release-notes/content-status/#content-status","text":"The Egeria Community is constantly innovating in the field of metadata integration and governance. The code is developed using an agile process. As such, new code is continuously introduced. The benefit is that there is plenty of opportunity to feedback and influence the development process. The downside is that the master branch contains code at different stages of development. The aim of this page is to document how we label the different modules, so you can choose what to consume. These labels are found in the README.md files at the top level of each module. Basically, a module will go through the phases in this order: In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Technical preview Technical preview function is in a state that it can be tried. The development is complete, there is documentation and there are samples, tutorials and hands-on labs as appropriate. The community is looking for feedback on the function before releasing it. This feedback may result in changes to the external interfaces. Released This function is complete and can be used. The interfaces will be supported until the function is removed from the project via the deprecation process. There will be ongoing extensions to this function, but it will be done to ensure backward compatibility as far as possible. If there is a need to break backward compatibility, this will be discussed and reviewed in the community, with a documented timeline. Deprecated This function has been previously released. However, the maintainers believe there is no one interested in using, maintaining and developing this function. It may be removed in the future if there is consensus in the community. The README for this module will describe the timeline for this process. If you see deprecated function that you need, please contact the community and vote for its continued support. The current phase is shown at the top of the page. The history of the phases that the module has gone through is shown towards the end of the page.","title":"Content status"},{"location":"release-notes/latest/","text":"3.0 (August 2021) \u00b6 Special note on Java support Java 11 is now required to build and run Egeria. Known issue: use chromium-based browser for UIs It is recommended to use a chromium-based browser such as Google Chrome, Microsoft Edge or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . Functional changes Java 11 required As of Release 3.0 of Egeria, Java 11 is required to build and run Egeria. Egeria will not build / run / be supported on Java 8. Developers are now able to use Java 11 only functionality. Java releases beyond Java 11 up to the current release have some informal testing, and we do build verification on the current release (currently 16). See Java for further information. Defaults to multiple topics for cohorts The option added in release 2.11 to allow multiple topics per cohort now becomes the default in release 3.0. Changed passwords Passwords for the sample Coco Pharmaceuticals users were changed to secret . Bug fixes and other updates For details, see the commit history in GitHub . Implementation status","title":"Latest Release"},{"location":"release-notes/latest/#30-august-2021","text":"Special note on Java support Java 11 is now required to build and run Egeria. Known issue: use chromium-based browser for UIs It is recommended to use a chromium-based browser such as Google Chrome, Microsoft Edge or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . Functional changes Java 11 required As of Release 3.0 of Egeria, Java 11 is required to build and run Egeria. Egeria will not build / run / be supported on Java 8. Developers are now able to use Java 11 only functionality. Java releases beyond Java 11 up to the current release have some informal testing, and we do build verification on the current release (currently 16). See Java for further information. Defaults to multiple topics for cohorts The option added in release 2.11 to allow multiple topics per cohort now becomes the default in release 3.0. Changed passwords Passwords for the sample Coco Pharmaceuticals users were changed to secret . Bug fixes and other updates For details, see the commit history in GitHub . Implementation status","title":"3.0 (August 2021)"},{"location":"release-notes/next/","text":"3.1 (expected September 2021) \u00b6 Special note on Java support Java 11 is now required to build and run Egeria. Known issue: use chromium-based browser for UIs It is recommended to use a chromium-based browser such as Google Chrome, Microsoft Edge or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . Functional changes docker-compose The docker-compose environment for running our Coco Pharmaceuticals lab demo/tutorial is now deprecated. The configuration is still available, but will not be further developed or tested, and will be removed in a future release. Our Kubernetes Helm charts are now recommended to quickly setup the same lab environment, and the documentation for these has been improved to cover a Kubernetes introduction, and example based on 'microk8s' which are suited to an end-user desktop environment (and can also be run in enterprise/cloud environments). Type changes: added, modified, deprecated Data processing purposes See new type descriptions in model 0485 . Distinguishing between virtual machines, containers and bare metal hardware There are now types for distinguishing between virtual machines and virtual containers as well as bare metal hardware. There are also new types for specific technologies such as HadoopCluster , KubernetesCluster and DockerContainer to provide concrete examples of different types of hosts using popular technologies. Storage volumes There are new types for defining a storage volume that has been attached to a host. See description in model 0036 . ApplicationService A new subtype of software server for reusable business functions (such as microservices) has been added called ApplicationService . See description in model 0057 . ServerEndpoint The ServerEndpoint relationship can now connect to any ITInfrastructure elements, not just SoftwareServers . See description in model 0040 . OperatingPlatform The OperatingPlatform entity can now record the patch level of the operating system. There are also new types for describing the contents of an operating platform. See description in model 0030 . DeployedVirtualContainer The DeployedVirtualContainer relationship has been deprecated in favor of a more generic HostedHost relationship. See description in model 0035 . BoundedSchemaType, BoundedSchemaElementType, ArraySchemaType, SetSchemaType See description in model 0507 . Bug fixes and other updates For details, see the commit history in GitHub . Implementation status","title":"Next Release"},{"location":"release-notes/next/#31-expected-september-2021","text":"Special note on Java support Java 11 is now required to build and run Egeria. Known issue: use chromium-based browser for UIs It is recommended to use a chromium-based browser such as Google Chrome, Microsoft Edge or Apple Safari for the Egeria React UI. Some parts of the UI experience such as Dino currently experience problems with Firefox. See odpi/egeria-react-ui#96 . Functional changes docker-compose The docker-compose environment for running our Coco Pharmaceuticals lab demo/tutorial is now deprecated. The configuration is still available, but will not be further developed or tested, and will be removed in a future release. Our Kubernetes Helm charts are now recommended to quickly setup the same lab environment, and the documentation for these has been improved to cover a Kubernetes introduction, and example based on 'microk8s' which are suited to an end-user desktop environment (and can also be run in enterprise/cloud environments). Type changes: added, modified, deprecated Data processing purposes See new type descriptions in model 0485 . Distinguishing between virtual machines, containers and bare metal hardware There are now types for distinguishing between virtual machines and virtual containers as well as bare metal hardware. There are also new types for specific technologies such as HadoopCluster , KubernetesCluster and DockerContainer to provide concrete examples of different types of hosts using popular technologies. Storage volumes There are new types for defining a storage volume that has been attached to a host. See description in model 0036 . ApplicationService A new subtype of software server for reusable business functions (such as microservices) has been added called ApplicationService . See description in model 0057 . ServerEndpoint The ServerEndpoint relationship can now connect to any ITInfrastructure elements, not just SoftwareServers . See description in model 0040 . OperatingPlatform The OperatingPlatform entity can now record the patch level of the operating system. There are also new types for describing the contents of an operating platform. See description in model 0030 . DeployedVirtualContainer The DeployedVirtualContainer relationship has been deprecated in favor of a more generic HostedHost relationship. See description in model 0035 . BoundedSchemaType, BoundedSchemaElementType, ArraySchemaType, SetSchemaType See description in model 0507 . Bug fixes and other updates For details, see the commit history in GitHub . Implementation status","title":"3.1 (expected September 2021)"},{"location":"release-notes/overview/","text":"Release Notes Overview \u00b6 The project aims to produce a new release about once a month. Each release includes new features and fixes to existing function. Backwards compatibility The team aims to provide complete backward compatibility for all components that are officially released 1 . If backwards compatible changes are not possible, it will be called out explicitly in the release notes with an explanation on how to upgrade. Each release will also upgrade the level of its dependencies to ensure Egeria is running with all the latest security patches. We therefore recommend that you keep moving forward with us to get the best Egeria experience possible. Feedback One way you can help us is to feedback on your experiences, both good and bad. We would love to hear from you! If you discover an issue in the release you are using, we recommend first upgrading to the latest available release. If this does not resolve the problem, please raise a new GitHub issue . You can also follow our discussions by joining us on Slack . Our master branch is currently taking code for all future releases. Many of the features are large and the teams integrate code for partial function as soon as it is stable and has no impact on released function. So you will see support for much more function than is officially released. This way you can monitor and feedback on future items as they are developed. The roadmap for Egeria describes the end vision for Egeria and our current status. To understand more about what it means to have \"released function\" see Egeria content status . \u21a9","title":"Release Notes"},{"location":"release-notes/overview/#release-notes-overview","text":"The project aims to produce a new release about once a month. Each release includes new features and fixes to existing function. Backwards compatibility The team aims to provide complete backward compatibility for all components that are officially released 1 . If backwards compatible changes are not possible, it will be called out explicitly in the release notes with an explanation on how to upgrade. Each release will also upgrade the level of its dependencies to ensure Egeria is running with all the latest security patches. We therefore recommend that you keep moving forward with us to get the best Egeria experience possible. Feedback One way you can help us is to feedback on your experiences, both good and bad. We would love to hear from you! If you discover an issue in the release you are using, we recommend first upgrading to the latest available release. If this does not resolve the problem, please raise a new GitHub issue . You can also follow our discussions by joining us on Slack . Our master branch is currently taking code for all future releases. Many of the features are large and the teams integrate code for partial function as soon as it is stable and has no impact on released function. So you will see support for much more function than is officially released. This way you can monitor and feedback on future items as they are developed. The roadmap for Egeria describes the end vision for Egeria and our current status. To understand more about what it means to have \"released function\" see Egeria content status . \u21a9","title":"Release Notes Overview"},{"location":"release-notes/roadmap/","text":"Roadmap \u00b6 Egeria is a large project with many different activities adding content to the project. This page provides an overview of the aims of the project and a reflection of where we are today. Capability layers \u00b6 Egeria aims to deliver against 5 capability layers: Governance solutions \u00b6 Support the leadership team for a governance program providing the ability to create common definitions and monitor the success of the governance efforts across the enterprise. The implementation of a governance solution is focused mainly on the extension of the Egeria UI to support additional roles and functions. They make use of the services provided by the developer platform and may exploit additional content, utilities and connector implementations from the integration platform. Education \u00b6 Provides educational resources for different personas and starting points. Egeria's education aims to broaden the knowledge of people who need to work with digital resources about metadata, governance practices and the use of Egeria. Since there are many types of professionals involved with different skill levels, there are different choices: The Egeria dojo is a deep dive into the Egeria code and community. It is aimed at individuals who wish to become contributors. The hands-on labs provide practical experiences in running the Egeria code and using the different services. It is based around the Coco Pharmaceuticals use case and is organized by persona so you can target your learning to your interests. The guidance on governance provides governance best practices and training using the same Coco Pharmaceuticals use cases as the hands-on labs. They aim to guide a team that is setting up or revising their governance program through common governance tasks. They link to the Egeria code samples and hands-on labs to show how these best practices could be implemented using Egeria. The edX courses are a new idea to provide a full curriculum and certification for governance professionals and architects. It is in the early phases of design. Integration platform \u00b6 Supports integration of popular technologies by installing and configuring Egeria. Minimal coding is still required around unusual and home-grown tools and technologies. Ecosystem UIs: server and platform configuration, ecosystem monitoring, type explorer and repository explorer. Utilities and converters: support for different standard formats to load industry standard definitions, models, glossaries, and other content packs built on industry standard definitions, models, glossaries and other content packs. Examples include JSON-LD, OWL/RDF, XML, ... Pre-canned connectors to third party technologies: popular metadata repositories, databases, data formats and platforms; data movement engines, data virtualization engines, dev ops tools, analytics/AI tools, data catalogs, MDM and user directories, CMDBs, SDLC tools, ... Conformance test suite : Supports the testing of third party connectors. Each type of connector or service is supported by its own test workbench. Developer platform \u00b6 Provides frameworks, APIs, and hosting platforms for building an integrated metadata and distributed governance solutions. The developer platform contains the core Egeria implementation and provides support for integrating third party technology into the open metadata ecosystem and extending Egeria to run in different environments or to use different infrastructure services. Its use is described in the developer's guide . Open Metadata and Governance ( OMAG ) registered services are dynamically loaded in the OMAG Server Platform. This means they can be added and removed as needed to create a customized platform. This may include registered services written by the Egeria community and supplied by third parties. The access services provide provide specialist APIs / events for different types of tools. They work with the pre-defined open metadata types and use the repository services to access metadata. Engine services provide the services that host a specific type of governance engine. The governance engines collectively provide active governance to the assets and their associated metadata. Integration services each provide a specialized API to integration connectors. These are hosted in an integration daemon . The purpose of the integration services is to simplify the implementation and management of connectors that integrate metadata exchange with third party technologies. View services provide the services used by UIs. They are typically fine-grained services and they run in the view server . The use of the separate server (and server platform) enables an extra firewall to be erected between the view servers and the metadata servers and governance servers, hiding the internal systems from end users. The open metadata types provide common definitions for the different types of metadata needed by an organization. The open metadata type system is extendable; however, by providing a comprehensive starter set, and encouraging tools to use them, Egeria ensures metadata can be seamlessly shared amongst them. The OMAG Server Platform provides a multi-tenant runtime platform for OMAG Servers . Each OMAG Server hosts the connectors along with the Egeria services to integrate third party technology. The server chassis uses Spring Boot to provide the web server and REST API support for the platform. The administration services supports configuring and operating the OMAG Platform and Servers. Details of how to use the admin services are provided in the administration guide The platform services provide the means to query the OMAG Servers and services running on an OMAG Server Platform. The multi-tenancy management module supports multiple OMAG Servers running on an OMAG Server Platform. The repository services provide the basic ability to share metadata between metadata repositories. The metadata repositories are organized into open metadata repository cohorts . These cohorts define the scope of the metadata sharing and ensure metadata is available to all consumers within the cohort. The metadata security module provides customizable authorization checks for calls to the OMAG Server Platform, OMAG Server and the open metadata instances themselves. A governance server makes use of open metadata to actively manage an aspect of the digital landscape. The governance server services each provide the principle subsystem of a type of governance server . The generic handlers provide support for the type specific maintenance and retrieval of metadata that follows the open metadata types . This includes managing visibility of metadata through the Governance Zones , calls to Open Metadata Security and metadata management using templates . The open metadata frameworks define the interfaces implemented by components that \"plug-in\" to Egeria, either to integrate calls to third party technology or extend the function of Egeria. The frameworks are as follows: Open Connector Framework ( OCF ) - base framework for all types of plug-in components called connectors. Open Discovery Framework ( ODF ) - specialized connectors called discovery services that support automated metadata discovery, Governance Action Framework ( GAF ) - specialized connectors for the triage and remediation of issues found in the digital landscape. Audit Log Framework ( ALF ) - extensions for all types of connectors to enable natural language diagnostics such as exceptions and audit log messages. Deployment resources \u00b6 Aim to simplify the process of deploying the OMAG Server Platform and its connectors into an operational environment. The Egeria docker image is built daily and pushed to DockerHub. It contains an OMAG Server Platform. You can download it and use it in your own container environments. The Kubernetes Helm charts make use of the docker image to create a rich Egeria deployment used in the hands-on labs . The Kubernetes operators are in development. They will provide an easy way to control an Egeria deployment running on Kubernetes. Understanding the roadmap \u00b6 Current status \u00b6 Following is an overview of the current status of the functions in Egeria today: Green means that there is function that is either released or in technical preview . Orange means there is work in progress. Red means it is planned but not started. This chart is being updated with each release. As you can see, some progress has been made on all layers. However, since they do build on one another, most of the early work has been focused on establishing the frameworks, connector APIs and other services to provide the developer platform. The developer platform provides the libraries and interfaces to build connectors to integrate third party tools along with the runtime to host these connectors and manage the metadata exchange. Today we have a robust OMAG Server Platform and the ability to configure OMAG Servers that host specific types of connectors to third party tools. The initial focus was to enable third party metadata servers to connect together in the peer-to-peer open metadata repository cohort. This capability is delivered along with two repository connectors for the following third party connectors: IBM Information Governance Catalog ( IGC ) Apache Atlas History \u00b6 Through 2020, our focus shifted to the integration platform as we added connector implementations for popular third party technologies and standards (see connector catalog ) and built out the ecosystem user interface (UI) that enables an organization to: configure OMAG Servers on OMAG Server Platforms visualize the open metadata types through the type explorer (TEX) visualize open metadata instances in a single repository or across the open metadata repository cohorts that a server is connected to. visualize to cohort and query the operational status of the OMAG Servers and services operating in the open metadata ecosystem configure OMAG Servers and deploy them to OMAG Server Platforms The ecosystem UI makes calls to specialized REST services supported by a type of OMAG Server called the view server . The view server is new for 2020 and enables the REST APIs to the UIs to be deployed in a DMZ and the metadata servers to be behind an additional firewall. It also takes much of the load for supporting end users off of the metadata servers. In 2020 support for a new type of OMAG Server called the integration daemon was also added. This server supports integration services that can host integration connectors dedicated to exchanging metadata with specific third party technologies. Plans \u00b6 2021 has a focus on governing metadata. There is a new OMAG Server called the engine host that runs metadata discovery engines and governance engines. These are supported by new access services for governance. Support for the governance solutions naturally follows along, building on the two lower levels. The governance solutions themselves complement specific metadata and governance solutions available in the market today. Egeria is focused on filling in the gaps to support individuals that are setting up and running an open metadata ecosystem and wish to take advantage of the enterprise perspective it brings. The first solution is Historical Lineage Exploration . This was made available as a tech preview in late 2020. This provides a user interface for finding assets and viewing their lineage along with a dedicated governance server called the open lineage server . Next will be the Subject Area Management solution closely followed by the others in 2021 and beyond.","title":"Roadmap"},{"location":"release-notes/roadmap/#roadmap","text":"Egeria is a large project with many different activities adding content to the project. This page provides an overview of the aims of the project and a reflection of where we are today.","title":"Roadmap"},{"location":"release-notes/roadmap/#capability-layers","text":"Egeria aims to deliver against 5 capability layers:","title":"Capability layers"},{"location":"release-notes/roadmap/#governance-solutions","text":"Support the leadership team for a governance program providing the ability to create common definitions and monitor the success of the governance efforts across the enterprise. The implementation of a governance solution is focused mainly on the extension of the Egeria UI to support additional roles and functions. They make use of the services provided by the developer platform and may exploit additional content, utilities and connector implementations from the integration platform.","title":"Governance solutions"},{"location":"release-notes/roadmap/#education","text":"Provides educational resources for different personas and starting points. Egeria's education aims to broaden the knowledge of people who need to work with digital resources about metadata, governance practices and the use of Egeria. Since there are many types of professionals involved with different skill levels, there are different choices: The Egeria dojo is a deep dive into the Egeria code and community. It is aimed at individuals who wish to become contributors. The hands-on labs provide practical experiences in running the Egeria code and using the different services. It is based around the Coco Pharmaceuticals use case and is organized by persona so you can target your learning to your interests. The guidance on governance provides governance best practices and training using the same Coco Pharmaceuticals use cases as the hands-on labs. They aim to guide a team that is setting up or revising their governance program through common governance tasks. They link to the Egeria code samples and hands-on labs to show how these best practices could be implemented using Egeria. The edX courses are a new idea to provide a full curriculum and certification for governance professionals and architects. It is in the early phases of design.","title":"Education"},{"location":"release-notes/roadmap/#integration-platform","text":"Supports integration of popular technologies by installing and configuring Egeria. Minimal coding is still required around unusual and home-grown tools and technologies. Ecosystem UIs: server and platform configuration, ecosystem monitoring, type explorer and repository explorer. Utilities and converters: support for different standard formats to load industry standard definitions, models, glossaries, and other content packs built on industry standard definitions, models, glossaries and other content packs. Examples include JSON-LD, OWL/RDF, XML, ... Pre-canned connectors to third party technologies: popular metadata repositories, databases, data formats and platforms; data movement engines, data virtualization engines, dev ops tools, analytics/AI tools, data catalogs, MDM and user directories, CMDBs, SDLC tools, ... Conformance test suite : Supports the testing of third party connectors. Each type of connector or service is supported by its own test workbench.","title":"Integration platform"},{"location":"release-notes/roadmap/#developer-platform","text":"Provides frameworks, APIs, and hosting platforms for building an integrated metadata and distributed governance solutions. The developer platform contains the core Egeria implementation and provides support for integrating third party technology into the open metadata ecosystem and extending Egeria to run in different environments or to use different infrastructure services. Its use is described in the developer's guide . Open Metadata and Governance ( OMAG ) registered services are dynamically loaded in the OMAG Server Platform. This means they can be added and removed as needed to create a customized platform. This may include registered services written by the Egeria community and supplied by third parties. The access services provide provide specialist APIs / events for different types of tools. They work with the pre-defined open metadata types and use the repository services to access metadata. Engine services provide the services that host a specific type of governance engine. The governance engines collectively provide active governance to the assets and their associated metadata. Integration services each provide a specialized API to integration connectors. These are hosted in an integration daemon . The purpose of the integration services is to simplify the implementation and management of connectors that integrate metadata exchange with third party technologies. View services provide the services used by UIs. They are typically fine-grained services and they run in the view server . The use of the separate server (and server platform) enables an extra firewall to be erected between the view servers and the metadata servers and governance servers, hiding the internal systems from end users. The open metadata types provide common definitions for the different types of metadata needed by an organization. The open metadata type system is extendable; however, by providing a comprehensive starter set, and encouraging tools to use them, Egeria ensures metadata can be seamlessly shared amongst them. The OMAG Server Platform provides a multi-tenant runtime platform for OMAG Servers . Each OMAG Server hosts the connectors along with the Egeria services to integrate third party technology. The server chassis uses Spring Boot to provide the web server and REST API support for the platform. The administration services supports configuring and operating the OMAG Platform and Servers. Details of how to use the admin services are provided in the administration guide The platform services provide the means to query the OMAG Servers and services running on an OMAG Server Platform. The multi-tenancy management module supports multiple OMAG Servers running on an OMAG Server Platform. The repository services provide the basic ability to share metadata between metadata repositories. The metadata repositories are organized into open metadata repository cohorts . These cohorts define the scope of the metadata sharing and ensure metadata is available to all consumers within the cohort. The metadata security module provides customizable authorization checks for calls to the OMAG Server Platform, OMAG Server and the open metadata instances themselves. A governance server makes use of open metadata to actively manage an aspect of the digital landscape. The governance server services each provide the principle subsystem of a type of governance server . The generic handlers provide support for the type specific maintenance and retrieval of metadata that follows the open metadata types . This includes managing visibility of metadata through the Governance Zones , calls to Open Metadata Security and metadata management using templates . The open metadata frameworks define the interfaces implemented by components that \"plug-in\" to Egeria, either to integrate calls to third party technology or extend the function of Egeria. The frameworks are as follows: Open Connector Framework ( OCF ) - base framework for all types of plug-in components called connectors. Open Discovery Framework ( ODF ) - specialized connectors called discovery services that support automated metadata discovery, Governance Action Framework ( GAF ) - specialized connectors for the triage and remediation of issues found in the digital landscape. Audit Log Framework ( ALF ) - extensions for all types of connectors to enable natural language diagnostics such as exceptions and audit log messages.","title":"Developer platform"},{"location":"release-notes/roadmap/#deployment-resources","text":"Aim to simplify the process of deploying the OMAG Server Platform and its connectors into an operational environment. The Egeria docker image is built daily and pushed to DockerHub. It contains an OMAG Server Platform. You can download it and use it in your own container environments. The Kubernetes Helm charts make use of the docker image to create a rich Egeria deployment used in the hands-on labs . The Kubernetes operators are in development. They will provide an easy way to control an Egeria deployment running on Kubernetes.","title":"Deployment resources"},{"location":"release-notes/roadmap/#understanding-the-roadmap","text":"","title":"Understanding the roadmap"},{"location":"release-notes/roadmap/#current-status","text":"Following is an overview of the current status of the functions in Egeria today: Green means that there is function that is either released or in technical preview . Orange means there is work in progress. Red means it is planned but not started. This chart is being updated with each release. As you can see, some progress has been made on all layers. However, since they do build on one another, most of the early work has been focused on establishing the frameworks, connector APIs and other services to provide the developer platform. The developer platform provides the libraries and interfaces to build connectors to integrate third party tools along with the runtime to host these connectors and manage the metadata exchange. Today we have a robust OMAG Server Platform and the ability to configure OMAG Servers that host specific types of connectors to third party tools. The initial focus was to enable third party metadata servers to connect together in the peer-to-peer open metadata repository cohort. This capability is delivered along with two repository connectors for the following third party connectors: IBM Information Governance Catalog ( IGC ) Apache Atlas","title":"Current status"},{"location":"release-notes/roadmap/#history","text":"Through 2020, our focus shifted to the integration platform as we added connector implementations for popular third party technologies and standards (see connector catalog ) and built out the ecosystem user interface (UI) that enables an organization to: configure OMAG Servers on OMAG Server Platforms visualize the open metadata types through the type explorer (TEX) visualize open metadata instances in a single repository or across the open metadata repository cohorts that a server is connected to. visualize to cohort and query the operational status of the OMAG Servers and services operating in the open metadata ecosystem configure OMAG Servers and deploy them to OMAG Server Platforms The ecosystem UI makes calls to specialized REST services supported by a type of OMAG Server called the view server . The view server is new for 2020 and enables the REST APIs to the UIs to be deployed in a DMZ and the metadata servers to be behind an additional firewall. It also takes much of the load for supporting end users off of the metadata servers. In 2020 support for a new type of OMAG Server called the integration daemon was also added. This server supports integration services that can host integration connectors dedicated to exchanging metadata with specific third party technologies.","title":"History"},{"location":"release-notes/roadmap/#plans","text":"2021 has a focus on governing metadata. There is a new OMAG Server called the engine host that runs metadata discovery engines and governance engines. These are supported by new access services for governance. Support for the governance solutions naturally follows along, building on the two lower levels. The governance solutions themselves complement specific metadata and governance solutions available in the market today. Egeria is focused on filling in the gaps to support individuals that are setting up and running an open metadata ecosystem and wish to take advantage of the enterprise perspective it brings. The first solution is Historical Lineage Exploration . This was made available as a tech preview in late 2020. This provides a user interface for finding assets and viewing their lineage along with a dedicated governance server called the open lineage server . Next will be the Subject Area Management solution closely followed by the others in 2021 and beyond.","title":"Plans"},{"location":"services/platform/","text":"Released This function is complete and can be used. The interfaces will be supported until the function is removed from the project via the deprecation process. There will be ongoing extensions to this function, but it will be done to ensure backward compatibility as far as possible. If there is a need to break backward compatibility, this will be discussed and reviewed in the community, with a documented timeline. Platform Services \u00b6 The platform services provide the APIs for querying the Open Metadata and Governance ( OMAG ) Server Platform and discovering information about the OMAG Servers that it is hosting. There are two parts to the platform services: The OMAG Server Platform Origin Service returns a platform identifier that indicates which version of the platform is running. The OMAG Server Platform Active Service returns information about the known and active servers running on the platform along with definitions of each type of Open Metadata and Governance ( OMAG ) services.","title":"Platform Services"},{"location":"services/platform/#platform-services","text":"The platform services provide the APIs for querying the Open Metadata and Governance ( OMAG ) Server Platform and discovering information about the OMAG Servers that it is hosting. There are two parts to the platform services: The OMAG Server Platform Origin Service returns a platform identifier that indicates which version of the platform is running. The OMAG Server Platform Active Service returns information about the known and active servers running on the platform along with definitions of each type of Open Metadata and Governance ( OMAG ) services.","title":"Platform Services"},{"location":"services/omas/","text":"Open Metadata Access Services ( OMAS ) \u00b6 The Open Metadata Access Services ( OMAS ) provide domain-specific services for data tools, engines and platforms to integrate with open metadata. The access services are as follows: OMAS Description analytics-modeling The Analytics Modeling OMAS configures and manages metadata for modeling analytics and reporting services. asset-catalog The Asset Catalog OMAS provides search and query capabilities for tools and applications to support an asset catalog function. It supports search requests for assets with specific characteristics and returns summaries of the matching assets, plus methods to allow drill-down into the details of a specific asset to related metadata. asset-consumer The Asset Consumer OMAS is designed for applications that are using OCF connectors to access data stores, APIs and functions such as analytics. The Asset Consumer OMAS provides a factory function for the connectors, the ability to retrieve all of the metadata about the asset and the ability to add feedback on the asset. asset-lineage The Asset Lineage OMAS listens to relevant lineage related events on the enterprise topic level and publishes these on the Asset Lineage OutTopic, combined with relevant context information on the described entities. These events are listened to by the open lineage services governance server. asset-manager The Asset Managner OMAS manages the exchange of metadata with third party metadata catalogs and asset managers. It is typically called by the Catalog Integrator OMIS to send and receive asset information, including schemas, profiles, policies and lineage information with a third party asset manager. Typical examples of asset managers include data catalogs that are managing metadata for a collection of data assets for a data-serving solution. asset-owner The Asset Owner OMAS provides services for an asset owner to curate metadata about their asset(s) and understand how these assets are being used and governed. community-profile The Community Profile OMAS supports the administration for a community and related user profiles. These communities are involved in reviewing and crowd-sourcing knowledge about the data assets and their use. data-engine The Data Engine OMAS provides APIs and events for a data movement/processing engine to record the changes it is making the the data landscape. This information forms a key part of asset lineage. data-manager The Data Manager OMAS provides an integration point to enable technologies that manage collections of data such as database servers, file systems, file managers and content managers to publish metadata to the metadata repositories about the changing structures and content stored in the data platform. It is typically called from the Database Integrator OMIS and Files Integrator OMIS integration services. data-privacy The Data Privacy OMAS supports a privacy officer as they manage data privacy in their organization. This includes managing privacy impact assessments and reviews of software services that use personal data as they move through their development, deployment and use. data-science The Data Science OMAS provides access to metadata for data assets, connections and projects, plus the ability to maintain metadata about data science notebooks and models and log activity during the analytics development process. It is designed for data science and analytics management tools. design-model The Design Model OMAS provides the ability to manage information from all types of design models. These models may come from tools or be part of a packaged standard. This content is useful for governance, system integration and software development. dev-ops The DevOps OMAS provides services for a DevOps pipeline to query and maintain metadata about systems, processes and software components that are being deployed into the information landscape. digital-architecture The Digital Architecture OMAS provides the ability to define information standards, definitions, solution blueprints and models for an organization. It is designed for architecture tools. It is able to support the definition and management of a digital service through concept to deployment. digital-service The Digital Service OMAS provides services for a managing the lifecycle of an Egeria Digital Service. discovery-engine The Discovery Engine OMAS provides an API for a discovery engine to access and store metadata from an open metadata repository (or open metadata repository cohort). governance-engine The Governance Engine OMAS provides APIs and events that retrieve and manage metadata for governance engines. Governance engines ensure that the infrastructure supporting the data landscape is operating according to the governance program. For example, the governance engine may be ensuring that individuals and servers only have access to the data they have been authorized to see. governance-program The Governance Program OMAS provides the ability to maintain a governance program in the open metadata repositories. It is designed for governance and CDO tools. it-infrastructure The IT Infrastructure OMAS provides support for the design and planning of the information infrastructure that supports the data assets. This includes the development of system blueprints that link down to the metadata about real infrastructure components. This metadata helps in the linkage between information governance metadata and IT infrastructure management (ITIL) metadata typically stored in a Configuration Management Database (CMDB). project-management The Project Management OMAS supports the metadata associated with projects and campaigns. These projects and campaigns may be for governance projects, or generic data use projects. security-manager The Security Manager OMAS provides the services to exchange security tags with access control and data protection technology services. It is called by the Security Integrator OMIS . security-officer The Security Officer OMAS provides the services to support the definition of roles and rules for managing the protection of metadata and assets, plus work with the audit logs captured by the open metadata and governance tools. It is typically used by the security, compliance and auditing teams. software-developer The Software Development OMAS provides access to metadata needed to build compliant APIs, data stores and related software components. stewardship-action The Stewardship Action OMAS provides services for managing exceptions discovered in the information landscape that need correcting. These exceptions may be quality errors, missing or outdated information, invalid licensing, job failures, and many more. The Stewardship Action OMAS also enables the review and triage of the exceptions, simple remediation and status reporting. subject-area The Subject Area OMAS is for tools that support subject matter experts who are defining glossaries, reference data and rules around data for a specific subject area, such as \"customer data\". It supports the development of a comprehensive definition of the subject area and the standards that support it. These definitions can then be folded into the Governance Program, and used by Asset Owner's to improve the findability and understandability of their assets by linking their asset's structure to relevant parts of the subject area definition. Using the OMAS 's \u00b6 The OMAS 's run in either a metadata access point or a metadata server . They can be configured and activated individually or as a complete set. The administration services provide the ability to configure, start and stop the access services. Each OMAS typically supports a REST API, a topic where it publishes notifications of interest to its users, and a topic where new metadata requests can be posted to the OMAS . It also has a Java client that provides access to its API and topics. This java client is embedded in the governance servers and view servers . They can also be downloaded and used independently with the Egeria Client Package .","title":"Open Metadata Access Services"},{"location":"services/omas/#open-metadata-access-services-omas","text":"The Open Metadata Access Services ( OMAS ) provide domain-specific services for data tools, engines and platforms to integrate with open metadata. The access services are as follows: OMAS Description analytics-modeling The Analytics Modeling OMAS configures and manages metadata for modeling analytics and reporting services. asset-catalog The Asset Catalog OMAS provides search and query capabilities for tools and applications to support an asset catalog function. It supports search requests for assets with specific characteristics and returns summaries of the matching assets, plus methods to allow drill-down into the details of a specific asset to related metadata. asset-consumer The Asset Consumer OMAS is designed for applications that are using OCF connectors to access data stores, APIs and functions such as analytics. The Asset Consumer OMAS provides a factory function for the connectors, the ability to retrieve all of the metadata about the asset and the ability to add feedback on the asset. asset-lineage The Asset Lineage OMAS listens to relevant lineage related events on the enterprise topic level and publishes these on the Asset Lineage OutTopic, combined with relevant context information on the described entities. These events are listened to by the open lineage services governance server. asset-manager The Asset Managner OMAS manages the exchange of metadata with third party metadata catalogs and asset managers. It is typically called by the Catalog Integrator OMIS to send and receive asset information, including schemas, profiles, policies and lineage information with a third party asset manager. Typical examples of asset managers include data catalogs that are managing metadata for a collection of data assets for a data-serving solution. asset-owner The Asset Owner OMAS provides services for an asset owner to curate metadata about their asset(s) and understand how these assets are being used and governed. community-profile The Community Profile OMAS supports the administration for a community and related user profiles. These communities are involved in reviewing and crowd-sourcing knowledge about the data assets and their use. data-engine The Data Engine OMAS provides APIs and events for a data movement/processing engine to record the changes it is making the the data landscape. This information forms a key part of asset lineage. data-manager The Data Manager OMAS provides an integration point to enable technologies that manage collections of data such as database servers, file systems, file managers and content managers to publish metadata to the metadata repositories about the changing structures and content stored in the data platform. It is typically called from the Database Integrator OMIS and Files Integrator OMIS integration services. data-privacy The Data Privacy OMAS supports a privacy officer as they manage data privacy in their organization. This includes managing privacy impact assessments and reviews of software services that use personal data as they move through their development, deployment and use. data-science The Data Science OMAS provides access to metadata for data assets, connections and projects, plus the ability to maintain metadata about data science notebooks and models and log activity during the analytics development process. It is designed for data science and analytics management tools. design-model The Design Model OMAS provides the ability to manage information from all types of design models. These models may come from tools or be part of a packaged standard. This content is useful for governance, system integration and software development. dev-ops The DevOps OMAS provides services for a DevOps pipeline to query and maintain metadata about systems, processes and software components that are being deployed into the information landscape. digital-architecture The Digital Architecture OMAS provides the ability to define information standards, definitions, solution blueprints and models for an organization. It is designed for architecture tools. It is able to support the definition and management of a digital service through concept to deployment. digital-service The Digital Service OMAS provides services for a managing the lifecycle of an Egeria Digital Service. discovery-engine The Discovery Engine OMAS provides an API for a discovery engine to access and store metadata from an open metadata repository (or open metadata repository cohort). governance-engine The Governance Engine OMAS provides APIs and events that retrieve and manage metadata for governance engines. Governance engines ensure that the infrastructure supporting the data landscape is operating according to the governance program. For example, the governance engine may be ensuring that individuals and servers only have access to the data they have been authorized to see. governance-program The Governance Program OMAS provides the ability to maintain a governance program in the open metadata repositories. It is designed for governance and CDO tools. it-infrastructure The IT Infrastructure OMAS provides support for the design and planning of the information infrastructure that supports the data assets. This includes the development of system blueprints that link down to the metadata about real infrastructure components. This metadata helps in the linkage between information governance metadata and IT infrastructure management (ITIL) metadata typically stored in a Configuration Management Database (CMDB). project-management The Project Management OMAS supports the metadata associated with projects and campaigns. These projects and campaigns may be for governance projects, or generic data use projects. security-manager The Security Manager OMAS provides the services to exchange security tags with access control and data protection technology services. It is called by the Security Integrator OMIS . security-officer The Security Officer OMAS provides the services to support the definition of roles and rules for managing the protection of metadata and assets, plus work with the audit logs captured by the open metadata and governance tools. It is typically used by the security, compliance and auditing teams. software-developer The Software Development OMAS provides access to metadata needed to build compliant APIs, data stores and related software components. stewardship-action The Stewardship Action OMAS provides services for managing exceptions discovered in the information landscape that need correcting. These exceptions may be quality errors, missing or outdated information, invalid licensing, job failures, and many more. The Stewardship Action OMAS also enables the review and triage of the exceptions, simple remediation and status reporting. subject-area The Subject Area OMAS is for tools that support subject matter experts who are defining glossaries, reference data and rules around data for a specific subject area, such as \"customer data\". It supports the development of a comprehensive definition of the subject area and the standards that support it. These definitions can then be folded into the Governance Program, and used by Asset Owner's to improve the findability and understandability of their assets by linking their asset's structure to relevant parts of the subject area definition.","title":"Open Metadata Access Services (OMAS)"},{"location":"services/omas/#using-the-omass","text":"The OMAS 's run in either a metadata access point or a metadata server . They can be configured and activated individually or as a complete set. The administration services provide the ability to configure, start and stop the access services. Each OMAS typically supports a REST API, a topic where it publishes notifications of interest to its users, and a topic where new metadata requests can be posted to the OMAS . It also has a Java client that provides access to its API and topics. This java client is embedded in the governance servers and view servers . They can also be downloaded and used independently with the Egeria Client Package .","title":"Using the OMAS's"},{"location":"services/omas/client-server/","text":"Client-server concepts for the Open Metadata Access Services ( OMAS ) \u00b6 In Topic \u00b6 The InTopic of an Open Metadata Access Service is the location where the OMAS will receive incoming events from its consumers. Out Topic \u00b6 The OutTopic of an Open Metadata Access Service is the location where the OMAS will send outgoing events to its consumers.","title":"Client server"},{"location":"services/omas/client-server/#client-server-concepts-for-the-open-metadata-access-services-omas","text":"","title":"Client-server concepts for the Open Metadata Access Services (OMAS)"},{"location":"services/omas/client-server/#in-topic","text":"The InTopic of an Open Metadata Access Service is the location where the OMAS will receive incoming events from its consumers.","title":"In Topic"},{"location":"services/omas/client-server/#out-topic","text":"The OutTopic of an Open Metadata Access Service is the location where the OMAS will send outgoing events to its consumers.","title":"Out Topic"},{"location":"services/omas/analytics-modeling/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Analytics Modeling Open Metadata Access Service ( OMAS ) \u00b6 The Analytics Modeling OMAS configures and manages metadata for modeling analytics and reporting services.","title":"Analytics Modeling OMAS"},{"location":"services/omas/analytics-modeling/#analytics-modeling-open-metadata-access-service-omas","text":"The Analytics Modeling OMAS configures and manages metadata for modeling analytics and reporting services.","title":"Analytics Modeling Open Metadata Access Service (OMAS)"},{"location":"services/omas/asset-catalog/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Asset Catalog Open Metadata Access Service ( OMAS ) \u00b6 The Asset Catalog OMAS provides services to search for data assets including: data stores event feeds APIs data sets The search locates assets based on the content of the Asset metadata itself and the metadata that links to it. This includes: glossary terms schema elements assets The Asset Catalog REST API supports: the retrieval of assets based on unique identifiers the retrieval of asset's relationships and classifications the retrieval of assets based on known classification or relationship to query for related assets and to retrieve an asset neighborhood The module structure for the Asset Catalog OMAS is as follows: asset-catalog-client supports the client library. asset-catalog-api supports the common Java classes that are used both by the client and the server. asset-catalog-server supports the server side implementation of the access service. This includes the interaction with the administration services for registration, configuration, initialization and termination of the access service. interaction with the repository services to work with open metadata from the cohort . support for the access service's API and its related event management. asset-catalog-spring supports the REST API using the Spring libraries. Search solution \u00b6 The search will return Assets, Glossary Terms and Schema Elements that match the search criteria. As the asset search is to be performed against on one or more repositories a search engine will be used. The search will be performed using the existing properties of the asset, glossary terms and/or schema elements. Indexing will be performed by the Asset Catalog OMAS according to supported zones. The search result will contain: guid, name (name or displayName), description, qualifiedName, classifications, zoneMembership (the basic properties of the element). In order to get the full context of the element, a second call is performed. At this step, the specific relationships are traverse for getting the connection to the asset and to get the schema type that is behind the given asset. This call is using the asset global identifier and the asset type. Figure 1:Integration of search engine Other Services \u00b6 Asset Catalog OMAS provides services to fetch the asset * classifications * relationships * specific entities that connects two assets * relationships between two known entities * related assets","title":"Asset Catalog OMAS"},{"location":"services/omas/asset-catalog/#asset-catalog-open-metadata-access-service-omas","text":"The Asset Catalog OMAS provides services to search for data assets including: data stores event feeds APIs data sets The search locates assets based on the content of the Asset metadata itself and the metadata that links to it. This includes: glossary terms schema elements assets The Asset Catalog REST API supports: the retrieval of assets based on unique identifiers the retrieval of asset's relationships and classifications the retrieval of assets based on known classification or relationship to query for related assets and to retrieve an asset neighborhood The module structure for the Asset Catalog OMAS is as follows: asset-catalog-client supports the client library. asset-catalog-api supports the common Java classes that are used both by the client and the server. asset-catalog-server supports the server side implementation of the access service. This includes the interaction with the administration services for registration, configuration, initialization and termination of the access service. interaction with the repository services to work with open metadata from the cohort . support for the access service's API and its related event management. asset-catalog-spring supports the REST API using the Spring libraries.","title":"Asset Catalog Open Metadata Access Service (OMAS)"},{"location":"services/omas/asset-catalog/#search-solution","text":"The search will return Assets, Glossary Terms and Schema Elements that match the search criteria. As the asset search is to be performed against on one or more repositories a search engine will be used. The search will be performed using the existing properties of the asset, glossary terms and/or schema elements. Indexing will be performed by the Asset Catalog OMAS according to supported zones. The search result will contain: guid, name (name or displayName), description, qualifiedName, classifications, zoneMembership (the basic properties of the element). In order to get the full context of the element, a second call is performed. At this step, the specific relationships are traverse for getting the connection to the asset and to get the schema type that is behind the given asset. This call is using the asset global identifier and the asset type. Figure 1:Integration of search engine","title":"Search solution"},{"location":"services/omas/asset-catalog/#other-services","text":"Asset Catalog OMAS provides services to fetch the asset * classifications * relationships * specific entities that connects two assets * relationships between two known entities * related assets","title":"Other Services"},{"location":"services/omas/asset-consumer/","text":"Released This function is complete and can be used. The interfaces will be supported until the function is removed from the project via the deprecation process. There will be ongoing extensions to this function, but it will be done to ensure backward compatibility as far as possible. If there is a need to break backward compatibility, this will be discussed and reviewed in the community, with a documented timeline. Asset Consumer Open Metadata Access Service ( OMAS ) \u00b6 The Asset Consumer OMAS provides services to an individual who wants to work with assets such as: data stores, data sets and data feeds reports APIs functions such as analytical services It supports: the retrieval of connection objects from the open metadata repositories. A connection object is used to create a connector to an asset. the creation of a connector based on the properties in a connection object. the retrieval of properties about an asset. These properties are called the connected asset properties . the adding of feedback (comments, ratings and likes) to an asset. the attachment of informal tags to an asset. the adding of an audit log record for an asset. the publishing of notifications about assets on Asset Consumer OMAS 's out topic . Adding feedback through the Asset Consumer OMAS results in Karma Points being awarded to the individual. These are maintained in the individual's profile. A karma point is awarded for each contribution of feedback through the API. (The awarding of Karma points is managed by the Community Profile OMAS .) The connectors returned by the Asset Consumer OMAS are Open Connector Framework ( OCF ) connectors. The caller can use the connector to access the contents of the asset itself and the properties about the asset it is accessing. This service is provided by the OCF Metadata Management Common Service . Digging Deeper \u00b6 User Documentation Design Documentation","title":"Asset Consumer OMAS"},{"location":"services/omas/asset-consumer/#asset-consumer-open-metadata-access-service-omas","text":"The Asset Consumer OMAS provides services to an individual who wants to work with assets such as: data stores, data sets and data feeds reports APIs functions such as analytical services It supports: the retrieval of connection objects from the open metadata repositories. A connection object is used to create a connector to an asset. the creation of a connector based on the properties in a connection object. the retrieval of properties about an asset. These properties are called the connected asset properties . the adding of feedback (comments, ratings and likes) to an asset. the attachment of informal tags to an asset. the adding of an audit log record for an asset. the publishing of notifications about assets on Asset Consumer OMAS 's out topic . Adding feedback through the Asset Consumer OMAS results in Karma Points being awarded to the individual. These are maintained in the individual's profile. A karma point is awarded for each contribution of feedback through the API. (The awarding of Karma points is managed by the Community Profile OMAS .) The connectors returned by the Asset Consumer OMAS are Open Connector Framework ( OCF ) connectors. The caller can use the connector to access the contents of the asset itself and the properties about the asset it is accessing. This service is provided by the OCF Metadata Management Common Service .","title":"Asset Consumer Open Metadata Access Service (OMAS)"},{"location":"services/omas/asset-consumer/#digging-deeper","text":"User Documentation Design Documentation","title":"Digging Deeper"},{"location":"services/omas/asset-consumer/concepts/","text":"Asset Consumer OMAS Concepts \u00b6 Assets Asset Log Messages Connected Asset Properties Connectors Connections Glossary Terms Feedback on Assets Tagging Assets License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/asset-consumer/concepts/#asset-consumer-omas-concepts","text":"Assets Asset Log Messages Connected Asset Properties Connectors Connections Glossary Terms Feedback on Assets Tagging Assets License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Asset Consumer OMAS Concepts"},{"location":"services/omas/asset-consumer/concepts/asset-log-message/","text":"Asset log message \u00b6 The asset log message is a type of log record that can be added to the local server's audit log . It is used to record that a specific action has been taken on an asset. The log record contains the following information: userId - userId of user making request. requestType - unique id for the asset. connectorInstanceId - (optional) id of connector in use (if any). connectionName - (optional) name of the connection (extracted from the connector). connectorType - (optional) type of connector in use (if any). contextId - (optional) function name, or processId of the activity that the caller is performing. message - log record content. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Asset log message"},{"location":"services/omas/asset-consumer/concepts/asset-log-message/#asset-log-message","text":"The asset log message is a type of log record that can be added to the local server's audit log . It is used to record that a specific action has been taken on an asset. The log record contains the following information: userId - userId of user making request. requestType - unique id for the asset. connectorInstanceId - (optional) id of connector in use (if any). connectionName - (optional) name of the connection (extracted from the connector). connectorType - (optional) type of connector in use (if any). contextId - (optional) function name, or processId of the activity that the caller is performing. message - log record content. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Asset log message"},{"location":"services/omas/asset-consumer/design/","text":"Asset Consumer OMAS Design \u00b6 The module structure for the Asset Consumer OMAS is as follows: asset-consumer-client supports the client library. asset-consumer-api supports the common Java classes that are used both by the client and the server. asset-consumer-server supports in implementation of the access service and its related event management. asset-consumer-spring supports the REST API using the Spring libraries. The Asset Consumer OMAS interfaces are heavily influenced by the Open Connector Framework (OCF) . It also uses the client and server side support provided by the ocf-metadata-management common services. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/asset-consumer/design/#asset-consumer-omas-design","text":"The module structure for the Asset Consumer OMAS is as follows: asset-consumer-client supports the client library. asset-consumer-api supports the common Java classes that are used both by the client and the server. asset-consumer-server supports in implementation of the access service and its related event management. asset-consumer-spring supports the REST API using the Spring libraries. The Asset Consumer OMAS interfaces are heavily influenced by the Open Connector Framework (OCF) . It also uses the client and server side support provided by the ocf-metadata-management common services. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Asset Consumer OMAS Design"},{"location":"services/omas/asset-consumer/scenarios/","text":"Asset Consumer OMAS Scenarios \u00b6 This is the list of documented scenarios: Working with Connectors (overview) Creating a connector Locating the connected asset Retrieving asset properties Logging audit messages about an event Adding Feedback to an Asset Looking up the meanings of terms assigned to an asset Adding descriptive tags to an asset License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/asset-consumer/scenarios/#asset-consumer-omas-scenarios","text":"This is the list of documented scenarios: Working with Connectors (overview) Creating a connector Locating the connected asset Retrieving asset properties Logging audit messages about an event Adding Feedback to an Asset Looking up the meanings of terms assigned to an asset Adding descriptive tags to an asset License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Asset Consumer OMAS Scenarios"},{"location":"services/omas/asset-consumer/scenarios/adding-feedback-to-an-asset/","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding feedback to an asset"},{"location":"services/omas/asset-consumer/scenarios/creating-a-connector/","text":"Creating a connector for application use \u00b6 The Asset Consumer OMAS supports a REST API to extract metadata from the open metadata repositories linked to the same open metadata cohort as the Asset Consumer OMAS. It also has a Java client that provides an equivalent interface to the REST API plus connector factory methods supported by an embedded Connector Broker. The Connector Broker is an Open Connector Framework (OCF) component that is able to create and configure instances of compliant connectors. It is passed a Connection object which has all of the properties needed to create the connector. The Asset Consumer OMAS java client extracts the Connection object from the open metadata repositories and then calls the Connector Broker. The code sample below creates the Asset Consumer OMAS Client. It is passed the server name and platform root URL for the metadata server that will supply the connection object. AssetConsumer client = new AssetConsumer(serverName, serverURLRoot); The next code samples show how to create the connector. There are three approaches: Supplying a hard-coded Connection object. Supplying the unique identifier (guid) of the Connection object in the metadata repositories that Asset Consumer OMAS has access to. Supplying the unique identifier (guid) of an Asset. The connector is created using the Connection linked to the asset. In this first example, the connector is created from the hard-coded connection. /** * This method creates a connector using a hard coded connection object. This connector will be able * to retrieve the data from the file, but it will not be able to retrieve the metadata about * the file. * * @return connector to requested file */ private CSVFileStoreConnector getConnectorUsingHardCodedConnection() { CSVFileStoreConnector connector = null; try { AssetConsumer client = new AssetConsumer(serverName, serverURLRoot); connector = (CSVFileStoreConnector)client.getConnectorByConnection(clientUserId, getHardCodedConnection(fileName)); } catch (Exception error) { System.out.println(\"The connector can not be created with Asset Consumer OMAS.\"); } return connector; } /** * This method creates a connection. The connection object provides the OCF with the properties to create the * connector and the properties needed by the connector to access the asset. * * The Connection object includes a Connector Type object. This defines the type of connector to create. * The Connection object also includes an Endpoint object. This is used by the connector to locate and connect * to the Asset. * * @param fileName name of the file to connect to * @return connection object */ private Connection getHardCodedConnection(String fileName) { final String endpointGUID = \"8bf8f5fa-b5d8-40e1-a00e-e4a0c59fd6c0\"; final String connectorTypeGUID = \"2e1556a3-908f-4303-812d-d81b48b19bab\"; final String connectionGUID = \"b9af734f-f005-4085-9975-bf46c67a099a\"; final String endpointDescription = \"File name.\"; String endpointName = \"CSVFileStore.Endpoint.\" + fileName; Endpoint endpoint = new Endpoint(); endpoint.setType(Endpoint.getEndpointType()); endpoint.setGUID(endpointGUID); endpoint.setQualifiedName(endpointName); endpoint.setDisplayName(endpointName); endpoint.setDescription(endpointDescription); endpoint.setAddress(fileName); final String connectorTypeDescription = \"CSVFileStore connector type.\"; final String connectorTypeJavaClassName = CSVFileStoreProvider.class.getName(); String connectorTypeName = \"CSVFileStore.ConnectorType.Test\"; ConnectorType connectorType = new ConnectorType(); connectorType.setType(ConnectorType.getConnectorTypeType()); connectorType.setGUID(connectorTypeGUID); connectorType.setQualifiedName(connectorTypeName); connectorType.setDisplayName(connectorTypeName); connectorType.setDescription(connectorTypeDescription); connectorType.setConnectorProviderClassName(connectorTypeJavaClassName); final String connectionDescription = \"CSVFileStore connection.\"; String connectionName = \"CSVFileStore.Connection.Test\"; Connection connection = new Connection(); connection.setType(Connection.getConnectionType()); connection.setGUID(connectionGUID); connection.setQualifiedName(connectionName); connection.setDisplayName(connectionName); connection.setDescription(connectionDescription); connection.setEndpoint(endpoint); connection.setConnectorType(connectorType); return connection; } The next example, uses the Connection guid: try { /* * Return a connector for the connection. */ return (CSVFileStoreConnector) client.getConnectorByGUID(clientUserId, connectionGUID); } catch (Exception error) { System.out.println(\"Unable to create connector for connection: \" + connectionGUID); } The final example uses the Asset guid: try { /* * Return a connector for the asset. */ return (CSVFileStoreConnector) client.getConnectorForAsset(clientUserId, assetGUID); } catch (Exception error) { System.out.println(\"Unable to create connector for asset: \" + assetGUID); } Why use the Asset Consumer OMAS java client rather than the ConnectorBroker? \u00b6 Each connector has a method call called getConnectedAssetProperties . This returns the metadata known about the asset that the connector is accessing. When you create a connector using the Asset Consumer OMAS java client, and the Connection used is linked to an asset then getConnectedAssetProperties returns all of the metadata known about the asset. ConnectedAssetProperties assetProperties = connector.getConnectedAssetProperties(clientUserId); if (assetProperties != null) { AssetUniverse assetUniverse = assetProperties.getAssetUniverse(); if (assetUniverse != null) { System.out.println(\"Type Name: \" + assetUniverse.getAssetTypeName()); System.out.println(\"Qualified Name: \" + assetUniverse.getQualifiedName()); } else { System.out.println(assetProperties.toString()); } } else { System.out.println(\"No asset properties ...\"); } Return to Asset Consumer OMAS User Guide License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Creating a connector"},{"location":"services/omas/asset-consumer/scenarios/creating-a-connector/#creating-a-connector-for-application-use","text":"The Asset Consumer OMAS supports a REST API to extract metadata from the open metadata repositories linked to the same open metadata cohort as the Asset Consumer OMAS. It also has a Java client that provides an equivalent interface to the REST API plus connector factory methods supported by an embedded Connector Broker. The Connector Broker is an Open Connector Framework (OCF) component that is able to create and configure instances of compliant connectors. It is passed a Connection object which has all of the properties needed to create the connector. The Asset Consumer OMAS java client extracts the Connection object from the open metadata repositories and then calls the Connector Broker. The code sample below creates the Asset Consumer OMAS Client. It is passed the server name and platform root URL for the metadata server that will supply the connection object. AssetConsumer client = new AssetConsumer(serverName, serverURLRoot); The next code samples show how to create the connector. There are three approaches: Supplying a hard-coded Connection object. Supplying the unique identifier (guid) of the Connection object in the metadata repositories that Asset Consumer OMAS has access to. Supplying the unique identifier (guid) of an Asset. The connector is created using the Connection linked to the asset. In this first example, the connector is created from the hard-coded connection. /** * This method creates a connector using a hard coded connection object. This connector will be able * to retrieve the data from the file, but it will not be able to retrieve the metadata about * the file. * * @return connector to requested file */ private CSVFileStoreConnector getConnectorUsingHardCodedConnection() { CSVFileStoreConnector connector = null; try { AssetConsumer client = new AssetConsumer(serverName, serverURLRoot); connector = (CSVFileStoreConnector)client.getConnectorByConnection(clientUserId, getHardCodedConnection(fileName)); } catch (Exception error) { System.out.println(\"The connector can not be created with Asset Consumer OMAS.\"); } return connector; } /** * This method creates a connection. The connection object provides the OCF with the properties to create the * connector and the properties needed by the connector to access the asset. * * The Connection object includes a Connector Type object. This defines the type of connector to create. * The Connection object also includes an Endpoint object. This is used by the connector to locate and connect * to the Asset. * * @param fileName name of the file to connect to * @return connection object */ private Connection getHardCodedConnection(String fileName) { final String endpointGUID = \"8bf8f5fa-b5d8-40e1-a00e-e4a0c59fd6c0\"; final String connectorTypeGUID = \"2e1556a3-908f-4303-812d-d81b48b19bab\"; final String connectionGUID = \"b9af734f-f005-4085-9975-bf46c67a099a\"; final String endpointDescription = \"File name.\"; String endpointName = \"CSVFileStore.Endpoint.\" + fileName; Endpoint endpoint = new Endpoint(); endpoint.setType(Endpoint.getEndpointType()); endpoint.setGUID(endpointGUID); endpoint.setQualifiedName(endpointName); endpoint.setDisplayName(endpointName); endpoint.setDescription(endpointDescription); endpoint.setAddress(fileName); final String connectorTypeDescription = \"CSVFileStore connector type.\"; final String connectorTypeJavaClassName = CSVFileStoreProvider.class.getName(); String connectorTypeName = \"CSVFileStore.ConnectorType.Test\"; ConnectorType connectorType = new ConnectorType(); connectorType.setType(ConnectorType.getConnectorTypeType()); connectorType.setGUID(connectorTypeGUID); connectorType.setQualifiedName(connectorTypeName); connectorType.setDisplayName(connectorTypeName); connectorType.setDescription(connectorTypeDescription); connectorType.setConnectorProviderClassName(connectorTypeJavaClassName); final String connectionDescription = \"CSVFileStore connection.\"; String connectionName = \"CSVFileStore.Connection.Test\"; Connection connection = new Connection(); connection.setType(Connection.getConnectionType()); connection.setGUID(connectionGUID); connection.setQualifiedName(connectionName); connection.setDisplayName(connectionName); connection.setDescription(connectionDescription); connection.setEndpoint(endpoint); connection.setConnectorType(connectorType); return connection; } The next example, uses the Connection guid: try { /* * Return a connector for the connection. */ return (CSVFileStoreConnector) client.getConnectorByGUID(clientUserId, connectionGUID); } catch (Exception error) { System.out.println(\"Unable to create connector for connection: \" + connectionGUID); } The final example uses the Asset guid: try { /* * Return a connector for the asset. */ return (CSVFileStoreConnector) client.getConnectorForAsset(clientUserId, assetGUID); } catch (Exception error) { System.out.println(\"Unable to create connector for asset: \" + assetGUID); }","title":"Creating a connector for application use"},{"location":"services/omas/asset-consumer/scenarios/creating-a-connector/#why-use-the-asset-consumer-omas-java-client-rather-than-the-connectorbroker","text":"Each connector has a method call called getConnectedAssetProperties . This returns the metadata known about the asset that the connector is accessing. When you create a connector using the Asset Consumer OMAS java client, and the Connection used is linked to an asset then getConnectedAssetProperties returns all of the metadata known about the asset. ConnectedAssetProperties assetProperties = connector.getConnectedAssetProperties(clientUserId); if (assetProperties != null) { AssetUniverse assetUniverse = assetProperties.getAssetUniverse(); if (assetUniverse != null) { System.out.println(\"Type Name: \" + assetUniverse.getAssetTypeName()); System.out.println(\"Qualified Name: \" + assetUniverse.getQualifiedName()); } else { System.out.println(assetProperties.toString()); } } else { System.out.println(\"No asset properties ...\"); } Return to Asset Consumer OMAS User Guide License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Why use the Asset Consumer OMAS java client rather than the ConnectorBroker?"},{"location":"services/omas/asset-consumer/scenarios/locating-the-connected-asset/","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Locating the connected asset"},{"location":"services/omas/asset-consumer/scenarios/logging-messages-about-an-asset/","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Logging messages about an asset"},{"location":"services/omas/asset-consumer/scenarios/looking-up-meanings-of-terms/","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Looking up meanings of terms"},{"location":"services/omas/asset-consumer/scenarios/retrieving-asset-properties/","text":"Retrieving asset properties \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Retrieving asset properties"},{"location":"services/omas/asset-consumer/scenarios/retrieving-asset-properties/#retrieving-asset-properties","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Retrieving asset properties"},{"location":"services/omas/asset-consumer/scenarios/tagging-an-asset/","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Tagging an asset"},{"location":"services/omas/asset-consumer/scenarios/working-with-connectors/","text":"Working with connectors \u00b6 An open connector is a Java client to an Asset that implements the Connector interface defined in the Open Connector Framework (OCF) . It has 2 parts to its interface: The specialized interface to work with the specific contents of the asset. For example, if the connector was for data stored in a relational database, this interface would probably follow the Java Database Connectivity (JDBC) specification. The documentation for this interface is found with the specific connector. These are the connectors supported by ODPi Egeria with links to the documentation: basic-file-connector provides connector to read a file. It has no special knowledge of the format. avro-file-connector provides connector to read files that have an Apache Avro format. csv-file-connector provides connector to read files that have a CSV tabular format. data-folder-connector provides connector to read a data set that is made up of many files stored within a data folder. A generalized interface to extract all of the open metadata known about the asset. This is referred to as the connected asset properties . This interface is documented here . An application creates a connector using the Asset Consumer OMAS client . When an Asset is cataloged in the open metadata repository, there is a Connection object linked to it. This defines all of the properties required to create the connector. See Creating a connector for step by step instructions on creating connectors. Asset Consumer OMAS looks up the Connection object and calls the Connector Broker to create the connector. Once the connector is created, an application may use it to retrieve the content of the asset and the connected asset properties. When the application has finished with the connector, it should call disconnect() to release any resources that the connector may be holding. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Working with connectors"},{"location":"services/omas/asset-consumer/scenarios/working-with-connectors/#working-with-connectors","text":"An open connector is a Java client to an Asset that implements the Connector interface defined in the Open Connector Framework (OCF) . It has 2 parts to its interface: The specialized interface to work with the specific contents of the asset. For example, if the connector was for data stored in a relational database, this interface would probably follow the Java Database Connectivity (JDBC) specification. The documentation for this interface is found with the specific connector. These are the connectors supported by ODPi Egeria with links to the documentation: basic-file-connector provides connector to read a file. It has no special knowledge of the format. avro-file-connector provides connector to read files that have an Apache Avro format. csv-file-connector provides connector to read files that have a CSV tabular format. data-folder-connector provides connector to read a data set that is made up of many files stored within a data folder. A generalized interface to extract all of the open metadata known about the asset. This is referred to as the connected asset properties . This interface is documented here . An application creates a connector using the Asset Consumer OMAS client . When an Asset is cataloged in the open metadata repository, there is a Connection object linked to it. This defines all of the properties required to create the connector. See Creating a connector for step by step instructions on creating connectors. Asset Consumer OMAS looks up the Connection object and calls the Connector Broker to create the connector. Once the connector is created, an application may use it to retrieve the content of the asset and the connected asset properties. When the application has finished with the connector, it should call disconnect() to release any resources that the connector may be holding. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Working with connectors"},{"location":"services/omas/asset-consumer/user/","text":"Asset Consumer OMAS User Guide \u00b6 The Asset Consumer OMAS is designed for use by an application that is accessing data sources and services through connectors . These data sources and services are called Assets , hence the name of this OMAS is Asset Consumer . Typically the first action to take is to create the connector to get access to the asset content and its properties . Connectors are created from Connection objects. Connection objects can be created by the calling application, or stored in one of the open metadata repositories that are accessible to the Asset Consumer OMAS. Alternatively, if the consumer only needs access to the asset's properties, they can use the Asset Consumer OMAS to locate the identifier of the asset and then retrieve the asset properties . Within the asset properties are links to glossary terms. It is possible to look up the full description of a term to further understand the asset. There are also capabilities to log messages about the asset , add feedback to the asset in terms of likes, star ratings, reviews and comments, and add tags to the asset . Interface choices \u00b6 The Asset Consumer OMAS offers the following types of interface: Java client , REST API and Out Topic Events for receiving events about assets. Connectors are only available through the Java client. Configuration \u00b6 Details of how to configure the Asset Consumer OMAS can be found here License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/asset-consumer/user/#asset-consumer-omas-user-guide","text":"The Asset Consumer OMAS is designed for use by an application that is accessing data sources and services through connectors . These data sources and services are called Assets , hence the name of this OMAS is Asset Consumer . Typically the first action to take is to create the connector to get access to the asset content and its properties . Connectors are created from Connection objects. Connection objects can be created by the calling application, or stored in one of the open metadata repositories that are accessible to the Asset Consumer OMAS. Alternatively, if the consumer only needs access to the asset's properties, they can use the Asset Consumer OMAS to locate the identifier of the asset and then retrieve the asset properties . Within the asset properties are links to glossary terms. It is possible to look up the full description of a term to further understand the asset. There are also capabilities to log messages about the asset , add feedback to the asset in terms of likes, star ratings, reviews and comments, and add tags to the asset .","title":"Asset Consumer OMAS User Guide"},{"location":"services/omas/asset-consumer/user/#interface-choices","text":"The Asset Consumer OMAS offers the following types of interface: Java client , REST API and Out Topic Events for receiving events about assets. Connectors are only available through the Java client.","title":"Interface choices"},{"location":"services/omas/asset-consumer/user/#configuration","text":"Details of how to configure the Asset Consumer OMAS can be found here License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Configuration"},{"location":"services/omas/asset-lineage/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Asset Lineage Open Metadata Access Service ( OMAS ) \u00b6 The Asset Lineage OMAS provides services to query the lineage of business terms and data assets. This access service is used to build Vertical Lineage and Horizontal Lineage functionality. On the output topic, it publishes out events that contains full context of data assets and glossary terms involved in lineage. These events are consumed by the external tools that build the lineage graph in a specific format. Also, the Asset Lineage OMAS provides an endpoint that publishes the lineage events associated with the entities involved in lineage. Digging Deeper \u00b6 User Documentation Design Documentation","title":"Asset Lineage OMAS"},{"location":"services/omas/asset-lineage/#asset-lineage-open-metadata-access-service-omas","text":"The Asset Lineage OMAS provides services to query the lineage of business terms and data assets. This access service is used to build Vertical Lineage and Horizontal Lineage functionality. On the output topic, it publishes out events that contains full context of data assets and glossary terms involved in lineage. These events are consumed by the external tools that build the lineage graph in a specific format. Also, the Asset Lineage OMAS provides an endpoint that publishes the lineage events associated with the entities involved in lineage.","title":"Asset Lineage Open Metadata Access Service (OMAS)"},{"location":"services/omas/asset-lineage/#digging-deeper","text":"User Documentation Design Documentation","title":"Digging Deeper"},{"location":"services/omas/asset-lineage/design/","text":"Asset Lineage OMAS Design Documentation \u00b6 The module structure for the Asset Lineage OMAS is as follows: asset-lineage-client supports the client library. asset-lineage-api supports the common Java classes that are used both by the client and the server. asset-lineage-server supports in implementation of the access service and its related event management. asset-lineage-spring supports the REST API using the Spring libraries. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/asset-lineage/design/#asset-lineage-omas-design-documentation","text":"The module structure for the Asset Lineage OMAS is as follows: asset-lineage-client supports the client library. asset-lineage-api supports the common Java classes that are used both by the client and the server. asset-lineage-server supports in implementation of the access service and its related event management. asset-lineage-spring supports the REST API using the Spring libraries. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Asset Lineage OMAS Design Documentation"},{"location":"services/omas/asset-lineage/user/","text":"Asset Lineage OMAS User Documentation \u00b6 The Asset Lineage OMAS is designed to publish the events that can be used by external tools and engines to build lineage graphs. These complex events are constructed using the Enterprise Connector in oredr to fetch the full context of the data assets. The link between Glossary Terms and Schema Elements/Glossary Categories is named vertical lineage . The horizontal lineage traces the path which data flows starting system of records to the point of usage. Most of the interaction with the Asset Lineage OMAS will be driven by the external tools used to build lineage. To understand how to configure: * Configuring the Asset Lineage OMAS \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/asset-lineage/user/#asset-lineage-omas-user-documentation","text":"The Asset Lineage OMAS is designed to publish the events that can be used by external tools and engines to build lineage graphs. These complex events are constructed using the Enterprise Connector in oredr to fetch the full context of the data assets. The link between Glossary Terms and Schema Elements/Glossary Categories is named vertical lineage . The horizontal lineage traces the path which data flows starting system of records to the point of usage. Most of the interaction with the Asset Lineage OMAS will be driven by the external tools used to build lineage. To understand how to configure:","title":"Asset Lineage OMAS User Documentation"},{"location":"services/omas/asset-lineage/user/#configuring-the-asset-lineage-omas","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"* Configuring the Asset Lineage OMAS"},{"location":"services/omas/asset-manager/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Asset Manager Open Metadata Access Service ( OMAS ) \u00b6 The Asset Manager OMAS provides APIs for supporting the exchange of metadata with a third party asset manager. An asset manager is typically a catalog of assets . It is responsible for maintaining details of the assets including their characteristics, ownership, assessments and governance requirements. The Asset Manager OMAS (in conjunction with the Catalog Integrator OMIS ) provides a new integration path for metadata catalogs the goes via an integration service hosted in an integration daemon . Catalogs will be able to have a two-way integration through this path without needing to conform to the repository service rules for managing home and reference copies. This is possible for two reasons: Since the metadata from the catalog passes through an OMAS , Egeria will be able to have a better control of the metadata from the catalog. Since the catalog is not part of a federated query, any inconsistent updates to metadata that occurs in its repository, only impacts the users of that catalog and not the whole cohort. External Identifiers \u00b6 A major challenge when exchanging metadata with third party asset managers is that there is often a mismatch between the structure of the metadata in a third party asset manager and the structure of the open metadata types used by the Asset Manager OMAS . For this reason, the Asset Manager OMAS supports the ability to associate multiple external identifiers with an open metadata instance. These external identifiers are scoped to a particular third party asset manager so that there is no confusion if two third party asset managers happen to use the same unique identifier within their repositories. Each of these external identifiers can be mapped to the appropriate open metadata instances without confusion. There are also API calls for querying open metadata instances using external identifiers and the identifier of the third party asset manager. More information on the use of external identifiers to map between metadata elements in third party asset managers and open metadata instances can be found here . Supplementary Properties \u00b6 It is common for external asset managers to include extensive descriptive properties for assets that include both a technical name and description as well as a business name and description. The Asset Manager OMAS supports this distinction and stores the technical name and description in an Asset metadata instance and the business name and description in a glossary term metadata instance that is linked to the asset using a MoreInformation relationship . The properties that are stored in the glossary term are referred to as supplementary properties.","title":"Index"},{"location":"services/omas/asset-manager/#asset-manager-open-metadata-access-service-omas","text":"The Asset Manager OMAS provides APIs for supporting the exchange of metadata with a third party asset manager. An asset manager is typically a catalog of assets . It is responsible for maintaining details of the assets including their characteristics, ownership, assessments and governance requirements. The Asset Manager OMAS (in conjunction with the Catalog Integrator OMIS ) provides a new integration path for metadata catalogs the goes via an integration service hosted in an integration daemon . Catalogs will be able to have a two-way integration through this path without needing to conform to the repository service rules for managing home and reference copies. This is possible for two reasons: Since the metadata from the catalog passes through an OMAS , Egeria will be able to have a better control of the metadata from the catalog. Since the catalog is not part of a federated query, any inconsistent updates to metadata that occurs in its repository, only impacts the users of that catalog and not the whole cohort.","title":"Asset Manager Open Metadata Access Service (OMAS)"},{"location":"services/omas/asset-manager/#external-identifiers","text":"A major challenge when exchanging metadata with third party asset managers is that there is often a mismatch between the structure of the metadata in a third party asset manager and the structure of the open metadata types used by the Asset Manager OMAS . For this reason, the Asset Manager OMAS supports the ability to associate multiple external identifiers with an open metadata instance. These external identifiers are scoped to a particular third party asset manager so that there is no confusion if two third party asset managers happen to use the same unique identifier within their repositories. Each of these external identifiers can be mapped to the appropriate open metadata instances without confusion. There are also API calls for querying open metadata instances using external identifiers and the identifier of the third party asset manager. More information on the use of external identifiers to map between metadata elements in third party asset managers and open metadata instances can be found here .","title":"External Identifiers"},{"location":"services/omas/asset-manager/#supplementary-properties","text":"It is common for external asset managers to include extensive descriptive properties for assets that include both a technical name and description as well as a business name and description. The Asset Manager OMAS supports this distinction and stores the technical name and description in an Asset metadata instance and the business name and description in a glossary term metadata instance that is linked to the asset using a MoreInformation relationship . The properties that are stored in the glossary term are referred to as supplementary properties.","title":"Supplementary Properties"},{"location":"services/omas/asset-owner/","text":"Released This function is complete and can be used. The interfaces will be supported until the function is removed from the project via the deprecation process. There will be ongoing extensions to this function, but it will be done to ensure backward compatibility as far as possible. If there is a need to break backward compatibility, this will be discussed and reviewed in the community, with a documented timeline. Asset Owner Open Metadata Access Service ( OMAS ) \u00b6 The Asset Owner OMAS provides APIs and notifications for tools and applications supporting the work of Asset Owners in protecting and enhancing their assets. Every asset has an owner property. This is the userId of the owner. The owner is responsible for the correct classification of assets and the assignment of connection(s) to the asset. The owner typically links the asset (or more likely the asset's schema elements) to glossary terms and declares the asset's associated licenses and certifications. Asset owners can maintain a note log for each of their assets. They can view the feedback and respond to it. The Asset Owner OMAS generates notifications about new feedback linked to an asset.","title":"Asset Owner OMAS"},{"location":"services/omas/asset-owner/#asset-owner-open-metadata-access-service-omas","text":"The Asset Owner OMAS provides APIs and notifications for tools and applications supporting the work of Asset Owners in protecting and enhancing their assets. Every asset has an owner property. This is the userId of the owner. The owner is responsible for the correct classification of assets and the assignment of connection(s) to the asset. The owner typically links the asset (or more likely the asset's schema elements) to glossary terms and declares the asset's associated licenses and certifications. Asset owners can maintain a note log for each of their assets. They can view the feedback and respond to it. The Asset Owner OMAS generates notifications about new feedback linked to an asset.","title":"Asset Owner Open Metadata Access Service (OMAS)"},{"location":"services/omas/asset-owner/design/","text":"Asset Owner OMAS design \u00b6 The module structure for the Asset Owner OMAS follows the standard pattern as follows: asset-owner-client supports the client library. asset-owner-api supports the common Java classes that are used both by the client and the server. asset-owner-server supports in implementation of the access service and its related event management. asset-owner-spring supports the REST API using the Spring libraries. It makes use of the ocf-metadata-management for its server side interaction with the metadata repository and so the primary function of the Asset Owner OMAS is to manage the APIs for the Asset Owner and translate between them and the Open Connector Framework (OCF) oriented interfaces of ocf-metadata-management. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/asset-owner/design/#asset-owner-omas-design","text":"The module structure for the Asset Owner OMAS follows the standard pattern as follows: asset-owner-client supports the client library. asset-owner-api supports the common Java classes that are used both by the client and the server. asset-owner-server supports in implementation of the access service and its related event management. asset-owner-spring supports the REST API using the Spring libraries. It makes use of the ocf-metadata-management for its server side interaction with the metadata repository and so the primary function of the Asset Owner OMAS is to manage the APIs for the Asset Owner and translate between them and the Open Connector Framework (OCF) oriented interfaces of ocf-metadata-management. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Asset Owner OMAS design"},{"location":"services/omas/asset-owner/user/","text":"\u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/asset-owner/user/#_1","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":""},{"location":"services/omas/community-profile/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Community Profile Open Metadata Access Service ( OMAS ) \u00b6 The Community Profile OMAS provides APIs and events for tools and applications that are managing information about people and the way they work together. In particular the Community Profile OMAS supports personal profiles , organizations and teams along with communities of people collaborating around data. With this information, open metadata reduces the friction between people from different silos of an organization that can prevent the effective use of data. For example, an Asset Owner can monitor who is using their asset and for what purposes. At the same time a data scientist or business analyst can find out which assets their colleagues are using, and any feedback that they gave, which helps to guide them to the most appropriate assets for their work. Is this metadata ? \u00b6 Data about people and organizations is not strictly metadata. It is master data. This means it is a type of data that is widely duplicated across an organization's systems but mercifully slowly changing. Open metadata is therefore only one of many systems making use of this data. Many organizations use a centralized user directory, such as LDAP, for their employees. In addition large organizations with thousands of systems may also have a master data management (MDM) program to keep their data about people and organization's synchronized amongst their systems. With or without MDM, it is important that the Community Profile OMAS can operate as a downstream consumer of this data, rather than operating as an independent island. This way it adds value to the organization by enabling the recording of asset ownership, use and feedback, without an excessive amount of additional administration. Digging Deeper \u00b6 User Documentation Design Documentation","title":"Community Profile OMAS"},{"location":"services/omas/community-profile/#community-profile-open-metadata-access-service-omas","text":"The Community Profile OMAS provides APIs and events for tools and applications that are managing information about people and the way they work together. In particular the Community Profile OMAS supports personal profiles , organizations and teams along with communities of people collaborating around data. With this information, open metadata reduces the friction between people from different silos of an organization that can prevent the effective use of data. For example, an Asset Owner can monitor who is using their asset and for what purposes. At the same time a data scientist or business analyst can find out which assets their colleagues are using, and any feedback that they gave, which helps to guide them to the most appropriate assets for their work.","title":"Community Profile Open Metadata Access Service (OMAS)"},{"location":"services/omas/community-profile/#is-this-metadata","text":"Data about people and organizations is not strictly metadata. It is master data. This means it is a type of data that is widely duplicated across an organization's systems but mercifully slowly changing. Open metadata is therefore only one of many systems making use of this data. Many organizations use a centralized user directory, such as LDAP, for their employees. In addition large organizations with thousands of systems may also have a master data management (MDM) program to keep their data about people and organization's synchronized amongst their systems. With or without MDM, it is important that the Community Profile OMAS can operate as a downstream consumer of this data, rather than operating as an independent island. This way it adds value to the organization by enabling the recording of asset ownership, use and feedback, without an excessive amount of additional administration.","title":"Is this metadata ?"},{"location":"services/omas/community-profile/#digging-deeper","text":"User Documentation Design Documentation","title":"Digging Deeper"},{"location":"services/omas/community-profile/concepts/","text":"Community Profile OMAS Concepts and Vocabulary \u00b6 Below are the list of concepts that are core to the Community Profile OMAS. The link for each concept takes you to more detail about the concept itself, how it is implemented and the function supported for this concept. The list is in alphabetical order. Action Anchor Collection (of favorite things) Comments Communities Community Forums and Contributions Community Member Community Roles Community Administrator Community Contributor Community Leader Community Observer Contact Method and Contact Details Departmental Structure External References and External Reference List Favorite Things Collections Head count limit (for Roles) Karma Points Karma Point Plateau Like Master Data Manager Organization Peer Network Personal Messages Personal Note Log and Notes Personal Profiles Personal Roles Review Role Tag Team Team Leader Team Member To Do Useful Resources and Useful Resources List License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/community-profile/concepts/#community-profile-omas-concepts-and-vocabulary","text":"Below are the list of concepts that are core to the Community Profile OMAS. The link for each concept takes you to more detail about the concept itself, how it is implemented and the function supported for this concept. The list is in alphabetical order. Action Anchor Collection (of favorite things) Comments Communities Community Forums and Contributions Community Member Community Roles Community Administrator Community Contributor Community Leader Community Observer Contact Method and Contact Details Departmental Structure External References and External Reference List Favorite Things Collections Head count limit (for Roles) Karma Points Karma Point Plateau Like Master Data Manager Organization Peer Network Personal Messages Personal Note Log and Notes Personal Profiles Personal Roles Review Role Tag Team Team Leader Team Member To Do Useful Resources and Useful Resources List License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community Profile OMAS Concepts and Vocabulary"},{"location":"services/omas/community-profile/concepts/collection/","text":"Collections \u00b6 A collection is a reusable resource list . Assets and other resources can be linked to a collection. The collection itself can then be added to a resource list, say for a community or a personal profile . License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Collection"},{"location":"services/omas/community-profile/concepts/collection/#collections","text":"A collection is a reusable resource list . Assets and other resources can be linked to a collection. The collection itself can then be added to a resource list, say for a community or a personal profile . License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Collections"},{"location":"services/omas/community-profile/concepts/comment/","text":"Comments \u00b6 [Comments] are informal messages or feedback. They can be attached to many places, for example a personal note , a community , a community forum or community forum contribution , a review , an external reference , or another comment. Sometimes specific names are used for comments depending on what they are attached to. For example: * a personal note comment is a comment attached to a personal note. * a forum comment is a comment attached to a community forum either directly or via a community forum contribution. * a comment reply is a comment attached to another comment. The ability to reply to a comment (or a comment reply) means that comments can be chained together to show a detailed conversation on a topic. The owner of a personal profile or the administrator of a community are able to remove inappropriate or out-of-date comments attached to their personal profile or community respectively. See: * [Removing] License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Comment"},{"location":"services/omas/community-profile/concepts/comment/#comments","text":"[Comments] are informal messages or feedback. They can be attached to many places, for example a personal note , a community , a community forum or community forum contribution , a review , an external reference , or another comment. Sometimes specific names are used for comments depending on what they are attached to. For example: * a personal note comment is a comment attached to a personal note. * a forum comment is a comment attached to a community forum either directly or via a community forum contribution. * a comment reply is a comment attached to another comment. The ability to reply to a comment (or a comment reply) means that comments can be chained together to show a detailed conversation on a topic. The owner of a personal profile or the administrator of a community are able to remove inappropriate or out-of-date comments attached to their personal profile or community respectively. See: * [Removing] License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Comments"},{"location":"services/omas/community-profile/concepts/community-administrator/","text":"Community Administrator \u00b6 A community administrator is someone who is responsible for managing the membership list of a community . When a community is created, the administrator defaults to the person who created it. This can be changed. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community administrator"},{"location":"services/omas/community-profile/concepts/community-administrator/#community-administrator","text":"A community administrator is someone who is responsible for managing the membership list of a community . When a community is created, the administrator defaults to the person who created it. This can be changed. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community Administrator"},{"location":"services/omas/community-profile/concepts/community-contributor/","text":"Community Contributor \u00b6 A community contributor is a role for a community member who is able to create content for the community . This could be a new forum and contributions to any forum, comments, likes, tags and review. See community roles for more information on the other roles. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community contributor"},{"location":"services/omas/community-profile/concepts/community-contributor/#community-contributor","text":"A community contributor is a role for a community member who is able to create content for the community . This could be a new forum and contributions to any forum, comments, likes, tags and review. See community roles for more information on the other roles. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community Contributor"},{"location":"services/omas/community-profile/concepts/community-forum/","text":"Community Forum \u00b6 A community forum provides a place for a community to store notes and comments about a specific topic. Forums can be created by community contributors, administrators and leaders . Each forum has a name, description and a list of forum contributions . Forum contributions can be created by community members, leaders and administrators. Each contribution can be updated/deleted by the community administrators. Anyone can add comments to a forum contribution. Thus it is possible for a forum contribution to simply pose a question to the membership and the membership creates the discussion and possible answers using comments. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community forum"},{"location":"services/omas/community-profile/concepts/community-forum/#community-forum","text":"A community forum provides a place for a community to store notes and comments about a specific topic. Forums can be created by community contributors, administrators and leaders . Each forum has a name, description and a list of forum contributions . Forum contributions can be created by community members, leaders and administrators. Each contribution can be updated/deleted by the community administrators. Anyone can add comments to a forum contribution. Thus it is possible for a forum contribution to simply pose a question to the membership and the membership creates the discussion and possible answers using comments. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community Forum"},{"location":"services/omas/community-profile/concepts/community-leader/","text":"Community Leader \u00b6 A community leader is a person who gives direction to a community . See community roles . License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community leader"},{"location":"services/omas/community-profile/concepts/community-leader/#community-leader","text":"A community leader is a person who gives direction to a community . See community roles . License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community Leader"},{"location":"services/omas/community-profile/concepts/community-member/","text":"Community Member \u00b6 A community member is a person whose profile is linked to a community . There are different roles a community member can be assigned to. Each has different privileges. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community member"},{"location":"services/omas/community-profile/concepts/community-member/#community-member","text":"A community member is a person whose profile is linked to a community . There are different roles a community member can be assigned to. Each has different privileges. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community Member"},{"location":"services/omas/community-profile/concepts/community-observer/","text":"Community Observer \u00b6 A community observer is someone who is interested in a community but is not contributing. See community roles for more information. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community observer"},{"location":"services/omas/community-profile/concepts/community-observer/#community-observer","text":"A community observer is someone who is interested in a community but is not contributing. See community roles for more information. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community Observer"},{"location":"services/omas/community-profile/concepts/community-roles/","text":"Community roles \u00b6 Members of a community are assigned a role. The roles are additive. At a minimum, a members is a Community Observer . This means they receive notifications from the community. This can be upgraded to Community Contributor to add the right to post content to the community. Next is a Community Administrator who is given the additional ability to add members and delete inappropriate content. Finally the Community Leader has all of the powers of the administrator but really focuses on the health and content of the community - typically creating forums, keeping the exchange of information flowing, ensuring the lists of useful resources and external references are up to date, and more. The person creating a community is the first community leader. This can be changed. Typically the community leader assigns administrators to manage the membership lists to free them up to manage the content. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community roles"},{"location":"services/omas/community-profile/concepts/community-roles/#community-roles","text":"Members of a community are assigned a role. The roles are additive. At a minimum, a members is a Community Observer . This means they receive notifications from the community. This can be upgraded to Community Contributor to add the right to post content to the community. Next is a Community Administrator who is given the additional ability to add members and delete inappropriate content. Finally the Community Leader has all of the powers of the administrator but really focuses on the health and content of the community - typically creating forums, keeping the exchange of information flowing, ensuring the lists of useful resources and external references are up to date, and more. The person creating a community is the first community leader. This can be changed. Typically the community leader assigns administrators to manage the membership lists to free them up to manage the content. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community roles"},{"location":"services/omas/community-profile/concepts/community/","text":"Community \u00b6 A community is a anchor point for grouping people and resources together around a common interest. Communities are typically long running endeavours. Communities have members . These are the people working together in the community. The community can gather together a list of useful resources and a list of external references . The members can create forums on different topics to share information. The members can attach comments to the community itself and to forum contributions, and add replies to them. Members can also provide reviews to forum contributions. This includes a star rating and review comment. Members can add tags to the community and any of its contents. Members can also add a like to the community and any of its contents. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community"},{"location":"services/omas/community-profile/concepts/community/#community","text":"A community is a anchor point for grouping people and resources together around a common interest. Communities are typically long running endeavours. Communities have members . These are the people working together in the community. The community can gather together a list of useful resources and a list of external references . The members can create forums on different topics to share information. The members can attach comments to the community itself and to forum contributions, and add replies to them. Members can also provide reviews to forum contributions. This includes a star rating and review comment. Members can add tags to the community and any of its contents. Members can also add a like to the community and any of its contents. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community"},{"location":"services/omas/community-profile/concepts/contact-method/","text":"Contact method \u00b6 A contact method provides a means to send a person or a team a message. This includes email, phone, or through their personal profile . License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Contact method"},{"location":"services/omas/community-profile/concepts/contact-method/#contact-method","text":"A contact method provides a means to send a person or a team a message. This includes email, phone, or through their personal profile . License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Contact method"},{"location":"services/omas/community-profile/concepts/external-reference/","text":"External references \u00b6 External references describe and contain a URL to a document or online resource outside of open metadata. A list of external references can be attached to a personal profile , a community or a team License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"External reference"},{"location":"services/omas/community-profile/concepts/external-reference/#external-references","text":"External references describe and contain a URL to a document or online resource outside of open metadata. A list of external references can be attached to a personal profile , a community or a team License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"External references"},{"location":"services/omas/community-profile/concepts/favorite-things-collection/","text":"Collections \u00b6 Collections are sharable lists of things. A collection may, for example, be a member of a useful resource list that is attached to multiple anchors such as communities and personal profiles. There are three special types of collections managed by Community Profile OMAS that are only attached to a single personal profile . They are to manage collections of assets , projects and communities for the individual owner of the License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Favorite things collection"},{"location":"services/omas/community-profile/concepts/favorite-things-collection/#collections","text":"Collections are sharable lists of things. A collection may, for example, be a member of a useful resource list that is attached to multiple anchors such as communities and personal profiles. There are three special types of collections managed by Community Profile OMAS that are only attached to a single personal profile . They are to manage collections of assets , projects and communities for the individual owner of the License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Collections"},{"location":"services/omas/community-profile/concepts/head-count-limit-for-role/","text":"Head count limit \u00b6 The head count limit is an optional value that can be set in a personal role . This determines how many people are funded for the role. Open metadata does not prevent more people than this limit being appointed to the role, but it does send a notification to indicate that the limit has been breached. The organization can choose to increase the head count limit or remove one of the appointed people. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Head count limit for role"},{"location":"services/omas/community-profile/concepts/head-count-limit-for-role/#head-count-limit","text":"The head count limit is an optional value that can be set in a personal role . This determines how many people are funded for the role. Open metadata does not prevent more people than this limit being appointed to the role, but it does send a notification to indicate that the limit has been breached. The organization can choose to increase the head count limit or remove one of the appointed people. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Head count limit"},{"location":"services/omas/community-profile/concepts/karma-point-plateau/","text":"Karma Point Plateau \u00b6 A karma point plateau identifies a significant contribution to open metadata. By default, a karma point plateau is achieved for every 500 karma points awarded. However the threshold for the karma point plateau can be changed at server start up . The Community Profile OMAS generates a Karma Point Plateau Event each time a person achieves a karma point plateau. This is sent on the Community Profile OMAS's OutTopic License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Karma point plateau"},{"location":"services/omas/community-profile/concepts/karma-point-plateau/#karma-point-plateau","text":"A karma point plateau identifies a significant contribution to open metadata. By default, a karma point plateau is achieved for every 500 karma points awarded. However the threshold for the karma point plateau can be changed at server start up . The Community Profile OMAS generates a Karma Point Plateau Event each time a person achieves a karma point plateau. This is sent on the Community Profile OMAS's OutTopic License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Karma Point Plateau"},{"location":"services/omas/community-profile/concepts/karma-point/","text":"Karma Point \u00b6 A karma point is a reward given to an individual for making a contribution to open metadata. This may be for: Creating some information Correcting or enhancing some information Linking information together Removing obsolete information Karma points are awarded automatically. They are stored in an individual's personal profile . When an individual's karma point total reaches a multiple of the karma point notification threshold, the Community Profile OMAS sends a notification on its OutTopic . Related information \u00b6 [Configuring the notification threshold for karma points] License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Karma point"},{"location":"services/omas/community-profile/concepts/karma-point/#karma-point","text":"A karma point is a reward given to an individual for making a contribution to open metadata. This may be for: Creating some information Correcting or enhancing some information Linking information together Removing obsolete information Karma points are awarded automatically. They are stored in an individual's personal profile . When an individual's karma point total reaches a multiple of the karma point notification threshold, the Community Profile OMAS sends a notification on its OutTopic .","title":"Karma Point"},{"location":"services/omas/community-profile/concepts/karma-point/#related-information","text":"[Configuring the notification threshold for karma points] License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Related information"},{"location":"services/omas/community-profile/concepts/like/","text":"Like \u00b6 A \"like\" is an attachment that can be made to a personal message, personal note, forums and forum contributions, comments, teams and communities. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Like"},{"location":"services/omas/community-profile/concepts/like/#like","text":"A \"like\" is an attachment that can be made to a personal message, personal note, forums and forum contributions, comments, teams and communities. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Like"},{"location":"services/omas/community-profile/concepts/peer-network/","text":"Peer Network \u00b6 An individual can maintain a list of their close/important colleagues. This is called their peer network and it is chained off of their personal profile . It is important to note that the perspective on who is a close/important colleague is a personal perspective. Therefore Community Profile OMAS separates the concept of who has linked to them from who they have linked to. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Peer network"},{"location":"services/omas/community-profile/concepts/peer-network/#peer-network","text":"An individual can maintain a list of their close/important colleagues. This is called their peer network and it is chained off of their personal profile . It is important to note that the perspective on who is a close/important colleague is a personal perspective. Therefore Community Profile OMAS separates the concept of who has linked to them from who they have linked to. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Peer Network"},{"location":"services/omas/community-profile/concepts/personal-message/","text":"Personal Message \u00b6 A personal message is a message that is attached to a personal profile, or as a reply to another personal message. If is possible to add tags and likes to a personal message. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Personal message"},{"location":"services/omas/community-profile/concepts/personal-message/#personal-message","text":"A personal message is a message that is attached to a personal profile, or as a reply to another personal message. If is possible to add tags and likes to a personal message. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Personal Message"},{"location":"services/omas/community-profile/concepts/personal-notes/","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Personal notes"},{"location":"services/omas/community-profile/concepts/personal-profile/","text":"Personal Profile \u00b6 A personal profile provides a place for an individual to share information about themselves with the other people they are collaborating with. It is associated with one or more of the person's userIds. Each userId is linked to the profile as a UserIdentity object. There can be more than one userId for a profile (for example if a user has an administrator userId and a normal userId) However, the same userId can not be linked to two profiles. This means we can retrieve a profile from the UserId. Each profile has a qualified name that should uniquely identify the individual. For example, an employee identifier. There is space to provide the name the individual wants to be known as, and their full name, along with a job title. An individual can also maintain collections of their favourite Assets, Projects and Communities and control notifications about changes to the member of these lists. Working with personal profiles \u00b6 Retrieving my personal profile Creating my personal profile Updating my personal profile Removing my personal profile Retrieving a personal profile for another user Loading personal profiles of existing members of an organization Synchronizing updates to personal profiles from another system License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Personal profile"},{"location":"services/omas/community-profile/concepts/personal-profile/#personal-profile","text":"A personal profile provides a place for an individual to share information about themselves with the other people they are collaborating with. It is associated with one or more of the person's userIds. Each userId is linked to the profile as a UserIdentity object. There can be more than one userId for a profile (for example if a user has an administrator userId and a normal userId) However, the same userId can not be linked to two profiles. This means we can retrieve a profile from the UserId. Each profile has a qualified name that should uniquely identify the individual. For example, an employee identifier. There is space to provide the name the individual wants to be known as, and their full name, along with a job title. An individual can also maintain collections of their favourite Assets, Projects and Communities and control notifications about changes to the member of these lists.","title":"Personal Profile"},{"location":"services/omas/community-profile/concepts/personal-profile/#working-with-personal-profiles","text":"Retrieving my personal profile Creating my personal profile Updating my personal profile Removing my personal profile Retrieving a personal profile for another user Loading personal profiles of existing members of an organization Synchronizing updates to personal profiles from another system License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Working with personal profiles"},{"location":"services/omas/community-profile/concepts/personal-roles/","text":"Personal roles \u00b6 Personal roles are the list of person roles that an individual is currently appointed to. Community Member , Team Leader and Team Member are examples of personal roles. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Personal roles"},{"location":"services/omas/community-profile/concepts/personal-roles/#personal-roles","text":"Personal roles are the list of person roles that an individual is currently appointed to. Community Member , Team Leader and Team Member are examples of personal roles. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Personal roles"},{"location":"services/omas/community-profile/concepts/review/","text":"Reviews \u00b6 A review is an attachment that can be made to a personal note or forum contribution. It includes a star rating and an optional review comment. Reviews are used to provide feedback on ideas that are proposed in personal notes and forums. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Review"},{"location":"services/omas/community-profile/concepts/review/#reviews","text":"A review is an attachment that can be made to a personal note or forum contribution. It includes a star rating and an optional review comment. Reviews are used to provide feedback on ideas that are proposed in personal notes and forums. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Reviews"},{"location":"services/omas/community-profile/concepts/tag/","text":"Tag \u00b6 A tag is a descriptive name with optional description that can be attached to resources such as personal profiles, personal messages, personal notes, community forums and forum contributions, comments and communities. The process of adding a tag to an object is called tagging . A tag can be public (visible to all users) or private (visible only to the user that created it). Working with tags \u00b6 Finding existing tags Accessing resources attached to a tag Accessing my tags Creating a tag Attaching a tag Detaching my tag Detaching a tag from a resource Deleting my private tag Deleting public tags License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Tag"},{"location":"services/omas/community-profile/concepts/tag/#tag","text":"A tag is a descriptive name with optional description that can be attached to resources such as personal profiles, personal messages, personal notes, community forums and forum contributions, comments and communities. The process of adding a tag to an object is called tagging . A tag can be public (visible to all users) or private (visible only to the user that created it).","title":"Tag"},{"location":"services/omas/community-profile/concepts/tag/#working-with-tags","text":"Finding existing tags Accessing resources attached to a tag Accessing my tags Creating a tag Attaching a tag Detaching my tag Detaching a tag from a resource Deleting my private tag Deleting public tags License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Working with tags"},{"location":"services/omas/community-profile/concepts/to-do/","text":"To Dos \u00b6 A to do is a record of an action. It includes details of the action to perform and the time frame it needs to be completed for. To dos are normally created as a result of a meeting, or a process, such as a discovery process . The requested action may be ad hoc in nature, or part of a stewardship process that identified a specific action for an individual, or group of individuals. The creator of a to do is called the originator . The person, or people assigned to complete the task are called the assigned resources . Assigned to dos are attached to one of their roles rather than directly to their personal profile . Each to do has a priority and a status: * Open - no work has started. * In Progress - work is ongoing. * Waiting - work is on hold, typically blocked. * Complete - Work is completed. * Abandoned - No work will happen. Working with to dos \u00b6 Below are the descriptions of how to use the Community Profile OMAS to work with to dos. Accessing my to dos Creating a to do Managing a to do Design information \u00b6 ToDo Bean Open Metadata Type License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"To do"},{"location":"services/omas/community-profile/concepts/to-do/#to-dos","text":"A to do is a record of an action. It includes details of the action to perform and the time frame it needs to be completed for. To dos are normally created as a result of a meeting, or a process, such as a discovery process . The requested action may be ad hoc in nature, or part of a stewardship process that identified a specific action for an individual, or group of individuals. The creator of a to do is called the originator . The person, or people assigned to complete the task are called the assigned resources . Assigned to dos are attached to one of their roles rather than directly to their personal profile . Each to do has a priority and a status: * Open - no work has started. * In Progress - work is ongoing. * Waiting - work is on hold, typically blocked. * Complete - Work is completed. * Abandoned - No work will happen.","title":"To Dos"},{"location":"services/omas/community-profile/concepts/to-do/#working-with-to-dos","text":"Below are the descriptions of how to use the Community Profile OMAS to work with to dos. Accessing my to dos Creating a to do Managing a to do","title":"Working with to dos"},{"location":"services/omas/community-profile/concepts/to-do/#design-information","text":"ToDo Bean Open Metadata Type License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Design information"},{"location":"services/omas/community-profile/concepts/useful-resource/","text":"Useful Resources \u00b6 A resource is definition that is used frequently and so it is helpful to have a link to it. Examples of resources include glossaries, or categories and terms from a glossary, governance definitions, models, system definitions or process definitions. It is possible to maintain a list of useful resources for a personal profile , a team or a community . Working with resources and resource lists \u00b6 Finding a resource License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Useful resource"},{"location":"services/omas/community-profile/concepts/useful-resource/#useful-resources","text":"A resource is definition that is used frequently and so it is helpful to have a link to it. Examples of resources include glossaries, or categories and terms from a glossary, governance definitions, models, system definitions or process definitions. It is possible to maintain a list of useful resources for a personal profile , a team or a community .","title":"Useful Resources"},{"location":"services/omas/community-profile/concepts/useful-resource/#working-with-resources-and-resource-lists","text":"Finding a resource License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Working with resources and resource lists"},{"location":"services/omas/community-profile/concepts/user-identity/","text":"User identity \u00b6 The user identity is the unique name of a user's logon account. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"User identity"},{"location":"services/omas/community-profile/concepts/user-identity/#user-identity","text":"The user identity is the unique name of a user's logon account. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"User identity"},{"location":"services/omas/community-profile/design/","text":"Community Profile Open Metadata Access Service (OMAS) Design Documentation \u00b6 The Community Profile OMAS is implemented in four modules: community-profile-api supports the common Java classes that are used both by the client and the server. This includes the Java API, events, beans, events and REST API structures. community-profile-client supports the Java client library that allows applications and tools to call the remote REST APIs. community-profile-server supports the server side implementation of the access service. This includes the interaction with the administration services for registration, configuration, initialization and termination of the access service. interaction with the repository services to work with open metadata from the cohort . support for the access service's API and its related event management. community-profile-spring supports the REST API using the Spring libraries. Further documentation can be found: Java API Beans Exceptions Event Payloads REST API Operations Client-side design Server-side design License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/community-profile/design/#community-profile-open-metadata-access-service-omas-design-documentation","text":"The Community Profile OMAS is implemented in four modules: community-profile-api supports the common Java classes that are used both by the client and the server. This includes the Java API, events, beans, events and REST API structures. community-profile-client supports the Java client library that allows applications and tools to call the remote REST APIs. community-profile-server supports the server side implementation of the access service. This includes the interaction with the administration services for registration, configuration, initialization and termination of the access service. interaction with the repository services to work with open metadata from the cohort . support for the access service's API and its related event management. community-profile-spring supports the REST API using the Spring libraries. Further documentation can be found: Java API Beans Exceptions Event Payloads REST API Operations Client-side design Server-side design License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community Profile Open Metadata Access Service (OMAS) Design Documentation"},{"location":"services/omas/community-profile/scenarios/","text":"Using the Community Profile OMAS \u00b6 Below are the list of tasks supported by the Community Profile OMAS. One time set up by the administrator \u00b6 Many organizations already have a system that maintains information about their employees and/or customers and/or business partners . The Community Profile OMAS therefore supports an event exchange with such a system to keep the profiles synchronized. The following tasks cover the work of the IT team to integrate the Community Profile OMAS with other systems. Loading personal profiles of existing members of an organization into an open metadata repository. Synchronizing updates to personal profiles from another system Loading the organization's departmental structure into an open metadata repository. Synchronizing the organization's departmental structure with another system Capturing karma point plateaus emitted from the Community Profile OMAS OutTopic . Synchronizing collaboration activity with another system Individuals working with their personal profile \u00b6 A personal profile provides a place for an individual to share information about themselves with the other people they are collaborating with. Each personal profile is associated with one or more of the person's userIds. It is retrieved using one of these userIds. Retrieving my personal profile For organizations where the personal profile is not loaded from another system (see above) an individual can maintain their own profile. Creating my personal profile Updating my personal profile Removing my personal profile Once an individual has a personal profile, they will be awarded karma points when they contribute content to open metadata. An individual can query their karma points. Retrieving my karma points The individual can also maintain collections of their favourite Assets , Projects and Communities and control notifications about changes to the contents of these lists. Accessing my favorite assets Managing my favorite assets Accessing my favorite projects Managing my favorite projects Accessing my favorite communities Managing my favorite communities An individual can link other resources such as glossaries, and external references to their profile. Finding resources Accessing my resource list Managing my resource list Accessing my external reference list Managing my external reference list An individual can create a series of personal notes. These are like a personal blog. They are visible to other users who can comment on and review the content. Setting up my personal notes Accessing my personal notes Removing my personal notes Adding a personal note Updating a personal note Removing a personal note An individual can query the teams and communities they belong to. Accessing my communities Accessing my teams An individual can access an manage a list of close colleagues called their peer network . Accessing my peer network Managing my peer network An individual can query their roles in the organization and any actions (to dos) that have been assigned to them as part of one of these roles. Accessing my roles Accessing my to dos for each role Managing to dos An individual can add feedback to their profile and others (see below). Providing feedback and content to personal profiles \u00b6 An individual can send a personal message to themselves, or another user. This message is attached to the recipient's profile. Sending a personal message Replying to a personal message Updating a personal message Removing a personal message It is possible to add comments, likes and reviews to a personal note. Adding a comment to a personal note Updating a comment to a personal note Removing my comment from a personal note Adding a review to a personal note Updating my review to a personal note Removing my review from a personal note Adding a like to a personal note Removing a like from a personal note It is possible to create and attach tags to personal profiles, personal notes, and comments either for your personal profile or someone else's. Finding existing tags Accessing resources attached to a tag Accessing my tags Creating a tag Attaching a tag Detaching my tag Detaching a tag from a resource Deleting my private tag Deleting public tags Individuals searching for other people and teams \u00b6 Below are different ways to locate people in the organization. Finding a Person Querying another's personal profile Navigating the Departmental Structure Finding a Team Accessing my teams Viewing Leaders of a Team Viewing Members of a Team Communities \u00b6 Communities collect together resources, best practices and ideas for a group of people who are collaborating on a specific topic or skill. Anyone can create a community. Creating a community The person creating the community is the community leader . They can then add other people as community members with different community roles . Adding a new community member Changing a community member's role Removing a community member Community leaders and administrators can remove inappropriate content from a community and close it. Removing a comment from a community Removing a note from a community Removing a review from a community Removing a resource from a community Closing a community Individuals can locate and connect with a community. Finding the communities I am a member of Finding a community Querying a community Watching a community Joining a community Leaving a community Once someone is a member of a community they can add content to it. Adding a comment to a community Replying to a community comment Removing my comment from a community Adding a resource to a community Creating a community forum Adding a contribution to a community forum Removing my contribution from a community forum Community content can have feedback attached in the form of tags, reviews and likes. Attaching feedback to a community Attaching feedback to a community comment Attaching feedback to a community forum Attaching feedback to a community forum contribution Related information \u00b6 Community Profile OMAS Concepts Community Profile OMAS Design License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/community-profile/scenarios/#using-the-community-profile-omas","text":"Below are the list of tasks supported by the Community Profile OMAS.","title":"Using the Community Profile OMAS"},{"location":"services/omas/community-profile/scenarios/#one-time-set-up-by-the-administrator","text":"Many organizations already have a system that maintains information about their employees and/or customers and/or business partners . The Community Profile OMAS therefore supports an event exchange with such a system to keep the profiles synchronized. The following tasks cover the work of the IT team to integrate the Community Profile OMAS with other systems. Loading personal profiles of existing members of an organization into an open metadata repository. Synchronizing updates to personal profiles from another system Loading the organization's departmental structure into an open metadata repository. Synchronizing the organization's departmental structure with another system Capturing karma point plateaus emitted from the Community Profile OMAS OutTopic . Synchronizing collaboration activity with another system","title":"One time set up by the administrator"},{"location":"services/omas/community-profile/scenarios/#individuals-working-with-their-personal-profile","text":"A personal profile provides a place for an individual to share information about themselves with the other people they are collaborating with. Each personal profile is associated with one or more of the person's userIds. It is retrieved using one of these userIds. Retrieving my personal profile For organizations where the personal profile is not loaded from another system (see above) an individual can maintain their own profile. Creating my personal profile Updating my personal profile Removing my personal profile Once an individual has a personal profile, they will be awarded karma points when they contribute content to open metadata. An individual can query their karma points. Retrieving my karma points The individual can also maintain collections of their favourite Assets , Projects and Communities and control notifications about changes to the contents of these lists. Accessing my favorite assets Managing my favorite assets Accessing my favorite projects Managing my favorite projects Accessing my favorite communities Managing my favorite communities An individual can link other resources such as glossaries, and external references to their profile. Finding resources Accessing my resource list Managing my resource list Accessing my external reference list Managing my external reference list An individual can create a series of personal notes. These are like a personal blog. They are visible to other users who can comment on and review the content. Setting up my personal notes Accessing my personal notes Removing my personal notes Adding a personal note Updating a personal note Removing a personal note An individual can query the teams and communities they belong to. Accessing my communities Accessing my teams An individual can access an manage a list of close colleagues called their peer network . Accessing my peer network Managing my peer network An individual can query their roles in the organization and any actions (to dos) that have been assigned to them as part of one of these roles. Accessing my roles Accessing my to dos for each role Managing to dos An individual can add feedback to their profile and others (see below).","title":"Individuals working with their personal profile"},{"location":"services/omas/community-profile/scenarios/#providing-feedback-and-content-to-personal-profiles","text":"An individual can send a personal message to themselves, or another user. This message is attached to the recipient's profile. Sending a personal message Replying to a personal message Updating a personal message Removing a personal message It is possible to add comments, likes and reviews to a personal note. Adding a comment to a personal note Updating a comment to a personal note Removing my comment from a personal note Adding a review to a personal note Updating my review to a personal note Removing my review from a personal note Adding a like to a personal note Removing a like from a personal note It is possible to create and attach tags to personal profiles, personal notes, and comments either for your personal profile or someone else's. Finding existing tags Accessing resources attached to a tag Accessing my tags Creating a tag Attaching a tag Detaching my tag Detaching a tag from a resource Deleting my private tag Deleting public tags","title":"Providing feedback and content to personal profiles"},{"location":"services/omas/community-profile/scenarios/#individuals-searching-for-other-people-and-teams","text":"Below are different ways to locate people in the organization. Finding a Person Querying another's personal profile Navigating the Departmental Structure Finding a Team Accessing my teams Viewing Leaders of a Team Viewing Members of a Team","title":"Individuals searching for other people and teams"},{"location":"services/omas/community-profile/scenarios/#communities","text":"Communities collect together resources, best practices and ideas for a group of people who are collaborating on a specific topic or skill. Anyone can create a community. Creating a community The person creating the community is the community leader . They can then add other people as community members with different community roles . Adding a new community member Changing a community member's role Removing a community member Community leaders and administrators can remove inappropriate content from a community and close it. Removing a comment from a community Removing a note from a community Removing a review from a community Removing a resource from a community Closing a community Individuals can locate and connect with a community. Finding the communities I am a member of Finding a community Querying a community Watching a community Joining a community Leaving a community Once someone is a member of a community they can add content to it. Adding a comment to a community Replying to a community comment Removing my comment from a community Adding a resource to a community Creating a community forum Adding a contribution to a community forum Removing my contribution from a community forum Community content can have feedback attached in the form of tags, reviews and likes. Attaching feedback to a community Attaching feedback to a community comment Attaching feedback to a community forum Attaching feedback to a community forum contribution","title":"Communities"},{"location":"services/omas/community-profile/scenarios/#related-information","text":"Community Profile OMAS Concepts Community Profile OMAS Design License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Related information"},{"location":"services/omas/community-profile/scenarios/accessing-a-useful-resource-list/","text":"Accessing a Useful Resource List \u00b6 It is possible to maintain a list of useful resources with a personal profile , a team or a community . First locate the unique identifier (guid) of the element acting as an anchor ofr License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing a useful resource list"},{"location":"services/omas/community-profile/scenarios/accessing-a-useful-resource-list/#accessing-a-useful-resource-list","text":"It is possible to maintain a list of useful resources with a personal profile , a team or a community . First locate the unique identifier (guid) of the element acting as an anchor ofr License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing a Useful Resource List"},{"location":"services/omas/community-profile/scenarios/accessing-an-external-references-list/","text":"Accessing an External References List \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing an external references list"},{"location":"services/omas/community-profile/scenarios/accessing-an-external-references-list/#accessing-an-external-references-list","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing an External References List"},{"location":"services/omas/community-profile/scenarios/accessing-my-communities/","text":"Accessing my communities \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my communities"},{"location":"services/omas/community-profile/scenarios/accessing-my-communities/#accessing-my-communities","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my communities"},{"location":"services/omas/community-profile/scenarios/accessing-my-external-references/","text":"Accessing My External References \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my external references"},{"location":"services/omas/community-profile/scenarios/accessing-my-external-references/#accessing-my-external-references","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing My External References"},{"location":"services/omas/community-profile/scenarios/accessing-my-favorite-assets/","text":"Accessing my favorite assets \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my favorite assets"},{"location":"services/omas/community-profile/scenarios/accessing-my-favorite-assets/#accessing-my-favorite-assets","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my favorite assets"},{"location":"services/omas/community-profile/scenarios/accessing-my-favorite-communities/","text":"Accessing my favorite communities \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my favorite communities"},{"location":"services/omas/community-profile/scenarios/accessing-my-favorite-communities/#accessing-my-favorite-communities","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my favorite communities"},{"location":"services/omas/community-profile/scenarios/accessing-my-favorite-projects/","text":"Accessing my favorite projects \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my favorite projects"},{"location":"services/omas/community-profile/scenarios/accessing-my-favorite-projects/#accessing-my-favorite-projects","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my favorite projects"},{"location":"services/omas/community-profile/scenarios/accessing-my-peer-network/","text":"Accessing my peer network \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my peer network"},{"location":"services/omas/community-profile/scenarios/accessing-my-peer-network/#accessing-my-peer-network","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my peer network"},{"location":"services/omas/community-profile/scenarios/accessing-my-personal-notes/","text":"Accessing my roles \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my personal notes"},{"location":"services/omas/community-profile/scenarios/accessing-my-personal-notes/#accessing-my-roles","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my roles"},{"location":"services/omas/community-profile/scenarios/accessing-my-resource-list/","text":"Accessing my Resource List \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my resource list"},{"location":"services/omas/community-profile/scenarios/accessing-my-resource-list/#accessing-my-resource-list","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my Resource List"},{"location":"services/omas/community-profile/scenarios/accessing-my-roles/","text":"Accessing my roles \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my roles"},{"location":"services/omas/community-profile/scenarios/accessing-my-roles/#accessing-my-roles","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my roles"},{"location":"services/omas/community-profile/scenarios/accessing-my-tags/","text":"Accessing my tags \u00b6 Tags provide an informal way of identifying particular types of resources. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my tags"},{"location":"services/omas/community-profile/scenarios/accessing-my-tags/#accessing-my-tags","text":"Tags provide an informal way of identifying particular types of resources. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my tags"},{"location":"services/omas/community-profile/scenarios/accessing-my-teams/","text":"Accessing the team I am a member of \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my teams"},{"location":"services/omas/community-profile/scenarios/accessing-my-teams/#accessing-the-team-i-am-a-member-of","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing the team I am a member of"},{"location":"services/omas/community-profile/scenarios/accessing-my-to-dos/","text":"Accessing my to dos \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my to dos"},{"location":"services/omas/community-profile/scenarios/accessing-my-to-dos/#accessing-my-to-dos","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing my to dos"},{"location":"services/omas/community-profile/scenarios/accessing-tagged-resources/","text":"Accessing tagged resources \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing tagged resources"},{"location":"services/omas/community-profile/scenarios/accessing-tagged-resources/#accessing-tagged-resources","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Accessing tagged resources"},{"location":"services/omas/community-profile/scenarios/adding-a-comment-to-a-community/","text":"Adding a comment to a community \u00b6 Can be added to the community, or to a a forum contribution. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a comment to a community"},{"location":"services/omas/community-profile/scenarios/adding-a-comment-to-a-community/#adding-a-comment-to-a-community","text":"Can be added to the community, or to a a forum contribution. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a comment to a community"},{"location":"services/omas/community-profile/scenarios/adding-a-comment-to-a-personal-note/","text":"Adding a comment to a personal note \u00b6 Can add to personal note linked to your profile or someone else's. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a comment to a personal note"},{"location":"services/omas/community-profile/scenarios/adding-a-comment-to-a-personal-note/#adding-a-comment-to-a-personal-note","text":"Can add to personal note linked to your profile or someone else's. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a comment to a personal note"},{"location":"services/omas/community-profile/scenarios/adding-a-comment/","text":"Adding a comment \u00b6 Maybe to a community, community forum contribution License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a comment"},{"location":"services/omas/community-profile/scenarios/adding-a-comment/#adding-a-comment","text":"Maybe to a community, community forum contribution License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a comment"},{"location":"services/omas/community-profile/scenarios/adding-a-contribution-to-a-forum/","text":"Adding a Contribution to a Community Forum \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a contribution to a forum"},{"location":"services/omas/community-profile/scenarios/adding-a-contribution-to-a-forum/#adding-a-contribution-to-a-community-forum","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a Contribution to a Community Forum"},{"location":"services/omas/community-profile/scenarios/adding-a-like-to-a-personal-note/","text":"Adding a like to a personal note \u00b6 Can add like to personal note linked to your profile or someone else's. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a like to a personal note"},{"location":"services/omas/community-profile/scenarios/adding-a-like-to-a-personal-note/#adding-a-like-to-a-personal-note","text":"Can add like to personal note linked to your profile or someone else's. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a like to a personal note"},{"location":"services/omas/community-profile/scenarios/adding-a-new-community-member/","text":"Adding a new community member \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a new community member"},{"location":"services/omas/community-profile/scenarios/adding-a-new-community-member/#adding-a-new-community-member","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a new community member"},{"location":"services/omas/community-profile/scenarios/adding-a-personal-note/","text":"Adding a personal note \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a personal note"},{"location":"services/omas/community-profile/scenarios/adding-a-personal-note/#adding-a-personal-note","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a personal note"},{"location":"services/omas/community-profile/scenarios/adding-a-resource-to-a-resource-list/","text":"Adding a Resource to a Resource List \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a resource to a resource list"},{"location":"services/omas/community-profile/scenarios/adding-a-resource-to-a-resource-list/#adding-a-resource-to-a-resource-list","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a Resource to a Resource List"},{"location":"services/omas/community-profile/scenarios/adding-a-review-to-a-personal-note/","text":"Adding a review to a personal note \u00b6 Can add to personal note linked to your profile or someone else's. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a review to a personal note"},{"location":"services/omas/community-profile/scenarios/adding-a-review-to-a-personal-note/#adding-a-review-to-a-personal-note","text":"Can add to personal note linked to your profile or someone else's. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Adding a review to a personal note"},{"location":"services/omas/community-profile/scenarios/attaching-a-tag/","text":"Attaching a tag to a resource \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Attaching a tag"},{"location":"services/omas/community-profile/scenarios/attaching-a-tag/#attaching-a-tag-to-a-resource","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Attaching a tag to a resource"},{"location":"services/omas/community-profile/scenarios/attaching-feedback-to-a-community-comment/","text":"Attaching feedback to a community comment \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Attaching feedback to a community comment"},{"location":"services/omas/community-profile/scenarios/attaching-feedback-to-a-community-comment/#attaching-feedback-to-a-community-comment","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Attaching feedback to a community comment"},{"location":"services/omas/community-profile/scenarios/attaching-feedback-to-a-community-forum-contribution/","text":"Attaching feedback to a community forum contribution \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Attaching feedback to a community forum contribution"},{"location":"services/omas/community-profile/scenarios/attaching-feedback-to-a-community-forum-contribution/#attaching-feedback-to-a-community-forum-contribution","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Attaching feedback to a community forum contribution"},{"location":"services/omas/community-profile/scenarios/attaching-feedback-to-a-community-forum/","text":"Attaching feedback to a community forum \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Attaching feedback to a community forum"},{"location":"services/omas/community-profile/scenarios/attaching-feedback-to-a-community-forum/#attaching-feedback-to-a-community-forum","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Attaching feedback to a community forum"},{"location":"services/omas/community-profile/scenarios/attaching-feedback-to-a-community/","text":"Attaching feedback to a community \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Attaching feedback to a community"},{"location":"services/omas/community-profile/scenarios/attaching-feedback-to-a-community/#attaching-feedback-to-a-community","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Attaching feedback to a community"},{"location":"services/omas/community-profile/scenarios/capturing-karma-point-plateaus/","text":"Capturing karma point plateaus \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Capturing karma point plateaus"},{"location":"services/omas/community-profile/scenarios/capturing-karma-point-plateaus/#capturing-karma-point-plateaus","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Capturing karma point plateaus"},{"location":"services/omas/community-profile/scenarios/changing-community-member-role/","text":"Changing a community member's role \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Changing community member role"},{"location":"services/omas/community-profile/scenarios/changing-community-member-role/#changing-a-community-members-role","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Changing a community member's role"},{"location":"services/omas/community-profile/scenarios/closing-a-community/","text":"Closing a community \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Closing a community"},{"location":"services/omas/community-profile/scenarios/closing-a-community/#closing-a-community","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Closing a community"},{"location":"services/omas/community-profile/scenarios/creating-a-community-forum/","text":"Creating a community forum \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Creating a community forum"},{"location":"services/omas/community-profile/scenarios/creating-a-community-forum/#creating-a-community-forum","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Creating a community forum"},{"location":"services/omas/community-profile/scenarios/creating-a-community/","text":"Creating a community \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Creating a community"},{"location":"services/omas/community-profile/scenarios/creating-a-community/#creating-a-community","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Creating a community"},{"location":"services/omas/community-profile/scenarios/creating-a-tag/","text":"Creating a tag \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Creating a tag"},{"location":"services/omas/community-profile/scenarios/creating-a-tag/#creating-a-tag","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Creating a tag"},{"location":"services/omas/community-profile/scenarios/creating-a-to-do/","text":"Creating a to do \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Creating a to do"},{"location":"services/omas/community-profile/scenarios/creating-a-to-do/#creating-a-to-do","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Creating a to do"},{"location":"services/omas/community-profile/scenarios/creating-my-personal-profile/","text":"Creating my personal profile \u00b6 If your organization has not already created a personal profile for you then it is possible to create the profile yourself. The information that you will need is as follows. Many of the fields are optional but the more information that you supply, the easier it will be for people to locate you. qualified name - this is a unique identifier for you - for example if you are an employee of the organization then use your employee number. name - this is the name that you want to be known by. full name - this is your full legal name. This is optional. job title - this is also optional. job description - this is a short paragraph describing what your role is. It is optional. contact details such as email address, phone number social media account. See also \u00b6 Managing my contact details Design note \u00b6 The Community Profile OMAS will emit an [event] whenever a new profile is created using this approach License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Creating my personal profile"},{"location":"services/omas/community-profile/scenarios/creating-my-personal-profile/#creating-my-personal-profile","text":"If your organization has not already created a personal profile for you then it is possible to create the profile yourself. The information that you will need is as follows. Many of the fields are optional but the more information that you supply, the easier it will be for people to locate you. qualified name - this is a unique identifier for you - for example if you are an employee of the organization then use your employee number. name - this is the name that you want to be known by. full name - this is your full legal name. This is optional. job title - this is also optional. job description - this is a short paragraph describing what your role is. It is optional. contact details such as email address, phone number social media account.","title":"Creating my personal profile"},{"location":"services/omas/community-profile/scenarios/creating-my-personal-profile/#see-also","text":"Managing my contact details","title":"See also"},{"location":"services/omas/community-profile/scenarios/creating-my-personal-profile/#design-note","text":"The Community Profile OMAS will emit an [event] whenever a new profile is created using this approach License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Design note"},{"location":"services/omas/community-profile/scenarios/detaching-a-tag/","text":"Detaching a tag from a resource \u00b6 Find the resource and detach the tag. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Detaching a tag"},{"location":"services/omas/community-profile/scenarios/detaching-a-tag/#detaching-a-tag-from-a-resource","text":"Find the resource and detach the tag. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Detaching a tag from a resource"},{"location":"services/omas/community-profile/scenarios/detaching-my-tag/","text":"Detaching my tag from a resource \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Detaching my tag"},{"location":"services/omas/community-profile/scenarios/detaching-my-tag/#detaching-my-tag-from-a-resource","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Detaching my tag from a resource"},{"location":"services/omas/community-profile/scenarios/finding-a-community/","text":"Finding a community \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Finding a community"},{"location":"services/omas/community-profile/scenarios/finding-a-community/#finding-a-community","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Finding a community"},{"location":"services/omas/community-profile/scenarios/finding-a-person/","text":"Finding a person \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Finding a person"},{"location":"services/omas/community-profile/scenarios/finding-a-person/#finding-a-person","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Finding a person"},{"location":"services/omas/community-profile/scenarios/finding-a-resource/","text":"Finding a resource \u00b6 See also \u00b6 Finding resources by tags License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Finding a resource"},{"location":"services/omas/community-profile/scenarios/finding-a-resource/#finding-a-resource","text":"","title":"Finding a resource"},{"location":"services/omas/community-profile/scenarios/finding-a-resource/#see-also","text":"Finding resources by tags License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"See also"},{"location":"services/omas/community-profile/scenarios/finding-a-tag/","text":"Finding a tag \u00b6 Find request, then browse results. Then query? License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Finding a tag"},{"location":"services/omas/community-profile/scenarios/finding-a-tag/#finding-a-tag","text":"Find request, then browse results. Then query? License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Finding a tag"},{"location":"services/omas/community-profile/scenarios/finding-a-team/","text":"Finding a team \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Finding a team"},{"location":"services/omas/community-profile/scenarios/finding-a-team/#finding-a-team","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Finding a team"},{"location":"services/omas/community-profile/scenarios/joining-a-community/","text":"Joining a community \u00b6 Add a to do on community administrators License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Joining a community"},{"location":"services/omas/community-profile/scenarios/joining-a-community/#joining-a-community","text":"Add a to do on community administrators License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Joining a community"},{"location":"services/omas/community-profile/scenarios/leaving-a-community/","text":"Leaving a community \u00b6 Add a to-do on a community administrator License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Leaving a community"},{"location":"services/omas/community-profile/scenarios/leaving-a-community/#leaving-a-community","text":"Add a to-do on a community administrator License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Leaving a community"},{"location":"services/omas/community-profile/scenarios/loading-departmental-structure/","text":"Loading an organization's departmental structure \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Loading departmental structure"},{"location":"services/omas/community-profile/scenarios/loading-departmental-structure/#loading-an-organizations-departmental-structure","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Loading an organization's departmental structure"},{"location":"services/omas/community-profile/scenarios/loading-personal-profiles/","text":"Loading the personal profiles of existing members of an organization \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Loading personal profiles"},{"location":"services/omas/community-profile/scenarios/loading-personal-profiles/#loading-the-personal-profiles-of-existing-members-of-an-organization","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Loading the personal profiles of existing members of an organization"},{"location":"services/omas/community-profile/scenarios/managing-a-to-do/","text":"Managing an action (to do) \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Managing a to do"},{"location":"services/omas/community-profile/scenarios/managing-a-to-do/#managing-an-action-to-do","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Managing an action (to do)"},{"location":"services/omas/community-profile/scenarios/managing-my-contact-details/","text":"Managing my contact details \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Managing my contact details"},{"location":"services/omas/community-profile/scenarios/managing-my-contact-details/#managing-my-contact-details","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Managing my contact details"},{"location":"services/omas/community-profile/scenarios/managing-my-external-references/","text":"Managing My External References \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Managing my external references"},{"location":"services/omas/community-profile/scenarios/managing-my-external-references/#managing-my-external-references","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Managing My External References"},{"location":"services/omas/community-profile/scenarios/managing-my-favorite-assets/","text":"Managing my favorite assets \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Managing my favorite assets"},{"location":"services/omas/community-profile/scenarios/managing-my-favorite-assets/#managing-my-favorite-assets","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Managing my favorite assets"},{"location":"services/omas/community-profile/scenarios/managing-my-favorite-communities/","text":"Managing my favorite communities \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Managing my favorite communities"},{"location":"services/omas/community-profile/scenarios/managing-my-favorite-communities/#managing-my-favorite-communities","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Managing my favorite communities"},{"location":"services/omas/community-profile/scenarios/managing-my-favorite-projects/","text":"Managing my favorite projects \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Managing my favorite projects"},{"location":"services/omas/community-profile/scenarios/managing-my-favorite-projects/#managing-my-favorite-projects","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Managing my favorite projects"},{"location":"services/omas/community-profile/scenarios/managing-my-peer-network/","text":"Managing my peer network \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Managing my peer network"},{"location":"services/omas/community-profile/scenarios/managing-my-peer-network/#managing-my-peer-network","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Managing my peer network"},{"location":"services/omas/community-profile/scenarios/managing-my-resource-list/","text":"Managing My Resource List \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Managing my resource list"},{"location":"services/omas/community-profile/scenarios/managing-my-resource-list/#managing-my-resource-list","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Managing My Resource List"},{"location":"services/omas/community-profile/scenarios/navigating-the-departmental-structure/","text":"Navigating the departmental structure of an organization \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Navigating the departmental structure"},{"location":"services/omas/community-profile/scenarios/navigating-the-departmental-structure/#navigating-the-departmental-structure-of-an-organization","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Navigating the departmental structure of an organization"},{"location":"services/omas/community-profile/scenarios/querying-a-community/","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Querying a community"},{"location":"services/omas/community-profile/scenarios/querying-anothers-personal-profile/","text":"Querying another person's personal profile \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Querying anothers personal profile"},{"location":"services/omas/community-profile/scenarios/querying-anothers-personal-profile/#querying-another-persons-personal-profile","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Querying another person's personal profile"},{"location":"services/omas/community-profile/scenarios/removing-a-community-comment/","text":"Removing a comment from a community \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a community comment"},{"location":"services/omas/community-profile/scenarios/removing-a-community-comment/#removing-a-comment-from-a-community","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a comment from a community"},{"location":"services/omas/community-profile/scenarios/removing-a-community-member/","text":"Removing a community member \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a community member"},{"location":"services/omas/community-profile/scenarios/removing-a-community-member/#removing-a-community-member","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a community member"},{"location":"services/omas/community-profile/scenarios/removing-a-community-note/","text":"Removing a community note \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a community note"},{"location":"services/omas/community-profile/scenarios/removing-a-community-note/#removing-a-community-note","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a community note"},{"location":"services/omas/community-profile/scenarios/removing-a-community-resource/","text":"Removing a resource from a community \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a community resource"},{"location":"services/omas/community-profile/scenarios/removing-a-community-resource/#removing-a-resource-from-a-community","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a resource from a community"},{"location":"services/omas/community-profile/scenarios/removing-a-contribution-from-a-forum/","text":"Removing a Contribution from a Community Forum \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a contribution from a forum"},{"location":"services/omas/community-profile/scenarios/removing-a-contribution-from-a-forum/#removing-a-contribution-from-a-community-forum","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a Contribution from a Community Forum"},{"location":"services/omas/community-profile/scenarios/removing-a-like/","text":"Removing a like \u00b6 This is from anything License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a like"},{"location":"services/omas/community-profile/scenarios/removing-a-like/#removing-a-like","text":"This is from anything License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a like"},{"location":"services/omas/community-profile/scenarios/removing-a-personal-message/","text":"Removing a personal message \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a personal message"},{"location":"services/omas/community-profile/scenarios/removing-a-personal-message/#removing-a-personal-message","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a personal message"},{"location":"services/omas/community-profile/scenarios/removing-a-personal-note/","text":"Removing a personal note \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a personal note"},{"location":"services/omas/community-profile/scenarios/removing-a-personal-note/#removing-a-personal-note","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a personal note"},{"location":"services/omas/community-profile/scenarios/removing-a-review/","text":"Removing a review \u00b6 Can do if own the thing review is being removed from. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a review"},{"location":"services/omas/community-profile/scenarios/removing-a-review/#removing-a-review","text":"Can do if own the thing review is being removed from. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a review"},{"location":"services/omas/community-profile/scenarios/removing-a-tag/","text":"Removing a public tag \u00b6 Can do it if not attached to anything License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a tag"},{"location":"services/omas/community-profile/scenarios/removing-a-tag/#removing-a-public-tag","text":"Can do it if not attached to anything License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a public tag"},{"location":"services/omas/community-profile/scenarios/removing-my-comment-from-a-community/","text":"Removing a comment that I made on a community \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing my comment from a community"},{"location":"services/omas/community-profile/scenarios/removing-my-comment-from-a-community/#removing-a-comment-that-i-made-on-a-community","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a comment that I made on a community"},{"location":"services/omas/community-profile/scenarios/removing-my-comment-from-a-personal-note/","text":"Removing a comment that I made to a personal note \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing my comment from a personal note"},{"location":"services/omas/community-profile/scenarios/removing-my-comment-from-a-personal-note/#removing-a-comment-that-i-made-to-a-personal-note","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a comment that I made to a personal note"},{"location":"services/omas/community-profile/scenarios/removing-my-like-from-a-personal-note/","text":"Removing a like that I made to a personal note \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing my like from a personal note"},{"location":"services/omas/community-profile/scenarios/removing-my-like-from-a-personal-note/#removing-a-like-that-i-made-to-a-personal-note","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing a like that I made to a personal note"},{"location":"services/omas/community-profile/scenarios/removing-my-personal-notes/","text":"Removing my personal notes \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing my personal notes"},{"location":"services/omas/community-profile/scenarios/removing-my-personal-notes/#removing-my-personal-notes","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing my personal notes"},{"location":"services/omas/community-profile/scenarios/removing-my-personal-profile/","text":"Removing my personal profile \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing my personal profile"},{"location":"services/omas/community-profile/scenarios/removing-my-personal-profile/#removing-my-personal-profile","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing my personal profile"},{"location":"services/omas/community-profile/scenarios/removing-my-review/","text":"Removing my review \u00b6 Can do if own the review. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing my review"},{"location":"services/omas/community-profile/scenarios/removing-my-review/#removing-my-review","text":"Can do if own the review. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing my review"},{"location":"services/omas/community-profile/scenarios/removing-my-tag/","text":"Removing one of my private tags \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing my tag"},{"location":"services/omas/community-profile/scenarios/removing-my-tag/#removing-one-of-my-private-tags","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Removing one of my private tags"},{"location":"services/omas/community-profile/scenarios/replying-to-a-comment/","text":"Replying to a comment \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Replying to a comment"},{"location":"services/omas/community-profile/scenarios/replying-to-a-comment/#replying-to-a-comment","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Replying to a comment"},{"location":"services/omas/community-profile/scenarios/replying-to-a-personal-message/","text":"Replying to a personal message \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Replying to a personal message"},{"location":"services/omas/community-profile/scenarios/replying-to-a-personal-message/#replying-to-a-personal-message","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Replying to a personal message"},{"location":"services/omas/community-profile/scenarios/retrieving-my-karma-points/","text":"Retrieving my karma points \u00b6 If an individual has a Personal profile the Community Profile OMAS will reward him/her whenever they contribute to open metadata. These rewards are in the form of karma points . The Community Profile OMAS is responsible for maintaining the count of the karma points. It does this by listening to the metadata changes occurring in the metadata repositories and updates the personal profile of each user making a contribution. The Community Profile OMAS provides a method/operation to allow an individual to retrieve their current karma point total. [Using Java to query my karma points] [Using the REST API to query my karma points] The access service option property \"KarmaPointPlateau\" indicates the multiple of karma points for an individual that results in an external event being published - the default is 500. This means that when an individual gets to 500 karma points, and event is sent, and other event is sent when they get to 1000 karma points and so on. These events can be used to trigger additional recognition activities for the individuals concerned. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Retrieving my karma points"},{"location":"services/omas/community-profile/scenarios/retrieving-my-karma-points/#retrieving-my-karma-points","text":"If an individual has a Personal profile the Community Profile OMAS will reward him/her whenever they contribute to open metadata. These rewards are in the form of karma points . The Community Profile OMAS is responsible for maintaining the count of the karma points. It does this by listening to the metadata changes occurring in the metadata repositories and updates the personal profile of each user making a contribution. The Community Profile OMAS provides a method/operation to allow an individual to retrieve their current karma point total. [Using Java to query my karma points] [Using the REST API to query my karma points] The access service option property \"KarmaPointPlateau\" indicates the multiple of karma points for an individual that results in an external event being published - the default is 500. This means that when an individual gets to 500 karma points, and event is sent, and other event is sent when they get to 1000 karma points and so on. These events can be used to trigger additional recognition activities for the individuals concerned. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Retrieving my karma points"},{"location":"services/omas/community-profile/scenarios/retrieving-my-personal-profile/","text":"Retrieving my personal profile \u00b6 Every user accessing the open metadata ecosystem has a unique user identity (userId). This identity is used for authentication, authorization and auditing of activity related to open metadata and governance. Optionally a user identity can be associated with a personal profile . This provides information about the user behind the userId and aims to improve collaboration across the organization. There are two mechanisms for retrieving your personal profile: Retrieving your personal profile using Java Retrieving your personal profile using the REST API If you do not have a personal profile \u00b6 An organization can choose to load personal profiles automatically Alternatively, it can be left to the individual to create their own personal profile . License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Retrieving my personal profile"},{"location":"services/omas/community-profile/scenarios/retrieving-my-personal-profile/#retrieving-my-personal-profile","text":"Every user accessing the open metadata ecosystem has a unique user identity (userId). This identity is used for authentication, authorization and auditing of activity related to open metadata and governance. Optionally a user identity can be associated with a personal profile . This provides information about the user behind the userId and aims to improve collaboration across the organization. There are two mechanisms for retrieving your personal profile: Retrieving your personal profile using Java Retrieving your personal profile using the REST API","title":"Retrieving my personal profile"},{"location":"services/omas/community-profile/scenarios/retrieving-my-personal-profile/#if-you-do-not-have-a-personal-profile","text":"An organization can choose to load personal profiles automatically Alternatively, it can be left to the individual to create their own personal profile . License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"If you do not have a personal profile"},{"location":"services/omas/community-profile/scenarios/sending-a-personal-message/","text":"Sending a personal message \u00b6 Link through to adding a comment or separate method? License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Sending a personal message"},{"location":"services/omas/community-profile/scenarios/sending-a-personal-message/#sending-a-personal-message","text":"Link through to adding a comment or separate method? License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Sending a personal message"},{"location":"services/omas/community-profile/scenarios/setting-up-my-personal-notes/","text":"Setting up my personal notes \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Setting up my personal notes"},{"location":"services/omas/community-profile/scenarios/setting-up-my-personal-notes/#setting-up-my-personal-notes","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Setting up my personal notes"},{"location":"services/omas/community-profile/scenarios/synchronizing-collaboration-activity/","text":"Synchronizing collaboration activity with another system \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Synchronizing collaboration activity"},{"location":"services/omas/community-profile/scenarios/synchronizing-collaboration-activity/#synchronizing-collaboration-activity-with-another-system","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Synchronizing collaboration activity with another system"},{"location":"services/omas/community-profile/scenarios/synchronizing-departmental-structure/","text":"Synchronizing Departmental Structure \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Synchronizing departmental structure"},{"location":"services/omas/community-profile/scenarios/synchronizing-departmental-structure/#synchronizing-departmental-structure","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Synchronizing Departmental Structure"},{"location":"services/omas/community-profile/scenarios/synchronizing-personal-profiles/","text":"Synchronizing updates to personal profiles from another system \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Synchronizing personal profiles"},{"location":"services/omas/community-profile/scenarios/synchronizing-personal-profiles/#synchronizing-updates-to-personal-profiles-from-another-system","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Synchronizing updates to personal profiles from another system"},{"location":"services/omas/community-profile/scenarios/updating-a-comment/","text":"Updating a comment \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Updating a comment"},{"location":"services/omas/community-profile/scenarios/updating-a-comment/#updating-a-comment","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Updating a comment"},{"location":"services/omas/community-profile/scenarios/updating-a-personal-message/","text":"Updating a personal message \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Updating a personal message"},{"location":"services/omas/community-profile/scenarios/updating-a-personal-message/#updating-a-personal-message","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Updating a personal message"},{"location":"services/omas/community-profile/scenarios/updating-a-personal-note/","text":"Updating a personal note \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Updating a personal note"},{"location":"services/omas/community-profile/scenarios/updating-a-personal-note/#updating-a-personal-note","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Updating a personal note"},{"location":"services/omas/community-profile/scenarios/updating-my-personal-profile/","text":"Updating my personal profile \u00b6 Include adding another userId to the profile See also \u00b6 Managing my contact details License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Updating my personal profile"},{"location":"services/omas/community-profile/scenarios/updating-my-personal-profile/#updating-my-personal-profile","text":"Include adding another userId to the profile","title":"Updating my personal profile"},{"location":"services/omas/community-profile/scenarios/updating-my-personal-profile/#see-also","text":"Managing my contact details License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"See also"},{"location":"services/omas/community-profile/scenarios/updating-my-review/","text":"Updating my review \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Updating my review"},{"location":"services/omas/community-profile/scenarios/updating-my-review/#updating-my-review","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Updating my review"},{"location":"services/omas/community-profile/scenarios/viewing-leaders-of-a-team/","text":"Viewing leaders of a team \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Viewing leaders of a team"},{"location":"services/omas/community-profile/scenarios/viewing-leaders-of-a-team/#viewing-leaders-of-a-team","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Viewing leaders of a team"},{"location":"services/omas/community-profile/scenarios/viewing-members-of-a-team/","text":"Viewing members of a team \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Viewing members of a team"},{"location":"services/omas/community-profile/scenarios/viewing-members-of-a-team/#viewing-members-of-a-team","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Viewing members of a team"},{"location":"services/omas/community-profile/scenarios/watching-a-community/","text":"Watching a community \u00b6 Add yourself as an observer. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Watching a community"},{"location":"services/omas/community-profile/scenarios/watching-a-community/#watching-a-community","text":"Add yourself as an observer. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Watching a community"},{"location":"services/omas/community-profile/user/","text":"Community Profile OMAS User Documentation \u00b6 The Community Profile OMAS is designed to cover many of the administrative tasks relating to managing information about people, teams and communities. Most of the interaction with the Community Profile OMAS will be driven by individuals. All users will be able to manage their personal profile and lists of favourite assets, projects and communities . All users will be able to manage any to dos that have been assigned to them. All users will be able to create a community and administer it. This includes managing members, the communities resources and the notifications sent to the members. All users can also query the communities and teams they are a member of. All users will be able to search for people, teams and communities. There is also support for an administrator to create and delete personal profiles as individuals join and leave the organization and also manage the organization's departmental structure. The assumption is that the organization already has at least one system that manages this information, and so Community Profile OMAS is designed to be integrated with existing systems in order to keep the profiles and departmental structure up-to-date. To understand more see: Configuring the Community Profile OMAS Using the Community Profile OMAS License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/community-profile/user/#community-profile-omas-user-documentation","text":"The Community Profile OMAS is designed to cover many of the administrative tasks relating to managing information about people, teams and communities. Most of the interaction with the Community Profile OMAS will be driven by individuals. All users will be able to manage their personal profile and lists of favourite assets, projects and communities . All users will be able to manage any to dos that have been assigned to them. All users will be able to create a community and administer it. This includes managing members, the communities resources and the notifications sent to the members. All users can also query the communities and teams they are a member of. All users will be able to search for people, teams and communities. There is also support for an administrator to create and delete personal profiles as individuals join and leave the organization and also manage the organization's departmental structure. The assumption is that the organization already has at least one system that manages this information, and so Community Profile OMAS is designed to be integrated with existing systems in order to keep the profiles and departmental structure up-to-date. To understand more see: Configuring the Community Profile OMAS Using the Community Profile OMAS License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Community Profile OMAS User Documentation"},{"location":"services/omas/data-engine/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Data Engine Open Metadata Access Service ( OMAS ) \u00b6 The Data Engine OMAS provides APIs and events for data movement/processing engines to record the changes made to the data landscape. It provides the ability to register the data engine itself along with the lineage details of the ETL transformations. Data Engine OMAS APIs offer support for creating the corresponding open metadata types for assets and jobs. The module structure for the Data Engine OMAS is as follows: data-engine-api supports the common Java classes that are used both by the client and the server. This includes the Java API, beans and REST API structures. data-engine-client supports the Java client library that allows applications and tools to call the remote REST APIs. data-engine-server supports the server side implementation of the access service. This includes the interaction with the administration services for registration, configuration, initialization and termination of the access service. interaction with the repository services to work with open metadata from the cohort . support for the access service's API and its related event management. data-engine-spring supports the REST API using the Spring libraries. Digging Deeper \u00b6 User Documentation Design Documentation","title":"Data Engine OMAS"},{"location":"services/omas/data-engine/#data-engine-open-metadata-access-service-omas","text":"The Data Engine OMAS provides APIs and events for data movement/processing engines to record the changes made to the data landscape. It provides the ability to register the data engine itself along with the lineage details of the ETL transformations. Data Engine OMAS APIs offer support for creating the corresponding open metadata types for assets and jobs. The module structure for the Data Engine OMAS is as follows: data-engine-api supports the common Java classes that are used both by the client and the server. This includes the Java API, beans and REST API structures. data-engine-client supports the Java client library that allows applications and tools to call the remote REST APIs. data-engine-server supports the server side implementation of the access service. This includes the interaction with the administration services for registration, configuration, initialization and termination of the access service. interaction with the repository services to work with open metadata from the cohort . support for the access service's API and its related event management. data-engine-spring supports the REST API using the Spring libraries.","title":"Data Engine Open Metadata Access Service (OMAS)"},{"location":"services/omas/data-engine/#digging-deeper","text":"User Documentation Design Documentation","title":"Digging Deeper"},{"location":"services/omas/data-engine/design/","text":"Data Engine OMAS Design \u00b6 The module structure for the Data Engine OMAS is as follows: data-engine-api supports the common Java classes that are used both by the client and the server. This includes the Java API, beans and REST API structures. data-engine-client supports the Java client library that allows applications and tools to call the remote REST APIs. data-engine-server supports the server side implementation of the access service.This includes the interaction with the administration services for registration, configuration, initialization and termination of the access service. interaction with the repository services to work with open metadata from the cohort . support for the access service's API and its related event management. data-engine-spring supports the REST API using the Spring libraries. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/data-engine/design/#data-engine-omas-design","text":"The module structure for the Data Engine OMAS is as follows: data-engine-api supports the common Java classes that are used both by the client and the server. This includes the Java API, beans and REST API structures. data-engine-client supports the Java client library that allows applications and tools to call the remote REST APIs. data-engine-server supports the server side implementation of the access service.This includes the interaction with the administration services for registration, configuration, initialization and termination of the access service. interaction with the repository services to work with open metadata from the cohort . support for the access service's API and its related event management. data-engine-spring supports the REST API using the Spring libraries. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Data Engine OMAS Design"},{"location":"services/omas/data-engine/samples/collections/","text":"Samples \u00b6 DataEngine-asset-endpoints.postman_collection.json \u00b6 This sample Postman collection illustrates DataEngine OMAS endpoints for creating/updating/deleting assets DataEngine-process-endpoints.postman_collection.json \u00b6 This sample Postman collection illustrates DataEngine OMAS endpoints for creating/updating/deleting processes DataEngineOMAS-local-graph-integration.postman_collection.json \u00b6 This sample Postman collection illustrate configuring and using the DataEngine OMAS with the Egeria graph repository. This script can be used to configure Egeria with Data Engine OMAS and the local graph repository. It can be used to run through a number of different tests of the REST endpoints that Data Engine OMAS exposes. Prerequisites: local-integration-tests.postman_environment.json - the environment used for running the tests locally Egeria by default uses https:// requests with a self-signed certificate. Any PostMan users therefore will need to go into settings->general and turn off 'SSL certificate verification' or requests will fail. Each step is sequentially numbered so that they can be executed in-order as part of a Postman \"Runner\", if desired. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/data-engine/samples/collections/#samples","text":"","title":"Samples"},{"location":"services/omas/data-engine/samples/collections/#dataengine-asset-endpointspostman_collectionjson","text":"This sample Postman collection illustrates DataEngine OMAS endpoints for creating/updating/deleting assets","title":"DataEngine-asset-endpoints.postman_collection.json"},{"location":"services/omas/data-engine/samples/collections/#dataengine-process-endpointspostman_collectionjson","text":"This sample Postman collection illustrates DataEngine OMAS endpoints for creating/updating/deleting processes","title":"DataEngine-process-endpoints.postman_collection.json"},{"location":"services/omas/data-engine/samples/collections/#dataengineomas-local-graph-integrationpostman_collectionjson","text":"This sample Postman collection illustrate configuring and using the DataEngine OMAS with the Egeria graph repository. This script can be used to configure Egeria with Data Engine OMAS and the local graph repository. It can be used to run through a number of different tests of the REST endpoints that Data Engine OMAS exposes. Prerequisites: local-integration-tests.postman_environment.json - the environment used for running the tests locally Egeria by default uses https:// requests with a self-signed certificate. Any PostMan users therefore will need to go into settings->general and turn off 'SSL certificate verification' or requests will fail. Each step is sequentially numbered so that they can be executed in-order as part of a Postman \"Runner\", if desired. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"DataEngineOMAS-local-graph-integration.postman_collection.json"},{"location":"services/omas/data-engine/samples/events/","text":"Samples \u00b6 DataEngine_upsert_events.txt - file with sample event types for upserting entities in Data Engine OMAS \u00b6 DataEngine_delete_events.txt - file with sample event types for deleting entities in Data Engine OMAS \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/data-engine/samples/events/#samples","text":"","title":"Samples"},{"location":"services/omas/data-engine/samples/events/#dataengine_upsert_eventstxt-file-with-sample-event-types-for-upserting-entities-in-data-engine-omas","text":"","title":"DataEngine_upsert_events.txt - file with sample event types for upserting entities in Data Engine OMAS"},{"location":"services/omas/data-engine/samples/events/#dataengine_delete_eventstxt-file-with-sample-event-types-for-deleting-entities-in-data-engine-omas","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"DataEngine_delete_events.txt - file with sample event types for deleting entities in Data Engine OMAS"},{"location":"services/omas/data-engine/scenarios/","text":"Using the Data Engine OMAS \u00b6 Below is the list of tasks supported by Data Engine OMAS. External Tool registration \u00b6 Typically the first action to take for an external tool is to register as a software-server-capability . External Tool lookup \u00b6 An external tool can lookup for the registered external tool. Create Schema Type \u00b6 Create Port Implementation with schema type \u00b6 Create Port Alias with delegation to a Port Implementation \u00b6 Create Process, with corresponding Port Aliases, Port Implementations and Schema Types \u00b6 Add lineage mappings to processes \u00b6 Create Database \u00b6 Create Relational Tables \u00b6 Create Data Files \u00b6 Delete Database \u00b6 Delete Relational Tables \u00b6 Delete Data Files \u00b6 Delete Connections \u00b6 Delete Endpoint \u00b6 Sample use case \u00b6 Initial load use case illustrates the integration between Data Engine OMAS and IBM's DataStage ETL tool. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/data-engine/scenarios/#using-the-data-engine-omas","text":"Below is the list of tasks supported by Data Engine OMAS.","title":"Using the Data Engine OMAS"},{"location":"services/omas/data-engine/scenarios/#external-tool-registration","text":"Typically the first action to take for an external tool is to register as a software-server-capability .","title":"External Tool registration"},{"location":"services/omas/data-engine/scenarios/#external-tool-lookup","text":"An external tool can lookup for the registered external tool.","title":"External Tool lookup"},{"location":"services/omas/data-engine/scenarios/#create-schema-type","text":"","title":"Create Schema Type"},{"location":"services/omas/data-engine/scenarios/#create-port-implementation-with-schema-type","text":"","title":"Create Port Implementation with schema type"},{"location":"services/omas/data-engine/scenarios/#create-port-alias-with-delegation-to-a-port-implementation","text":"","title":"Create Port Alias with delegation to a Port Implementation"},{"location":"services/omas/data-engine/scenarios/#create-process-with-corresponding-port-aliases-port-implementations-and-schema-types","text":"","title":"Create Process, with corresponding Port Aliases, Port Implementations and Schema Types"},{"location":"services/omas/data-engine/scenarios/#add-lineage-mappings-to-processes","text":"","title":"Add lineage mappings to processes"},{"location":"services/omas/data-engine/scenarios/#create-database","text":"","title":"Create Database"},{"location":"services/omas/data-engine/scenarios/#create-relational-tables","text":"","title":"Create Relational Tables"},{"location":"services/omas/data-engine/scenarios/#create-data-files","text":"","title":"Create Data Files"},{"location":"services/omas/data-engine/scenarios/#delete-database","text":"","title":"Delete Database"},{"location":"services/omas/data-engine/scenarios/#delete-relational-tables","text":"","title":"Delete Relational Tables"},{"location":"services/omas/data-engine/scenarios/#delete-data-files","text":"","title":"Delete Data Files"},{"location":"services/omas/data-engine/scenarios/#delete-connections","text":"","title":"Delete Connections"},{"location":"services/omas/data-engine/scenarios/#delete-endpoint","text":"","title":"Delete Endpoint"},{"location":"services/omas/data-engine/scenarios/#sample-use-case","text":"Initial load use case illustrates the integration between Data Engine OMAS and IBM's DataStage ETL tool. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Sample use case"},{"location":"services/omas/data-engine/scenarios/add-lineage-mappings/","text":"Add lineage mappings \u00b6 Add lineage mappings between schema types involved in a ETL transformation. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Add lineage mappings"},{"location":"services/omas/data-engine/scenarios/add-lineage-mappings/#add-lineage-mappings","text":"Add lineage mappings between schema types involved in a ETL transformation. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Add lineage mappings"},{"location":"services/omas/data-engine/scenarios/create-data-files/","text":"Create data files \u00b6 Create a data file with the associated schema, columns and folder hierarchy. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Create data files"},{"location":"services/omas/data-engine/scenarios/create-data-files/#create-data-files","text":"Create a data file with the associated schema, columns and folder hierarchy. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Create data files"},{"location":"services/omas/data-engine/scenarios/create-databases/","text":"Create processes \u00b6 Create a database with the associated schema type. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Create databases"},{"location":"services/omas/data-engine/scenarios/create-databases/#create-processes","text":"Create a database with the associated schema type. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Create processes"},{"location":"services/omas/data-engine/scenarios/create-port-aliases/","text":"Create port aliases \u00b6 Create a port alias with the delegated port implementation and the corresponding port delegation relationship. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Create port aliases"},{"location":"services/omas/data-engine/scenarios/create-port-aliases/#create-port-aliases","text":"Create a port alias with the delegated port implementation and the corresponding port delegation relationship. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Create port aliases"},{"location":"services/omas/data-engine/scenarios/create-port-implementations/","text":"Create port implementations \u00b6 Create a port implementation with the associated schema type and port schema relationship. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Create port implementations"},{"location":"services/omas/data-engine/scenarios/create-port-implementations/#create-port-implementations","text":"Create a port implementation with the associated schema type and port schema relationship. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Create port implementations"},{"location":"services/omas/data-engine/scenarios/create-processes/","text":"Create processes \u00b6 Create a process with the associated ports and process port relationships. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Create processes"},{"location":"services/omas/data-engine/scenarios/create-processes/#create-processes","text":"Create a process with the associated ports and process port relationships. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Create processes"},{"location":"services/omas/data-engine/scenarios/create-relational-tables/","text":"Create relational tables \u00b6 Create a relational table with the associated ports and process port relationships. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Create relational tables"},{"location":"services/omas/data-engine/scenarios/create-relational-tables/#create-relational-tables","text":"Create a relational table with the associated ports and process port relationships. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Create relational tables"},{"location":"services/omas/data-engine/scenarios/create-schema-types/","text":"Create schema types \u00b6 Create a schema type with all the schema attributes and relationships describing the columns involved in a transformation. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Create schema types"},{"location":"services/omas/data-engine/scenarios/create-schema-types/#create-schema-types","text":"Create a schema type with all the schema attributes and relationships describing the columns involved in a transformation. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Create schema types"},{"location":"services/omas/data-engine/scenarios/delete-connections/","text":"Delete connections \u00b6 Delete a connection License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete connections"},{"location":"services/omas/data-engine/scenarios/delete-connections/#delete-connections","text":"Delete a connection License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete connections"},{"location":"services/omas/data-engine/scenarios/delete-data-files/","text":"Delete data files \u00b6 Delete a data file with all the schema attributes and relationships describing the columns. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete data files"},{"location":"services/omas/data-engine/scenarios/delete-data-files/#delete-data-files","text":"Delete a data file with all the schema attributes and relationships describing the columns. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete data files"},{"location":"services/omas/data-engine/scenarios/delete-databases/","text":"Delete databases \u00b6 Delete a database with all the tables attached License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete databases"},{"location":"services/omas/data-engine/scenarios/delete-databases/#delete-databases","text":"Delete a database with all the tables attached License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete databases"},{"location":"services/omas/data-engine/scenarios/delete-endpoints/","text":"Delete endpoints \u00b6 Delete an endpoint License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete endpoints"},{"location":"services/omas/data-engine/scenarios/delete-endpoints/#delete-endpoints","text":"Delete an endpoint License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete endpoints"},{"location":"services/omas/data-engine/scenarios/delete-port-aliases/","text":"Delete port aliases \u00b6 Delete a port alias with the corresponding port delegation relationship. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete port aliases"},{"location":"services/omas/data-engine/scenarios/delete-port-aliases/#delete-port-aliases","text":"Delete a port alias with the corresponding port delegation relationship. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete port aliases"},{"location":"services/omas/data-engine/scenarios/delete-port-implementations/","text":"Delete port implementations \u00b6 Delete a port implementation with the associated schema type and port schema relationship. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete port implementations"},{"location":"services/omas/data-engine/scenarios/delete-port-implementations/#delete-port-implementations","text":"Delete a port implementation with the associated schema type and port schema relationship. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete port implementations"},{"location":"services/omas/data-engine/scenarios/delete-processes/","text":"Delete processes \u00b6 Delete a process with the associated ports and process port relationships. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete processes"},{"location":"services/omas/data-engine/scenarios/delete-processes/#delete-processes","text":"Delete a process with the associated ports and process port relationships. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete processes"},{"location":"services/omas/data-engine/scenarios/delete-relational-tables/","text":"Delete relational tables \u00b6 Delete a relational table with all the schema attributes and relationships describing the columns. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete relational tables"},{"location":"services/omas/data-engine/scenarios/delete-relational-tables/#delete-relational-tables","text":"Delete a relational table with all the schema attributes and relationships describing the columns. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete relational tables"},{"location":"services/omas/data-engine/scenarios/delete-schema-types/","text":"Delete schema types \u00b6 Delete a schema type with all the schema attributes and relationships describing the columns. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete schema types"},{"location":"services/omas/data-engine/scenarios/delete-schema-types/#delete-schema-types","text":"Delete a schema type with all the schema attributes and relationships describing the columns. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Delete schema types"},{"location":"services/omas/data-engine/scenarios/initial-load-igc-data-stage/","text":"Initial load use case \u00b6 Initial load use case shows the integration between Data Engine OMAS and IBM Data Stage. The calls from assets sample collection describe the operations needed for creating the open metadata entities corresponding to the Data Stage ETL job. Note: Data Engine OMAS must have access to the IGC entities. Check egeria-connector-ibm-information-server for more details. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Initial load igc data stage"},{"location":"services/omas/data-engine/scenarios/initial-load-igc-data-stage/#initial-load-use-case","text":"Initial load use case shows the integration between Data Engine OMAS and IBM Data Stage. The calls from assets sample collection describe the operations needed for creating the open metadata entities corresponding to the Data Stage ETL job. Note: Data Engine OMAS must have access to the IGC entities. Check egeria-connector-ibm-information-server for more details. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Initial load use case"},{"location":"services/omas/data-engine/scenarios/lookup-registration-tool/","text":"Lookup an external tool \u00b6 An external tool can lookup for the software server capability entity created at registration step. Request to use is: lookup-external-tool License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Lookup registration tool"},{"location":"services/omas/data-engine/scenarios/lookup-registration-tool/#lookup-an-external-tool","text":"An external tool can lookup for the software server capability entity created at registration step. Request to use is: lookup-external-tool License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Lookup an external tool"},{"location":"services/omas/data-engine/scenarios/register-external-tool/","text":"Registering an external tool \u00b6 For an external tool to submit metadata to Data Engine OMAS it needs to first register. This implies creating a SoftwareServerCapability entity with the properties defining the external tool. Request to use is: register-external-tool License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Register external tool"},{"location":"services/omas/data-engine/scenarios/register-external-tool/#registering-an-external-tool","text":"For an external tool to submit metadata to Data Engine OMAS it needs to first register. This implies creating a SoftwareServerCapability entity with the properties defining the external tool. Request to use is: register-external-tool License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Registering an external tool"},{"location":"services/omas/data-engine/user/","text":"Data Engine OMAS User Documentation \u00b6 The Data Engine OMAS manages the creation of open metadata types for all the assets and jobs that are involved in ETL transformation. Data Engine OMAS offers a Java client and REST API for creating the job metadata. To understand more see: Configuring the Data Engine OMAS Using the Data Engine OMAS License: CC BY 4.0 ,ky Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/data-engine/user/#data-engine-omas-user-documentation","text":"The Data Engine OMAS manages the creation of open metadata types for all the assets and jobs that are involved in ETL transformation. Data Engine OMAS offers a Java client and REST API for creating the job metadata. To understand more see: Configuring the Data Engine OMAS Using the Data Engine OMAS License: CC BY 4.0 ,ky Copyright Contributors to the ODPi Egeria project.","title":"Data Engine OMAS User Documentation"},{"location":"services/omas/data-manager/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Data Manager Open Metadata Access Service ( OMAS ) \u00b6 The Data Manager OMAS provides APIs for technologies wishing to register new data assets, connections and related schema from data resources located in database servers, file systems, event brokers, API gateways and file managers and content managers. The caller of this interface may be the data manager itself, or an integration daemon if the data manager does not support open metadata directly. The integration daemon calls the Data Manager OMAS client through the following integration services . API Integrator OMIS for API Gateways Database Integrator OMIS for database managers Display Integrator OMIS for reports and forms Files Integrator OMIS for file systems and file managers Topic Integrator OMIS for event-based brokers and managers There are specific APIs for different types of data managers and assets. These reflect the terminology typically associated with the specific type of data manager to make it easier for people to map the Data Manager OMAS APIs and events to the actual technology. However, the specific implementation objects supported by these APIs all inherit from common open metadata types so it is possible to work with the resulting metadata in a technology agnostic manner using services such as the Asset Consumer OMAS . Basic metadata model \u00b6 Figure 1 shows the types of metadata captured by the Data Manager OMAS . Figure 1: Basic metadata capture by the Data Manager OMAS These are: Asset - Asset describes the data asset such as the data set, database schema, topic, API etc. Connection , Connector Type and Endpoint are used to create a connector to access the data in the data asset. Schema Element(s) describe the structure of the data managed by the data asset. To make it possible to search for particular types of asset, there are many specialized asset types defined in Egeria. The full list is shown here , however Data Manager OMAS supports the following subtypes of Asset (and any additional subtypes of these types that you wish to define yourself). DeployedAPI for API descriptions. Topic for topics supported by an event manager. DataFile for a file with sub types of: CSVFile for CSV files. AvroFile for files using the Avro format. JSONFile for files using the JSON format. Database for databases. DeployedDatabaseSchema for schemas within a database. DeployedReport for reports. Form for interactive forms. The Data Manager OMAS APIs needs to accommodate slight variations between different vendor implementations of data managers, along with information relating to local deployment standards. As such there is provision in these interfaces to support: VendorProperties for properties unique to a specific vendor implementation, and AdditionalProperties for properties that the metadata team wish to add to the metadata. Data Managers \u00b6 The Data Manager OMAS Supports the following types of data managers: Icon Name Provenance Description File System Local Cohort Create metadata elements for files and folders along with their data connections and any known schema information. Catalogued files and folders are members of the local cohort because many different types of processes may work with them. File Manager External Create metadata elements for files and folders along with their data connections and any known schema information. Catalogued files and folders are members of the data manager's metadata collection because it is responsible for their maintenance. Database Manager External Create metadata elements for databases, database schemas, tables, views, columns, primary keys and foreign keys. Catalogued elements are members of the data manager's metadata collection because it is responsible for their maintenance. Event Broker Local Cohort or External Create metadata elements for topics and the event payloads they support. Catalogued elements are members of the data manager's metadata collection (ie External provenance) if it is responsible for their maintenance. API Manager Local Cohort or External Create metadata elements for APIs and their supported headers and payloads. Catalogued elements are members of the data manager's metadata collection (ie External provenance) if it is responsible for their maintenance. More information \u00b6 What is an Asset? Building an asset catalog Modeling schema structures Design information \u00b6 The module structure for the Data Manager OMAS is as follows: data-manager-client supports the client library. data-manager-api supports the common Java classes that are used both by the client and the server. data-manager-server supports in implementation of the access service and its related event management. data-manager-spring supports the REST API using the Spring libraries. data-manager-topic-connectors supports the connectors used to access the OutTopic events from the Data Manager OMAS .","title":"Data Manager OMAS"},{"location":"services/omas/data-manager/#data-manager-open-metadata-access-service-omas","text":"The Data Manager OMAS provides APIs for technologies wishing to register new data assets, connections and related schema from data resources located in database servers, file systems, event brokers, API gateways and file managers and content managers. The caller of this interface may be the data manager itself, or an integration daemon if the data manager does not support open metadata directly. The integration daemon calls the Data Manager OMAS client through the following integration services . API Integrator OMIS for API Gateways Database Integrator OMIS for database managers Display Integrator OMIS for reports and forms Files Integrator OMIS for file systems and file managers Topic Integrator OMIS for event-based brokers and managers There are specific APIs for different types of data managers and assets. These reflect the terminology typically associated with the specific type of data manager to make it easier for people to map the Data Manager OMAS APIs and events to the actual technology. However, the specific implementation objects supported by these APIs all inherit from common open metadata types so it is possible to work with the resulting metadata in a technology agnostic manner using services such as the Asset Consumer OMAS .","title":"Data Manager Open Metadata Access Service (OMAS)"},{"location":"services/omas/data-manager/#basic-metadata-model","text":"Figure 1 shows the types of metadata captured by the Data Manager OMAS . Figure 1: Basic metadata capture by the Data Manager OMAS These are: Asset - Asset describes the data asset such as the data set, database schema, topic, API etc. Connection , Connector Type and Endpoint are used to create a connector to access the data in the data asset. Schema Element(s) describe the structure of the data managed by the data asset. To make it possible to search for particular types of asset, there are many specialized asset types defined in Egeria. The full list is shown here , however Data Manager OMAS supports the following subtypes of Asset (and any additional subtypes of these types that you wish to define yourself). DeployedAPI for API descriptions. Topic for topics supported by an event manager. DataFile for a file with sub types of: CSVFile for CSV files. AvroFile for files using the Avro format. JSONFile for files using the JSON format. Database for databases. DeployedDatabaseSchema for schemas within a database. DeployedReport for reports. Form for interactive forms. The Data Manager OMAS APIs needs to accommodate slight variations between different vendor implementations of data managers, along with information relating to local deployment standards. As such there is provision in these interfaces to support: VendorProperties for properties unique to a specific vendor implementation, and AdditionalProperties for properties that the metadata team wish to add to the metadata.","title":"Basic metadata model"},{"location":"services/omas/data-manager/#data-managers","text":"The Data Manager OMAS Supports the following types of data managers: Icon Name Provenance Description File System Local Cohort Create metadata elements for files and folders along with their data connections and any known schema information. Catalogued files and folders are members of the local cohort because many different types of processes may work with them. File Manager External Create metadata elements for files and folders along with their data connections and any known schema information. Catalogued files and folders are members of the data manager's metadata collection because it is responsible for their maintenance. Database Manager External Create metadata elements for databases, database schemas, tables, views, columns, primary keys and foreign keys. Catalogued elements are members of the data manager's metadata collection because it is responsible for their maintenance. Event Broker Local Cohort or External Create metadata elements for topics and the event payloads they support. Catalogued elements are members of the data manager's metadata collection (ie External provenance) if it is responsible for their maintenance. API Manager Local Cohort or External Create metadata elements for APIs and their supported headers and payloads. Catalogued elements are members of the data manager's metadata collection (ie External provenance) if it is responsible for their maintenance.","title":"Data Managers"},{"location":"services/omas/data-manager/#more-information","text":"What is an Asset? Building an asset catalog Modeling schema structures","title":"More information"},{"location":"services/omas/data-manager/#design-information","text":"The module structure for the Data Manager OMAS is as follows: data-manager-client supports the client library. data-manager-api supports the common Java classes that are used both by the client and the server. data-manager-server supports in implementation of the access service and its related event management. data-manager-spring supports the REST API using the Spring libraries. data-manager-topic-connectors supports the connectors used to access the OutTopic events from the Data Manager OMAS .","title":"Design information"},{"location":"services/omas/data-privacy/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Data Privacy Open Metadata Access Service (OMAS) \u00b6 The Data Privacy OMAS provides APIs and events for tools that supports the operational side of a data privacy program. This includes: Reviewing the regulations and governance requirements defined in the governance program that related to privacy. Maintaining the definitions for personal data. Retrieving information about the location and protection of personal data. Retrieving information about the digital services in order to assess their compliance to the data privacy program. Recording data processing impact assessments. Managing incidents relating to data privacy. The module structure for the Data Privacy OMAS is as follows: data-privacy-client supports the client libraries for different languages. data-privacy-api supports the common Java classes that are used both by the client and the server. data-privacy-server supports in implementation of the access service and its related event management. data-privacy-spring supports the REST API using the Spring libraries. Return to the access-services module. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Data Privacy OMAS"},{"location":"services/omas/data-privacy/#data-privacy-open-metadata-access-service-omas","text":"The Data Privacy OMAS provides APIs and events for tools that supports the operational side of a data privacy program. This includes: Reviewing the regulations and governance requirements defined in the governance program that related to privacy. Maintaining the definitions for personal data. Retrieving information about the location and protection of personal data. Retrieving information about the digital services in order to assess their compliance to the data privacy program. Recording data processing impact assessments. Managing incidents relating to data privacy. The module structure for the Data Privacy OMAS is as follows: data-privacy-client supports the client libraries for different languages. data-privacy-api supports the common Java classes that are used both by the client and the server. data-privacy-server supports in implementation of the access service and its related event management. data-privacy-spring supports the REST API using the Spring libraries. Return to the access-services module. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Data Privacy Open Metadata Access Service (OMAS)"},{"location":"services/omas/data-science/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Data Science Open Metadata Access Service ( OMAS ) \u00b6 The Data Science OMAS provides APIs and events for tools and applications focused on building all types of analytics models such as predictive models and machine learning models. It provides the ability to define the purpose and requirements for a model, along with lineage and audit information relating to the development and validation process associated with the model. It also supports the packaging of the model into software components for consumption by the Software Developer OMAS , Digital Architecture OMAS and DevOps OMAS since the models ultimately provide the implementation of software components that form part of the implementation of a digital service . Design \u00b6 The module structure for the Data Science OMAS is as follows: data-science-client supports the client library. data-science-api supports the common Java classes that are used both by the client and the server. data-science-server supports in implementation of the access service and its related event management. data-science-spring supports the REST API using the Spring libraries.","title":"Data Science OMAS"},{"location":"services/omas/data-science/#data-science-open-metadata-access-service-omas","text":"The Data Science OMAS provides APIs and events for tools and applications focused on building all types of analytics models such as predictive models and machine learning models. It provides the ability to define the purpose and requirements for a model, along with lineage and audit information relating to the development and validation process associated with the model. It also supports the packaging of the model into software components for consumption by the Software Developer OMAS , Digital Architecture OMAS and DevOps OMAS since the models ultimately provide the implementation of software components that form part of the implementation of a digital service .","title":"Data Science Open Metadata Access Service (OMAS)"},{"location":"services/omas/data-science/#design","text":"The module structure for the Data Science OMAS is as follows: data-science-client supports the client library. data-science-api supports the common Java classes that are used both by the client and the server. data-science-server supports in implementation of the access service and its related event management. data-science-spring supports the REST API using the Spring libraries.","title":"Design"},{"location":"services/omas/design-model/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Design Model Open Metadata Access Service ( OMAS ) \u00b6 The Design Model OMAS supports the management of design model intellectual property that has either been provided as standard or created in a software architecture and design modeling tool. The module structure for the Design Model OMAS is as follows: design-model-client supports the client library. design-model-api supports the common Java classes that are used both by the client and the server. design-model-server supports in implementation of the access service and its related event management. design-model-spring supports the REST API using the Spring libraries.","title":"Design Model OMAS"},{"location":"services/omas/design-model/#design-model-open-metadata-access-service-omas","text":"The Design Model OMAS supports the management of design model intellectual property that has either been provided as standard or created in a software architecture and design modeling tool. The module structure for the Design Model OMAS is as follows: design-model-client supports the client library. design-model-api supports the common Java classes that are used both by the client and the server. design-model-server supports in implementation of the access service and its related event management. design-model-spring supports the REST API using the Spring libraries.","title":"Design Model Open Metadata Access Service (OMAS)"},{"location":"services/omas/dev-ops/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. DevOps Open Metadata Access Service ( OMAS ) \u00b6 The DevOps OMAS provides APIs and events for tools that play a role in a DevOps pipeline. It enables these tools to query information about the assets it is deploying, the infrastructure options and any governance actions that need to be performed. The module structure for the DevOps OMAS is as follows: dev-ops-client supports the client library. dev-ops-api supports the common Java classes that are used both by the client and the server. dev-ops-server supports in implementation of the access service and its related event management. dev-ops-spring supports the REST API using the Spring libraries.","title":"DevOps OMAS"},{"location":"services/omas/dev-ops/#devops-open-metadata-access-service-omas","text":"The DevOps OMAS provides APIs and events for tools that play a role in a DevOps pipeline. It enables these tools to query information about the assets it is deploying, the infrastructure options and any governance actions that need to be performed. The module structure for the DevOps OMAS is as follows: dev-ops-client supports the client library. dev-ops-api supports the common Java classes that are used both by the client and the server. dev-ops-server supports in implementation of the access service and its related event management. dev-ops-spring supports the REST API using the Spring libraries.","title":"DevOps Open Metadata Access Service (OMAS)"},{"location":"services/omas/digital-architecture/","text":"Technical preview Technical preview function is in a state that it can be tried. The development is complete, there is documentation and there are samples, tutorials and hands-on labs as appropriate. The community is looking for feedback on the function before releasing it. This feedback may result in changes to the external interfaces. Digital Architecture Open Metadata Access Service ( OMAS ) \u00b6 The Digital Architecture OMAS provides APIs for tools and applications managing the design of data structures, software and the IT infrastructure that supports the operations of the organization. It is primarily supporting architects and their tools as they are setting up common definitions and standards that help to increase the consistency of the IT landscape. Further information is available below: User Documentation Design Documentation","title":"Digital Architecture OMAS"},{"location":"services/omas/digital-architecture/#digital-architecture-open-metadata-access-service-omas","text":"The Digital Architecture OMAS provides APIs for tools and applications managing the design of data structures, software and the IT infrastructure that supports the operations of the organization. It is primarily supporting architects and their tools as they are setting up common definitions and standards that help to increase the consistency of the IT landscape. Further information is available below: User Documentation Design Documentation","title":"Digital Architecture Open Metadata Access Service (OMAS)"},{"location":"services/omas/digital-architecture/design/","text":"Digital Architecture OMAS Design \u00b6 The module structure for the Digital Architecture OMAS follows the standard pattern as follows: digital-architecture-client supports the client library. digital-architecture-api supports the common Java classes that are used both by the client and the server. digital-architecture-server supports in implementation of the access service and its related event management. digital-architecture-spring supports the REST API using the Spring libraries. It makes use of the ocf-metadata-management for its server side interaction with the metadata repository and so the primary function of the Digital Architecture OMAS is to manage the APIs for the architects and translate between them and the Open Connector Framework (OCF) oriented interfaces of ocf-metadata-management. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/digital-architecture/design/#digital-architecture-omas-design","text":"The module structure for the Digital Architecture OMAS follows the standard pattern as follows: digital-architecture-client supports the client library. digital-architecture-api supports the common Java classes that are used both by the client and the server. digital-architecture-server supports in implementation of the access service and its related event management. digital-architecture-spring supports the REST API using the Spring libraries. It makes use of the ocf-metadata-management for its server side interaction with the metadata repository and so the primary function of the Digital Architecture OMAS is to manage the APIs for the architects and translate between them and the Open Connector Framework (OCF) oriented interfaces of ocf-metadata-management. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Digital Architecture OMAS Design"},{"location":"services/omas/digital-architecture/user/","text":"Digital Architecture OMAS User Documentation \u00b6 License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/digital-architecture/user/#digital-architecture-omas-user-documentation","text":"License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Digital Architecture OMAS User Documentation"},{"location":"services/omas/digital-service/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Digital Service Open Metadata Access Service ( OMAS ) \u00b6 The Digital Service OMAS supports the business capability owners as they track the development and use of digital services that support the operation of the organization. Digital services are business services that are implemented in software. An example of a digital service is one that allows a customer to buy a product from the organization. Digital services are implemented using a variety of software components. These components are defined and deployed using the Software Developer OMAS and DevOps OMAS respectively. The Digital Service OMAS is responsible for: Recording the key business capabilities of the organization - with particular focus on those business capabilities involved in the business transformation. Assigning the owner of each business capability. Linking the business capability to the governance definitions defined by the governance program . For each business capability, defining the digital services that support it. Assigning ownership to each digital service. Supporting the owner of a digital service throughout the digital service's lifecycle. Supporting the business capability owners with the ability to review the status of the digital services within their business capability. Design \u00b6 The module structure for the Digital Service OMAS is as follows: digital-service-api supports the common Java classes that are used both by the client and the server. digital-service-client supports the client library. digital-service-server supports in implementation of the access service and its related event management. digital-service-spring supports the REST API using the Spring libraries.","title":"Digital Service OMAS"},{"location":"services/omas/digital-service/#digital-service-open-metadata-access-service-omas","text":"The Digital Service OMAS supports the business capability owners as they track the development and use of digital services that support the operation of the organization. Digital services are business services that are implemented in software. An example of a digital service is one that allows a customer to buy a product from the organization. Digital services are implemented using a variety of software components. These components are defined and deployed using the Software Developer OMAS and DevOps OMAS respectively. The Digital Service OMAS is responsible for: Recording the key business capabilities of the organization - with particular focus on those business capabilities involved in the business transformation. Assigning the owner of each business capability. Linking the business capability to the governance definitions defined by the governance program . For each business capability, defining the digital services that support it. Assigning ownership to each digital service. Supporting the owner of a digital service throughout the digital service's lifecycle. Supporting the business capability owners with the ability to review the status of the digital services within their business capability.","title":"Digital Service Open Metadata Access Service (OMAS)"},{"location":"services/omas/digital-service/#design","text":"The module structure for the Digital Service OMAS is as follows: digital-service-api supports the common Java classes that are used both by the client and the server. digital-service-client supports the client library. digital-service-server supports in implementation of the access service and its related event management. digital-service-spring supports the REST API using the Spring libraries.","title":"Design"},{"location":"services/omas/discovery-engine/","text":"Technical preview Technical preview function is in a state that it can be tried. The development is complete, there is documentation and there are samples, tutorials and hands-on labs as appropriate. The community is looking for feedback on the function before releasing it. This feedback may result in changes to the external interfaces. Discovery Engine Open Metadata Access Service ( OMAS ) \u00b6 The Discovery Engine OMAS provides APIs and events for metadata discovery tools that are surveying the data landscape and recording information in metadata repositories. These types of tools are called Discovery Engines in the Open Discovery Framework ( ODF ) , which is why this access service is called the Discovery Engine OMAS . The Open Discovery Framework ( ODF ) provides a comprehensive set of open APIs that describe the interaction between metadata discovery tools and a metadata server. The aim is to make it easy for metadata discovery tools to work with open metadata repositories. The capabilities defined in the ODF fall into 4 broad categories. The metadata server APIs - these are implemented by the Discovery Engine OMAS and include: Discovery configuration API - for configuring discovery engines and services - and also retrieving this configuration. Asset catalog API - for finding assets in the metadata repository. Asset store API - for retrieving a specific asset's metadata and connector. Annotation store API - for storing new metadata about the asset. The discovery services - these are the specialist plugin services that each perform a particular type of analysis. These are implemented by the metadata discovery tool (or interface with the discovery tool's APIs to drive specific types of analysis). The discovery engines - these manage the work of a collection of related discovery services. The discovery server - this hosts one or more discovery engines. It provides a REST API to request specific analysis on particular assets, monitor progress of the discovery services and review the results. In Egeria, the discovery server is implemented by the Asset Analysis OMES running in an engine host . Figure 1 shows how these capabilities work together. Figure 1: Interfaces of the Discovery Engine OMAS The engine host server retrieves configuration from the Governance Engine OMAS . When a discovery engine receives a request to analyse an asset, it retrieves the annotations from previous analysis of this asset. While the discovery service is running, it is writing new annotations about the asset through the Discovery Engine OMAS . More details of this processing follows. Discovery Engine Configuration \u00b6 The configuration of the discovery engines and the discovery services that they support are managed in the metadata server through the Governance Engine OMAS . The Engine Host OMAG Server is typically located close to the data assets to minimize the network traffic resulting from the analysis. Where the data assets are distributed in multiple locations, it is possible to deploy an Engine Host server in each location so the discovery workload is kept close to the data. A single Discovery Engine OMAS can support multiple engine hosts deployed in this way. The Asset Analysis OMES on the engine host server is configured with the location of the metadata server where the Discovery Engine OMAS is running along with the names of the discovery engines it will host. The same discovery engine can simultaneously run on multiple engine host servers. This means the Asset Analysis OMES can host all of the discovery engines it needs to analyse the assets at its location. When the Asset Analysis OMES starts in the engine host, it calls the Governance Engine OMAS to retrieve the configuration for each of its discovery engines (see Figure 1, number 1). It also connects to the Governance Engine OMAS 's out topic to receive any updates on this configuration while it is running. Within the discovery engine's configuration are the list of discovery request types it supports that are in turn each linked to the discovery service that should run when one of these discovery types is requested to be run against a specific asset. This is shown in figure 2. Figure 2: Discovery Engine Configuration Processing Discovery Requests \u00b6 When a discovery request is made, the discovery engine creates an instance of the discovery service and gives it access to a discovery context . The discovery context provides access to existing metadata known about the Asset, a connector to access the data stored in the asset and a store to record the new metadata it has discovered about the asset. Behind the scenes, the discovery context is calling the Discovery Engine OMAS to both retrieve metadata about the Asset and its connector (see Figure 1, number 2), and to store the new metadata (Figure 1, number 3). Further Information \u00b6 The Open Discovery Framework ( ODF ) provides more information about the discovery engines and discovery services along with the metadata APIs. In Egeria, both the metadata server where the Discovery Engine OMAS runs and the engine host whether the Asset Analysis OMES runs are types of OMAG Servers . More information on the operation of the engine host can be found under the Engine Services . An overview of automated metadata discovery approaches is available here . Design information \u00b6 The module structure for the Discovery Engine OMAS is as follows: discovery-engine-client supports the client library that is used by the discovery server (and the discovery engines and discovery services it hosts) to access the Discovery Engine OMAS 's REST API and out topic. discovery-engine-api supports the common Java classes that are used both by the client and the server. Since the Open Discovery Framework ( ODF ) defines most of the interfaces for the Discovery Engine OMAS , this module only needs to provide the interfaces associated with the out topic. discovery-engine-server supports in implementation of the metadata interfaces defined by the Open Discovery Framework ( ODF ) and its related event management. discovery-engine-spring supports the REST API using the Spring libraries. This module has no business logic associated with it. Each REST API endpoint delegates immediately to an equivalent function in the server module. It is, however, a useful place to look to get a view of the REST API supported by this OMAS .","title":"Discovery Engine OMAS"},{"location":"services/omas/discovery-engine/#discovery-engine-open-metadata-access-service-omas","text":"The Discovery Engine OMAS provides APIs and events for metadata discovery tools that are surveying the data landscape and recording information in metadata repositories. These types of tools are called Discovery Engines in the Open Discovery Framework ( ODF ) , which is why this access service is called the Discovery Engine OMAS . The Open Discovery Framework ( ODF ) provides a comprehensive set of open APIs that describe the interaction between metadata discovery tools and a metadata server. The aim is to make it easy for metadata discovery tools to work with open metadata repositories. The capabilities defined in the ODF fall into 4 broad categories. The metadata server APIs - these are implemented by the Discovery Engine OMAS and include: Discovery configuration API - for configuring discovery engines and services - and also retrieving this configuration. Asset catalog API - for finding assets in the metadata repository. Asset store API - for retrieving a specific asset's metadata and connector. Annotation store API - for storing new metadata about the asset. The discovery services - these are the specialist plugin services that each perform a particular type of analysis. These are implemented by the metadata discovery tool (or interface with the discovery tool's APIs to drive specific types of analysis). The discovery engines - these manage the work of a collection of related discovery services. The discovery server - this hosts one or more discovery engines. It provides a REST API to request specific analysis on particular assets, monitor progress of the discovery services and review the results. In Egeria, the discovery server is implemented by the Asset Analysis OMES running in an engine host . Figure 1 shows how these capabilities work together. Figure 1: Interfaces of the Discovery Engine OMAS The engine host server retrieves configuration from the Governance Engine OMAS . When a discovery engine receives a request to analyse an asset, it retrieves the annotations from previous analysis of this asset. While the discovery service is running, it is writing new annotations about the asset through the Discovery Engine OMAS . More details of this processing follows.","title":"Discovery Engine Open Metadata Access Service (OMAS)"},{"location":"services/omas/discovery-engine/#discovery-engine-configuration","text":"The configuration of the discovery engines and the discovery services that they support are managed in the metadata server through the Governance Engine OMAS . The Engine Host OMAG Server is typically located close to the data assets to minimize the network traffic resulting from the analysis. Where the data assets are distributed in multiple locations, it is possible to deploy an Engine Host server in each location so the discovery workload is kept close to the data. A single Discovery Engine OMAS can support multiple engine hosts deployed in this way. The Asset Analysis OMES on the engine host server is configured with the location of the metadata server where the Discovery Engine OMAS is running along with the names of the discovery engines it will host. The same discovery engine can simultaneously run on multiple engine host servers. This means the Asset Analysis OMES can host all of the discovery engines it needs to analyse the assets at its location. When the Asset Analysis OMES starts in the engine host, it calls the Governance Engine OMAS to retrieve the configuration for each of its discovery engines (see Figure 1, number 1). It also connects to the Governance Engine OMAS 's out topic to receive any updates on this configuration while it is running. Within the discovery engine's configuration are the list of discovery request types it supports that are in turn each linked to the discovery service that should run when one of these discovery types is requested to be run against a specific asset. This is shown in figure 2. Figure 2: Discovery Engine Configuration","title":"Discovery Engine Configuration"},{"location":"services/omas/discovery-engine/#processing-discovery-requests","text":"When a discovery request is made, the discovery engine creates an instance of the discovery service and gives it access to a discovery context . The discovery context provides access to existing metadata known about the Asset, a connector to access the data stored in the asset and a store to record the new metadata it has discovered about the asset. Behind the scenes, the discovery context is calling the Discovery Engine OMAS to both retrieve metadata about the Asset and its connector (see Figure 1, number 2), and to store the new metadata (Figure 1, number 3).","title":"Processing Discovery Requests"},{"location":"services/omas/discovery-engine/#further-information","text":"The Open Discovery Framework ( ODF ) provides more information about the discovery engines and discovery services along with the metadata APIs. In Egeria, both the metadata server where the Discovery Engine OMAS runs and the engine host whether the Asset Analysis OMES runs are types of OMAG Servers . More information on the operation of the engine host can be found under the Engine Services . An overview of automated metadata discovery approaches is available here .","title":"Further Information"},{"location":"services/omas/discovery-engine/#design-information","text":"The module structure for the Discovery Engine OMAS is as follows: discovery-engine-client supports the client library that is used by the discovery server (and the discovery engines and discovery services it hosts) to access the Discovery Engine OMAS 's REST API and out topic. discovery-engine-api supports the common Java classes that are used both by the client and the server. Since the Open Discovery Framework ( ODF ) defines most of the interfaces for the Discovery Engine OMAS , this module only needs to provide the interfaces associated with the out topic. discovery-engine-server supports in implementation of the metadata interfaces defined by the Open Discovery Framework ( ODF ) and its related event management. discovery-engine-spring supports the REST API using the Spring libraries. This module has no business logic associated with it. Each REST API endpoint delegates immediately to an equivalent function in the server module. It is, however, a useful place to look to get a view of the REST API supported by this OMAS .","title":"Design information"},{"location":"services/omas/governance-engine/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Governance Engine Open Metadata Access Service ( OMAS ) \u00b6 The Governance Engine OMAS supports the implementation of a governance program by providing the metadata services for running governance engines . A governance engine is a collection of related governance services that provide pluggable governance functions. The governance services are implemented as specialist connectors that are defined by: Open Discovery Framework ( ODF ) for Open Discovery Services that analyse the content of resources in the digital landscape. Governance Action Framework ( GAF ) of Governance Action Services that monitor, assess and maintain metadata. The governance services run in the Engine Host OMAG Server supported by the Open Metadata Engine Services ( OMES ) . The Governance Engine OMAS has the following capabilities: Creating the definitions for governance engines and their governance services . Providing the APIs and events that enable the Engine Host OMAG Server to retrieve the definitions of the governance engines and services and be notified of any changes to them. Creating the definitions for governance action processes that control the sequencing of governance actions . Providing APIs to create governance actions explicitly and incident reports . Initiation and choreography of governance actions based on the template provided by a governance actions process . Notification of new governance actions to the Engine Host OMAG Servers that then invoke the appropriate governance services to action them. Supporting the metadata requirements for many of the engine services . Linking the governance actions, governance action processes and governance services to the governance definitions supported by the Governance Program OMAS . Providing APIs to query the status of the governance capabilities implemented through the governance engines. Documentation \u00b6 Governance Engine OMAS has a User Guide that covers the Governance Engine OMAS 's APIs and events. The documentation for writing governance services is located: Open Discovery Framework ( ODF ) for Open Discovery Services. Governance Action Framework ( GAF ) for the Governance Action Services: Watchdog Governance Services, Triage Governance Services, Verification Governance Services, Remediation Governance Services and Provisioning Governance Services. Internals \u00b6 The module structure for the Governance Engine OMAS is as follows: governance-engine-client supports the client library. governance-engine-api supports the common Java classes that are used both by the client and the server. governance-engine-topic-connectors provides access to this modules In and Out Topics. governance-engine-server supports in implementation of the access service and its related event management. governance-engine-spring supports the REST API using the Spring libraries.","title":"Governance Engine OMAS"},{"location":"services/omas/governance-engine/#governance-engine-open-metadata-access-service-omas","text":"The Governance Engine OMAS supports the implementation of a governance program by providing the metadata services for running governance engines . A governance engine is a collection of related governance services that provide pluggable governance functions. The governance services are implemented as specialist connectors that are defined by: Open Discovery Framework ( ODF ) for Open Discovery Services that analyse the content of resources in the digital landscape. Governance Action Framework ( GAF ) of Governance Action Services that monitor, assess and maintain metadata. The governance services run in the Engine Host OMAG Server supported by the Open Metadata Engine Services ( OMES ) . The Governance Engine OMAS has the following capabilities: Creating the definitions for governance engines and their governance services . Providing the APIs and events that enable the Engine Host OMAG Server to retrieve the definitions of the governance engines and services and be notified of any changes to them. Creating the definitions for governance action processes that control the sequencing of governance actions . Providing APIs to create governance actions explicitly and incident reports . Initiation and choreography of governance actions based on the template provided by a governance actions process . Notification of new governance actions to the Engine Host OMAG Servers that then invoke the appropriate governance services to action them. Supporting the metadata requirements for many of the engine services . Linking the governance actions, governance action processes and governance services to the governance definitions supported by the Governance Program OMAS . Providing APIs to query the status of the governance capabilities implemented through the governance engines.","title":"Governance Engine Open Metadata Access Service (OMAS)"},{"location":"services/omas/governance-engine/#documentation","text":"Governance Engine OMAS has a User Guide that covers the Governance Engine OMAS 's APIs and events. The documentation for writing governance services is located: Open Discovery Framework ( ODF ) for Open Discovery Services. Governance Action Framework ( GAF ) for the Governance Action Services: Watchdog Governance Services, Triage Governance Services, Verification Governance Services, Remediation Governance Services and Provisioning Governance Services.","title":"Documentation"},{"location":"services/omas/governance-engine/#internals","text":"The module structure for the Governance Engine OMAS is as follows: governance-engine-client supports the client library. governance-engine-api supports the common Java classes that are used both by the client and the server. governance-engine-topic-connectors provides access to this modules In and Out Topics. governance-engine-server supports in implementation of the access service and its related event management. governance-engine-spring supports the REST API using the Spring libraries.","title":"Internals"},{"location":"services/omas/governance-engine/concepts/","text":"Governance Engine Open Metadata Access Service (OMAS) Concepts \u00b6 The concepts introduced by the Governance Engine OMAS are (in alphabetical order): Governance Action Governance Action Process Governance Action Type Governance Engine Governance Request Type Governance Service Incident Report Return to module overview License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/governance-engine/concepts/#governance-engine-open-metadata-access-service-omas-concepts","text":"The concepts introduced by the Governance Engine OMAS are (in alphabetical order): Governance Action Governance Action Process Governance Action Type Governance Engine Governance Request Type Governance Service Incident Report Return to module overview License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Governance Engine Open Metadata Access Service (OMAS) Concepts"},{"location":"services/omas/governance-engine/concepts/governance-action-process/","text":"Governance Action Process \u00b6 A governance action process is a predefined sequence of governance actions that are coordinated by the Governance Engine OMAS . The steps in a governance action process are defined by linked governance action types . Each governance action type provides the specification of the governance action to run. The links between then show which guards cause the governance action to run. Details of how to set up governance action process is described in the Governance Engine OMAS User Guide . Open metadata types \u00b6 0462 Governance Action Types shows the structure of the incident report. It is a Referenceable so it can support comments and have governance actions linked to it. Further information \u00b6 The Open Metadata Engine Services (OMES) provide the mechanisms that support the different types of governance engines . These engines run the governance services that execute the governance actions defined by the governance action process. Return to Governance Engine OMAS Concepts Return to Governance Engine OMAS Overview License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Governance action process"},{"location":"services/omas/governance-engine/concepts/governance-action-process/#governance-action-process","text":"A governance action process is a predefined sequence of governance actions that are coordinated by the Governance Engine OMAS . The steps in a governance action process are defined by linked governance action types . Each governance action type provides the specification of the governance action to run. The links between then show which guards cause the governance action to run. Details of how to set up governance action process is described in the Governance Engine OMAS User Guide .","title":"Governance Action Process"},{"location":"services/omas/governance-engine/concepts/governance-action-process/#open-metadata-types","text":"0462 Governance Action Types shows the structure of the incident report. It is a Referenceable so it can support comments and have governance actions linked to it.","title":"Open metadata types"},{"location":"services/omas/governance-engine/concepts/governance-action-process/#further-information","text":"The Open Metadata Engine Services (OMES) provide the mechanisms that support the different types of governance engines . These engines run the governance services that execute the governance actions defined by the governance action process. Return to Governance Engine OMAS Concepts Return to Governance Engine OMAS Overview License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Further information"},{"location":"services/omas/governance-engine/concepts/governance-action-type/","text":"Governance Action Type \u00b6 A governance action type is a template for a governance action . A set of linked governance action types form the definition of a governance action process . Governance action types are defined through the Governance Engine OMAS and this OMAS also coordinates the create of a governance action from the governance action type as part of its execution of the governance action process. Open metadata type definition \u00b6 The governance action type is defined in the 0462 Governance Action Type model of the Open Metadata Types. Return to Governance Engine OMAS Concepts Return to Governance Engine OMAS Overview License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Governance action type"},{"location":"services/omas/governance-engine/concepts/governance-action-type/#governance-action-type","text":"A governance action type is a template for a governance action . A set of linked governance action types form the definition of a governance action process . Governance action types are defined through the Governance Engine OMAS and this OMAS also coordinates the create of a governance action from the governance action type as part of its execution of the governance action process.","title":"Governance Action Type"},{"location":"services/omas/governance-engine/concepts/governance-action-type/#open-metadata-type-definition","text":"The governance action type is defined in the 0462 Governance Action Type model of the Open Metadata Types. Return to Governance Engine OMAS Concepts Return to Governance Engine OMAS Overview License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Open metadata type definition"},{"location":"services/omas/governance-engine/concepts/governance-action/","text":"Governance Action \u00b6 A governance action describes a specific governance activity that needs to be performed on one or more metadata elements, or their counterparts in the digital landscape. A governance action is represented as a metadata entity in the open metadata repositories and linked to: The source (cause) of the governance action. The target elements that need to be acted upon. The governance engine that will run the governance service that implements the desired behavior. The governance action metadata entity is used to coordinate the desired activity in the governance engine, record its current state and act as a record of the activity for future audits. Governance actions can be created through the Governance Engine OMAS API . Some governance services (for example, the Watchdog Governance Action Service ) can create governance actions when they run. Governance services produce output strings called guards that indicate specific conditions or outcomes. These guards can be used to trigger new governance actions. Triggered governance actions are linked to their predecessor so it possible to trace through the governance actions that ran. The governance action process defines the flow of governance actions. It uses governance action types to build up a template of possible governance actions linked via the guards. When the process runs, its linked governance action types control the triggering of new governance actions. If the start date of the governance action is in the future, the Engine Host Services running in the same Engine Host OMAG Server as the nominated governance engine will schedule the governance service to run soon after the requested start date. If the start date is left blank, the requested governance service is run as soon as possible. Return to Governance Engine OMAS Concepts Return to Governance Engine OMAS Overview License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Governance action"},{"location":"services/omas/governance-engine/concepts/governance-action/#governance-action","text":"A governance action describes a specific governance activity that needs to be performed on one or more metadata elements, or their counterparts in the digital landscape. A governance action is represented as a metadata entity in the open metadata repositories and linked to: The source (cause) of the governance action. The target elements that need to be acted upon. The governance engine that will run the governance service that implements the desired behavior. The governance action metadata entity is used to coordinate the desired activity in the governance engine, record its current state and act as a record of the activity for future audits. Governance actions can be created through the Governance Engine OMAS API . Some governance services (for example, the Watchdog Governance Action Service ) can create governance actions when they run. Governance services produce output strings called guards that indicate specific conditions or outcomes. These guards can be used to trigger new governance actions. Triggered governance actions are linked to their predecessor so it possible to trace through the governance actions that ran. The governance action process defines the flow of governance actions. It uses governance action types to build up a template of possible governance actions linked via the guards. When the process runs, its linked governance action types control the triggering of new governance actions. If the start date of the governance action is in the future, the Engine Host Services running in the same Engine Host OMAG Server as the nominated governance engine will schedule the governance service to run soon after the requested start date. If the start date is left blank, the requested governance service is run as soon as possible. Return to Governance Engine OMAS Concepts Return to Governance Engine OMAS Overview License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Governance Action"},{"location":"services/omas/governance-engine/concepts/governance-engine/","text":"Governance Engine \u00b6 A governance engine is responsible for executing requests to a collection of related governance services . The implementation of a governance engine is handled by an Open Metadata Engine Service (OMES) running in an Engine Host OMAG Server. There is a specific engine service for each type of governance engine/service pair. Governance Engines \u00b6 Governance engines define a collection of related governance services. Governance services are specialized connectors that implement a single specialized governance activity. There are six types of governance service: Open Discovery Service for analysing the content of an Asset's real-world counterpart in the digital landscape. (For example, if the asset describes a file, the discovery service analyses the data stored in the file). Open Watchdog Service for monitoring changes to open metadata elements and when certain changes occur (such as the creation of a new Asset ) the watchdog service requests action from other governance services by creating either a Governance Action , a Governance Action Process or an Incident Report . Open Verification Service for testing the properties of specific open metadata elements to ensure they are set up correctly or do not indicate a situation where governance activity is required. The results returned from the verification service can be used to trigger other governance services as part of a Governance Action Process . Open Triage Service for making decisions on how to handle a specific situation or incident. Often this involves a human decision maker. Open Remediation Service for correcting errors in open metadata or the digital landscape it represents. Open Provisioning Service for configuring, enabling, provisioning resources in the digital landscape. Often these provisioning services manage the cataloguing of new assets and the lineage between them. There is a different Open Metadata Engine Service (OMES) depending on the type of governance service. The engine services support the specialist REST APIs and event handling needed for the specific type of governance service. Governance Service Engine Service Open Discovery Service Asset Analysis OMES Watchdog Governance Service Governance Action OMES Verification Governance Service Governance Action OMES Triage Governance Service Governance Action OMES Remediation Governance Service Governance Action OMES Provisioning Governance Service Governance Action OMES Each governance engine has a unique name. A governance engine definition for this unique name is created using the Governance Engine OMAS API . Figure 1 shows the structure of a governance engine definition. The open metadata types for this definition are in model 0461 - Governance Engines (see Governance Engine , GovernanceService linked by the SupportedGovernanceService relationship. Figure 1: The structure of a governance engine definition When a governance engine is called, it is passed a request type and request parameters. This is mapped to a call to a governance service . Return to Governance Engine OMAS Concepts Return to Governance Engine OMAS Overview License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Governance engine"},{"location":"services/omas/governance-engine/concepts/governance-engine/#governance-engine","text":"A governance engine is responsible for executing requests to a collection of related governance services . The implementation of a governance engine is handled by an Open Metadata Engine Service (OMES) running in an Engine Host OMAG Server. There is a specific engine service for each type of governance engine/service pair.","title":"Governance Engine"},{"location":"services/omas/governance-engine/concepts/governance-engine/#governance-engines","text":"Governance engines define a collection of related governance services. Governance services are specialized connectors that implement a single specialized governance activity. There are six types of governance service: Open Discovery Service for analysing the content of an Asset's real-world counterpart in the digital landscape. (For example, if the asset describes a file, the discovery service analyses the data stored in the file). Open Watchdog Service for monitoring changes to open metadata elements and when certain changes occur (such as the creation of a new Asset ) the watchdog service requests action from other governance services by creating either a Governance Action , a Governance Action Process or an Incident Report . Open Verification Service for testing the properties of specific open metadata elements to ensure they are set up correctly or do not indicate a situation where governance activity is required. The results returned from the verification service can be used to trigger other governance services as part of a Governance Action Process . Open Triage Service for making decisions on how to handle a specific situation or incident. Often this involves a human decision maker. Open Remediation Service for correcting errors in open metadata or the digital landscape it represents. Open Provisioning Service for configuring, enabling, provisioning resources in the digital landscape. Often these provisioning services manage the cataloguing of new assets and the lineage between them. There is a different Open Metadata Engine Service (OMES) depending on the type of governance service. The engine services support the specialist REST APIs and event handling needed for the specific type of governance service. Governance Service Engine Service Open Discovery Service Asset Analysis OMES Watchdog Governance Service Governance Action OMES Verification Governance Service Governance Action OMES Triage Governance Service Governance Action OMES Remediation Governance Service Governance Action OMES Provisioning Governance Service Governance Action OMES Each governance engine has a unique name. A governance engine definition for this unique name is created using the Governance Engine OMAS API . Figure 1 shows the structure of a governance engine definition. The open metadata types for this definition are in model 0461 - Governance Engines (see Governance Engine , GovernanceService linked by the SupportedGovernanceService relationship. Figure 1: The structure of a governance engine definition When a governance engine is called, it is passed a request type and request parameters. This is mapped to a call to a governance service . Return to Governance Engine OMAS Concepts Return to Governance Engine OMAS Overview License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Governance Engines"},{"location":"services/omas/governance-engine/concepts/governance-request-type/","text":"Governance Request Type \u00b6 The governance request type defines the descriptive name of a specific governance activity that the organization wishes to run. The request type is mapped to a governance service implementation along with request parameters to configure the behaviour of the service in a governance engine definition as shown in Figure 1. Figure 1: Governance request types as part of a governance engine definition Governance services are run by the Open Metadata Engine Services (OMES) in an Engine Host OMAG Server. The Engine Host Services called the Governance Engine OMAS They are used by the Governance Engines to determine which Governance Service to run. Related Information \u00b6 The Open Metadata Types model 0461 Governance Action Engines shows how the request type links the governance engine to the governance service via the SupportedGovernanceService relationship. Return to Governance Engine OMAS Concepts Return to Governance Engine OMAS Overview License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Governance request type"},{"location":"services/omas/governance-engine/concepts/governance-request-type/#governance-request-type","text":"The governance request type defines the descriptive name of a specific governance activity that the organization wishes to run. The request type is mapped to a governance service implementation along with request parameters to configure the behaviour of the service in a governance engine definition as shown in Figure 1. Figure 1: Governance request types as part of a governance engine definition Governance services are run by the Open Metadata Engine Services (OMES) in an Engine Host OMAG Server. The Engine Host Services called the Governance Engine OMAS They are used by the Governance Engines to determine which Governance Service to run.","title":"Governance Request Type"},{"location":"services/omas/governance-engine/concepts/governance-request-type/#related-information","text":"The Open Metadata Types model 0461 Governance Action Engines shows how the request type links the governance engine to the governance service via the SupportedGovernanceService relationship. Return to Governance Engine OMAS Concepts Return to Governance Engine OMAS Overview License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Related Information"},{"location":"services/omas/governance-engine/concepts/governance-service/","text":"Governance Service \u00b6 A governance service is a specialized connector that implements a specific governance activity. There are six types of governance services: Open Discovery Service for analysing the content of an Asset's real-world counterpart in the digital landscape. (For example, if the asset describes a file, the discovery service analyses the data stored in the file). Watchdog Governance Service for monitoring changes to open metadata elements and when certain changes occur (such as the creation of a new Asset ) the watchdog service requests action from other governance services by creating either a Governance Action , a Governance Action Process or an Incident Report . Verification Governance Service for testing the properties of specific open metadata elements to ensure they are set up correctly or do not indicate a situation where governance activity is required. The results returned from the verification service can be used to trigger other governance services as part of a Governance Action Process . Triage Governance Service for making decisions on how to handle a specific situation or incident. Often this involves a human decision maker. Remediation Governance Service for correcting errors in open metadata or the digital landscape it represents. Provisioning Governance Service for configuring, enabling, provisioning resources in the digital landscape. Often these provisioning services manage the cataloguing of new assets and the lineage between them. The Governance Action Open Metadata Engine Service (OMES) supports the execution of the governance action service. It supports the specialist REST APIs and event handling needed for the specific type of governance action service. Governance Service Engine Service Open Discovery Service Asset Analysis OMES Watchdog Governance Service Governance Action OMES Verification Governance Service Governance Action OMES Triage Governance Service Governance Action OMES Remediation Governance Service Governance Action OMES Provisioning Governance Service Governance Action OMES Support for implementing governance services \u00b6 The interface for the Open Discovery Service is defined by the Open Discovery Framework (ODF) and the rest are defined by the Governance Action Framework (GAF) . These frameworks provide the guidance to developers of new governance services. Support for running governance services \u00b6 Related governance services are configured together as a governance engine and they run in the appropriate Open Metadata Engine Service (OMES) . The Governance Engine OMAS provides: * The API to create governance engine definitions for the governance services. * The API to link governance services together into governance action processes . * The metadata support for the Engine Host Services to drive the governance services in an Engine Host OMAG Server. Return to Governance Engine OMAS Concepts Return to Governance Engine OMAS Overview License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Governance service"},{"location":"services/omas/governance-engine/concepts/governance-service/#governance-service","text":"A governance service is a specialized connector that implements a specific governance activity. There are six types of governance services: Open Discovery Service for analysing the content of an Asset's real-world counterpart in the digital landscape. (For example, if the asset describes a file, the discovery service analyses the data stored in the file). Watchdog Governance Service for monitoring changes to open metadata elements and when certain changes occur (such as the creation of a new Asset ) the watchdog service requests action from other governance services by creating either a Governance Action , a Governance Action Process or an Incident Report . Verification Governance Service for testing the properties of specific open metadata elements to ensure they are set up correctly or do not indicate a situation where governance activity is required. The results returned from the verification service can be used to trigger other governance services as part of a Governance Action Process . Triage Governance Service for making decisions on how to handle a specific situation or incident. Often this involves a human decision maker. Remediation Governance Service for correcting errors in open metadata or the digital landscape it represents. Provisioning Governance Service for configuring, enabling, provisioning resources in the digital landscape. Often these provisioning services manage the cataloguing of new assets and the lineage between them. The Governance Action Open Metadata Engine Service (OMES) supports the execution of the governance action service. It supports the specialist REST APIs and event handling needed for the specific type of governance action service. Governance Service Engine Service Open Discovery Service Asset Analysis OMES Watchdog Governance Service Governance Action OMES Verification Governance Service Governance Action OMES Triage Governance Service Governance Action OMES Remediation Governance Service Governance Action OMES Provisioning Governance Service Governance Action OMES","title":"Governance Service"},{"location":"services/omas/governance-engine/concepts/governance-service/#support-for-implementing-governance-services","text":"The interface for the Open Discovery Service is defined by the Open Discovery Framework (ODF) and the rest are defined by the Governance Action Framework (GAF) . These frameworks provide the guidance to developers of new governance services.","title":"Support for implementing governance services"},{"location":"services/omas/governance-engine/concepts/governance-service/#support-for-running-governance-services","text":"Related governance services are configured together as a governance engine and they run in the appropriate Open Metadata Engine Service (OMES) . The Governance Engine OMAS provides: * The API to create governance engine definitions for the governance services. * The API to link governance services together into governance action processes . * The metadata support for the Engine Host Services to drive the governance services in an Engine Host OMAG Server. Return to Governance Engine OMAS Concepts Return to Governance Engine OMAS Overview License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Support for running governance services"},{"location":"services/omas/governance-engine/concepts/incident-report/","text":"Incident Report \u00b6 An incident report provides a record of an error or situation that needs some governance action. Return to Governance Engine OMAS Concepts Return to Governance Engine OMAS Overview License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Incident report"},{"location":"services/omas/governance-engine/concepts/incident-report/#incident-report","text":"An incident report provides a record of an error or situation that needs some governance action. Return to Governance Engine OMAS Concepts Return to Governance Engine OMAS Overview License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Incident Report"},{"location":"services/omas/governance-engine/user/","text":"Governance Engine Open Metadata Access Service (OMAS) User Guide \u00b6 Defining Governance Engines and Governance Services \u00b6 Figure 1: Governance request types as part of a governance engine definition Defining Governance Action Processes \u00b6 Return to the module overview . License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Index"},{"location":"services/omas/governance-engine/user/#governance-engine-open-metadata-access-service-omas-user-guide","text":"","title":"Governance Engine Open Metadata Access Service (OMAS) User Guide"},{"location":"services/omas/governance-engine/user/#defining-governance-engines-and-governance-services","text":"Figure 1: Governance request types as part of a governance engine definition","title":"Defining Governance Engines and Governance Services"},{"location":"services/omas/governance-engine/user/#defining-governance-action-processes","text":"Return to the module overview . License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Defining Governance Action Processes"},{"location":"services/omas/governance-program/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Governance Program Open Metadata Access Service ( OMAS ) \u00b6 The Governance Program OMAS provides APIs and events for tools and applications focused on defining a data strategy, planning support for a regulation and/or developing a governance program for the data landscape. It assumes an organization is operating an active governance program that is iteratively reviewed and developed. It covers: Understanding the business drivers and regulations that provide the motivation and direction to the governance program. Laying down the governance policies (principles, obligations and approaches) that frame the governance program. Planning and defining the governance controls that detail how these governance policies will be implemented in the organization, and enumerating the implications of these decisions and the expected outcomes. Defining the organization's roles and responsibilities that will support the governance program. Defining the classifications and governance zones that will organize the assets being governed. Defining the subject areas that will organize the data-oriented definitions such as glossary terms, valid values and quality rules. Reviewing the impact of the governance program. adjusting governance definitions and metrics as necessary. Reviewing the strategy, business and regulatory landscape. adjusting the governance definitions and metrics as necessary. Related OMASs \u00b6 The Community Profile OMAS supports the definition of the profiles for people and teams that will support the governance program. These are linked to the governance roles defined by the governance program. The Project Management OMAS supports the rollout of the governance program by commissioning campaigns and projects to implement the governance controls and the collection of measurements to assess the success of the program. The Digital Architecture OMAS provides the set up of the digital landscape that supports the governance program. This includes the definitions of the information supply chains and solution components that support the organization's activities. The Digital Service OMAS documents the business capabilities along with their digital services that are supported by the governance program. The Governance Engine OMAS supports the implementation of technical controls and the choreography of their execution. The Stewardship Action OMAS supports the stewards as they manage the exceptions detected to the governance program. The Data Privacy OMAS supports the operational aspects of managing privacy as part of the organization's activities. The Subject Area OMAS supports the definitions of the vocabularies associated with a subject area. The Data Manager OMAS support the automated cataloging of assets and configuration of technology that is managing them. The Security Manager OMAS support the configuration of technology that is managing the security of assets. The Security Officer OMAS support the definitions of users and groups and related definitions that make up the user directory. The Asset Manager OMAS supports the automated exchange of governance definitions between catalogs and asset managers to create a consistent rollout of governance requirements. The Asset Owner OMAS supports the linking of governance definitions and classifications to assets to define how they should be governed. The Asset Consumer OMAS supports the visibility of the governance definitions and classification by consumers of the assets. Design Information \u00b6 The Governance Program OMAS provides both a Java and a REST API for managing the definitions for a governance program. It has the following modules: governance-program-api defines the Java API and the common Java classes that are used both by the client and the server. governance-program-client supports the client library. This is used by tools that help organizations to plan and manage their governance program. governance-program-server supports in implementation of the access service and its related event management. governance-program-spring supports the REST API using the Spring libraries.","title":"Governance Program OMAS"},{"location":"services/omas/governance-program/#governance-program-open-metadata-access-service-omas","text":"The Governance Program OMAS provides APIs and events for tools and applications focused on defining a data strategy, planning support for a regulation and/or developing a governance program for the data landscape. It assumes an organization is operating an active governance program that is iteratively reviewed and developed. It covers: Understanding the business drivers and regulations that provide the motivation and direction to the governance program. Laying down the governance policies (principles, obligations and approaches) that frame the governance program. Planning and defining the governance controls that detail how these governance policies will be implemented in the organization, and enumerating the implications of these decisions and the expected outcomes. Defining the organization's roles and responsibilities that will support the governance program. Defining the classifications and governance zones that will organize the assets being governed. Defining the subject areas that will organize the data-oriented definitions such as glossary terms, valid values and quality rules. Reviewing the impact of the governance program. adjusting governance definitions and metrics as necessary. Reviewing the strategy, business and regulatory landscape. adjusting the governance definitions and metrics as necessary.","title":"Governance Program Open Metadata Access Service (OMAS)"},{"location":"services/omas/governance-program/#related-omass","text":"The Community Profile OMAS supports the definition of the profiles for people and teams that will support the governance program. These are linked to the governance roles defined by the governance program. The Project Management OMAS supports the rollout of the governance program by commissioning campaigns and projects to implement the governance controls and the collection of measurements to assess the success of the program. The Digital Architecture OMAS provides the set up of the digital landscape that supports the governance program. This includes the definitions of the information supply chains and solution components that support the organization's activities. The Digital Service OMAS documents the business capabilities along with their digital services that are supported by the governance program. The Governance Engine OMAS supports the implementation of technical controls and the choreography of their execution. The Stewardship Action OMAS supports the stewards as they manage the exceptions detected to the governance program. The Data Privacy OMAS supports the operational aspects of managing privacy as part of the organization's activities. The Subject Area OMAS supports the definitions of the vocabularies associated with a subject area. The Data Manager OMAS support the automated cataloging of assets and configuration of technology that is managing them. The Security Manager OMAS support the configuration of technology that is managing the security of assets. The Security Officer OMAS support the definitions of users and groups and related definitions that make up the user directory. The Asset Manager OMAS supports the automated exchange of governance definitions between catalogs and asset managers to create a consistent rollout of governance requirements. The Asset Owner OMAS supports the linking of governance definitions and classifications to assets to define how they should be governed. The Asset Consumer OMAS supports the visibility of the governance definitions and classification by consumers of the assets.","title":"Related OMASs"},{"location":"services/omas/governance-program/#design-information","text":"The Governance Program OMAS provides both a Java and a REST API for managing the definitions for a governance program. It has the following modules: governance-program-api defines the Java API and the common Java classes that are used both by the client and the server. governance-program-client supports the client library. This is used by tools that help organizations to plan and manage their governance program. governance-program-server supports in implementation of the access service and its related event management. governance-program-spring supports the REST API using the Spring libraries.","title":"Design Information"},{"location":"services/omas/it-infrastructure/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. IT Infrastructure Open Metadata Access Service ( OMAS ) \u00b6 The IT Infrastructure OMAS provides APIs for tools and applications managing the IT infrastructure that supports the data assets and software. It is typically used by the Infrastructure Integrator OMIS to capture metadata from deployment artifacts, or to generate deployment artifacts from open metadata. The other major use of IT Infrastructure OMAS is to manually catalog the key pieces of IT Infrastructure used by an organization. Often the creation of this metadata is used to kick of the automated cataloging of the capabilities and assets associated with the infrastructure. Basic Concepts \u00b6 At the base is the notion of a Host . This could be: * BareMetalComputer - traditional computer hardware or * VirtualMachine - a virtualized machine (such as VMWare or VirtualBox) that uses a hypervisor to virtualize machine hardware or * VirtualContainer - a software system using a container library to virtualize the operating system it uses or * HostCluster - a cluster of Hosts that are operating as a single operational environment (such as a Hadoop cluster or kubernetes cluster). Hosts are composable and reusable. For example, figure 1 shows a Virtual Container that is deployed to two Bare Metal Computers. The relationship between them is called HostedHost . Figure 1: A virtual container deployed to two bare metal computers Figure 2 shows an example of Host Cluster, such as a Hadoop Cluster, that manages multiple Hosts (Bare Metal Computers in this example). The relationship between the Host Cluster and each subordinate Host is HostClusterMember . The host cluster may not have all of its members active. The membership denotes the pool of resources that the cluster has to work with. Figure 2: A cluster of bare metal computers operating as single host A Host typically runs an operating system and may have various hardware characteristics of interest (such as byte ordering). Collectively this information is called the OperatingPlatform . The Operating Platform can be linked to many hosts through the HostOperatingPlatform relationship. This is shown in figure 2 and figure 3. It documents that these hosts are running the software identified by the operating platform. This is particularly useful if you using standard software stack builds that are deployed to multiple hosts since it is easy to see which hosts are at which levels of the software and to manage the rollout of upgrades to the stack. Figure 3: A collection of hosts using the same operating platform The SoftwarePackageManifest shown in figure 4 details the software stack for the operating platform. It is represented as a Collection linked to the operating platform using the OperatingPlatformManifest relationship. Figure 4: The software stack definition for an operating platform Linked to a Host via the SoftwareServerPlatformDeployment are the SoftwareServerPlatform s. They describe the running processes running on the Host that use software described in the Operating Platform. This is shown in figure 5. Figure 5: The software server platforms running on the host The SoftwareServerPlatform itself may run one or many SoftwareServers where a Software Server is a collection of SoftwareServerCapabilities that access the operating system through the software server platform. The Software Server Capabilities deliver the function of the server. They typically host/manage assets such as DataSets , DataStores and Processes that are linked to the Software Server Capability using the ServerAssetUse relationship. Some technologies are written as a single stack. This means there is a single Software Server for the Software Server Platform. Figure 6 shows the structure of the metadata elements for a technology - such as Apache Kafka or a Database Server - that is a single stack. Figure 6: The metadata elements to represent a single stack technology Figure 7 shows the representation of the stack of a platform that allows multiple servers to be configured. Egeria's OMAG Server Platform is an example of this type of technology. The Software Servers are OMAG Servers and the registered services: Open Metadata Access Services (OMASs), Open Metadata Engine Services (OMESs), Open Metadata Integration Services (OMISs) and Open Metadata View Services (OMVSs) are Software Server Capabilities of types: MetadataAccessService , EngineHostingService , MetadataIntegrationService and UserViewService respectively. Figure 7: The metadata elements to represent a configurable software server platform Software Server Capabilities \u00b6 There are many types of software server capability. The list below contains the Software Server Capabilities defined in the open metadata types. These are the software server capabilities defined in the open types: APIManager - A capability that manages callable APIs that typically delegate onto Software Services. Application - A capability supporting a specific business function. Catalog - A capability that manages collections of descriptions about people, places, digital assets, things, ... DataManager - A capability that manages collections of data. Engine - A programmable engine for running automated processes. Workflow Engine - An engine capable of running a mixture of human and automated tasks as part of a workflow process. Reporting Engine - An engine capable of creating reports by combining information from multiple data sets. Analytics Engine - An engine capable of running analytics models using data from one or more data sets. Data Movement Engine - An engine capable of copying data from one data store to another. Data Virtualization Engine - An engine capable of creating new data sets by dynamically combining data from one or more data stores or data sets. EventBroker - A capability that supports event-based services, typically around topics. Software Services - A capability that provides externally callable functions to other services. Application Service - A software service that supports a reusable business function. Metadata Integration Service - A software service that exchanges metadata between servers. Metadata Access Service - A software service that provides access to stored metadata. Engine Hosting Service - A software service that provides services that delegate to a hosted engine. User View Service - A software service that provides user interfaces access to digital resources. Network Gateway - A connection point enabling network traffic to pass between two networks. Database Manager - A capability that manages data organized as relational schemas. Enterprise Access Layer - Repository services for the Open Metadata Access Services ( OMAS ) supporting federated queries and aggregated events from the connected cohorts. Cohort Member - A capability enabling a server to access an open metadata repository cohort. Governance Engine - A collection of related governance services of the same type. Governance Action Engine - A collection of related governance services supporting the Governance Action Framework ( GAF ). Open Discovery Engine - A collection of related governance services supporting the Open Discovery Framework ( ODF ). In addition it is possible to augment software server capabilities with classifications. The following classifications are typically associated with the DataManager : Content Collection Manager - A manager of controlled documents and related media. File System - A capability that supports a store of files organized into a hierarchy of file folders for general use. File Manager - A manager of a collection of files and folders. The following are more generally applied. * Notification Manager - A server capability that is distributing events from a topic to its subscriber list. * Cloud Service - A capability enabled for a tenant on a cloud platform. Technology Examples \u00b6 Using the basic concepts described above, here are some examples of metadata for different types of technologies. Figure 8 shows the example of the software stack for Apache Kafka. Figure 8: The metadata elements to represent a Kafka Server Attachments and Classifications \u00b6 Locations Zones Ownership External References Infrastructure Managers \u00b6 When the IT Infrastructure OMAS is capturing metadata from deployment artifacts that are managed wholly by a specific technology or automated process, this technology should be catalogued as a SoftwareServerCapability and its guid and qualifiedName passed as the infrastructureManagerGUID and infrastructureManagerName parameters on the API. This will mark the metadata elements as managed by an external source which makes the metadata read-only to all but the caller responsible for cataloguing the artifact. Where Egeria is the primary catalog of the infrastructure metadata, or deployment artifacts that the metadata is derived from are either manually created or maintained by multiple process, the infrastructure manager identifiers are left blank so the resulting metadata elements are editable by any authorized caller. See Metadata Provenance for more information about the use of external source identifiers. Module Design \u00b6 The module structure for the IT Infrastructure OMAS is as follows: it-infrastructure-client supports the client library. it-infrastructure-api supports the common Java classes that are used both by the client and the server. it-infrastructure-server supports in implementation of the access service and its related event management. it-infrastructure-spring supports the REST API using the Spring libraries.","title":"IT Infrastructure OMAS"},{"location":"services/omas/it-infrastructure/#it-infrastructure-open-metadata-access-service-omas","text":"The IT Infrastructure OMAS provides APIs for tools and applications managing the IT infrastructure that supports the data assets and software. It is typically used by the Infrastructure Integrator OMIS to capture metadata from deployment artifacts, or to generate deployment artifacts from open metadata. The other major use of IT Infrastructure OMAS is to manually catalog the key pieces of IT Infrastructure used by an organization. Often the creation of this metadata is used to kick of the automated cataloging of the capabilities and assets associated with the infrastructure.","title":"IT Infrastructure Open Metadata Access Service (OMAS)"},{"location":"services/omas/it-infrastructure/#basic-concepts","text":"At the base is the notion of a Host . This could be: * BareMetalComputer - traditional computer hardware or * VirtualMachine - a virtualized machine (such as VMWare or VirtualBox) that uses a hypervisor to virtualize machine hardware or * VirtualContainer - a software system using a container library to virtualize the operating system it uses or * HostCluster - a cluster of Hosts that are operating as a single operational environment (such as a Hadoop cluster or kubernetes cluster). Hosts are composable and reusable. For example, figure 1 shows a Virtual Container that is deployed to two Bare Metal Computers. The relationship between them is called HostedHost . Figure 1: A virtual container deployed to two bare metal computers Figure 2 shows an example of Host Cluster, such as a Hadoop Cluster, that manages multiple Hosts (Bare Metal Computers in this example). The relationship between the Host Cluster and each subordinate Host is HostClusterMember . The host cluster may not have all of its members active. The membership denotes the pool of resources that the cluster has to work with. Figure 2: A cluster of bare metal computers operating as single host A Host typically runs an operating system and may have various hardware characteristics of interest (such as byte ordering). Collectively this information is called the OperatingPlatform . The Operating Platform can be linked to many hosts through the HostOperatingPlatform relationship. This is shown in figure 2 and figure 3. It documents that these hosts are running the software identified by the operating platform. This is particularly useful if you using standard software stack builds that are deployed to multiple hosts since it is easy to see which hosts are at which levels of the software and to manage the rollout of upgrades to the stack. Figure 3: A collection of hosts using the same operating platform The SoftwarePackageManifest shown in figure 4 details the software stack for the operating platform. It is represented as a Collection linked to the operating platform using the OperatingPlatformManifest relationship. Figure 4: The software stack definition for an operating platform Linked to a Host via the SoftwareServerPlatformDeployment are the SoftwareServerPlatform s. They describe the running processes running on the Host that use software described in the Operating Platform. This is shown in figure 5. Figure 5: The software server platforms running on the host The SoftwareServerPlatform itself may run one or many SoftwareServers where a Software Server is a collection of SoftwareServerCapabilities that access the operating system through the software server platform. The Software Server Capabilities deliver the function of the server. They typically host/manage assets such as DataSets , DataStores and Processes that are linked to the Software Server Capability using the ServerAssetUse relationship. Some technologies are written as a single stack. This means there is a single Software Server for the Software Server Platform. Figure 6 shows the structure of the metadata elements for a technology - such as Apache Kafka or a Database Server - that is a single stack. Figure 6: The metadata elements to represent a single stack technology Figure 7 shows the representation of the stack of a platform that allows multiple servers to be configured. Egeria's OMAG Server Platform is an example of this type of technology. The Software Servers are OMAG Servers and the registered services: Open Metadata Access Services (OMASs), Open Metadata Engine Services (OMESs), Open Metadata Integration Services (OMISs) and Open Metadata View Services (OMVSs) are Software Server Capabilities of types: MetadataAccessService , EngineHostingService , MetadataIntegrationService and UserViewService respectively. Figure 7: The metadata elements to represent a configurable software server platform","title":"Basic Concepts"},{"location":"services/omas/it-infrastructure/#software-server-capabilities","text":"There are many types of software server capability. The list below contains the Software Server Capabilities defined in the open metadata types. These are the software server capabilities defined in the open types: APIManager - A capability that manages callable APIs that typically delegate onto Software Services. Application - A capability supporting a specific business function. Catalog - A capability that manages collections of descriptions about people, places, digital assets, things, ... DataManager - A capability that manages collections of data. Engine - A programmable engine for running automated processes. Workflow Engine - An engine capable of running a mixture of human and automated tasks as part of a workflow process. Reporting Engine - An engine capable of creating reports by combining information from multiple data sets. Analytics Engine - An engine capable of running analytics models using data from one or more data sets. Data Movement Engine - An engine capable of copying data from one data store to another. Data Virtualization Engine - An engine capable of creating new data sets by dynamically combining data from one or more data stores or data sets. EventBroker - A capability that supports event-based services, typically around topics. Software Services - A capability that provides externally callable functions to other services. Application Service - A software service that supports a reusable business function. Metadata Integration Service - A software service that exchanges metadata between servers. Metadata Access Service - A software service that provides access to stored metadata. Engine Hosting Service - A software service that provides services that delegate to a hosted engine. User View Service - A software service that provides user interfaces access to digital resources. Network Gateway - A connection point enabling network traffic to pass between two networks. Database Manager - A capability that manages data organized as relational schemas. Enterprise Access Layer - Repository services for the Open Metadata Access Services ( OMAS ) supporting federated queries and aggregated events from the connected cohorts. Cohort Member - A capability enabling a server to access an open metadata repository cohort. Governance Engine - A collection of related governance services of the same type. Governance Action Engine - A collection of related governance services supporting the Governance Action Framework ( GAF ). Open Discovery Engine - A collection of related governance services supporting the Open Discovery Framework ( ODF ). In addition it is possible to augment software server capabilities with classifications. The following classifications are typically associated with the DataManager : Content Collection Manager - A manager of controlled documents and related media. File System - A capability that supports a store of files organized into a hierarchy of file folders for general use. File Manager - A manager of a collection of files and folders. The following are more generally applied. * Notification Manager - A server capability that is distributing events from a topic to its subscriber list. * Cloud Service - A capability enabled for a tenant on a cloud platform.","title":"Software Server Capabilities"},{"location":"services/omas/it-infrastructure/#technology-examples","text":"Using the basic concepts described above, here are some examples of metadata for different types of technologies. Figure 8 shows the example of the software stack for Apache Kafka. Figure 8: The metadata elements to represent a Kafka Server","title":"Technology Examples"},{"location":"services/omas/it-infrastructure/#attachments-and-classifications","text":"Locations Zones Ownership External References","title":"Attachments and Classifications"},{"location":"services/omas/it-infrastructure/#infrastructure-managers","text":"When the IT Infrastructure OMAS is capturing metadata from deployment artifacts that are managed wholly by a specific technology or automated process, this technology should be catalogued as a SoftwareServerCapability and its guid and qualifiedName passed as the infrastructureManagerGUID and infrastructureManagerName parameters on the API. This will mark the metadata elements as managed by an external source which makes the metadata read-only to all but the caller responsible for cataloguing the artifact. Where Egeria is the primary catalog of the infrastructure metadata, or deployment artifacts that the metadata is derived from are either manually created or maintained by multiple process, the infrastructure manager identifiers are left blank so the resulting metadata elements are editable by any authorized caller. See Metadata Provenance for more information about the use of external source identifiers.","title":"Infrastructure Managers"},{"location":"services/omas/it-infrastructure/#module-design","text":"The module structure for the IT Infrastructure OMAS is as follows: it-infrastructure-client supports the client library. it-infrastructure-api supports the common Java classes that are used both by the client and the server. it-infrastructure-server supports in implementation of the access service and its related event management. it-infrastructure-spring supports the REST API using the Spring libraries.","title":"Module Design"},{"location":"services/omas/project-management/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Project Management Open Metadata Access Service ( OMAS ) \u00b6 The Project Management OMAS provides APIs and events for tools and applications that support project leaders - particularly those who are leading governance projects. The interface support the setting up, management and completion of projects along with the teams that are working on them. It is aimed at linking the projects and tasks associated with the rollout and management of capability used to govern the organization's digital landscape and the people who use it. The module structure for the Project Management OMAS is as follows: project-management-client supports the client library. project-management-api supports the common Java classes that are used both by the client and the server. project-management-server supports in implementation of the access service and its related event management. project-management-spring supports the REST API using the Spring libraries.","title":"Project Management OMAS"},{"location":"services/omas/project-management/#project-management-open-metadata-access-service-omas","text":"The Project Management OMAS provides APIs and events for tools and applications that support project leaders - particularly those who are leading governance projects. The interface support the setting up, management and completion of projects along with the teams that are working on them. It is aimed at linking the projects and tasks associated with the rollout and management of capability used to govern the organization's digital landscape and the people who use it. The module structure for the Project Management OMAS is as follows: project-management-client supports the client library. project-management-api supports the common Java classes that are used both by the client and the server. project-management-server supports in implementation of the access service and its related event management. project-management-spring supports the REST API using the Spring libraries.","title":"Project Management Open Metadata Access Service (OMAS)"},{"location":"services/omas/security-manager/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Security Manager Open Metadata Access Service ( OMAS ) \u00b6 The Security Manager OMAS provides APIs for technologies wishing to register new data assets, connections and related schema from data resources located in database servers, file systems, file managers and content managers. The caller of this interface may be the security manager itself, or an integration daemon if the security manager does not support open metadata directly. There are specific APIs for different types of security managers and assets. These reflect the terminology typically associated with the specific type of security manager to make it easier for people to map the Security Manager OMAS APIs and events to the actual technology. However, the specific implementation objects supported by these APIs all inherit from common open metadata types so it is possible to work with the resulting metadata in a technology agnostic manner using services such as the Asset Consumer OMAS . The Security Manager OMAS APIs needs to accommodate slight variations between different vendor implementations of security managers, along with information relating to local deployment standards. As such there is provision in these interfaces to support: VendorProperties for properties unique to a specific vendor implementation, and AdditionalProperties for properties that the metadata team wish to add to the metadata. The module structure for the Security Manager OMAS is as follows: security-manager-client supports the client library. security-manager-api supports the common Java classes that are used both by the client and the server. security-manager-server supports in implementation of the access service and its related event management. security-manager-spring supports the REST API using the Spring libraries. security-manager-topic-connectors supports the connectors used to access the InTopic and OutTopic events from the Security Manager OMAS .","title":"Security Manager OMAS"},{"location":"services/omas/security-manager/#security-manager-open-metadata-access-service-omas","text":"The Security Manager OMAS provides APIs for technologies wishing to register new data assets, connections and related schema from data resources located in database servers, file systems, file managers and content managers. The caller of this interface may be the security manager itself, or an integration daemon if the security manager does not support open metadata directly. There are specific APIs for different types of security managers and assets. These reflect the terminology typically associated with the specific type of security manager to make it easier for people to map the Security Manager OMAS APIs and events to the actual technology. However, the specific implementation objects supported by these APIs all inherit from common open metadata types so it is possible to work with the resulting metadata in a technology agnostic manner using services such as the Asset Consumer OMAS . The Security Manager OMAS APIs needs to accommodate slight variations between different vendor implementations of security managers, along with information relating to local deployment standards. As such there is provision in these interfaces to support: VendorProperties for properties unique to a specific vendor implementation, and AdditionalProperties for properties that the metadata team wish to add to the metadata. The module structure for the Security Manager OMAS is as follows: security-manager-client supports the client library. security-manager-api supports the common Java classes that are used both by the client and the server. security-manager-server supports in implementation of the access service and its related event management. security-manager-spring supports the REST API using the Spring libraries. security-manager-topic-connectors supports the connectors used to access the InTopic and OutTopic events from the Security Manager OMAS .","title":"Security Manager Open Metadata Access Service (OMAS)"},{"location":"services/omas/security-officer/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Security Officer Open Metadata Access Service ( OMAS ) \u00b6 The Security Officer Open Metadata Access Service ( OMAS ) provides access to metadata for policy enforcement frameworks such as Apache Ranger. This API simplifies the internal models and structures of the open metadata type model and related structure for the consumers. As an example, Apache Ranger needs to know how a particular entity is classified so that the classification can be used within a policy (rule). The Open Metadata Types define a complex graph-oriented model, within which classifications can be multi level - for example a column may be classified as employee_salary whilst employee_salary itself may be classified as SPI . Ranger however just needs to know that the column is SPI, not how we got there. So we convert this complex model into something much more operationally-focused in the form of SecurityTags and deliver that over the API. The implementation will follow this graph, and build up a list of all tags that are appropriate to use. Note that in the case of Ranger it is actually the tagsync process that will call the Governance Engine OMAS for this classification information Ranger can do this today, but via a large number of individual requests to retrieve types and entities. Rather than these lower level queries to the metadata repository services, the Security Officer OMAS offers result sets to make queries more efficient, and more appropriate notifications. More details on Apache Ranger's Tag Propagation Process is found in the docs directory by following the link. Internals \u00b6 The module structure for the Security Officer OMAS is as follows: security-officer-api supports the common Java classes that are used both by the client and the server. security-officer-topic-connectors supports the common Java classes that are used both by the client and the server. security-officer-client supports the client library. security-officer-server supports in implementation of the access service and its related event management. security-officer-spring supports the REST API using the Spring libraries.","title":"Security Officer OMAS"},{"location":"services/omas/security-officer/#security-officer-open-metadata-access-service-omas","text":"The Security Officer Open Metadata Access Service ( OMAS ) provides access to metadata for policy enforcement frameworks such as Apache Ranger. This API simplifies the internal models and structures of the open metadata type model and related structure for the consumers. As an example, Apache Ranger needs to know how a particular entity is classified so that the classification can be used within a policy (rule). The Open Metadata Types define a complex graph-oriented model, within which classifications can be multi level - for example a column may be classified as employee_salary whilst employee_salary itself may be classified as SPI . Ranger however just needs to know that the column is SPI, not how we got there. So we convert this complex model into something much more operationally-focused in the form of SecurityTags and deliver that over the API. The implementation will follow this graph, and build up a list of all tags that are appropriate to use. Note that in the case of Ranger it is actually the tagsync process that will call the Governance Engine OMAS for this classification information Ranger can do this today, but via a large number of individual requests to retrieve types and entities. Rather than these lower level queries to the metadata repository services, the Security Officer OMAS offers result sets to make queries more efficient, and more appropriate notifications. More details on Apache Ranger's Tag Propagation Process is found in the docs directory by following the link.","title":"Security Officer Open Metadata Access Service (OMAS)"},{"location":"services/omas/security-officer/#internals","text":"The module structure for the Security Officer OMAS is as follows: security-officer-api supports the common Java classes that are used both by the client and the server. security-officer-topic-connectors supports the common Java classes that are used both by the client and the server. security-officer-client supports the client library. security-officer-server supports in implementation of the access service and its related event management. security-officer-spring supports the REST API using the Spring libraries.","title":"Internals"},{"location":"services/omas/security-officer/TagPropogation/TagPropogation/","text":"Tag Propagation \u00b6 Introduction \u00b6 This document aims to explain how tag propagation will be handled within Governance Engine OMAS. The terms tag and classification should be treated interchangeably.... Tag propagation is the process by which we figure out which classifications should apply to a particular asset. For example a classification may be associated directly with an asset, or instead an asset may be associated with a business term that itself is classified. We also have to resolve conflicts. A series of scenarios will be presented. Scenarios \u00b6 Direct Classification \u00b6 The Asset (EmpSal) is classified as Sensitive This is a direct classification, exactly once, - there is nothing to resolve This classification will be exposed via GE-omas against the EmpSal resource Direct Multiple Classification \u00b6 The Asset (EmpSal) is classified as Sensitive and TopSecret. This is invalid - there should only be a single governance classification applied. No attempt will be made to reconcile these multiple classifications - ie to determine which should take precedence, which is most restrictive, are they the same etc. instead as this is an error, the asset will not be exposed at GE-omas at all. An Audit log entry will be created reporting the error. Classification via Business Term \u00b6 The EmpSal asset is assigned the business term 'EmployeeSalary' which gives it meaning. This business term is classified as Sensitive, so the resultant classification for EmpSal is Sensitive Classification via Business Term and Directly \u00b6 The EmpSal asset is assigned the business term 'EmployeeSalary' which gives it meaning. This business term is classified as Sensitive However the asset is also classified as Top Secret Since this has narrower scope, and as long as it is the only direct classification, this will be used So the resultant classification here is TopSecret. Classification via multiple business terms \u00b6 The Asset (EmpSal) is assigned the two business terms EmployeeBonus & EmployeeSalary This is invalid - there should only be a single business term applied. No attempt will be made to reconcile these multiple terms, or their classifications - ie to determine which should take precedence, which is most restrictive, are they the same etc. instead as this is an error, the asset will not be exposed at GE-omas at all. An Audit log entry will be created reporting the error. Reference material \u00b6 Cary Workshop \u00b6 In November 2018 design meetings in Cary came up with the following diagrams, which hopefully have been explained and the points addressed in this document. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"TagPropogation"},{"location":"services/omas/security-officer/TagPropogation/TagPropogation/#tag-propagation","text":"","title":"Tag Propagation"},{"location":"services/omas/security-officer/TagPropogation/TagPropogation/#introduction","text":"This document aims to explain how tag propagation will be handled within Governance Engine OMAS. The terms tag and classification should be treated interchangeably.... Tag propagation is the process by which we figure out which classifications should apply to a particular asset. For example a classification may be associated directly with an asset, or instead an asset may be associated with a business term that itself is classified. We also have to resolve conflicts. A series of scenarios will be presented.","title":"Introduction"},{"location":"services/omas/security-officer/TagPropogation/TagPropogation/#scenarios","text":"","title":"Scenarios"},{"location":"services/omas/security-officer/TagPropogation/TagPropogation/#direct-classification","text":"The Asset (EmpSal) is classified as Sensitive This is a direct classification, exactly once, - there is nothing to resolve This classification will be exposed via GE-omas against the EmpSal resource","title":"Direct Classification"},{"location":"services/omas/security-officer/TagPropogation/TagPropogation/#direct-multiple-classification","text":"The Asset (EmpSal) is classified as Sensitive and TopSecret. This is invalid - there should only be a single governance classification applied. No attempt will be made to reconcile these multiple classifications - ie to determine which should take precedence, which is most restrictive, are they the same etc. instead as this is an error, the asset will not be exposed at GE-omas at all. An Audit log entry will be created reporting the error.","title":"Direct Multiple Classification"},{"location":"services/omas/security-officer/TagPropogation/TagPropogation/#classification-via-business-term","text":"The EmpSal asset is assigned the business term 'EmployeeSalary' which gives it meaning. This business term is classified as Sensitive, so the resultant classification for EmpSal is Sensitive","title":"Classification via Business Term"},{"location":"services/omas/security-officer/TagPropogation/TagPropogation/#classification-via-business-term-and-directly","text":"The EmpSal asset is assigned the business term 'EmployeeSalary' which gives it meaning. This business term is classified as Sensitive However the asset is also classified as Top Secret Since this has narrower scope, and as long as it is the only direct classification, this will be used So the resultant classification here is TopSecret.","title":"Classification via Business Term and Directly"},{"location":"services/omas/security-officer/TagPropogation/TagPropogation/#classification-via-multiple-business-terms","text":"The Asset (EmpSal) is assigned the two business terms EmployeeBonus & EmployeeSalary This is invalid - there should only be a single business term applied. No attempt will be made to reconcile these multiple terms, or their classifications - ie to determine which should take precedence, which is most restrictive, are they the same etc. instead as this is an error, the asset will not be exposed at GE-omas at all. An Audit log entry will be created reporting the error.","title":"Classification via multiple business terms"},{"location":"services/omas/security-officer/TagPropogation/TagPropogation/#reference-material","text":"","title":"Reference material"},{"location":"services/omas/security-officer/TagPropogation/TagPropogation/#cary-workshop","text":"In November 2018 design meetings in Cary came up with the following diagrams, which hopefully have been explained and the points addressed in this document. License: CC BY 4.0 , Copyright Contributors to the ODPi Egeria project.","title":"Cary Workshop"},{"location":"services/omas/software-developer/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Software Developer Open Metadata Access Service ( OMAS ) \u00b6 The Software Developer OMAS provides APIs and events for software developer tools and applications that help developers make good use of the standards and best practices defined for the data landscape. It supports the documentation of the component structure of a software capability and the ability to link it to the digital services it supports for the business. As the software developer works on the implementation, the Software Developer OMAS supports APIs to search for data structure implementation snippets based on search criteria such as glossary terms and/or language. It also provides information about the most appropriate data sources to use for particular situations along with details of reference data values and sets. Using these services augments the software component model for the software capability. Finally it enables the documentation of the packaging as the components are moved into the DevOps Pipeline . Design \u00b6 The module structure for the Software Developer OMAS is as follows: software-developer-client supports the client library. software-developer-api supports the common Java classes that are used both by the client and the server. software-developer-server supports in implementation of the access service and its related event management. software-developer-spring supports the REST API using the Spring libraries.","title":"Software Developer OMAS"},{"location":"services/omas/software-developer/#software-developer-open-metadata-access-service-omas","text":"The Software Developer OMAS provides APIs and events for software developer tools and applications that help developers make good use of the standards and best practices defined for the data landscape. It supports the documentation of the component structure of a software capability and the ability to link it to the digital services it supports for the business. As the software developer works on the implementation, the Software Developer OMAS supports APIs to search for data structure implementation snippets based on search criteria such as glossary terms and/or language. It also provides information about the most appropriate data sources to use for particular situations along with details of reference data values and sets. Using these services augments the software component model for the software capability. Finally it enables the documentation of the packaging as the components are moved into the DevOps Pipeline .","title":"Software Developer Open Metadata Access Service (OMAS)"},{"location":"services/omas/software-developer/#design","text":"The module structure for the Software Developer OMAS is as follows: software-developer-client supports the client library. software-developer-api supports the common Java classes that are used both by the client and the server. software-developer-server supports in implementation of the access service and its related event management. software-developer-spring supports the REST API using the Spring libraries.","title":"Design"},{"location":"services/omas/stewardship-action/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Stewardship Action Open Metadata Access Service ( OMAS ) \u00b6 The Stewardship Action OMAS provides APIs and events for tools and applications focused on resolving issues detected in the data landscape. It works in partnership with the stewardship server to execute functions that detect, report on and implement the resolution of issues. These functions are called stewardship actions . For example, when a change is detected in an Asset metadata entity, the Stewardship Action OMAS runs the stewardship actions associated with that asset. These could be: * detect and emit an event if a new asset is created without an owner. * create an entry in a maintenance NoteLog to record each change to the Asset. Where a stewardship action creates an event, this is published to the stewardship action OMASs OutTopic where it is picked up by the Stewardship server to process. The stewardship server is configured with the stewardship actions to process ite incoming events. For example, there could be a stewardship action to assign an owner to an Asset without one based on values in a reference table or if there was no default owner defined for the asset type, to initiate a workflow to enable a human steward to assign it. Implementing Stewardship Actions \u00b6 The implementation of stewardship actions is through the Governance Action Framework ( GAF ) . The GAF defines plug-in components that can be configured and executed by the stewardship action OMAS and in the stewardship server. The the stewardship actions are open connectors (see Open Connector Framework ( OCF ) ) that implement the interfaces defined by the governance action framework. Configuring the Stewardship Action OMAS \u00b6 The stewardship action OMAS is controlled by the definition of StewardshipAction and RequestForAction entities in the metadata repositories. StewardshipAction entities relate to conditions found in the metadata and include details of how to detect the condition and the action to take. RequestForAction entities refer to issues found in the data landscape itself. They do not contain information about how to fix the problem. This is decided in the stewardship server. These entities are linked to the assets that need the automated stewardship actions. They can be created through the stewardship action OMAS API, or by other OMASs. For example, the Asset Owner OMAS may set up stewardship actions on behalf of the owner of the asset. A discovery service may use the Discovery Engine OMAS to attach RequestForAction entities to an asset as a result of issues it has found in the data access through the asset. The stewardship action OMAS is triggered by listening for metadata changes through its OMRS topic listener. It detects new RequestForAction entities being created and it listens for changes to the assets. All new RequestForAction entities are actioned by sending an event on the stewardship action OMAS OutTopic. Asset changes are assessed using information in any attached StewardshipAction entities. If the assessment determines action is required then an event is sent through the stewardship action OMAS OutTopic. The stewardship server is listening for these events and running the GAF stewardship actions specified in the request for action. These actions may call back to the open metadata access services. Module structure \u00b6 The module structure for the Stewardship Action OMAS is as follows: stewardship-action-client supports the client library. stewardship-action-api supports the common Java classes that are used both by the client and the server. stewardship-action-server supports in implementation of the access service and its related event management. stewardship-action-spring supports the REST API using the Spring libraries.","title":"Stewardship Action OMAS"},{"location":"services/omas/stewardship-action/#stewardship-action-open-metadata-access-service-omas","text":"The Stewardship Action OMAS provides APIs and events for tools and applications focused on resolving issues detected in the data landscape. It works in partnership with the stewardship server to execute functions that detect, report on and implement the resolution of issues. These functions are called stewardship actions . For example, when a change is detected in an Asset metadata entity, the Stewardship Action OMAS runs the stewardship actions associated with that asset. These could be: * detect and emit an event if a new asset is created without an owner. * create an entry in a maintenance NoteLog to record each change to the Asset. Where a stewardship action creates an event, this is published to the stewardship action OMASs OutTopic where it is picked up by the Stewardship server to process. The stewardship server is configured with the stewardship actions to process ite incoming events. For example, there could be a stewardship action to assign an owner to an Asset without one based on values in a reference table or if there was no default owner defined for the asset type, to initiate a workflow to enable a human steward to assign it.","title":"Stewardship Action Open Metadata Access Service (OMAS)"},{"location":"services/omas/stewardship-action/#implementing-stewardship-actions","text":"The implementation of stewardship actions is through the Governance Action Framework ( GAF ) . The GAF defines plug-in components that can be configured and executed by the stewardship action OMAS and in the stewardship server. The the stewardship actions are open connectors (see Open Connector Framework ( OCF ) ) that implement the interfaces defined by the governance action framework.","title":"Implementing Stewardship Actions"},{"location":"services/omas/stewardship-action/#configuring-the-stewardship-action-omas","text":"The stewardship action OMAS is controlled by the definition of StewardshipAction and RequestForAction entities in the metadata repositories. StewardshipAction entities relate to conditions found in the metadata and include details of how to detect the condition and the action to take. RequestForAction entities refer to issues found in the data landscape itself. They do not contain information about how to fix the problem. This is decided in the stewardship server. These entities are linked to the assets that need the automated stewardship actions. They can be created through the stewardship action OMAS API, or by other OMASs. For example, the Asset Owner OMAS may set up stewardship actions on behalf of the owner of the asset. A discovery service may use the Discovery Engine OMAS to attach RequestForAction entities to an asset as a result of issues it has found in the data access through the asset. The stewardship action OMAS is triggered by listening for metadata changes through its OMRS topic listener. It detects new RequestForAction entities being created and it listens for changes to the assets. All new RequestForAction entities are actioned by sending an event on the stewardship action OMAS OutTopic. Asset changes are assessed using information in any attached StewardshipAction entities. If the assessment determines action is required then an event is sent through the stewardship action OMAS OutTopic. The stewardship server is listening for these events and running the GAF stewardship actions specified in the request for action. These actions may call back to the open metadata access services.","title":"Configuring the Stewardship Action OMAS"},{"location":"services/omas/stewardship-action/#module-structure","text":"The module structure for the Stewardship Action OMAS is as follows: stewardship-action-client supports the client library. stewardship-action-api supports the common Java classes that are used both by the client and the server. stewardship-action-server supports in implementation of the access service and its related event management. stewardship-action-spring supports the REST API using the Spring libraries.","title":"Module structure"},{"location":"services/omas/subject-area/","text":"Technical preview Technical preview function is in a state that it can be tried. The development is complete, there is documentation and there are samples, tutorials and hands-on labs as appropriate. The community is looking for feedback on the function before releasing it. This feedback may result in changes to the external interfaces. Subject Area Open Metadata Access Service ( OMAS ) \u00b6 The Subject Area OMAS supports subject matter experts who are documenting their knowledge about a particular subject. This includes: glossary terms reference data validation rules The Subject Area API enables subject matter experts to author glossary content. The operations include Find, Create, Read, Update and Delete (CRUD) operations on Glossary, Term and Category objects. These structures are defined as POJO property objects (aka beans). The module structure for the Subject Area OMAS is as follows: subject-area-client supports the client library. subject-area-api supports the common Java classes that are used both by the client and the server. subject-area-server supports an implementation of the access service and its related event management. subject-area-spring supports the REST API using the Spring libraries. The implementation is not complete. The following has been implemented : Java and REST API for create, get and update for Glossary, Category, Term , SubjectAreaDefinition. Java and REST API for the Term to Term relationships HAS-A, RelatedTerm, Synonym, Antonym, Translations, used in context, preferred terms, valid values, replacement terms, typed by, is a, is a type of. Java and REST API for the Term to Category relationships TermCategorization. TermAnchor and CategoryAnchor relationships can be created , deleted, purged and restored. As there are no properties, there are no update or replace operations. getTermRelationships, get GlossaryRelationships and getCategoryRelationships findTerm, findCategory and findGlossary This has been verified by running the Subject Area samples and Subject Area FVT against an in-memory repository Example REST calls: \u00b6 Create Glossary instance \u00b6 POST url: localhost:9443/servers/{serverName}/open-metadata/access-services/subject-area/users/{user}/glossaries JSON body: { \"name\" : \"Test glossary 1\" , \"description\" : \"This is a Glossary for testing.\" , \"usage\" : \"for test\" } Get Glossary instance \u00b6 Get Glossary instance (where {serverName} is the name of the server, {guid} is the guid in the Glossary create response and {user} is the userid ) GET url: localhost:9443/servers/{serverName}/open-metadata/access-services/subject-area/users/{user}/glossaries/{guid} (where {guid} is the GUID in the Glossary create response) Delete Glossary instance \u00b6 Delete Glossary instance (where {guid} is the guid in the Glossary create response and {user} is the userid ) DELETE url : localhost:9443/open-metadata/access-services/subject-area/users/{user}/glossaries/{guid} The Subject Area OMAS philosophy \u00b6 The Subject area OMAS is the access service that subject area experts should use. The intent is that the APIs that are exposed are natural for the tasks that a subject area expert is performing. At this time the Subject area OMAS exposes APIs around the task of Glossary authoring, focusing on Glossary, Category and Term objects. The Subject Area OMAS architecture \u00b6 The Subject Area main objects are the Glossary, Category and Terms. There map onto the OMRS types Glossary, GlossaryCategory and GlossaryTerm. The mapping is not one to one, because the OMAS API is looking to emphasise certain content and hide some of the OMRS details that the subject area expert is not concerned with. Subject Area OMAS mapping to OMRS entities considerations: * Glossary, Category and Term objects have associated icons, these are embedded objects rather than relationships. In this way icon content is shown as important to the subject area expert as they are like to be working with the glossary content visually * The icon embedded object is an IconSummary object. This is an example of other object in the OMAS API whose names end with \"Summary\". These objects represent a summary of the entity at the end of a certain type of OMRS relationships. Note the icon function has not been implemented in the OMAS yet. * Categories and Terms have a GlossarySummary , this is there associated Glossary. Good practice is to have Terms and Categories within a Glossary, so the Subject area API Term and Category create and update APIs expect a glossary to be supplied. See effective date considerations. * OMRS relationships can be managed via the Subject Area relationships API. Some of these relationships may appear as summary objects. The Subject Area OMAS API overview. \u00b6 There are a number of types of APIs associated with the Subject Area OMAS . * Create, update, replace, get, delete (hard and soft) and restore for Glossary, Category, Term and relationships. * get relationships associated with a Term - implemented * Find APIs allow content to be found - findTerm, findCategory and findGlossary implemented * Collaboration APIs allow comments and TODO and the like to be associate with glossary content * A report API, allows glossary content to be analysed, the API response highlights areas that the subject Area Expert might want to amend. - not implemented yet * Node orientated APIs How the Subject Area OMAS deals with effective dates \u00b6 The OMRS entities, relationships and classifications have optional effective From and To dates. These dates are exposed in the Term, Category and Glossary objects as attributes. * create, update and replace calls to the subject Area for Term, Category, glossary and relationships omas can specify an effective date range in the request, allowing the subject area OMAS to manage effective dates. The null value or when it is not specified To date means there is no limit in the future for the objects effectivity. A null or unspecified from date means that this no starting restriction for effectivity. The date must not be in the past. The From date should be prior to the To Date. * create, update, replace, restore, soft delete responses may return Summary objects that are not in the effective date range of the associated Term, Category or Glossary object. This is to allow glossaries content to be 'messy' and allow the subject area expert to fix it up. * A get of a Term, Glossary or Category that has potentially associated Summary objects. Because the Subject Area OMAS is an authoring interface, the user needs to see all content irrespective of the effectivity date. So associated summary objects are exposed even if they are not effective. The summary objects contain the effectivity dates of the relationship and the connected object. The Subject Area user can see these dates and maker a decision as to whether they want to amend them. * create, update, delete restore and replace operations are exposed for relationships that appear as summary objects - so that their effectivity ranges can be managed by the subject area expert. How the Subject Area OMAS deals with finds \u00b6 The find APIs in the Subject Area do not accept input from the user that will be interpreted as a regex. Instead 2 flags are supplied, with the searchCriteria: exactValue and mixedCase. The search criteria from API is a literal and is then extended appropriately to form a regex expression - implementing the requested exactValue and mixedCase.","title":"Subject Area OMAS"},{"location":"services/omas/subject-area/#subject-area-open-metadata-access-service-omas","text":"The Subject Area OMAS supports subject matter experts who are documenting their knowledge about a particular subject. This includes: glossary terms reference data validation rules The Subject Area API enables subject matter experts to author glossary content. The operations include Find, Create, Read, Update and Delete (CRUD) operations on Glossary, Term and Category objects. These structures are defined as POJO property objects (aka beans). The module structure for the Subject Area OMAS is as follows: subject-area-client supports the client library. subject-area-api supports the common Java classes that are used both by the client and the server. subject-area-server supports an implementation of the access service and its related event management. subject-area-spring supports the REST API using the Spring libraries. The implementation is not complete. The following has been implemented : Java and REST API for create, get and update for Glossary, Category, Term , SubjectAreaDefinition. Java and REST API for the Term to Term relationships HAS-A, RelatedTerm, Synonym, Antonym, Translations, used in context, preferred terms, valid values, replacement terms, typed by, is a, is a type of. Java and REST API for the Term to Category relationships TermCategorization. TermAnchor and CategoryAnchor relationships can be created , deleted, purged and restored. As there are no properties, there are no update or replace operations. getTermRelationships, get GlossaryRelationships and getCategoryRelationships findTerm, findCategory and findGlossary This has been verified by running the Subject Area samples and Subject Area FVT against an in-memory repository","title":"Subject Area Open Metadata Access Service (OMAS)"},{"location":"services/omas/subject-area/#example-rest-calls","text":"","title":"Example REST calls:"},{"location":"services/omas/subject-area/#create-glossary-instance","text":"POST url: localhost:9443/servers/{serverName}/open-metadata/access-services/subject-area/users/{user}/glossaries JSON body: { \"name\" : \"Test glossary 1\" , \"description\" : \"This is a Glossary for testing.\" , \"usage\" : \"for test\" }","title":"Create Glossary instance"},{"location":"services/omas/subject-area/#get-glossary-instance","text":"Get Glossary instance (where {serverName} is the name of the server, {guid} is the guid in the Glossary create response and {user} is the userid ) GET url: localhost:9443/servers/{serverName}/open-metadata/access-services/subject-area/users/{user}/glossaries/{guid} (where {guid} is the GUID in the Glossary create response)","title":"Get Glossary instance"},{"location":"services/omas/subject-area/#delete-glossary-instance","text":"Delete Glossary instance (where {guid} is the guid in the Glossary create response and {user} is the userid ) DELETE url : localhost:9443/open-metadata/access-services/subject-area/users/{user}/glossaries/{guid}","title":"Delete Glossary instance"},{"location":"services/omas/subject-area/#the-subject-area-omas-philosophy","text":"The Subject area OMAS is the access service that subject area experts should use. The intent is that the APIs that are exposed are natural for the tasks that a subject area expert is performing. At this time the Subject area OMAS exposes APIs around the task of Glossary authoring, focusing on Glossary, Category and Term objects.","title":"The Subject Area OMAS philosophy"},{"location":"services/omas/subject-area/#the-subject-area-omas-architecture","text":"The Subject Area main objects are the Glossary, Category and Terms. There map onto the OMRS types Glossary, GlossaryCategory and GlossaryTerm. The mapping is not one to one, because the OMAS API is looking to emphasise certain content and hide some of the OMRS details that the subject area expert is not concerned with. Subject Area OMAS mapping to OMRS entities considerations: * Glossary, Category and Term objects have associated icons, these are embedded objects rather than relationships. In this way icon content is shown as important to the subject area expert as they are like to be working with the glossary content visually * The icon embedded object is an IconSummary object. This is an example of other object in the OMAS API whose names end with \"Summary\". These objects represent a summary of the entity at the end of a certain type of OMRS relationships. Note the icon function has not been implemented in the OMAS yet. * Categories and Terms have a GlossarySummary , this is there associated Glossary. Good practice is to have Terms and Categories within a Glossary, so the Subject area API Term and Category create and update APIs expect a glossary to be supplied. See effective date considerations. * OMRS relationships can be managed via the Subject Area relationships API. Some of these relationships may appear as summary objects.","title":"The Subject Area OMAS architecture"},{"location":"services/omas/subject-area/#the-subject-area-omas-api-overview","text":"There are a number of types of APIs associated with the Subject Area OMAS . * Create, update, replace, get, delete (hard and soft) and restore for Glossary, Category, Term and relationships. * get relationships associated with a Term - implemented * Find APIs allow content to be found - findTerm, findCategory and findGlossary implemented * Collaboration APIs allow comments and TODO and the like to be associate with glossary content * A report API, allows glossary content to be analysed, the API response highlights areas that the subject Area Expert might want to amend. - not implemented yet * Node orientated APIs","title":"The Subject Area OMAS API overview."},{"location":"services/omas/subject-area/#how-the-subject-area-omas-deals-with-effective-dates","text":"The OMRS entities, relationships and classifications have optional effective From and To dates. These dates are exposed in the Term, Category and Glossary objects as attributes. * create, update and replace calls to the subject Area for Term, Category, glossary and relationships omas can specify an effective date range in the request, allowing the subject area OMAS to manage effective dates. The null value or when it is not specified To date means there is no limit in the future for the objects effectivity. A null or unspecified from date means that this no starting restriction for effectivity. The date must not be in the past. The From date should be prior to the To Date. * create, update, replace, restore, soft delete responses may return Summary objects that are not in the effective date range of the associated Term, Category or Glossary object. This is to allow glossaries content to be 'messy' and allow the subject area expert to fix it up. * A get of a Term, Glossary or Category that has potentially associated Summary objects. Because the Subject Area OMAS is an authoring interface, the user needs to see all content irrespective of the effectivity date. So associated summary objects are exposed even if they are not effective. The summary objects contain the effectivity dates of the relationship and the connected object. The Subject Area user can see these dates and maker a decision as to whether they want to amend them. * create, update, delete restore and replace operations are exposed for relationships that appear as summary objects - so that their effectivity ranges can be managed by the subject area expert.","title":"How the Subject Area OMAS deals with effective dates"},{"location":"services/omas/subject-area/#how-the-subject-area-omas-deals-with-finds","text":"The find APIs in the Subject Area do not accept input from the user that will be interpreted as a regex. Instead 2 flags are supplied, with the searchCriteria: exactValue and mixedCase. The search criteria from API is a literal and is then extended appropriately to form a regex expression - implementing the requested exactValue and mixedCase.","title":"How the Subject Area OMAS deals with finds"},{"location":"services/omes/","text":"Open Metadata Engine Services ( OMES ) \u00b6 The engine services are each able to host a specific type of governance engine. Broadly there are two types of governance engines: Open discovery engines that run automatic metadata discovery requests to analyze the content of an asset's real-world counterpart. Governance action engines that manage the processing supporting governance processing such as the resolution of issues reported in the open metadata ecosystem or the assets it supports. These engines are supported by the Asset Analysis OMES and Governance Action OMES respectively.","title":"Open Metadata Engine Services"},{"location":"services/omes/#open-metadata-engine-services-omes","text":"The engine services are each able to host a specific type of governance engine. Broadly there are two types of governance engines: Open discovery engines that run automatic metadata discovery requests to analyze the content of an asset's real-world counterpart. Governance action engines that manage the processing supporting governance processing such as the resolution of issues reported in the open metadata ecosystem or the assets it supports. These engines are supported by the Asset Analysis OMES and Governance Action OMES respectively.","title":"Open Metadata Engine Services (OMES)"},{"location":"services/omes/asset-analysis/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Asset Analysis Open Metadata Engine Service ( OMES ) \u00b6 The Asset Analysis OMES provides support for open discovery engines that are part of the Open Discovery Service ( ODF ) . Open discovery engines \u00b6 A discovery engine hosts automated metadata discovery . The Asset Analysis OMES is capable of hosting one or more discovery engines and supports a REST API to request that a discovery engine runs an discovery service to analyse an asset and to access the results. The results of each of these calls is a discovery analysis report . The REST API also supports a request to a discovery engine to run a specific open discovery service against each asset it has access to. The discovery engine services call the Discovery Engine Open Metadata Access Service ( OMAS ) running in an open metadata server to retrieve information about assets and to store the results of the discovery services.","title":"Asset Analysis"},{"location":"services/omes/asset-analysis/#asset-analysis-open-metadata-engine-service-omes","text":"The Asset Analysis OMES provides support for open discovery engines that are part of the Open Discovery Service ( ODF ) .","title":"Asset Analysis Open Metadata Engine Service (OMES)"},{"location":"services/omes/asset-analysis/#open-discovery-engines","text":"A discovery engine hosts automated metadata discovery . The Asset Analysis OMES is capable of hosting one or more discovery engines and supports a REST API to request that a discovery engine runs an discovery service to analyse an asset and to access the results. The results of each of these calls is a discovery analysis report . The REST API also supports a request to a discovery engine to run a specific open discovery service against each asset it has access to. The discovery engine services call the Discovery Engine Open Metadata Access Service ( OMAS ) running in an open metadata server to retrieve information about assets and to store the results of the discovery services.","title":"Open discovery engines"},{"location":"services/omes/governance-action/","text":"In development A subsystem that is in development means that the Egeria community is still building the function. The code is added continuously in small pieces to help the review and socialization process. It may not run, or do something useful - it only promises not to break other function. There will be git issues describing the end state. Governance Action Open Metadata Engine Services ( OMES ) \u00b6 The Governance Action Open Metadata Engine Service ( OMES ) runs in an Engine Host OMAG Server . It provides access to the open metadata ecosystem for governance action services . These are pluggable connectors that manage governance of open metadata. Their interfaces are defined by the Governance Action Framework ( GAF ) and supported by the Governance Action OMES . The governance Action OMES also provides an API to allow a third party tool to validate that a specific governance action service will load in the engine host server, and it returns the usage information encoded in the service's implementation. Using the Governance Action OMES \u00b6 Governance action services are defined and linked to one or more governance action engines using the Governance Engine OMAS . The definitions for both the governance action engines and their linked services are stored in a metadata server . When the Governance Action OMES is configured in the engine host, a list of governance action engines is supplied. This determines which governance action engines and hence governance action services that it supports. The Governance Action OMES is responsible for initializing the governance action engines and providing the context and runtime environment for governance action services when they are requested by third party technologies or through the governance action processing of the Governance Engine OMAS .","title":"Governance Action"},{"location":"services/omes/governance-action/#governance-action-open-metadata-engine-services-omes","text":"The Governance Action Open Metadata Engine Service ( OMES ) runs in an Engine Host OMAG Server . It provides access to the open metadata ecosystem for governance action services . These are pluggable connectors that manage governance of open metadata. Their interfaces are defined by the Governance Action Framework ( GAF ) and supported by the Governance Action OMES . The governance Action OMES also provides an API to allow a third party tool to validate that a specific governance action service will load in the engine host server, and it returns the usage information encoded in the service's implementation.","title":"Governance Action Open Metadata Engine Services (OMES)"},{"location":"services/omes/governance-action/#using-the-governance-action-omes","text":"Governance action services are defined and linked to one or more governance action engines using the Governance Engine OMAS . The definitions for both the governance action engines and their linked services are stored in a metadata server . When the Governance Action OMES is configured in the engine host, a list of governance action engines is supplied. This determines which governance action engines and hence governance action services that it supports. The Governance Action OMES is responsible for initializing the governance action engines and providing the context and runtime environment for governance action services when they are requested by third party technologies or through the governance action processing of the Governance Engine OMAS .","title":"Using the Governance Action OMES"},{"location":"services/omrs/","text":"Repository Services ( OMRS ) Design \u00b6 On the left-hand side is the administration interface supported by the OMAG Server. This is where configuration is passed to the OMRS , and status and other relevant information is made available to the OMAG Administration Services. Along the top is the interface with the Open Metadata Access Services ( OMAS ) . The OMRS provides access to the open metadata repositories through both APIs (see Enterprise OMRS Repository Connector ) and events (see Enterprise OMRS Topic ). Along the bottom are the six types of connectors that provide the OMRS with access to the stores and system resources it needs to support the OMAS . These connectors enable the OMRS to be deployed into different types of server environments and connect with different types of infrastructure and services. Repository connectors provide a common interface for metadata repositories. The OMRS store connectors can range from simple file stores to enterprise / cloud provider admin repositories and the event topic can support different types of messaging infrastructure. OMRS subsystems \u00b6 Inside the OMRS are 4 major subsystems (groups of components). Enterprise repository services \u00b6 The enterprise repository services provide a virtual metadata repository by combining the content of multiple open metadata repositories and delivering this metadata through a single API and event topic. The Enterprise Repository Services provide the enterprise access metadata support for the OMASs, supporting the federation of metadata across all of the repositories that are members of the cohort. The services include the following components: Enterprise Connector Manager - Manages the list of open metadata repositories that the Enterprise OMRS Repository Connector should call to retrieve an enterprise view of the metadata collections supported by these repositories. Enterprise Repository Connector - Supports federated queries. Enterprise OMRS Connector Provider - The OCF Connector Provider factory for the Enterprise OMRS Repository Connector. Enterprise OMRS Repository Connector - Implements the OMRS Repository Connector interface that supports enterprise access to the list of open metadata repositories registered with the OMRS Enterprise Connector Manager. Enterprise OMRS Metadata Collection - Manages calls to the list of open metadata repositories registered with the OMRS Enterprise Connector Manager on behalf of the Enterprise OMRS Repository Connector. Enterprise OMRS Connector Properties - Provides the connected asset properties for the Enterprise OMRS Repository Connector. The enterprise repository services are enabled automatically in a metadata server when one or more Open Metadata Access Services ( OMAS ) are configured. Administration services \u00b6 The administration services drive the initialization of the OMRS at server startup, provide access to the OMRS 's internal status and coordinate the orderly termination of OMRS when the open metadata services are deactivated. OMRS 's administration services are called by the server's administration services. It is supplied with configuration information including: Connections for the connectors it should use. Information about the local repository (if any). Whether the enterprise repository services should be initialized. Details of any cohorts it should join. The administration services include the following components: OMRS Operational Services - supports the admin interface for the OMRS . OMRS Configuration Factory - manages default values and creation of connectors. OMRS Audit Log - manages the storage and retrieval of audit log records. OMRS Archive Manager - manages the loading of open metadata archives. Cohort services \u00b6 The cohort services manage the exchange of metadata between a repository and other members of an open metadata repository cohort . It includes the following components: OMRS Metadata Highway Manager - manages the OMRS Cohort Manager for each open metadata repository cohort that the local server belongs to. OMRS Cohort Manager - manages the components needed in the local server for it to act as a member of an open metadata repository cohort. OMRS Cohort Registry - manages registration exchanges with other members of a cohort on behalf of the local server. Local repository services \u00b6 The local repository services manage the interaction with the local server's metadata collection (stored in the local repository). They include the following components: Local OMRS Repository Connector - Implements the OMRS Repository Connector interface that supports access to the local metadata repository. Local OMRS Connector Provider - The OCF Connector Provider factory for the Local OMRS Repository Connector. Local OMRS Metadata Collection - Manages metadata requests for the local repository. Local OMRS Repository Content Manager - Provides an in-memory cache of open metadata type definitions (TypeDefs) that are used for validating of TypeDefs from other open metadata repositories and creation of new open metadata instances (entities and relationships). Local OMRS Instance Event Processor - Processes inbound Instance Events on behalf of the local repository. These events may come from one of the connected open metadata repository cohorts or the OMRS Archive Manager. OMRS REST Repository Services - Implements the server-side of the In-memory OMRS Repository Connector. OMRS REST Repository Connector - Implements the OMRS Repository Connector interface that supports metadata access to a remote open metadata repository service via the OMRS Repository REST API. OMRS REST Connector Provider - The OCF Connector Provider factory for the OMRS REST Repository Connector. OMRS REST Metadata Collection - Manages calls to the OMRS REST Repository Services in a remote open metadata repository. Event management services \u00b6 Isn't this a fifth subsystem? The event management services are not illustrated in the overview of OMRS subsystems as we do not consider them to be one of the major subsystems that can be used on its own. It is nonetheless a subsystem of the OMRS , and hence is described here for completeness -- you will also see it illustrated further below in the component-level diagram in the purple box. The event management services provide event passing: Inbound event passing from the cohort services to the optional local repository services and enterprise repository services. Outbound event passing from the local repository services to the optional cohort services and enterprise repository services. The event management services ensure that the other subsystems do not need to be aware of whether the other subsystems are active or not. The event management services include the following components: OMRS Repository Event Manager - Manages the distribution of repository events (TypeDef and Instance Events) within the local server's OMRS components. OMRS Event Listener - Receives Registry and Repository (TypeDef and Instance) events from the OMRS Topic for a cohort. OMRS Event Publisher - Sends Registry and Repository (TypeDef and Instance) events to the OMRS Topic. This may be the OMRS Topic for a cohort, or the OMRS Topic used by the Open Metadata Access Services ( OMAS ). Patterns \u00b6 The OMRS is highly configurable and runs in every type of OMAG Server . The figures below show the different combinations. Local repository (only) \u00b6 The OMRS can support the OMAS 's with access to a single, local-only repository - with no connectivity to other open metadata repositories. This is what runs in a metadata server that is not connected to an open metadata repository cohort . Access services (only) \u00b6 The OMRS can also support a server without any local repository - so that all metadata for the OMAS 's is coming through the cohort services from remote metadata repositories. This is the caller integration pattern supported by the metadata access point OMAG Server . Repository proxy \u00b6 The OMRS can support a server where the OMAS 's are not deployed and the local repository is configured to connect as an adapter for a non-native open metadata repository. The cohort services connect this metadata repository with other members in one or more cohorts. This is called the adapter integration pattern and is used in a repository proxy OMAG Server . Connected metadata server \u00b6 Of course, it is also possible to run all the OMRS components together as well, supporting the OMAS 's with a local repository and connectivity to other repositories through the cohort servers. This is what runs in a metadata server that is connected to an open metadata repository cohort . Administration subsystem (alone) \u00b6 Finally, the administration subsystem alone is active in the servers that are not cohort members , that is the Governance Servers and the view servers . OMRS components \u00b6 The different combinations of operation required means that the OMRS components need to be flexible and communicate with one another through well-defined interfaces so that component implementations can be swapped in and out to support different configurations.","title":"Open Metadata Repository Services"},{"location":"services/omrs/#repository-services-omrs-design","text":"On the left-hand side is the administration interface supported by the OMAG Server. This is where configuration is passed to the OMRS , and status and other relevant information is made available to the OMAG Administration Services. Along the top is the interface with the Open Metadata Access Services ( OMAS ) . The OMRS provides access to the open metadata repositories through both APIs (see Enterprise OMRS Repository Connector ) and events (see Enterprise OMRS Topic ). Along the bottom are the six types of connectors that provide the OMRS with access to the stores and system resources it needs to support the OMAS . These connectors enable the OMRS to be deployed into different types of server environments and connect with different types of infrastructure and services. Repository connectors provide a common interface for metadata repositories. The OMRS store connectors can range from simple file stores to enterprise / cloud provider admin repositories and the event topic can support different types of messaging infrastructure.","title":"Repository Services (OMRS) Design"},{"location":"services/omrs/#omrs-subsystems","text":"Inside the OMRS are 4 major subsystems (groups of components).","title":"OMRS subsystems"},{"location":"services/omrs/#enterprise-repository-services","text":"The enterprise repository services provide a virtual metadata repository by combining the content of multiple open metadata repositories and delivering this metadata through a single API and event topic. The Enterprise Repository Services provide the enterprise access metadata support for the OMASs, supporting the federation of metadata across all of the repositories that are members of the cohort. The services include the following components: Enterprise Connector Manager - Manages the list of open metadata repositories that the Enterprise OMRS Repository Connector should call to retrieve an enterprise view of the metadata collections supported by these repositories. Enterprise Repository Connector - Supports federated queries. Enterprise OMRS Connector Provider - The OCF Connector Provider factory for the Enterprise OMRS Repository Connector. Enterprise OMRS Repository Connector - Implements the OMRS Repository Connector interface that supports enterprise access to the list of open metadata repositories registered with the OMRS Enterprise Connector Manager. Enterprise OMRS Metadata Collection - Manages calls to the list of open metadata repositories registered with the OMRS Enterprise Connector Manager on behalf of the Enterprise OMRS Repository Connector. Enterprise OMRS Connector Properties - Provides the connected asset properties for the Enterprise OMRS Repository Connector. The enterprise repository services are enabled automatically in a metadata server when one or more Open Metadata Access Services ( OMAS ) are configured.","title":"Enterprise repository services"},{"location":"services/omrs/#administration-services","text":"The administration services drive the initialization of the OMRS at server startup, provide access to the OMRS 's internal status and coordinate the orderly termination of OMRS when the open metadata services are deactivated. OMRS 's administration services are called by the server's administration services. It is supplied with configuration information including: Connections for the connectors it should use. Information about the local repository (if any). Whether the enterprise repository services should be initialized. Details of any cohorts it should join. The administration services include the following components: OMRS Operational Services - supports the admin interface for the OMRS . OMRS Configuration Factory - manages default values and creation of connectors. OMRS Audit Log - manages the storage and retrieval of audit log records. OMRS Archive Manager - manages the loading of open metadata archives.","title":"Administration services"},{"location":"services/omrs/#cohort-services","text":"The cohort services manage the exchange of metadata between a repository and other members of an open metadata repository cohort . It includes the following components: OMRS Metadata Highway Manager - manages the OMRS Cohort Manager for each open metadata repository cohort that the local server belongs to. OMRS Cohort Manager - manages the components needed in the local server for it to act as a member of an open metadata repository cohort. OMRS Cohort Registry - manages registration exchanges with other members of a cohort on behalf of the local server.","title":"Cohort services"},{"location":"services/omrs/#local-repository-services","text":"The local repository services manage the interaction with the local server's metadata collection (stored in the local repository). They include the following components: Local OMRS Repository Connector - Implements the OMRS Repository Connector interface that supports access to the local metadata repository. Local OMRS Connector Provider - The OCF Connector Provider factory for the Local OMRS Repository Connector. Local OMRS Metadata Collection - Manages metadata requests for the local repository. Local OMRS Repository Content Manager - Provides an in-memory cache of open metadata type definitions (TypeDefs) that are used for validating of TypeDefs from other open metadata repositories and creation of new open metadata instances (entities and relationships). Local OMRS Instance Event Processor - Processes inbound Instance Events on behalf of the local repository. These events may come from one of the connected open metadata repository cohorts or the OMRS Archive Manager. OMRS REST Repository Services - Implements the server-side of the In-memory OMRS Repository Connector. OMRS REST Repository Connector - Implements the OMRS Repository Connector interface that supports metadata access to a remote open metadata repository service via the OMRS Repository REST API. OMRS REST Connector Provider - The OCF Connector Provider factory for the OMRS REST Repository Connector. OMRS REST Metadata Collection - Manages calls to the OMRS REST Repository Services in a remote open metadata repository.","title":"Local repository services"},{"location":"services/omrs/#event-management-services","text":"Isn't this a fifth subsystem? The event management services are not illustrated in the overview of OMRS subsystems as we do not consider them to be one of the major subsystems that can be used on its own. It is nonetheless a subsystem of the OMRS , and hence is described here for completeness -- you will also see it illustrated further below in the component-level diagram in the purple box. The event management services provide event passing: Inbound event passing from the cohort services to the optional local repository services and enterprise repository services. Outbound event passing from the local repository services to the optional cohort services and enterprise repository services. The event management services ensure that the other subsystems do not need to be aware of whether the other subsystems are active or not. The event management services include the following components: OMRS Repository Event Manager - Manages the distribution of repository events (TypeDef and Instance Events) within the local server's OMRS components. OMRS Event Listener - Receives Registry and Repository (TypeDef and Instance) events from the OMRS Topic for a cohort. OMRS Event Publisher - Sends Registry and Repository (TypeDef and Instance) events to the OMRS Topic. This may be the OMRS Topic for a cohort, or the OMRS Topic used by the Open Metadata Access Services ( OMAS ).","title":"Event management services"},{"location":"services/omrs/#patterns","text":"The OMRS is highly configurable and runs in every type of OMAG Server . The figures below show the different combinations.","title":"Patterns"},{"location":"services/omrs/#local-repository-only","text":"The OMRS can support the OMAS 's with access to a single, local-only repository - with no connectivity to other open metadata repositories. This is what runs in a metadata server that is not connected to an open metadata repository cohort .","title":"Local repository (only)"},{"location":"services/omrs/#access-services-only","text":"The OMRS can also support a server without any local repository - so that all metadata for the OMAS 's is coming through the cohort services from remote metadata repositories. This is the caller integration pattern supported by the metadata access point OMAG Server .","title":"Access services (only)"},{"location":"services/omrs/#repository-proxy","text":"The OMRS can support a server where the OMAS 's are not deployed and the local repository is configured to connect as an adapter for a non-native open metadata repository. The cohort services connect this metadata repository with other members in one or more cohorts. This is called the adapter integration pattern and is used in a repository proxy OMAG Server .","title":"Repository proxy"},{"location":"services/omrs/#connected-metadata-server","text":"Of course, it is also possible to run all the OMRS components together as well, supporting the OMAS 's with a local repository and connectivity to other repositories through the cohort servers. This is what runs in a metadata server that is connected to an open metadata repository cohort .","title":"Connected metadata server"},{"location":"services/omrs/#administration-subsystem-alone","text":"Finally, the administration subsystem alone is active in the servers that are not cohort members , that is the Governance Servers and the view servers .","title":"Administration subsystem (alone)"},{"location":"services/omrs/#omrs-components","text":"The different combinations of operation required means that the OMRS components need to be flexible and communicate with one another through well-defined interfaces so that component implementations can be swapped in and out to support different configurations.","title":"OMRS components"},{"location":"services/omrs/cohort/","text":"Cohorts \u00b6 An Open Metadata Repository Cohort (or more simply, just a cohort ) is a collection of servers sharing metadata using the Open Metadata Repository Services ( OMRS ) . This sharing is peer-to-peer. Once a server becomes a member of the cohort, it can share metadata with, and receive metadata from, any other member. The cohort itself is self-configuring. At the heart of it is between one and four shared topics. OMRS needs to be flexible to support different performance and availability requirements. For example, where metadata is changing rapidly (such as in a data lake), this metadata should be dynamically queried from the repository where it was created and is being maintained because the rate of updates mean it would cost a lot of network traffic to keep a copy of this metadata up to date. The repository where a piece of metadata (metadata instance) was created and where it is maintained is called its home metadata repository . On the other hand, governance definitions (such as policies) and glossary terms rarely change. They are often administered centrally by the governance team and then linked to all metadata that describes the organization's data resources. Thus, it makes sense for this metadata to be replicated across the repositories within the cohort. These copies are called reference copies of the metadata, and they are read-only. The role of the OMRS is to optimize access to metadata across the cohort by using a combination of replication and federated queries, driven by the metadata workload from the connected tools. Formation of a cohort \u00b6 Cohort membership is established dynamically. This is through the cohort topic(s) . First server \u00b6 To join an open metadata repository cohort, a server must integrate with the OMRS module. OMRS then manages the metadata exchange. When OMRS running inside the server is configured to join a cohort it first adds a registration event to the cohort topic(s). This event identifies the server, its metadata repository (if any) and its capabilities. Subsequent servers \u00b6 When another server joins the cohort, it also adds its registration event to the cohort topic(s) and begins to receive the registration events from other members. The other members respond with re-registration events to ensure the new member has the latest information about the originator's capabilities. The exchange of registration information causes all members to verify that they have the latest information about their peers. This is maintained in their own cohort registry store so that they can reconfigure themselves on restart without needing the other members to resend their registration information. Peer-to-peer operation \u00b6 Once the registration information is exchanged and stored in each member's cohort registry store, it is ready to issue federated queries across the cohort, and respond to requests for metadata from other members. These requests can both retrieve metadata and maintain metadata in the home metadata repository . The management of federated queries and the routing of maintenance requests is managed by OMRS 's enterprise repository services . The enterprise repository services are configured with the registration information from across the cohort at the same time as the cohort registry store is updated. This process is managed by the cohort registry . The registration information includes the URL Root and server name of the member. The federation capability in each member allows it to issue metadata create, update, delete and search requests to each and every member of the cohort. Primary mechanism for accessing metadata This peer-to-peer operation and federated queries are the primary mechanism for accessing metadata, because the access services use federated queries for every request they make for metadata. Metadata exchange \u00b6 Once the cohort membership is established, the server begins publishing information using instance events about changes to the home metadata instances in their repository. These events can be used by other members to maintain a cache of reference copies of this metadata to improve availability of the metadata and retrieval performance. Updates to this metadata will, however, be automatically routed to the home repository by the enterprise repository services: Metadata refresh A member may also request that metadata is \"refreshed\" across the cohort. The originator of the requested metadata then sends the latest version of this metadata to the rest of the cohort through the cohort topic. This mechanism is useful to seed the cache in a new member of the cohort and is invoked as a result of a federated query issued from the new member. Dynamic changes to types \u00b6 Finally, as type definitions (TypeDefs) are added and updated, the cohort members send out events to allow the other members to verify that this type does not conflict with any of their types. Any conflicts in the types causes audit log messages to be logged in all members, prompting action to resolve the conflicts. Leaving the cohort \u00b6 When an OMAG Server permanently leaves the cohort, it sends an unregistration request. This enables the other members to remove the parting member from their registries. Enabling cohort membership \u00b6 Egeria provides a number of pre-built cohort members . One of them, the repository proxy provides a simple way to integrate a third party server into a cohort by creating an OMRS Repository Connector and optional Event Mapper Connector to map between the third party APIs/events and the repository service's equivalents A more bespoke integration involves: Creating an OMRS repository connector and optional event mapper connector Designing how to configure the OMRS Services for your metadata repository. Typically, this is done by extending the existing administration services of the metadata repository, but Egeria also offers some pre-built administration services that can be used or modified. Plugging the OMRS and any administration services into the metadata repository's security module so that requests to the server can be secured against unauthorized access. Integrating the OMRS , administration and security capability into your product. There are different integration patterns available to help you choose the best approach for your product. Each method is optimized for specific use cases and so the metadata repository can only play a full role in the open metadata use cases if it supports all integration methods. These are: Support for an OMRS repository connector to allow open metadata API calls to the repository to create, query, update and delete metadata stored in the repository. The OMRS connectors support the Open Connector Framework ( OCF ) to provide a call interface to the metadata repositories. The OMRS Repository Connector API is a standard interface for all metadata repositories. This enables services such as the Enterprise OMRS Repository Connector to interact with 1 or many metadata repositories through the same interface. The connection configuration it passes to the OCF determines which type of OMRS connector is returned by the OCF . Support for the OMRS event notifications that are used to synchronize selective metadata between the metadata repositories. Cohort members \u00b6 A cohort member is an OMAG Server that is registered with at least one open metadata repository cohort. Management of a server's membership is handled by the cohort services . The exchange of metadata uses the Open Metadata Repository Services ( OMRS ) interfaces which gives fine-grained 1 metadata notifications and updates. During server start up, the repository services detect the configuration of at least one cohort and starts the metadata highway manager . The metadata highway manager creates a cohort manager for each cohort configuration. The cohort manager manages the initialization and shutdown of the server's connectivity to a cohort, including the management of the cohort registry . The server's metadata security connector provides fine-grained control on which metadata is sent, received and/or stored by the server. This level of control is necessary for metadata repositories that are managing specific collections of valuable objects such as Assets . The types of cohort members include: Metadata server Metadata access point Repository proxy Conformance test server Explore hands-on The administration hands-on lab called \"Understanding Cohort Configuration Lab\" provides an opportunity to query the cohort registries of cohort members as they exchange metadata for Coco Pharmaceuticals. Cohort registration \u00b6 Each repository in the cohort has a cohort registry that supports the registration of the metadata repositories across the cohort. Through the registration process, each cohort registry assembles a list of all members of the cohort. This is saved in the cohort registry store . The list of connections to the remote members of the cohort are passed to the OMRS Enterprise Connector Manager that in turn manages the configuration of the Enterprise OMRS Repository Connectors. The Enterprise OMRS Connector provides federated query support across the metadata cohort for the Open Metadata Access Services ( OMAS ) . When a metadata repository registers with the cohort registry , the administrator may either supply a unique server identifier, or ask the OMRS to generate one. This server identifier (the metadata collection ID ) is used in the OMRS event notifications, and on OMRS repository connector calls to identify the location of the home copy of the metadata entities and to identify which repository is requesting a service or supports a particular function. Once the metadata repository has registered with the cohort registry , it is a member of the metadata repository cohort and can synchronize and share metadata with other repositories in the cohort through the OMRS topic(s) . Registering with multiple cohorts A single metadata repository can register with multiple metadata cohorts as long as its server identifier is unique across all cohorts that it joins and it manages the posting of events to the appropriate OMRS topic for each cohort it registers with. This capability is useful for a metadata repository that is aggregating reference copies of metadata from multiple open metadata repository cohorts. Cohort registry \u00b6 The cohort registry resides in each cohort member . It is responsible for registering a member with a specific open metadata repository cohort and maintaining a list of the other members of this cohort. The registration process is managed by exchanging registry events over the cohort topic(s) . The cohort registry maintains its record of the membership of the cohort in a cohort registry store . You may want to see the OMRS metamodel for more details on the granularity of metadata exchange. \u21a9","title":"Cohorts"},{"location":"services/omrs/cohort/#cohorts","text":"An Open Metadata Repository Cohort (or more simply, just a cohort ) is a collection of servers sharing metadata using the Open Metadata Repository Services ( OMRS ) . This sharing is peer-to-peer. Once a server becomes a member of the cohort, it can share metadata with, and receive metadata from, any other member. The cohort itself is self-configuring. At the heart of it is between one and four shared topics. OMRS needs to be flexible to support different performance and availability requirements. For example, where metadata is changing rapidly (such as in a data lake), this metadata should be dynamically queried from the repository where it was created and is being maintained because the rate of updates mean it would cost a lot of network traffic to keep a copy of this metadata up to date. The repository where a piece of metadata (metadata instance) was created and where it is maintained is called its home metadata repository . On the other hand, governance definitions (such as policies) and glossary terms rarely change. They are often administered centrally by the governance team and then linked to all metadata that describes the organization's data resources. Thus, it makes sense for this metadata to be replicated across the repositories within the cohort. These copies are called reference copies of the metadata, and they are read-only. The role of the OMRS is to optimize access to metadata across the cohort by using a combination of replication and federated queries, driven by the metadata workload from the connected tools.","title":"Cohorts"},{"location":"services/omrs/cohort/#formation-of-a-cohort","text":"Cohort membership is established dynamically. This is through the cohort topic(s) .","title":"Formation of a cohort"},{"location":"services/omrs/cohort/#first-server","text":"To join an open metadata repository cohort, a server must integrate with the OMRS module. OMRS then manages the metadata exchange. When OMRS running inside the server is configured to join a cohort it first adds a registration event to the cohort topic(s). This event identifies the server, its metadata repository (if any) and its capabilities.","title":"First server"},{"location":"services/omrs/cohort/#subsequent-servers","text":"When another server joins the cohort, it also adds its registration event to the cohort topic(s) and begins to receive the registration events from other members. The other members respond with re-registration events to ensure the new member has the latest information about the originator's capabilities. The exchange of registration information causes all members to verify that they have the latest information about their peers. This is maintained in their own cohort registry store so that they can reconfigure themselves on restart without needing the other members to resend their registration information.","title":"Subsequent servers"},{"location":"services/omrs/cohort/#peer-to-peer-operation","text":"Once the registration information is exchanged and stored in each member's cohort registry store, it is ready to issue federated queries across the cohort, and respond to requests for metadata from other members. These requests can both retrieve metadata and maintain metadata in the home metadata repository . The management of federated queries and the routing of maintenance requests is managed by OMRS 's enterprise repository services . The enterprise repository services are configured with the registration information from across the cohort at the same time as the cohort registry store is updated. This process is managed by the cohort registry . The registration information includes the URL Root and server name of the member. The federation capability in each member allows it to issue metadata create, update, delete and search requests to each and every member of the cohort. Primary mechanism for accessing metadata This peer-to-peer operation and federated queries are the primary mechanism for accessing metadata, because the access services use federated queries for every request they make for metadata.","title":"Peer-to-peer operation"},{"location":"services/omrs/cohort/#metadata-exchange","text":"Once the cohort membership is established, the server begins publishing information using instance events about changes to the home metadata instances in their repository. These events can be used by other members to maintain a cache of reference copies of this metadata to improve availability of the metadata and retrieval performance. Updates to this metadata will, however, be automatically routed to the home repository by the enterprise repository services: Metadata refresh A member may also request that metadata is \"refreshed\" across the cohort. The originator of the requested metadata then sends the latest version of this metadata to the rest of the cohort through the cohort topic. This mechanism is useful to seed the cache in a new member of the cohort and is invoked as a result of a federated query issued from the new member.","title":"Metadata exchange"},{"location":"services/omrs/cohort/#dynamic-changes-to-types","text":"Finally, as type definitions (TypeDefs) are added and updated, the cohort members send out events to allow the other members to verify that this type does not conflict with any of their types. Any conflicts in the types causes audit log messages to be logged in all members, prompting action to resolve the conflicts.","title":"Dynamic changes to types"},{"location":"services/omrs/cohort/#leaving-the-cohort","text":"When an OMAG Server permanently leaves the cohort, it sends an unregistration request. This enables the other members to remove the parting member from their registries.","title":"Leaving the cohort"},{"location":"services/omrs/cohort/#enabling-cohort-membership","text":"Egeria provides a number of pre-built cohort members . One of them, the repository proxy provides a simple way to integrate a third party server into a cohort by creating an OMRS Repository Connector and optional Event Mapper Connector to map between the third party APIs/events and the repository service's equivalents A more bespoke integration involves: Creating an OMRS repository connector and optional event mapper connector Designing how to configure the OMRS Services for your metadata repository. Typically, this is done by extending the existing administration services of the metadata repository, but Egeria also offers some pre-built administration services that can be used or modified. Plugging the OMRS and any administration services into the metadata repository's security module so that requests to the server can be secured against unauthorized access. Integrating the OMRS , administration and security capability into your product. There are different integration patterns available to help you choose the best approach for your product. Each method is optimized for specific use cases and so the metadata repository can only play a full role in the open metadata use cases if it supports all integration methods. These are: Support for an OMRS repository connector to allow open metadata API calls to the repository to create, query, update and delete metadata stored in the repository. The OMRS connectors support the Open Connector Framework ( OCF ) to provide a call interface to the metadata repositories. The OMRS Repository Connector API is a standard interface for all metadata repositories. This enables services such as the Enterprise OMRS Repository Connector to interact with 1 or many metadata repositories through the same interface. The connection configuration it passes to the OCF determines which type of OMRS connector is returned by the OCF . Support for the OMRS event notifications that are used to synchronize selective metadata between the metadata repositories.","title":"Enabling cohort membership"},{"location":"services/omrs/cohort/#cohort-members","text":"A cohort member is an OMAG Server that is registered with at least one open metadata repository cohort. Management of a server's membership is handled by the cohort services . The exchange of metadata uses the Open Metadata Repository Services ( OMRS ) interfaces which gives fine-grained 1 metadata notifications and updates. During server start up, the repository services detect the configuration of at least one cohort and starts the metadata highway manager . The metadata highway manager creates a cohort manager for each cohort configuration. The cohort manager manages the initialization and shutdown of the server's connectivity to a cohort, including the management of the cohort registry . The server's metadata security connector provides fine-grained control on which metadata is sent, received and/or stored by the server. This level of control is necessary for metadata repositories that are managing specific collections of valuable objects such as Assets . The types of cohort members include: Metadata server Metadata access point Repository proxy Conformance test server Explore hands-on The administration hands-on lab called \"Understanding Cohort Configuration Lab\" provides an opportunity to query the cohort registries of cohort members as they exchange metadata for Coco Pharmaceuticals.","title":"Cohort members"},{"location":"services/omrs/cohort/#cohort-registration","text":"Each repository in the cohort has a cohort registry that supports the registration of the metadata repositories across the cohort. Through the registration process, each cohort registry assembles a list of all members of the cohort. This is saved in the cohort registry store . The list of connections to the remote members of the cohort are passed to the OMRS Enterprise Connector Manager that in turn manages the configuration of the Enterprise OMRS Repository Connectors. The Enterprise OMRS Connector provides federated query support across the metadata cohort for the Open Metadata Access Services ( OMAS ) . When a metadata repository registers with the cohort registry , the administrator may either supply a unique server identifier, or ask the OMRS to generate one. This server identifier (the metadata collection ID ) is used in the OMRS event notifications, and on OMRS repository connector calls to identify the location of the home copy of the metadata entities and to identify which repository is requesting a service or supports a particular function. Once the metadata repository has registered with the cohort registry , it is a member of the metadata repository cohort and can synchronize and share metadata with other repositories in the cohort through the OMRS topic(s) . Registering with multiple cohorts A single metadata repository can register with multiple metadata cohorts as long as its server identifier is unique across all cohorts that it joins and it manages the posting of events to the appropriate OMRS topic for each cohort it registers with. This capability is useful for a metadata repository that is aggregating reference copies of metadata from multiple open metadata repository cohorts.","title":"Cohort registration"},{"location":"services/omrs/cohort/#cohort-registry","text":"The cohort registry resides in each cohort member . It is responsible for registering a member with a specific open metadata repository cohort and maintaining a list of the other members of this cohort. The registration process is managed by exchanging registry events over the cohort topic(s) . The cohort registry maintains its record of the membership of the cohort in a cohort registry store . You may want to see the OMRS metamodel for more details on the granularity of metadata exchange. \u21a9","title":"Cohort registry"},{"location":"services/omrs/metadata-events/","text":"Metadata events \u00b6 OMRS events are messages used to notify members of an open metadata repository cohort of changes to: The membership of the open metadata repository cohort. The types of metadata being managed by members of the open metadata repository cohort. The changes to the metadata instances stored by each of the members of the open metadata repository cohort. The motivation for sending OMRS events between the members of the open metadata repository cohort is to ensure open metadata is as widely available as security-permitted and access to it is as efficient as possible. The events are broadcast to the membership of the open metadata repository cohort through one or more OMRS event topic(s) and no replies are expected. Each member is expected to receive each event and make a local decision on whether to act on it or ignore it. OMRS event topic \u00b6 The OMRS event topic(s) are one or more topics provided through an event broker (typically Apache Kafka) that an open metadata repository cohort uses to synchronize metadata between metadata repositories. They are accessed by the Open Metadata Repository Services ( OMRS ) components through the OMRS topic connector . The OMRS topic connector is a pluggable OCF connector that allows the use of different messaging infrastructures to support the OMRS event topic(s) without affecting the implementation of the OMRS . OMRS events \u00b6 The OMRS event topic is used to send and receive OMRS events . There are three types of OMRS events that are sent/received on the OMRS event topics: TypeDef events and instance events are collectively called repository events because they affect the contents of the open metadata repositories. Cohort topic(s) \u00b6 A cohort can be configured to use: One OMRS event topic for all types of OMRS events Three OMRS event topics , one for each type of OMRS event Which to use? Using the single cohort topic is ok for small environments. However, the use of the three topics gives the best throughput, ensures rapid inclusion of new members in the cohort and is required for clustered members: when multiple instances of the same member operate in a cohort for high availability (HA). Notes for cohorts involving members at versions prior to 2.11 Versions of the OMRS prior to release 2.11 only support the single cohort topic. To allow a server running an older version of OMRS to join a cohort using the three topics, it is possible to configure the other members to use both the single topic and the three topics. This ensures all members see the same metadata, but the members configured to use both options will process all events twice. This configuration should only be used when absolutely needed and attention should be paid to upgrading the back-level server so it can use the three topics. Details of configuring the different topic options can be found in the administration guide . Enterprise OMRS event topic \u00b6 The enterprise repository services combine the OMRS events from all open metadata repository cohorts that the server is connected to and makes them available to each local Open Metadata Access Service ( OMAS ) . This is called the enterprise OMRS event topic . By default, it is implemented as an in-memory open metadata topic . Event types \u00b6 Every event has a: Timestamp - indicating the time the event was created. Originator - detailing the server that originated the message. The metadata collection ID of the sending open metadata repository's metadata collection. (The only time this is not set is when a message is sent from a server that does not have a local metadata repository configured.) The server name, type and organization are optional descriptive fields used in audit logging and problem determination. These values are set up through the administration services . Version - the version number of the event (set to OMRS V1.0 in the initial version). The setting of the category determines which category-specific section is used. Each category-specific section begins with a category-specific event type that describes the type of the event, and hence the properties that will be found in the category-specific section. If the event is reporting an error, there is also an optional error section. The error section has an error code, error message and a target metadata collection ID . The target metadata collection ID indicates which member of the cohort is the target for the error message. Other members may pick up the error and act on it as well. Registry events \u00b6 Registry events are used by metadata servers to register with an open metadata cohort . TypeDef events \u00b6 TypeDef events are used by members of an open metadata cohort to exchange information about the open metadata types they support. Instance events \u00b6 Instance events are used by members of an open metadata cohort to exchange information about changes to metadata instances.","title":"Metadata Events"},{"location":"services/omrs/metadata-events/#metadata-events","text":"OMRS events are messages used to notify members of an open metadata repository cohort of changes to: The membership of the open metadata repository cohort. The types of metadata being managed by members of the open metadata repository cohort. The changes to the metadata instances stored by each of the members of the open metadata repository cohort. The motivation for sending OMRS events between the members of the open metadata repository cohort is to ensure open metadata is as widely available as security-permitted and access to it is as efficient as possible. The events are broadcast to the membership of the open metadata repository cohort through one or more OMRS event topic(s) and no replies are expected. Each member is expected to receive each event and make a local decision on whether to act on it or ignore it.","title":"Metadata events"},{"location":"services/omrs/metadata-events/#omrs-event-topic","text":"The OMRS event topic(s) are one or more topics provided through an event broker (typically Apache Kafka) that an open metadata repository cohort uses to synchronize metadata between metadata repositories. They are accessed by the Open Metadata Repository Services ( OMRS ) components through the OMRS topic connector . The OMRS topic connector is a pluggable OCF connector that allows the use of different messaging infrastructures to support the OMRS event topic(s) without affecting the implementation of the OMRS .","title":"OMRS event topic"},{"location":"services/omrs/metadata-events/#omrs-events","text":"The OMRS event topic is used to send and receive OMRS events . There are three types of OMRS events that are sent/received on the OMRS event topics: TypeDef events and instance events are collectively called repository events because they affect the contents of the open metadata repositories.","title":"OMRS events"},{"location":"services/omrs/metadata-events/#cohort-topics","text":"A cohort can be configured to use: One OMRS event topic for all types of OMRS events Three OMRS event topics , one for each type of OMRS event Which to use? Using the single cohort topic is ok for small environments. However, the use of the three topics gives the best throughput, ensures rapid inclusion of new members in the cohort and is required for clustered members: when multiple instances of the same member operate in a cohort for high availability (HA). Notes for cohorts involving members at versions prior to 2.11 Versions of the OMRS prior to release 2.11 only support the single cohort topic. To allow a server running an older version of OMRS to join a cohort using the three topics, it is possible to configure the other members to use both the single topic and the three topics. This ensures all members see the same metadata, but the members configured to use both options will process all events twice. This configuration should only be used when absolutely needed and attention should be paid to upgrading the back-level server so it can use the three topics. Details of configuring the different topic options can be found in the administration guide .","title":"Cohort topic(s)"},{"location":"services/omrs/metadata-events/#enterprise-omrs-event-topic","text":"The enterprise repository services combine the OMRS events from all open metadata repository cohorts that the server is connected to and makes them available to each local Open Metadata Access Service ( OMAS ) . This is called the enterprise OMRS event topic . By default, it is implemented as an in-memory open metadata topic .","title":"Enterprise OMRS event topic"},{"location":"services/omrs/metadata-events/#event-types","text":"Every event has a: Timestamp - indicating the time the event was created. Originator - detailing the server that originated the message. The metadata collection ID of the sending open metadata repository's metadata collection. (The only time this is not set is when a message is sent from a server that does not have a local metadata repository configured.) The server name, type and organization are optional descriptive fields used in audit logging and problem determination. These values are set up through the administration services . Version - the version number of the event (set to OMRS V1.0 in the initial version). The setting of the category determines which category-specific section is used. Each category-specific section begins with a category-specific event type that describes the type of the event, and hence the properties that will be found in the category-specific section. If the event is reporting an error, there is also an optional error section. The error section has an error code, error message and a target metadata collection ID . The target metadata collection ID indicates which member of the cohort is the target for the error message. Other members may pick up the error and act on it as well.","title":"Event types"},{"location":"services/omrs/metadata-events/#registry-events","text":"Registry events are used by metadata servers to register with an open metadata cohort .","title":"Registry events"},{"location":"services/omrs/metadata-events/#typedef-events","text":"TypeDef events are used by members of an open metadata cohort to exchange information about the open metadata types they support.","title":"TypeDef events"},{"location":"services/omrs/metadata-events/#instance-events","text":"Instance events are used by members of an open metadata cohort to exchange information about changes to metadata instances.","title":"Instance events"},{"location":"services/omrs/metadata-repositories/","text":"Metadata repositories \u00b6 A metadata repository that supports the open metadata repository standards defined by the Open Metadata Repository Services ( OMRS ) is called an \"open metadata repository\". Home metadata repositories \u00b6 The metadata repository where a metadata entity or relationship is created is called its home repository . Metadata can only be updated and deleted in its home repository The Open Metadata Repository Services ( OMRS ) is responsible for sharing this metadata with other metadata repositories who are members of the same cohort . Reference copies \u00b6 These shared copies are called reference copies . Reference copies are read-only Update requests to a reference copy are automatically redirected to the home repository by the OMRS , without the caller being aware. Every metadata repository in a cohort has a unique identifier called the local metadata collection id . This identifier is set up in the server configuration and shared when this server connects to a cohort . When metadata is shared by OMRS , each element is tagged with the metadata collection id of its home repository. OMRS is able to route update requests to the right server by comparing the metadata collection id in the metadata instance with the cohort registration information passed between members of the cohort when they connect. Metadata collections \u00b6 A metadata collection refers to a set of metadata instances that are being maintained and accessed as a coherent set of information. These instances are explicitly typed using open metadata type definitions (TypeDefs) . Typically, metadata collections are stored in a single metadata repository. However, the Open Metadata Access Services ( OMAS ) provide access to a federated metadata collection called the enterprise metadata collection . Enterprise metadata collection \u00b6 The enterprise metadata collection is the metadata collection that is the combination of metadata from all metadata collections held by the open metadata repositories that are members of the same open metadata repository cohorts as the server where the enterprise repository services are enabled. Metadata collection ID \u00b6 Every metadata repository has a unique identifier called the local-metadata-collection-id . This identifier is assigned automatically during the configuration of the local repository but can be overridden through administrative commands. A new local metadata collection id is assigned when the local repository is set up In the following server configuration, the local metadata collection id is 1b96495f-82d3-4224-9fdd-31bcb84c224c . It also appears in an audit log message written at start up. OMRS-AUDIT-0001 The Open Metadata Repository Services (OMRS) is initializing : : : OMRS-AUDIT-0003 The local repository is initializing with metadata collection id 1b96495f-82d3-4224-9fdd-31bcb84c224c If the server is connected to a cohort, the local cohort registry sends the local metadata collection id and a registration event like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 { \"localRegistration\" : { \"metadataCollectionId\" : \"1b96495f-82d3-4224-9fdd-31bcb84c224c\" , \"serverName\" : \"cocoMDS1\" , \"serverType\" : \"Open Metadata and Governance Server\" , \"registrationTime\" : 1531820378765 , \"repositoryConnection\" : { \"class\" : \"Connection\" , \"type\" : { \"type\" : \"ElementType\" , \"elementTypeId\" : \"114e9f8f-5ff3-4c32-bd37-a7eb42712253\" , \"elementTypeName\" : \"Connection\" , \"elementTypeVersion\" : 1 , \"elementTypeDescription\" : \"A set of properties to identify and configure a connector instance.\" , \"elementOrigin\" : \"CONFIGURATION\" }, \"guid\" : \"858be98b-49d2-4ccf-9b23-01085a5f473f\" , \"qualifiedName\" : \"DefaultRepositoryRESTAPI.Connection.cocoMDS1\" , \"name\" : \"DefaultRepositoryRESTAPI.Connection.cocoMDS1\" , \"description\" : \"OMRS default repository REST API connection.\" , \"connectorType\" : { \"type\" : { \"type\" : \"ElementType\" , \"elementTypeId\" : \"954421eb-33a6-462d-a8ca-b5709a1bd0d4\" , \"elementTypeName\" : \"ConnectorType\" , \"elementTypeVersion\" : 1 , \"elementTypeDescription\" : \"A set of properties describing a type of connector.\" , \"elementOrigin\" : \"CONFIGURATION\" }, \"guid\" : \"64e67923-8190-45ea-8f96-39320d638c02\" , \"qualifiedName\" : \"DefaultRepositoryRESTAPI.ConnectorType.cocoMDS1\" , \"name\" : \"DefaultRepositoryRESTAPI.ConnectorType.cocoMDS1\" , \"description\" : \"OMRS default repository REST API connector type.\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.repositoryservices.rest.repositoryconnector.OMRSRESTRepositoryConnectorProvider\" }, \"endpoint\" : { \"type\" : { \"type\" : \"ElementType\" , \"elementTypeId\" : \"dbc20663-d705-4ff0-8424-80c262c6b8e7\" , \"elementTypeName\" : \"Endpoint\" , \"elementTypeVersion\" : 1 , \"elementTypeDescription\" : \"Description of the network address and related information needed to call a software service.\" , \"elementOrigin\" : \"CONFIGURATION\" }, \"guid\" : \"cee85898-43aa-4af5-9bbd-2bed809d1acb\" , \"qualifiedName\" : \"DefaultRepositoryRESTAPI.Endpoint.cocoMDS1\" , \"name\" : \"DefaultRepositoryRESTAPI.Endpoint.cocoMDS1\" , \"description\" : \"OMRS default repository REST API endpoint.\" , \"address\" : \"https://localhost:9443/openmetadata/repositoryservices/\" } } } }","title":"Metadata Repositories"},{"location":"services/omrs/metadata-repositories/#metadata-repositories","text":"A metadata repository that supports the open metadata repository standards defined by the Open Metadata Repository Services ( OMRS ) is called an \"open metadata repository\".","title":"Metadata repositories"},{"location":"services/omrs/metadata-repositories/#home-metadata-repositories","text":"The metadata repository where a metadata entity or relationship is created is called its home repository . Metadata can only be updated and deleted in its home repository The Open Metadata Repository Services ( OMRS ) is responsible for sharing this metadata with other metadata repositories who are members of the same cohort .","title":"Home metadata repositories"},{"location":"services/omrs/metadata-repositories/#reference-copies","text":"These shared copies are called reference copies . Reference copies are read-only Update requests to a reference copy are automatically redirected to the home repository by the OMRS , without the caller being aware. Every metadata repository in a cohort has a unique identifier called the local metadata collection id . This identifier is set up in the server configuration and shared when this server connects to a cohort . When metadata is shared by OMRS , each element is tagged with the metadata collection id of its home repository. OMRS is able to route update requests to the right server by comparing the metadata collection id in the metadata instance with the cohort registration information passed between members of the cohort when they connect.","title":"Reference copies"},{"location":"services/omrs/metadata-repositories/#metadata-collections","text":"A metadata collection refers to a set of metadata instances that are being maintained and accessed as a coherent set of information. These instances are explicitly typed using open metadata type definitions (TypeDefs) . Typically, metadata collections are stored in a single metadata repository. However, the Open Metadata Access Services ( OMAS ) provide access to a federated metadata collection called the enterprise metadata collection .","title":"Metadata collections"},{"location":"services/omrs/metadata-repositories/#enterprise-metadata-collection","text":"The enterprise metadata collection is the metadata collection that is the combination of metadata from all metadata collections held by the open metadata repositories that are members of the same open metadata repository cohorts as the server where the enterprise repository services are enabled.","title":"Enterprise metadata collection"},{"location":"services/omrs/metadata-repositories/#metadata-collection-id","text":"Every metadata repository has a unique identifier called the local-metadata-collection-id . This identifier is assigned automatically during the configuration of the local repository but can be overridden through administrative commands. A new local metadata collection id is assigned when the local repository is set up In the following server configuration, the local metadata collection id is 1b96495f-82d3-4224-9fdd-31bcb84c224c . It also appears in an audit log message written at start up. OMRS-AUDIT-0001 The Open Metadata Repository Services (OMRS) is initializing : : : OMRS-AUDIT-0003 The local repository is initializing with metadata collection id 1b96495f-82d3-4224-9fdd-31bcb84c224c If the server is connected to a cohort, the local cohort registry sends the local metadata collection id and a registration event like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 { \"localRegistration\" : { \"metadataCollectionId\" : \"1b96495f-82d3-4224-9fdd-31bcb84c224c\" , \"serverName\" : \"cocoMDS1\" , \"serverType\" : \"Open Metadata and Governance Server\" , \"registrationTime\" : 1531820378765 , \"repositoryConnection\" : { \"class\" : \"Connection\" , \"type\" : { \"type\" : \"ElementType\" , \"elementTypeId\" : \"114e9f8f-5ff3-4c32-bd37-a7eb42712253\" , \"elementTypeName\" : \"Connection\" , \"elementTypeVersion\" : 1 , \"elementTypeDescription\" : \"A set of properties to identify and configure a connector instance.\" , \"elementOrigin\" : \"CONFIGURATION\" }, \"guid\" : \"858be98b-49d2-4ccf-9b23-01085a5f473f\" , \"qualifiedName\" : \"DefaultRepositoryRESTAPI.Connection.cocoMDS1\" , \"name\" : \"DefaultRepositoryRESTAPI.Connection.cocoMDS1\" , \"description\" : \"OMRS default repository REST API connection.\" , \"connectorType\" : { \"type\" : { \"type\" : \"ElementType\" , \"elementTypeId\" : \"954421eb-33a6-462d-a8ca-b5709a1bd0d4\" , \"elementTypeName\" : \"ConnectorType\" , \"elementTypeVersion\" : 1 , \"elementTypeDescription\" : \"A set of properties describing a type of connector.\" , \"elementOrigin\" : \"CONFIGURATION\" }, \"guid\" : \"64e67923-8190-45ea-8f96-39320d638c02\" , \"qualifiedName\" : \"DefaultRepositoryRESTAPI.ConnectorType.cocoMDS1\" , \"name\" : \"DefaultRepositoryRESTAPI.ConnectorType.cocoMDS1\" , \"description\" : \"OMRS default repository REST API connector type.\" , \"connectorProviderClassName\" : \"org.odpi.openmetadata.adapters.repositoryservices.rest.repositoryconnector.OMRSRESTRepositoryConnectorProvider\" }, \"endpoint\" : { \"type\" : { \"type\" : \"ElementType\" , \"elementTypeId\" : \"dbc20663-d705-4ff0-8424-80c262c6b8e7\" , \"elementTypeName\" : \"Endpoint\" , \"elementTypeVersion\" : 1 , \"elementTypeDescription\" : \"Description of the network address and related information needed to call a software service.\" , \"elementOrigin\" : \"CONFIGURATION\" }, \"guid\" : \"cee85898-43aa-4af5-9bbd-2bed809d1acb\" , \"qualifiedName\" : \"DefaultRepositoryRESTAPI.Endpoint.cocoMDS1\" , \"name\" : \"DefaultRepositoryRESTAPI.Endpoint.cocoMDS1\" , \"description\" : \"OMRS default repository REST API endpoint.\" , \"address\" : \"https://localhost:9443/openmetadata/repositoryservices/\" } } } }","title":"Metadata collection ID"},{"location":"types/","text":"The open metadata type system \u00b6 Knowledge about data is spread amongst many people and systems. One of the roles of a metadata repository is to provide a place where this knowledge can be collected and correlated, as automated as possible. To enable different tools and processes to populate the metadata repository we need agreement on what data should be stored and in what format (structures). The different areas of metadata that we need to support for a wide range of metadata management and governance tasks include: This metadata may be spread across different metadata repositories that each specialize in particular use cases or communities of users. Area Description Area 0 describes base types and infrastructure. This includes the root type for all open metadata entities called OpenMetadataRoot and types for Asset , DataSet , Infrastructure , Process , Referenceable , SoftwareServer and Host . Area 1 collects information from people using the data assets. It includes their use of the assets and their feedback. It also manages crowd-sourced enhancements to the metadata from other areas before it is approved and incorporated into the governance program. Area 2 describes the data assets. These are the data sources, APIs, analytics models, transformation functions and rule implementations that store and manage data. The definitions in Area 2 include connectivity information that is used by the open connector framework (and other tools) to get access to the data assets. Area 3 describes the glossary. This is the definitions of terms and concepts and how they relate to one another. Linking the concepts/terms defined in the glossary to the data assets in Area 2 defines the meaning of the data that is managed by the data assets. This is a key relationship that helps people locate and understand the data assets they are working with. Area 4 defines how the data assets should be governed. This is where the classifications, policies and rules are defined. Area 5 is where standards are established. This includes data models, schema fragments and reference data that are used to assist developers and architects in using best practice data structures and valid values as they develop new capabilities around the data assets. Area 6 provides the additional information that automated metadata discovery engines have discovered about the data assets. This includes profile information, quality scores and suggested classifications. Area 7 provides the structures for recording lineage. The following diagram provides more detail of the metadata structures in each area and how they link together: Metadata is highly interconnected Bottom left is Area 0 - the foundation of the open metadata types along with the IT infrastructure that digital systems run on such as platforms, servers and network connections. Sitting on the foundation are the assets. The base definition for Asset is in Area 0 but Area 2 (middle bottom) builds out common types of assets that an organization uses. These assets are hosted and linked to the infrastructure described in Area 0. For example, a data set could be linked to the file system description to show where it is stored. Area 5 (right middle) focuses on defining the structure of data and the standard sets of values (called reference data). The structure of data is described in schemas and these are linked to the assets that use them. Many assets have technical names. Area 3 (top middle) captures business and real world terminologies and organizes them into glossaries. The individual terms described can be linked to the technical names and labels given to the assets and the data fields described in their schemas. Area 6 (bottom right) captures additional metadata captured through automated analysis of data. These analysis results are linked to the assets that hold the data so that data professionals can evaluate the suitability of the data for different purposes. Area 7 (left middle) captures the lineage of assets from a business and technical perspective. Above that in Area 4 are the definitions that control the governance of all of the assets. Finally, Area 1 (top right) captures information about users (people, automated process) their organization, such as teams and projects, and feedback. Within each area, the definitions are broken down into numbered packages to help identify groups of related elements. The numbering system relates to the area that the elements belong to. For example, area 1 has models 0100-0199, area 2 has models 0200-299, etc. Each area's sub-models are dispersed along its range, ensuring there is space to insert additional models in the future. Understanding the models \u00b6 The diagram above shows a few fragments from the models. Each of the UML classes represents an open metadata type. The stereotype on the UML class in the double angle brackets of entity , relationship and classification defines the category of type . The line between entities with the big arrow head means \"inheritance\". A type points to its supertype. The example on the left comes from model 0010 It shows that Asset inherits from Referenceable which inherits from OpenMetadataRoot . This means that Asset is a subtype of Referenceable , which is a subtype of OpenMetadataRoot . Alternatively, OpenMetadataRoot is the supertype of Referenceable , which is a supertype of Asset . This inheritance identifies which attributes (instance properties) are valid for an instance of a particular type since it is the aggregation of the attributes defined explicitly for the type and all of its supertypes. For example, Asset has two attributes defined: name and description . It also supports qualifiedName and additionalProperties because they are inherited from Referenceable . OpenMetadataRoot does not have any attributes defined so Asset gets nothing from it. The example on the right comes from model 0011 It shows the classification called Template that can be connected to a Referenceable . Since Referenceable is already defined in model 0010, it is shown without the white box where the attributes are show (called the \"attribute container\" in UML parlance). SourcedFrom is a relationship that connects two instances of Referenceable and any of its subtypes. This means SourcedFrom could connect two instances of type Asset together. The types of the instances connected do not need to be the same - SourcedFrom could connect a Referenceable instance with an Asset instance. The UML model diagrams show the currently active types. Some types and attributes have been deprecated and these have been removed from the model diagrams. However, there is a description of the deprecated types and which of the active types to use instead. Although the deprecated types can be used (for backwards compatibility) it is always preferable to use the latest types since they are typically more efficient and more consistent than their predecessors. Attribute type definitions \u00b6 The properties defined on each open metadata type will be one of the following attribute types: object boolean byte char short int long float double biginteger bigdecimal string date map<string,string> map<string,boolean> map<string,int> map<string,long> map<string,object> array<string> array<int>","title":"Open Metadata Type System"},{"location":"types/#the-open-metadata-type-system","text":"Knowledge about data is spread amongst many people and systems. One of the roles of a metadata repository is to provide a place where this knowledge can be collected and correlated, as automated as possible. To enable different tools and processes to populate the metadata repository we need agreement on what data should be stored and in what format (structures). The different areas of metadata that we need to support for a wide range of metadata management and governance tasks include: This metadata may be spread across different metadata repositories that each specialize in particular use cases or communities of users. Area Description Area 0 describes base types and infrastructure. This includes the root type for all open metadata entities called OpenMetadataRoot and types for Asset , DataSet , Infrastructure , Process , Referenceable , SoftwareServer and Host . Area 1 collects information from people using the data assets. It includes their use of the assets and their feedback. It also manages crowd-sourced enhancements to the metadata from other areas before it is approved and incorporated into the governance program. Area 2 describes the data assets. These are the data sources, APIs, analytics models, transformation functions and rule implementations that store and manage data. The definitions in Area 2 include connectivity information that is used by the open connector framework (and other tools) to get access to the data assets. Area 3 describes the glossary. This is the definitions of terms and concepts and how they relate to one another. Linking the concepts/terms defined in the glossary to the data assets in Area 2 defines the meaning of the data that is managed by the data assets. This is a key relationship that helps people locate and understand the data assets they are working with. Area 4 defines how the data assets should be governed. This is where the classifications, policies and rules are defined. Area 5 is where standards are established. This includes data models, schema fragments and reference data that are used to assist developers and architects in using best practice data structures and valid values as they develop new capabilities around the data assets. Area 6 provides the additional information that automated metadata discovery engines have discovered about the data assets. This includes profile information, quality scores and suggested classifications. Area 7 provides the structures for recording lineage. The following diagram provides more detail of the metadata structures in each area and how they link together: Metadata is highly interconnected Bottom left is Area 0 - the foundation of the open metadata types along with the IT infrastructure that digital systems run on such as platforms, servers and network connections. Sitting on the foundation are the assets. The base definition for Asset is in Area 0 but Area 2 (middle bottom) builds out common types of assets that an organization uses. These assets are hosted and linked to the infrastructure described in Area 0. For example, a data set could be linked to the file system description to show where it is stored. Area 5 (right middle) focuses on defining the structure of data and the standard sets of values (called reference data). The structure of data is described in schemas and these are linked to the assets that use them. Many assets have technical names. Area 3 (top middle) captures business and real world terminologies and organizes them into glossaries. The individual terms described can be linked to the technical names and labels given to the assets and the data fields described in their schemas. Area 6 (bottom right) captures additional metadata captured through automated analysis of data. These analysis results are linked to the assets that hold the data so that data professionals can evaluate the suitability of the data for different purposes. Area 7 (left middle) captures the lineage of assets from a business and technical perspective. Above that in Area 4 are the definitions that control the governance of all of the assets. Finally, Area 1 (top right) captures information about users (people, automated process) their organization, such as teams and projects, and feedback. Within each area, the definitions are broken down into numbered packages to help identify groups of related elements. The numbering system relates to the area that the elements belong to. For example, area 1 has models 0100-0199, area 2 has models 0200-299, etc. Each area's sub-models are dispersed along its range, ensuring there is space to insert additional models in the future.","title":"The open metadata type system"},{"location":"types/#understanding-the-models","text":"The diagram above shows a few fragments from the models. Each of the UML classes represents an open metadata type. The stereotype on the UML class in the double angle brackets of entity , relationship and classification defines the category of type . The line between entities with the big arrow head means \"inheritance\". A type points to its supertype. The example on the left comes from model 0010 It shows that Asset inherits from Referenceable which inherits from OpenMetadataRoot . This means that Asset is a subtype of Referenceable , which is a subtype of OpenMetadataRoot . Alternatively, OpenMetadataRoot is the supertype of Referenceable , which is a supertype of Asset . This inheritance identifies which attributes (instance properties) are valid for an instance of a particular type since it is the aggregation of the attributes defined explicitly for the type and all of its supertypes. For example, Asset has two attributes defined: name and description . It also supports qualifiedName and additionalProperties because they are inherited from Referenceable . OpenMetadataRoot does not have any attributes defined so Asset gets nothing from it. The example on the right comes from model 0011 It shows the classification called Template that can be connected to a Referenceable . Since Referenceable is already defined in model 0010, it is shown without the white box where the attributes are show (called the \"attribute container\" in UML parlance). SourcedFrom is a relationship that connects two instances of Referenceable and any of its subtypes. This means SourcedFrom could connect two instances of type Asset together. The types of the instances connected do not need to be the same - SourcedFrom could connect a Referenceable instance with an Asset instance. The UML model diagrams show the currently active types. Some types and attributes have been deprecated and these have been removed from the model diagrams. However, there is a description of the deprecated types and which of the active types to use instead. Although the deprecated types can be used (for backwards compatibility) it is always preferable to use the latest types since they are typically more efficient and more consistent than their predecessors.","title":"Understanding the models"},{"location":"types/#attribute-type-definitions","text":"The properties defined on each open metadata type will be one of the following attribute types: object boolean byte char short int long float double biginteger bigdecimal string date map<string,string> map<string,boolean> map<string,int> map<string,long> map<string,object> array<string> array<int>","title":"Attribute type definitions"},{"location":"types/metamodel/","text":"OMRS Metamodel \u00b6 OMRS 's metamodel defines the structures used to represent metadata, and can be broadly divided into the following two areas: Type Definitions ( TypeDef s and AttributeTypeDef s) Instances Type definitions \u00b6 TypeDef s define the general characteristics that are used to describe any general type -- whether native or not -- and typically fit into one of the following three general types ( TypeDefCategory ): EntityDef : the definition of a type of entity RelationshipDef : the definition of a type of relationship ClassificationDef : the definition of a type of classification TypeDef s are implemented as Java classes in Egeria (those in italics are abstract). Examples of type definitions For example, the native open metadata types that Egeria defines include: a generic Referenceable entity type, a GlossaryCategory entity type, a GlossaryTerm entity type, (and many others!) Rather than being another set of classes that inherit from the general TypeDef classes ( EntityDef , RelationshipDef , ClassificationDef ), these open types are instantiated objects of one of those TypeDef classes (in the above examples, of the EntityDef class). A set of TypeDefAttribute s within the TypeDef defines the list of additional characteristics (properties) that each of these entity types can possess -- things like names, descriptions, etc -- which may naturally vary from one entity type to another. (These property-level definitions are AttributeTypeDef s.) An example of a RelationshipDef is the linkage between GlossaryTerm and GlossaryCategory known as a TermCategorization , which is further defined to allow any number of linkages between the two, in either direction. Again, the TermCategorization will be an instantiated object of the class RelationshipDef , with various properties configured on it to define the endpoints of the relationship. An example ClassificationDef is Confidentiality : that can apply to any Referenceable entity (hence including both GlossaryTerm and GlossaryCategory , as both of these extend the Referenceable entity type). Example: Referenceable For example, Referenceable is defined as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 final String guid = \"a32316b8-dc8c-48c5-b12b-71c1b2a080bf\" ; final String name = \"Referenceable\" ; final String description = \"An open metadata entity that has a unique identifier.\" ; final String descriptionGUID = null ; // The EntityDef object itself EntityDef entityDef = archiveHelper . getDefaultEntityDef ( guid , name , null , description , descriptionGUID ); List < TypeDefAttribute > properties = new ArrayList <> (); TypeDefAttribute property ; final String attribute1Name = \"qualifiedName\" ; final String attribute1Description = \"Unique identifier for the entity.\" ; final String attribute1DescriptionGUID = null ; final String attribute2Name = \"additionalProperties\" ; final String attribute2Description = \"Additional properties for the element.\" ; final String attribute2DescriptionGUID = null ; // The TypeDefAttributes that define the properties of the Referenceable (EntityDef) object property = archiveHelper . getStringTypeDefAttribute ( attribute1Name , attribute1Description , attribute1DescriptionGUID ); property . setUnique ( true ); properties . add ( property ); property = archiveHelper . getMapStringStringTypeDefAttribute ( attribute2Name , attribute2Description , attribute2DescriptionGUID ); properties . add ( property ); entityDef . setPropertiesDefinition ( properties ); And this results in a JSON structure like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 { \"class\" : \"EntityDef\" , \"guid\" : \"a32316b8-dc8c-48c5-b12b-71c1b2a080bf\" , \"name\" : \"Referenceable\" , \"version\" : 1 , \"versionName\" : \"1.0\" , \"category\" : \"ENTITY_DEF\" , \"description\" : \"An open metadata entity that has a unique identifier.\" , \"origin\" : \"bce3b0a0-662a-4f87-b8dc-844078a11a6e\" , \"createdBy\" : \"ODPi Egeria (OMRS)\" , \"createTime\" : 1516313040008 , \"validInstanceStatusList\" : [ \"ACTIVE\" , \"DELETED\" ], \"initialStatus\" : \"ACTIVE\" , \"propertiesDefinition\" : [ { \"attributeName\" : \"qualifiedName\" , \"attributeType\" : { \"class\" : \"PrimitiveDef\" , \"version\" : 1 , \"versionName\" : \"1.0\" , \"category\" : \"PRIMITIVE\" , \"guid\" : \"b34a64b9-554a-42b1-8f8a-7d5c2339f9c4\" , \"name\" : \"string\" , \"primitiveDefCategory\" : \"OM_PRIMITIVE_TYPE_STRING\" }, \"attributeDescription\" : \"Unique identifier for the entity.\" , \"valuesMinCount\" : 0 , \"valuesMaxCount\" : 1 , \"indexable\" : true , \"attributeCardinality\" : \"AT_MOST_ONE\" , \"unique\" : true }, { \"attributeName\" : \"additionalProperties\" , \"attributeType\" : { \"class\" : \"CollectionDef\" , \"version\" : 1 , \"versionName\" : \"1.0\" , \"category\" : \"COLLECTION\" , \"guid\" : \"005c7c14-ac84-4136-beed-959401b041f8\" , \"name\" : \"map<string,string>\" , \"description\" : \"A map from String to String.\" , \"collectionDefCategory\" : \"OM_COLLECTION_MAP\" , \"argumentCount\" : 2 , \"argumentTypes\" : [ \"OM_PRIMITIVE_TYPE_STRING\" , \"OM_PRIMITIVE_TYPE_STRING\" ] }, \"attributeDescription\" : \"Additional properties for the element.\" , \"valuesMinCount\" : 0 , \"valuesMaxCount\" : 1 , \"indexable\" : true , \"attributeCardinality\" : \"AT_MOST_ONE\" , \"unique\" : false } ] } Instances \u00b6 Instances define individual instantiations of the TypeDefs: for example, individual GlossaryTerm entities (like \"Address Line 1\") or GlossaryCategory entities (like \"Coco Pharmaceuticals\"), Confidentiality classifications (like \"Sensitive\"), etc. Implemented instances will generally be objects of one of the following classes: How are entities modeled? EntitySummary , EntityProxy and EntityDetail Egeria models all entities using a general object -- EntitySummary -- from which more detailed representations (like EntityDetail ) are derived. Note that there is a property called type within this object (inherited from InstanceAuditHeader ) that defines the specific type of metadata the entity represents, rather than having a different type-specific object for every different type of entity. How are relationships modeled? Relationship and EntityProxy Egeria models all relationships using a general object -- Relationship -- which links together exactly two entities (using EntityProxy , itself an extension of EntitySummary). These EntityProxy s act as a sort of \"stub\" to which the relationship can point, without needing to be aware of the entire set of details of each entity involved in the relationship, and are therefore an important piece of ensuring that relationships are treated as \"first-class\" objects in their own right. As with entities, there is a property called type within this Relationship object (also inherited from InstanceAuditHeader ) that defines the specific type of metadata the relationship represents, rather than having a different type-specific object for every different type of relationship. How are classifications modeled? Classification Egeria models all classifications using a general object -- Classification -- any instance of which is possessed by exactly one entity (within the EntitySummary 's classifications property). As with the other kinds of instances, note that there is a property called type within this Classification object (also inherited from InstanceAuditHeader ) that defines the specific type of metadata the classification represents, rather than having a different type-specific object for every different type of classification. Furthermore, note that classifications in Egeria exist only as part of an entity: unlike EntitySummary and Relationship , they do not extend InstanceHeader and therefore are not assigned a GUID through which they could be independently retrieved or updated. Classifications are only retrievable or updatable through the entity by which they are possessed. Each of these will typically be further described by an InstanceProperties object that instantiates one or more properties used to describe the entity, classification or relationship. Instances as a concept are implemented as generic Java classes in Egeria (those in italics are abstract). Examples of instances Like TypeDefs, instances are managed as instantiated objects of the classes above (not as new classes that inherit from those above). For example, to represent an \"Address Line 1\" GlossaryTerm , there can be: an instantiated EntitySummary object providing a headline set of information about \"Address Line 1\" (i.e. its classifications), an instantiated EntityDetail object providing the full details of \"Address Line 1\" (all of its classifications and properties), an instantiated EntityProxy object defining the unique properties of \"Address Line 1\" (i.e. a qualifiedName property inherited from Referenceable which must be unique across all instances of the object) The InstanceProperties associated with the EntityDetail , Relationship and Classification objects give the further detail on the object (ie. its name, summary, etc). To represent a \"Sensitive Confidentiality\" concept that can be used to classify other information (like \"Address Line 1\"), we could have a \"Confidentiality\" Classification defined with a specific level (\"Sensitive\") through its InstanceProperties To represent the linkage between \"Address Line 1\" and a specific category location (like \"Coco Pharmaceuticals/Terms\") we could have a Relationship representing the TermCategorization with one end pointing to an EntityProxy for \"Address Line 1\" and one end pointing to an EntityProxy for \"Coco Pharmaceuticals/Terms\" Code example for creating the various types As code, this example could be implemented using something like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 // ENTITIES // Summary-level entity information (see end of CLASSIFICATION section for adding Classifications) EntitySummary summary = new EntitySummary (); summary . setGUID ( guid ); summary . setCreatedBy ( \"...user...\" ); summary . setCreateTime ( new Date (...)); summary . setUpdatedBy ( \"...user...\" ); summary . setUpdateTime ( new Date (...)); // Detailed-level entity information (see end of CLASSIFICATION section for adding Classifications) EntityDetail detail = repositoryConnector . getRepositoryHelper (). getSkeletonEntity ( sourceName , omrsRepositoryConnector . getMetadataCollectionId (), InstanceProvenanceType . LOCAL_COHORT , userId , \"GlossaryTerm\" ); detail . setStatus ( InstanceStatus . ACTIVE ); detail . setGUID ( guid ); detail . setCreatedBy ( \"...user...\" ); detail . setCreateTime ( new Date (...)); detail . setUpdatedBy ( \"...user...\" ); detail . setUpdateTime ( new Date (...)); InstanceProperties instanceProperties = new InstanceProperties (); PrimitivePropertyValue propertyValue = new PrimitivePropertyValue (); PrimitiveDef primitiveDef = new PrimitiveDef (); primitiveDef . setPrimitiveDefCategory ( PrimitiveDefCategory . OM_PRIMITIVE_TYPE_STRING ); propertyValue . setPrimitiveValue ( \"Address Line 1\" ); propertyValue . setPrimitiveDefCategory ( primitiveDef . getPrimitiveDefCategory ()); propertyValue . setTypeGUID ( primitiveDef . getGUID ()); propertyValue . setTypeName ( primitiveDef . getName ()); instanceProperties . setProperty ( \"qualifiedName\" , propertyValue ); propertyValue . setPrimitiveValue ( \"Street and street number\" ); instanceProperties . setProperty ( \"summary\" , propertyValue ); detail . setProperties ( instanceProperties ); // CLASSIFICATION InstanceProperties properties = new InstanceProperties (); // The \"Sensitive\" level for the classification EnumPropertyValue level = new EnumPropertyValue (); level . setSymbolicName ( \"Sensitive\" ); properties . setProperty ( \"level\" , level ); // Creating a new classification, of \"Confidentiality\", applying to \"Referenceable\", with // the \"Sensitive\" level defined above Classification classification = omrsRepositoryConnector . getRepositoryHelper (). getNewClassification ( sourceName , userId , \"Confidentiality\" , \"Referenceable\" , ClassificationOrigin . ASSIGNED , null , properties ); ArrayList < Classificaton > classifications = new ArrayList <> (); classifications . add ( classification ); // ... adding the Classification(s) to the Entity(Summary|Detail) summary . setClassifications ( classifications ); detail . setClassifications ( classifications ); // RELATIONSHIP // Define the qualifiedName property with the unique name for the first end of the relationship // (the GlossaryCategory end of the relationship) InstanceProperties uniqueProperties1 = new InstanceProperties (); PrimitivePropertyValue propertyValue = new PrimitivePropertyValue (); PrimitiveDef primitiveDef = new PrimitiveDef (); primitiveDef . setPrimitiveDefCategory ( PrimitiveDefCategory . OM_PRIMITIVE_TYPE_STRING ); propertyValue . setPrimitiveValue ( \"Coco Pharmaceuticals/Terms\" ); propertyValue . setPrimitiveDefCategory ( primitiveDef . getPrimitiveDefCategory ()); propertyValue . setTypeGUID ( primitiveDef . getGUID ()); propertyValue . setTypeName ( primitiveDef . getName ()); uniqueProperties1 . setProperty ( \"qualifiedName\" , propertyValue ); EntityProxy entityProxy1 = omrsRepositoryConnector . getRepositoryHelper (). getNewEntityProxy ( sourceName , omrsRepositoryConnector . getMetadataCollectionId (), InstanceProvenanceType . LOCAL_COHORT , userId , \"GlossaryCategory\" , uniqueProperties1 , null ); // Define the qualifiedName property with the unique name of the second end of the relationship // (the GlossaryTerm end of the relationship) InstanceProperties uniqueProperties2 = new InstanceProperties (); primitiveDef . setPrimitiveDefCategory ( PrimitiveDefCategory . OM_PRIMITIVE_TYPE_STRING ); propertyValue . setPrimitiveValue ( \"Address Line 1\" ); propertyValue . setPrimitiveDefCategory ( primitiveDef . getPrimitiveDefCategory ()); propertyValue . setTypeGUID ( primitiveDef . getGUID ()); propertyValue . setTypeName ( primitiveDef . getName ()); uniqueProperties2 . setProperty ( \"qualifiedName\" , propertyValue ); EntityProxy entityProxy2 = omrsRepositoryConnector . getRepositoryHelper (). getNewEntityProxy ( sourceName , omrsRepositoryConnector . getMetadataCollectionId (), InstanceProvenanceType . LOCAL_COHORT , userId , \"GlossaryTerm\" , uniqueProperties2 , null ); // Define the relationship between the two EntityProxy objects instantiated above Relationship omrsRelationship = new Relationship (); InstanceProperties relationshipProperties = new InstanceProperties (); PrimitivePropertyValue relationProperty = new PrimitivePropertyValue (); relationProperty . setTypeGUID ( \"...\" ); relationProperty . setTypeName ( \"???\" ); relationshipProperties . setProperty ( \"TermCategorization\" , relationProperty ); omrsRelationship . setProperties ( relationshipProperties ); omrsRelationship . setEntityOneProxy ( entityProxy1 ); omrsRelationship . setEntityTwoProxy ( entityProxy2 ); omrsRelationship . setGUID ( \"...\" );","title":"Metamodel"},{"location":"types/metamodel/#omrs-metamodel","text":"OMRS 's metamodel defines the structures used to represent metadata, and can be broadly divided into the following two areas: Type Definitions ( TypeDef s and AttributeTypeDef s) Instances","title":"OMRS Metamodel"},{"location":"types/metamodel/#type-definitions","text":"TypeDef s define the general characteristics that are used to describe any general type -- whether native or not -- and typically fit into one of the following three general types ( TypeDefCategory ): EntityDef : the definition of a type of entity RelationshipDef : the definition of a type of relationship ClassificationDef : the definition of a type of classification TypeDef s are implemented as Java classes in Egeria (those in italics are abstract). Examples of type definitions For example, the native open metadata types that Egeria defines include: a generic Referenceable entity type, a GlossaryCategory entity type, a GlossaryTerm entity type, (and many others!) Rather than being another set of classes that inherit from the general TypeDef classes ( EntityDef , RelationshipDef , ClassificationDef ), these open types are instantiated objects of one of those TypeDef classes (in the above examples, of the EntityDef class). A set of TypeDefAttribute s within the TypeDef defines the list of additional characteristics (properties) that each of these entity types can possess -- things like names, descriptions, etc -- which may naturally vary from one entity type to another. (These property-level definitions are AttributeTypeDef s.) An example of a RelationshipDef is the linkage between GlossaryTerm and GlossaryCategory known as a TermCategorization , which is further defined to allow any number of linkages between the two, in either direction. Again, the TermCategorization will be an instantiated object of the class RelationshipDef , with various properties configured on it to define the endpoints of the relationship. An example ClassificationDef is Confidentiality : that can apply to any Referenceable entity (hence including both GlossaryTerm and GlossaryCategory , as both of these extend the Referenceable entity type). Example: Referenceable For example, Referenceable is defined as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 final String guid = \"a32316b8-dc8c-48c5-b12b-71c1b2a080bf\" ; final String name = \"Referenceable\" ; final String description = \"An open metadata entity that has a unique identifier.\" ; final String descriptionGUID = null ; // The EntityDef object itself EntityDef entityDef = archiveHelper . getDefaultEntityDef ( guid , name , null , description , descriptionGUID ); List < TypeDefAttribute > properties = new ArrayList <> (); TypeDefAttribute property ; final String attribute1Name = \"qualifiedName\" ; final String attribute1Description = \"Unique identifier for the entity.\" ; final String attribute1DescriptionGUID = null ; final String attribute2Name = \"additionalProperties\" ; final String attribute2Description = \"Additional properties for the element.\" ; final String attribute2DescriptionGUID = null ; // The TypeDefAttributes that define the properties of the Referenceable (EntityDef) object property = archiveHelper . getStringTypeDefAttribute ( attribute1Name , attribute1Description , attribute1DescriptionGUID ); property . setUnique ( true ); properties . add ( property ); property = archiveHelper . getMapStringStringTypeDefAttribute ( attribute2Name , attribute2Description , attribute2DescriptionGUID ); properties . add ( property ); entityDef . setPropertiesDefinition ( properties ); And this results in a JSON structure like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 { \"class\" : \"EntityDef\" , \"guid\" : \"a32316b8-dc8c-48c5-b12b-71c1b2a080bf\" , \"name\" : \"Referenceable\" , \"version\" : 1 , \"versionName\" : \"1.0\" , \"category\" : \"ENTITY_DEF\" , \"description\" : \"An open metadata entity that has a unique identifier.\" , \"origin\" : \"bce3b0a0-662a-4f87-b8dc-844078a11a6e\" , \"createdBy\" : \"ODPi Egeria (OMRS)\" , \"createTime\" : 1516313040008 , \"validInstanceStatusList\" : [ \"ACTIVE\" , \"DELETED\" ], \"initialStatus\" : \"ACTIVE\" , \"propertiesDefinition\" : [ { \"attributeName\" : \"qualifiedName\" , \"attributeType\" : { \"class\" : \"PrimitiveDef\" , \"version\" : 1 , \"versionName\" : \"1.0\" , \"category\" : \"PRIMITIVE\" , \"guid\" : \"b34a64b9-554a-42b1-8f8a-7d5c2339f9c4\" , \"name\" : \"string\" , \"primitiveDefCategory\" : \"OM_PRIMITIVE_TYPE_STRING\" }, \"attributeDescription\" : \"Unique identifier for the entity.\" , \"valuesMinCount\" : 0 , \"valuesMaxCount\" : 1 , \"indexable\" : true , \"attributeCardinality\" : \"AT_MOST_ONE\" , \"unique\" : true }, { \"attributeName\" : \"additionalProperties\" , \"attributeType\" : { \"class\" : \"CollectionDef\" , \"version\" : 1 , \"versionName\" : \"1.0\" , \"category\" : \"COLLECTION\" , \"guid\" : \"005c7c14-ac84-4136-beed-959401b041f8\" , \"name\" : \"map<string,string>\" , \"description\" : \"A map from String to String.\" , \"collectionDefCategory\" : \"OM_COLLECTION_MAP\" , \"argumentCount\" : 2 , \"argumentTypes\" : [ \"OM_PRIMITIVE_TYPE_STRING\" , \"OM_PRIMITIVE_TYPE_STRING\" ] }, \"attributeDescription\" : \"Additional properties for the element.\" , \"valuesMinCount\" : 0 , \"valuesMaxCount\" : 1 , \"indexable\" : true , \"attributeCardinality\" : \"AT_MOST_ONE\" , \"unique\" : false } ] }","title":"Type definitions"},{"location":"types/metamodel/#instances","text":"Instances define individual instantiations of the TypeDefs: for example, individual GlossaryTerm entities (like \"Address Line 1\") or GlossaryCategory entities (like \"Coco Pharmaceuticals\"), Confidentiality classifications (like \"Sensitive\"), etc. Implemented instances will generally be objects of one of the following classes: How are entities modeled? EntitySummary , EntityProxy and EntityDetail Egeria models all entities using a general object -- EntitySummary -- from which more detailed representations (like EntityDetail ) are derived. Note that there is a property called type within this object (inherited from InstanceAuditHeader ) that defines the specific type of metadata the entity represents, rather than having a different type-specific object for every different type of entity. How are relationships modeled? Relationship and EntityProxy Egeria models all relationships using a general object -- Relationship -- which links together exactly two entities (using EntityProxy , itself an extension of EntitySummary). These EntityProxy s act as a sort of \"stub\" to which the relationship can point, without needing to be aware of the entire set of details of each entity involved in the relationship, and are therefore an important piece of ensuring that relationships are treated as \"first-class\" objects in their own right. As with entities, there is a property called type within this Relationship object (also inherited from InstanceAuditHeader ) that defines the specific type of metadata the relationship represents, rather than having a different type-specific object for every different type of relationship. How are classifications modeled? Classification Egeria models all classifications using a general object -- Classification -- any instance of which is possessed by exactly one entity (within the EntitySummary 's classifications property). As with the other kinds of instances, note that there is a property called type within this Classification object (also inherited from InstanceAuditHeader ) that defines the specific type of metadata the classification represents, rather than having a different type-specific object for every different type of classification. Furthermore, note that classifications in Egeria exist only as part of an entity: unlike EntitySummary and Relationship , they do not extend InstanceHeader and therefore are not assigned a GUID through which they could be independently retrieved or updated. Classifications are only retrievable or updatable through the entity by which they are possessed. Each of these will typically be further described by an InstanceProperties object that instantiates one or more properties used to describe the entity, classification or relationship. Instances as a concept are implemented as generic Java classes in Egeria (those in italics are abstract). Examples of instances Like TypeDefs, instances are managed as instantiated objects of the classes above (not as new classes that inherit from those above). For example, to represent an \"Address Line 1\" GlossaryTerm , there can be: an instantiated EntitySummary object providing a headline set of information about \"Address Line 1\" (i.e. its classifications), an instantiated EntityDetail object providing the full details of \"Address Line 1\" (all of its classifications and properties), an instantiated EntityProxy object defining the unique properties of \"Address Line 1\" (i.e. a qualifiedName property inherited from Referenceable which must be unique across all instances of the object) The InstanceProperties associated with the EntityDetail , Relationship and Classification objects give the further detail on the object (ie. its name, summary, etc). To represent a \"Sensitive Confidentiality\" concept that can be used to classify other information (like \"Address Line 1\"), we could have a \"Confidentiality\" Classification defined with a specific level (\"Sensitive\") through its InstanceProperties To represent the linkage between \"Address Line 1\" and a specific category location (like \"Coco Pharmaceuticals/Terms\") we could have a Relationship representing the TermCategorization with one end pointing to an EntityProxy for \"Address Line 1\" and one end pointing to an EntityProxy for \"Coco Pharmaceuticals/Terms\" Code example for creating the various types As code, this example could be implemented using something like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 // ENTITIES // Summary-level entity information (see end of CLASSIFICATION section for adding Classifications) EntitySummary summary = new EntitySummary (); summary . setGUID ( guid ); summary . setCreatedBy ( \"...user...\" ); summary . setCreateTime ( new Date (...)); summary . setUpdatedBy ( \"...user...\" ); summary . setUpdateTime ( new Date (...)); // Detailed-level entity information (see end of CLASSIFICATION section for adding Classifications) EntityDetail detail = repositoryConnector . getRepositoryHelper (). getSkeletonEntity ( sourceName , omrsRepositoryConnector . getMetadataCollectionId (), InstanceProvenanceType . LOCAL_COHORT , userId , \"GlossaryTerm\" ); detail . setStatus ( InstanceStatus . ACTIVE ); detail . setGUID ( guid ); detail . setCreatedBy ( \"...user...\" ); detail . setCreateTime ( new Date (...)); detail . setUpdatedBy ( \"...user...\" ); detail . setUpdateTime ( new Date (...)); InstanceProperties instanceProperties = new InstanceProperties (); PrimitivePropertyValue propertyValue = new PrimitivePropertyValue (); PrimitiveDef primitiveDef = new PrimitiveDef (); primitiveDef . setPrimitiveDefCategory ( PrimitiveDefCategory . OM_PRIMITIVE_TYPE_STRING ); propertyValue . setPrimitiveValue ( \"Address Line 1\" ); propertyValue . setPrimitiveDefCategory ( primitiveDef . getPrimitiveDefCategory ()); propertyValue . setTypeGUID ( primitiveDef . getGUID ()); propertyValue . setTypeName ( primitiveDef . getName ()); instanceProperties . setProperty ( \"qualifiedName\" , propertyValue ); propertyValue . setPrimitiveValue ( \"Street and street number\" ); instanceProperties . setProperty ( \"summary\" , propertyValue ); detail . setProperties ( instanceProperties ); // CLASSIFICATION InstanceProperties properties = new InstanceProperties (); // The \"Sensitive\" level for the classification EnumPropertyValue level = new EnumPropertyValue (); level . setSymbolicName ( \"Sensitive\" ); properties . setProperty ( \"level\" , level ); // Creating a new classification, of \"Confidentiality\", applying to \"Referenceable\", with // the \"Sensitive\" level defined above Classification classification = omrsRepositoryConnector . getRepositoryHelper (). getNewClassification ( sourceName , userId , \"Confidentiality\" , \"Referenceable\" , ClassificationOrigin . ASSIGNED , null , properties ); ArrayList < Classificaton > classifications = new ArrayList <> (); classifications . add ( classification ); // ... adding the Classification(s) to the Entity(Summary|Detail) summary . setClassifications ( classifications ); detail . setClassifications ( classifications ); // RELATIONSHIP // Define the qualifiedName property with the unique name for the first end of the relationship // (the GlossaryCategory end of the relationship) InstanceProperties uniqueProperties1 = new InstanceProperties (); PrimitivePropertyValue propertyValue = new PrimitivePropertyValue (); PrimitiveDef primitiveDef = new PrimitiveDef (); primitiveDef . setPrimitiveDefCategory ( PrimitiveDefCategory . OM_PRIMITIVE_TYPE_STRING ); propertyValue . setPrimitiveValue ( \"Coco Pharmaceuticals/Terms\" ); propertyValue . setPrimitiveDefCategory ( primitiveDef . getPrimitiveDefCategory ()); propertyValue . setTypeGUID ( primitiveDef . getGUID ()); propertyValue . setTypeName ( primitiveDef . getName ()); uniqueProperties1 . setProperty ( \"qualifiedName\" , propertyValue ); EntityProxy entityProxy1 = omrsRepositoryConnector . getRepositoryHelper (). getNewEntityProxy ( sourceName , omrsRepositoryConnector . getMetadataCollectionId (), InstanceProvenanceType . LOCAL_COHORT , userId , \"GlossaryCategory\" , uniqueProperties1 , null ); // Define the qualifiedName property with the unique name of the second end of the relationship // (the GlossaryTerm end of the relationship) InstanceProperties uniqueProperties2 = new InstanceProperties (); primitiveDef . setPrimitiveDefCategory ( PrimitiveDefCategory . OM_PRIMITIVE_TYPE_STRING ); propertyValue . setPrimitiveValue ( \"Address Line 1\" ); propertyValue . setPrimitiveDefCategory ( primitiveDef . getPrimitiveDefCategory ()); propertyValue . setTypeGUID ( primitiveDef . getGUID ()); propertyValue . setTypeName ( primitiveDef . getName ()); uniqueProperties2 . setProperty ( \"qualifiedName\" , propertyValue ); EntityProxy entityProxy2 = omrsRepositoryConnector . getRepositoryHelper (). getNewEntityProxy ( sourceName , omrsRepositoryConnector . getMetadataCollectionId (), InstanceProvenanceType . LOCAL_COHORT , userId , \"GlossaryTerm\" , uniqueProperties2 , null ); // Define the relationship between the two EntityProxy objects instantiated above Relationship omrsRelationship = new Relationship (); InstanceProperties relationshipProperties = new InstanceProperties (); PrimitivePropertyValue relationProperty = new PrimitivePropertyValue (); relationProperty . setTypeGUID ( \"...\" ); relationProperty . setTypeName ( \"???\" ); relationshipProperties . setProperty ( \"TermCategorization\" , relationProperty ); omrsRelationship . setProperties ( relationshipProperties ); omrsRelationship . setEntityOneProxy ( entityProxy1 ); omrsRelationship . setEntityTwoProxy ( entityProxy2 ); omrsRelationship . setGUID ( \"...\" );","title":"Instances"},{"location":"types/0/","text":"Area 0 Models - Common Definitions and Infrastructure \u00b6 Area 0 describes base types and infrastructure. This includes types for Asset , DataSet , Infrastructure , Process , Referenceable , Server and Host . 0010 Base Model 0011 Managing Referenceables 0012 Search Keywords 0015 Linked Media Types 0017 External Identifiers 0019 More Information 0020 Property Facets 0021 Collections 0025 Locations 0026 Endpoints 0030 Hosts and Platforms 0035 Complex Hosts 0036 Storage 0037 Software Server Platforms 0040 Software Servers 0042 Software Server Capabilities 0045 Servers and Assets 0050 Applications and Processes 0055 Data Processing Engines 0057 Integration Capabilities 0070 Networks and Gateways 0090 Cloud Platforms and Services","title":"Area 0 Base"},{"location":"types/0/#area-0-models-common-definitions-and-infrastructure","text":"Area 0 describes base types and infrastructure. This includes types for Asset , DataSet , Infrastructure , Process , Referenceable , Server and Host . 0010 Base Model 0011 Managing Referenceables 0012 Search Keywords 0015 Linked Media Types 0017 External Identifiers 0019 More Information 0020 Property Facets 0021 Collections 0025 Locations 0026 Endpoints 0030 Hosts and Platforms 0035 Complex Hosts 0036 Storage 0037 Software Server Platforms 0040 Software Servers 0042 Software Server Capabilities 0045 Servers and Assets 0050 Applications and Processes 0055 Data Processing Engines 0057 Integration Capabilities 0070 Networks and Gateways 0090 Cloud Platforms and Services","title":"Area 0 Models - Common Definitions and Infrastructure"},{"location":"types/0/0010-base-model/","text":"0010 Base model \u00b6 OpenMetadataRoot \u00b6 OpenMetadataRoot is the root entity for all open metadata entity types. Referenceable \u00b6 Referenceable is the super type for many of the open metadata entity types. A Referenceable is something that is important enough to be assigned a unique (qualified) name within its type. This unique name is often used outside the open metadata ecosystem as its unique identifier. Referenceable also has provision for storing additional properties. This is a set of name-value pairs (i.e. a map) where the values are all strings. Further information on the use of Referenceable. Asset \u00b6 Asset represents the most significant type of Referenceable . An Asset is something (either physical or digital) that is of value and so needs to be managed and governed. Deprecated attributes The Asset entity has the following deprecated attributes. Their values have been moved to classifications as shown in the table below. Many Asset s are created by their hosting technology and locked read-only to the broader metadata ecosystem (see external metadata provenance for more detail). By moving the governance related information to a classification, it can be maintained by a different service to the Asset creator. Deprecated attribute Moved to classification owner (type string ) Ownership ownerType (type AssetOwnerType enum) Ownership zoneMembership (type array<string> ) AssetZoneMembership latestChange (type string ) LatestChange Infrastructure , Process and DataSet are examples of Asset s. Infrastructure \u00b6 Infrastructure represents both the physical and digital assets that the organization runs its business on. There is more information on Infrastructure in: 0030 Hosts and platforms 0035 Complex hosts 0037 Software server platforms 0040 Software servers 0042 Software server capabilities Process \u00b6 Process describes a well-defined set of processing steps and decisions that drive a particular aspect of the organization's business. Most Process es are automated with software (see DeployedSoftwareComponent ) but they may also be a manual procedure. An automated process can be invoked from a remote server through a DeployedAPI . DataSet \u00b6 DataSet represents a collection of related data. This data does not need to be stored together. See DataStore for the Asset that represents a physical store. More information on assets can be found in building an Asset Catalog . Anchors \u00b6 The Anchors classification is used internally by the open metadata ecosystem to optimize the lookup of the entity at the root of a cluster of elements that represents a larger object. Currently, there is support for objects uniquely \"owned\" by an asset to store the GUID of that asset. Further information on the use of Anchors. Memento \u00b6 Finally, the Memento classification identifies that the Referenceable refers to a real-world asset/artifact that has either been deleted or archived offline. The metadata element has been retained to show its role in the lineage of other assets/artifacts . The properties in this classification identifies the archive processing and any information that helps to locate the asset/artifact in the archive (if applicable).","title":"Base Model"},{"location":"types/0/0010-base-model/#0010-base-model","text":"","title":"0010 Base model"},{"location":"types/0/0010-base-model/#openmetadataroot","text":"OpenMetadataRoot is the root entity for all open metadata entity types.","title":"OpenMetadataRoot"},{"location":"types/0/0010-base-model/#referenceable","text":"Referenceable is the super type for many of the open metadata entity types. A Referenceable is something that is important enough to be assigned a unique (qualified) name within its type. This unique name is often used outside the open metadata ecosystem as its unique identifier. Referenceable also has provision for storing additional properties. This is a set of name-value pairs (i.e. a map) where the values are all strings. Further information on the use of Referenceable.","title":"Referenceable"},{"location":"types/0/0010-base-model/#asset","text":"Asset represents the most significant type of Referenceable . An Asset is something (either physical or digital) that is of value and so needs to be managed and governed. Deprecated attributes The Asset entity has the following deprecated attributes. Their values have been moved to classifications as shown in the table below. Many Asset s are created by their hosting technology and locked read-only to the broader metadata ecosystem (see external metadata provenance for more detail). By moving the governance related information to a classification, it can be maintained by a different service to the Asset creator. Deprecated attribute Moved to classification owner (type string ) Ownership ownerType (type AssetOwnerType enum) Ownership zoneMembership (type array<string> ) AssetZoneMembership latestChange (type string ) LatestChange Infrastructure , Process and DataSet are examples of Asset s.","title":"Asset"},{"location":"types/0/0010-base-model/#infrastructure","text":"Infrastructure represents both the physical and digital assets that the organization runs its business on. There is more information on Infrastructure in: 0030 Hosts and platforms 0035 Complex hosts 0037 Software server platforms 0040 Software servers 0042 Software server capabilities","title":"Infrastructure"},{"location":"types/0/0010-base-model/#process","text":"Process describes a well-defined set of processing steps and decisions that drive a particular aspect of the organization's business. Most Process es are automated with software (see DeployedSoftwareComponent ) but they may also be a manual procedure. An automated process can be invoked from a remote server through a DeployedAPI .","title":"Process"},{"location":"types/0/0010-base-model/#dataset","text":"DataSet represents a collection of related data. This data does not need to be stored together. See DataStore for the Asset that represents a physical store. More information on assets can be found in building an Asset Catalog .","title":"DataSet"},{"location":"types/0/0010-base-model/#anchors","text":"The Anchors classification is used internally by the open metadata ecosystem to optimize the lookup of the entity at the root of a cluster of elements that represents a larger object. Currently, there is support for objects uniquely \"owned\" by an asset to store the GUID of that asset. Further information on the use of Anchors.","title":"Anchors"},{"location":"types/0/0010-base-model/#memento","text":"Finally, the Memento classification identifies that the Referenceable refers to a real-world asset/artifact that has either been deleted or archived offline. The metadata element has been retained to show its role in the lineage of other assets/artifacts . The properties in this classification identifies the archive processing and any information that helps to locate the asset/artifact in the archive (if applicable).","title":"Memento"},{"location":"types/0/0011-managing-referenceables/","text":"0011 Managing Referenceables \u00b6 Referenceable s can have chains of related feedback and additional knowledge attached to them. The following types help a metadata manager to process these collections of elements more efficiently. LatestChange \u00b6 The LatestChange classification is a convenience mechanism to indicate where the last change occurred. Components that are monitoring Referenceable s can use the open metadata events related to classifications to maintain a complete picture of the asset. Template \u00b6 The Template classification indicates that a Referenceable is a good element be used as a template when creating a new element of the same type. There is no restriction on using Referenceable s without this classification as templates. The Template classification is simply a useful marker to enable templates to be found. SourcedFrom \u00b6 When one Referenceable is created by using another Referencable as a template, the qualifiedName must be changed in the new Referenceable to give it a unique name - often the displayName changes, too. This makes it hard to identify which Referenceable s have been created from a template. The SourcedFrom relationship is used to show the provenance of the information from the template. This is useful to help trace where information has come from and to help understand any potential impact cause by a change to the template if this change also needs to be made to the elements that were copied from it. Deprecated types LastAttachment - use LatestChange instead LastAttachmentLink - use LatestChange instead Further information Cataloguing assets Using templates","title":"Managing Referenceables"},{"location":"types/0/0011-managing-referenceables/#0011-managing-referenceables","text":"Referenceable s can have chains of related feedback and additional knowledge attached to them. The following types help a metadata manager to process these collections of elements more efficiently.","title":"0011 Managing Referenceables"},{"location":"types/0/0011-managing-referenceables/#latestchange","text":"The LatestChange classification is a convenience mechanism to indicate where the last change occurred. Components that are monitoring Referenceable s can use the open metadata events related to classifications to maintain a complete picture of the asset.","title":"LatestChange"},{"location":"types/0/0011-managing-referenceables/#template","text":"The Template classification indicates that a Referenceable is a good element be used as a template when creating a new element of the same type. There is no restriction on using Referenceable s without this classification as templates. The Template classification is simply a useful marker to enable templates to be found.","title":"Template"},{"location":"types/0/0011-managing-referenceables/#sourcedfrom","text":"When one Referenceable is created by using another Referencable as a template, the qualifiedName must be changed in the new Referenceable to give it a unique name - often the displayName changes, too. This makes it hard to identify which Referenceable s have been created from a template. The SourcedFrom relationship is used to show the provenance of the information from the template. This is useful to help trace where information has come from and to help understand any potential impact cause by a change to the template if this change also needs to be made to the elements that were copied from it. Deprecated types LastAttachment - use LatestChange instead LastAttachmentLink - use LatestChange instead Further information Cataloguing assets Using templates","title":"SourcedFrom"},{"location":"types/0/0012-search-keywords/","text":"0012 - Search Keywords \u00b6 Some Referenceable s such as GlossaryTerm s have a lot of text in their definitions, making them easy to find using search strings. Asset definitions, on the other hand, typically have less text. Even the schema associated with the asset might only contain technical terms, and they are often abbreviated. The result is that assets can be hard for consumers to find. It is possible to link assets and schemas to glossary terms to make them findable by association. However, this linkage is a formal semantic relationship. Consumers of the assets can add InformalTag s to the assets and schemas, but only once they have found them. The search keywords provide a mechanism to allow the asset owner to tag the asset - and linked elements such as the schema - with a variety of keywords that can be matched during a search. This helps to boost relevant assets to the top of the search results. SearchKeyword \u00b6 The SearchKeyword entity stores the definition of the search keyword. SearchKeywordLink \u00b6 The SearchKeywordLink relationship links SearchKeyword s with the asset and their associated elements. RelatedKeyword \u00b6 Related keywords can be linked together using the RelatedKeyword relationship to allow simple synonym type expansions of the search. Further information Search keywords can be added manually through the Asset Owner OMAS . Some files such as documents and photos may have keywords embedded in them. These can be automatically discovered through metadata discovery and stored in their corresponding asset properties using these same search keyword elements.","title":"Search Keywords"},{"location":"types/0/0012-search-keywords/#0012-search-keywords","text":"Some Referenceable s such as GlossaryTerm s have a lot of text in their definitions, making them easy to find using search strings. Asset definitions, on the other hand, typically have less text. Even the schema associated with the asset might only contain technical terms, and they are often abbreviated. The result is that assets can be hard for consumers to find. It is possible to link assets and schemas to glossary terms to make them findable by association. However, this linkage is a formal semantic relationship. Consumers of the assets can add InformalTag s to the assets and schemas, but only once they have found them. The search keywords provide a mechanism to allow the asset owner to tag the asset - and linked elements such as the schema - with a variety of keywords that can be matched during a search. This helps to boost relevant assets to the top of the search results.","title":"0012 - Search Keywords"},{"location":"types/0/0012-search-keywords/#searchkeyword","text":"The SearchKeyword entity stores the definition of the search keyword.","title":"SearchKeyword"},{"location":"types/0/0012-search-keywords/#searchkeywordlink","text":"The SearchKeywordLink relationship links SearchKeyword s with the asset and their associated elements.","title":"SearchKeywordLink"},{"location":"types/0/0012-search-keywords/#relatedkeyword","text":"Related keywords can be linked together using the RelatedKeyword relationship to allow simple synonym type expansions of the search. Further information Search keywords can be added manually through the Asset Owner OMAS . Some files such as documents and photos may have keywords embedded in them. These can be automatically discovered through metadata discovery and stored in their corresponding asset properties using these same search keyword elements.","title":"RelatedKeyword"},{"location":"types/0/0015-linked-media-types/","text":"0015 Linked Media Types \u00b6 Linked media types describe the simple structures that are used repeatedly in open metadata to connect it to documents and entities in other types of repositories. ExternalReference \u00b6 ExternalReference s link metadata to elements in external repositories. RelatedMedia \u00b6 RelatedMedia such as images allow an icon, thumbnail and larger images to be associated with a metadata element. They are intended to be displayed with the metadata content. These images enrich the description of the object and may include, for example, design drawings, photographs or illustrations of the component in action.","title":"Linked Media"},{"location":"types/0/0015-linked-media-types/#0015-linked-media-types","text":"Linked media types describe the simple structures that are used repeatedly in open metadata to connect it to documents and entities in other types of repositories.","title":"0015 Linked Media Types"},{"location":"types/0/0015-linked-media-types/#externalreference","text":"ExternalReference s link metadata to elements in external repositories.","title":"ExternalReference"},{"location":"types/0/0015-linked-media-types/#relatedmedia","text":"RelatedMedia such as images allow an icon, thumbnail and larger images to be associated with a metadata element. They are intended to be displayed with the metadata content. These images enrich the description of the object and may include, for example, design drawings, photographs or illustrations of the component in action.","title":"RelatedMedia"},{"location":"types/0/0017-external-identifiers/","text":"0017 External Identifiers \u00b6 External identifiers are used to correlate the identifiers used in third party metadata catalogs with open metadata elements. ExternalId \u00b6 The ExternalId entity describes an external identifier from a specific third party metadata repository. It includes: The identifier value itself in identifier . The pattern used for the identifier (how it is generated and managed) is stored in keyPattern . These are the values it can take, with the default (and most used) being LOCAL_KEY : Value Meaning LOCAL_KEY Unique key allocated and used within the scope of a single system. RECYCLED_KEY Key allocated and used within the scope of a single system that is periodically reused for different records. NATURAL_KEY Key derived from an attribute of the entity, such as email address, passport number. MIRROR_KEY Key value copied from another system. AGGREGATE_KEY Key formed by combining keys from multiple systems. CALLERS_KEY Key from another system can bey used if system name provided. STABLE_KEY Key value will remain active even if records are merged. OTHER Another key pattern. ExternalIdLink \u00b6 The mapping of identifiers can be many-to-many, which is why you see that the ExternalIdLink relationship between the OpenMetadataRoot (open metadata resources) and the ExternalId is also many-to-many. This relationship includes properties to help to map the OpenMetadataRoot to the external identifier. ExternalIdScope \u00b6 There is no guarantee that external identifiers from a third party metadata catalog are globally unique and so the ExternalIdScope relationship links the external identifier to the Referenceable that represents the third party metadata catalog. Typically, this is a type of SoftwareServerCapability , for example, AssetManager . Further information There is an article on managing external identifiers to correlate metadata elements from different types of technologies.","title":"External Identifiers"},{"location":"types/0/0017-external-identifiers/#0017-external-identifiers","text":"External identifiers are used to correlate the identifiers used in third party metadata catalogs with open metadata elements.","title":"0017 External Identifiers"},{"location":"types/0/0017-external-identifiers/#externalid","text":"The ExternalId entity describes an external identifier from a specific third party metadata repository. It includes: The identifier value itself in identifier . The pattern used for the identifier (how it is generated and managed) is stored in keyPattern . These are the values it can take, with the default (and most used) being LOCAL_KEY : Value Meaning LOCAL_KEY Unique key allocated and used within the scope of a single system. RECYCLED_KEY Key allocated and used within the scope of a single system that is periodically reused for different records. NATURAL_KEY Key derived from an attribute of the entity, such as email address, passport number. MIRROR_KEY Key value copied from another system. AGGREGATE_KEY Key formed by combining keys from multiple systems. CALLERS_KEY Key from another system can bey used if system name provided. STABLE_KEY Key value will remain active even if records are merged. OTHER Another key pattern.","title":"ExternalId"},{"location":"types/0/0017-external-identifiers/#externalidlink","text":"The mapping of identifiers can be many-to-many, which is why you see that the ExternalIdLink relationship between the OpenMetadataRoot (open metadata resources) and the ExternalId is also many-to-many. This relationship includes properties to help to map the OpenMetadataRoot to the external identifier.","title":"ExternalIdLink"},{"location":"types/0/0017-external-identifiers/#externalidscope","text":"There is no guarantee that external identifiers from a third party metadata catalog are globally unique and so the ExternalIdScope relationship links the external identifier to the Referenceable that represents the third party metadata catalog. Typically, this is a type of SoftwareServerCapability , for example, AssetManager . Further information There is an article on managing external identifiers to correlate metadata elements from different types of technologies.","title":"ExternalIdScope"},{"location":"types/0/0019-more-information/","text":"0019 More Information \u00b6 MoreInformation \u00b6 The MoreInformation relationship enables Referenceable s of different subtypes to be associated in a way that indicates that one provides more detail about another. It can be used to show linkage between a glossary and the primary top level category, between a glossary term and its implementation. It is a looser association that a relationship such as SemanticAssignment . The Asset Manager OMAS makes use of this relationship to link an asset to a glossary term that is providing supplementary properties to the asset.","title":"More Information"},{"location":"types/0/0019-more-information/#0019-more-information","text":"","title":"0019 More Information"},{"location":"types/0/0019-more-information/#moreinformation","text":"The MoreInformation relationship enables Referenceable s of different subtypes to be associated in a way that indicates that one provides more detail about another. It can be used to show linkage between a glossary and the primary top level category, between a glossary term and its implementation. It is a looser association that a relationship such as SemanticAssignment . The Asset Manager OMAS makes use of this relationship to link an asset to a glossary term that is providing supplementary properties to the asset.","title":"MoreInformation"},{"location":"types/0/0020-property-facets/","text":"0020 Property Facets \u00b6 Property facets allow any entity to be extended with additional properties. This is particularly useful for storing metadata that originated in another type of metadata repository or tool, since it allows vendor-/tool-specific values to be stored. PropertyFacet \u00b6 The PropertyFacet entity describes the additional properties. ReferenceableFacet \u00b6 The ReferenceableFacet relationship indicates the source of the additional properties.","title":"Property Facets"},{"location":"types/0/0020-property-facets/#0020-property-facets","text":"Property facets allow any entity to be extended with additional properties. This is particularly useful for storing metadata that originated in another type of metadata repository or tool, since it allows vendor-/tool-specific values to be stored.","title":"0020 Property Facets"},{"location":"types/0/0020-property-facets/#propertyfacet","text":"The PropertyFacet entity describes the additional properties.","title":"PropertyFacet"},{"location":"types/0/0020-property-facets/#referenceablefacet","text":"The ReferenceableFacet relationship indicates the source of the additional properties.","title":"ReferenceableFacet"},{"location":"types/0/0021-collections/","text":"0021 Collections \u00b6 Collection \u00b6 Collection s provide a general mechanism for grouping resources together. Classifications \u00b6 The classifications associated with Collection allow it to be specialized for particular uses.","title":"Collections"},{"location":"types/0/0021-collections/#0021-collections","text":"","title":"0021 Collections"},{"location":"types/0/0021-collections/#collection","text":"Collection s provide a general mechanism for grouping resources together.","title":"Collection"},{"location":"types/0/0021-collections/#classifications","text":"The classifications associated with Collection allow it to be specialized for particular uses.","title":"Classifications"},{"location":"types/0/0025-locations/","text":"0025 Locations \u00b6 It is important to understand where assets are located to ensure they are properly protected and comply with data sovereignty laws. The open metadata model allows location information to be captured at many levels of granularity. NestedLocation \u00b6 The NestedLocation relationship allows hierarchical groupings of locations to be represented. Notice that locations can be organized into multiple hierarchies. AdjacentLocation \u00b6 The AdjacentLocation relationship links locations that touch one another. Classifications \u00b6 The notion of a location is variable, and the classifications help to clarify the nature of the location. FixedLocation \u00b6 FixedLocation means that the location represents a physical place where, for example, Host s , servers and hence data may be located. This could be an area of a data center, the building the data center is located in, or even the country where the server/data is located. The physical location may be defined using a postal address or coordinates. The coordinates should be accompanied by the type of map projection used. For example, Goode's Homolosine Equal Area Projection, Mercator Projection, Gall-Peters Projection, Miller Cylindrical Projection, Mollweide Projection, Sinusoidal EqualArea Projection or Robinson Projection. SecureLocation \u00b6 SecureLocation indicates that there is restricted access to the location. This can include a description of the type of security. CyberLocation \u00b6 CyberLocation means that the location describes something in cyberspace. It may include the network address of this location.","title":"Locations"},{"location":"types/0/0025-locations/#0025-locations","text":"It is important to understand where assets are located to ensure they are properly protected and comply with data sovereignty laws. The open metadata model allows location information to be captured at many levels of granularity.","title":"0025 Locations"},{"location":"types/0/0025-locations/#nestedlocation","text":"The NestedLocation relationship allows hierarchical groupings of locations to be represented. Notice that locations can be organized into multiple hierarchies.","title":"NestedLocation"},{"location":"types/0/0025-locations/#adjacentlocation","text":"The AdjacentLocation relationship links locations that touch one another.","title":"AdjacentLocation"},{"location":"types/0/0025-locations/#classifications","text":"The notion of a location is variable, and the classifications help to clarify the nature of the location.","title":"Classifications"},{"location":"types/0/0025-locations/#fixedlocation","text":"FixedLocation means that the location represents a physical place where, for example, Host s , servers and hence data may be located. This could be an area of a data center, the building the data center is located in, or even the country where the server/data is located. The physical location may be defined using a postal address or coordinates. The coordinates should be accompanied by the type of map projection used. For example, Goode's Homolosine Equal Area Projection, Mercator Projection, Gall-Peters Projection, Miller Cylindrical Projection, Mollweide Projection, Sinusoidal EqualArea Projection or Robinson Projection.","title":"FixedLocation"},{"location":"types/0/0025-locations/#securelocation","text":"SecureLocation indicates that there is restricted access to the location. This can include a description of the type of security.","title":"SecureLocation"},{"location":"types/0/0025-locations/#cyberlocation","text":"CyberLocation means that the location describes something in cyberspace. It may include the network address of this location.","title":"CyberLocation"},{"location":"types/0/0026-endpoints/","text":"0026 Endpoints \u00b6 Endpoint \u00b6 Endpoint s capture the network information needed to connect to a service. There is a wide variety of approaches to identifying the endpoint and so its properties will depend on how it is being used. Endpoints are part of a Connection . The connection provides the information to create an instance of a connector that is accessing a remote asset. In this situation the networkAddress is set up to the URL needed to connect to the specific asset. ServerEndpoint \u00b6 Endpoints can also be linked to infrastructure elements using the ServerEndpoint relationship to document their network address(s). These are often the values needed in the connection objects configured for integration connectors running in an integration daemon and so the endpoint can be looked up either as the integration connector is being configured, or dynamically when the integration connector is running. Endpoint examples The following picture illustrates the different uses of Endpoint . The top of the diagram shows the endpoint as part of a connection object used to create a connector to the real resource described by the Asset s shown in green. In the middle is an Endpoint tied to a SoftwareServerPlatform that is hosting assets. This endpoint can be queried when configuring integration connectors that are to connect to the platform and catalog the resources (assets) it is hosting. Finally, the VisibleEndpoint and NetworkEndpoint relationships shown at the bottom of the diagram help to document the visibility of an endpoint to a particular network and the host behind it.","title":"Endpoints"},{"location":"types/0/0026-endpoints/#0026-endpoints","text":"","title":"0026 Endpoints"},{"location":"types/0/0026-endpoints/#endpoint","text":"Endpoint s capture the network information needed to connect to a service. There is a wide variety of approaches to identifying the endpoint and so its properties will depend on how it is being used. Endpoints are part of a Connection . The connection provides the information to create an instance of a connector that is accessing a remote asset. In this situation the networkAddress is set up to the URL needed to connect to the specific asset.","title":"Endpoint"},{"location":"types/0/0026-endpoints/#serverendpoint","text":"Endpoints can also be linked to infrastructure elements using the ServerEndpoint relationship to document their network address(s). These are often the values needed in the connection objects configured for integration connectors running in an integration daemon and so the endpoint can be looked up either as the integration connector is being configured, or dynamically when the integration connector is running. Endpoint examples The following picture illustrates the different uses of Endpoint . The top of the diagram shows the endpoint as part of a connection object used to create a connector to the real resource described by the Asset s shown in green. In the middle is an Endpoint tied to a SoftwareServerPlatform that is hosting assets. This endpoint can be queried when configuring integration connectors that are to connect to the platform and catalog the resources (assets) it is hosting. Finally, the VisibleEndpoint and NetworkEndpoint relationships shown at the bottom of the diagram help to document the visibility of an endpoint to a particular network and the host behind it.","title":"ServerEndpoint"},{"location":"types/0/0030-hosts-and-platforms/","text":"0030 Hosts and Platforms \u00b6 The host and platform metadata entities provide a simple model for the IT infrastructure (nodes, computers, etc) that data resources are hosted on. ITInfrastructure \u00b6 ITInfrastructure is a type of Asset that supports the running of software systems. Host \u00b6 In today's systems, hardware is managed to get the maximum use out of it. Therefore, the concept of a Host is abstracted to describe a deployment environment that has access to hardware and has a basic software stack, typically including the operating systems. The host can be linked to its location through the AssetLocation relationship. OperatingPlatform \u00b6 The OperatingPlatform is an informational structure to describe the hardware characteristics and software stack (operating system, etc) of the host. OperatingPlatformManifest \u00b6 Details of the software stack can be captured in a Collection linked to the operating platform using the OperatingPlatformManifest . The collection may contain different types of details such as configuration files and software packages that can be organized into nested collections. SoftwarePackageManifest \u00b6 Collections that list software packages can be classified with the SoftwarePackageManifest classification. Many hosts could have the same operating platform. This means it can be used to represent standardized software stacks and which hosts they have been deployed to. Pipelines that manage the software stacks on these machines can use these elements to manage the rollout and update of the different software packages. Further information 0035 Complex Hosts describes how hardware is virtualized. 0037 Software Server Platform describes the software process that run on a host.","title":"Hosts and Platforms"},{"location":"types/0/0030-hosts-and-platforms/#0030-hosts-and-platforms","text":"The host and platform metadata entities provide a simple model for the IT infrastructure (nodes, computers, etc) that data resources are hosted on.","title":"0030 Hosts and Platforms"},{"location":"types/0/0030-hosts-and-platforms/#itinfrastructure","text":"ITInfrastructure is a type of Asset that supports the running of software systems.","title":"ITInfrastructure"},{"location":"types/0/0030-hosts-and-platforms/#host","text":"In today's systems, hardware is managed to get the maximum use out of it. Therefore, the concept of a Host is abstracted to describe a deployment environment that has access to hardware and has a basic software stack, typically including the operating systems. The host can be linked to its location through the AssetLocation relationship.","title":"Host"},{"location":"types/0/0030-hosts-and-platforms/#operatingplatform","text":"The OperatingPlatform is an informational structure to describe the hardware characteristics and software stack (operating system, etc) of the host.","title":"OperatingPlatform"},{"location":"types/0/0030-hosts-and-platforms/#operatingplatformmanifest","text":"Details of the software stack can be captured in a Collection linked to the operating platform using the OperatingPlatformManifest . The collection may contain different types of details such as configuration files and software packages that can be organized into nested collections.","title":"OperatingPlatformManifest"},{"location":"types/0/0030-hosts-and-platforms/#softwarepackagemanifest","text":"Collections that list software packages can be classified with the SoftwarePackageManifest classification. Many hosts could have the same operating platform. This means it can be used to represent standardized software stacks and which hosts they have been deployed to. Pipelines that manage the software stacks on these machines can use these elements to manage the rollout and update of the different software packages. Further information 0035 Complex Hosts describes how hardware is virtualized. 0037 Software Server Platform describes the software process that run on a host.","title":"SoftwarePackageManifest"},{"location":"types/0/0035-complex-hosts/","text":"0035 Complex Hosts \u00b6 In today's systems, hardware is managed to get the maximum use out of it. Therefore, the concept of a host is typically virtualized to allow a single computer to be used for many hosts and for multiple computers to collectively support a single host. The complex hosts handle environments where many nodes are acting together as a cluster, and where virtualized containers (such as Docker) are being used. BareMetalComputer \u00b6 A BareMetalComputer describes a connected set of physical hardware. The open metadata types today do not attempt to model hardware in detail but this could be easily added if a contributor with the appropriate expertise was willing to work on it. VirtualMachine \u00b6 A VirtualMachine provides virtualized hardware through a hypervisor that allows a single physical bare metal computer to run multiple virtual machines. VirtualContainer \u00b6 A VirtualContainer provides the services of a host to the software servers deployed on it. When the server makes requests for storage, network access, etc, the VirtualContainer delegates the requests to the equivalent services of the actual host it is deployed on. VirtualContainer s can be hosted on other VirtualContainer s, but to actually run they need to ultimately be deployed onto a real physical Host . DockerContainer \u00b6 DockerContainer provides a specific type for the popular container type called docker . HostCluster \u00b6 A HostCluster describes a collection of hosts that together are providing a service. Clusters are often used to provide horizontal scaling of services. There are two specific types of host clusters defined: in both, the hosts that they manage are often referred to as nodes . Within the host cluster is typically a special host (node) that is controlling the execution of the other members. This host is modelled with a SoftwareServerPlatform that describes the cluster management platform, and a SoftwareServer that groups the SoftwareServerCapabilities needed to manage the cluster. These software server capabilities are linked to the ITInfrastructure assets that are being managed by the cluster using the ServerAssetUse relationship. HadoopCluster \u00b6 HadoopCluster describes a Hadoop cluster that uses multiple bare metal computers/virtual machines to manage big data workloads. KuberenetesCluster \u00b6 KubernetesCluster describes a Kubernetes cluster that manages containerized applications across multiple bare metal computers/virtual machines. The containerized applications managed by Kubernetes are represented as VirtualContainer s. HostedHost \u00b6 The hosts can actually be virtualized through many levels. The HostedHost relationship is used to represent the layers of virtualized hosts. HostClusterMember \u00b6 The host cluster is linked to the hosts it is managing using the HostClusterMember relationship. Deprecated types DeployedVirtualContainer - use HostedHost , which is more general.","title":"Complex Hosts"},{"location":"types/0/0035-complex-hosts/#0035-complex-hosts","text":"In today's systems, hardware is managed to get the maximum use out of it. Therefore, the concept of a host is typically virtualized to allow a single computer to be used for many hosts and for multiple computers to collectively support a single host. The complex hosts handle environments where many nodes are acting together as a cluster, and where virtualized containers (such as Docker) are being used.","title":"0035 Complex Hosts"},{"location":"types/0/0035-complex-hosts/#baremetalcomputer","text":"A BareMetalComputer describes a connected set of physical hardware. The open metadata types today do not attempt to model hardware in detail but this could be easily added if a contributor with the appropriate expertise was willing to work on it.","title":"BareMetalComputer"},{"location":"types/0/0035-complex-hosts/#virtualmachine","text":"A VirtualMachine provides virtualized hardware through a hypervisor that allows a single physical bare metal computer to run multiple virtual machines.","title":"VirtualMachine"},{"location":"types/0/0035-complex-hosts/#virtualcontainer","text":"A VirtualContainer provides the services of a host to the software servers deployed on it. When the server makes requests for storage, network access, etc, the VirtualContainer delegates the requests to the equivalent services of the actual host it is deployed on. VirtualContainer s can be hosted on other VirtualContainer s, but to actually run they need to ultimately be deployed onto a real physical Host .","title":"VirtualContainer"},{"location":"types/0/0035-complex-hosts/#dockercontainer","text":"DockerContainer provides a specific type for the popular container type called docker .","title":"DockerContainer"},{"location":"types/0/0035-complex-hosts/#hostcluster","text":"A HostCluster describes a collection of hosts that together are providing a service. Clusters are often used to provide horizontal scaling of services. There are two specific types of host clusters defined: in both, the hosts that they manage are often referred to as nodes . Within the host cluster is typically a special host (node) that is controlling the execution of the other members. This host is modelled with a SoftwareServerPlatform that describes the cluster management platform, and a SoftwareServer that groups the SoftwareServerCapabilities needed to manage the cluster. These software server capabilities are linked to the ITInfrastructure assets that are being managed by the cluster using the ServerAssetUse relationship.","title":"HostCluster"},{"location":"types/0/0035-complex-hosts/#hadoopcluster","text":"HadoopCluster describes a Hadoop cluster that uses multiple bare metal computers/virtual machines to manage big data workloads.","title":"HadoopCluster"},{"location":"types/0/0035-complex-hosts/#kuberenetescluster","text":"KubernetesCluster describes a Kubernetes cluster that manages containerized applications across multiple bare metal computers/virtual machines. The containerized applications managed by Kubernetes are represented as VirtualContainer s.","title":"KuberenetesCluster"},{"location":"types/0/0035-complex-hosts/#hostedhost","text":"The hosts can actually be virtualized through many levels. The HostedHost relationship is used to represent the layers of virtualized hosts.","title":"HostedHost"},{"location":"types/0/0035-complex-hosts/#hostclustermember","text":"The host cluster is linked to the hosts it is managing using the HostClusterMember relationship. Deprecated types DeployedVirtualContainer - use HostedHost , which is more general.","title":"HostClusterMember"},{"location":"types/0/0036-storage/","text":"0036 Storage \u00b6 It is common for the processing running on a Host to need to persist data to storage. StorageVolume \u00b6 StorageVolume describes a persistent storage volume. AttachedStorage \u00b6 AttachedStorage identifies the host(s) that the StorageVolume is connected to.","title":"Storage"},{"location":"types/0/0036-storage/#0036-storage","text":"It is common for the processing running on a Host to need to persist data to storage.","title":"0036 Storage"},{"location":"types/0/0036-storage/#storagevolume","text":"StorageVolume describes a persistent storage volume.","title":"StorageVolume"},{"location":"types/0/0036-storage/#attachedstorage","text":"AttachedStorage identifies the host(s) that the StorageVolume is connected to.","title":"AttachedStorage"},{"location":"types/0/0037-software-server-platforms/","text":"0037 Software Server Platforms \u00b6 Software servers often use a software server platform to provide many of the services they use. The OMAG Server Platform is an example of a software server platform.","title":"Software Server Platforms"},{"location":"types/0/0037-software-server-platforms/#0037-software-server-platforms","text":"Software servers often use a software server platform to provide many of the services they use. The OMAG Server Platform is an example of a software server platform.","title":"0037 Software Server Platforms"},{"location":"types/0/0040-software-servers/","text":"0040 Software Servers \u00b6 Software servers describe the middleware software servers (such as application servers, data movement engines and database servers) that run on the Host s . SoftwareServer \u00b6 Within the SoftwareServer model we capture the userId that it operates under. Most metadata repositories are run in a secure mode requiring incoming requests to include the requester's security credentials. Therefore, we have an identifier for each unique logged on security identity ( userId ). This identity is recorded within specific entities and relationships when they are created or updated. By storing the user identifier for the server, it is possible to correlate the server with the changes to the metadata (and related data assets) that it makes. Further information See model 0110 Actors and 0117 IT Profiles for details of how user identifiers are correlated with ActorProfiles for people and teams. The ITProfile makes it possible to define a profile for a server's userId so that additional information about the userId can be captured. An OMAG Server is an example of a SoftwareServer .","title":"Software Servers"},{"location":"types/0/0040-software-servers/#0040-software-servers","text":"Software servers describe the middleware software servers (such as application servers, data movement engines and database servers) that run on the Host s .","title":"0040 Software Servers"},{"location":"types/0/0040-software-servers/#softwareserver","text":"Within the SoftwareServer model we capture the userId that it operates under. Most metadata repositories are run in a secure mode requiring incoming requests to include the requester's security credentials. Therefore, we have an identifier for each unique logged on security identity ( userId ). This identity is recorded within specific entities and relationships when they are created or updated. By storing the user identifier for the server, it is possible to correlate the server with the changes to the metadata (and related data assets) that it makes. Further information See model 0110 Actors and 0117 IT Profiles for details of how user identifiers are correlated with ActorProfiles for people and teams. The ITProfile makes it possible to define a profile for a server's userId so that additional information about the userId can be captured. An OMAG Server is an example of a SoftwareServer .","title":"SoftwareServer"},{"location":"types/0/0042-software-server-capabilities/","text":"0042 Software Server Capabilities \u00b6 SoftwareServerCapability \u00b6 Within a software server are many capabilities. Different organizations and tools can choose the granularity in which the capabilities are captured in order to provide appropriate context to data assets and the decisions made around them. These are the software server capabilities defined in the open types: APIManager - A capability that manages callable APIs that typically delegate onto Software Services. Application - A capability supporting a specific business function. Catalog - A capability that manages collections of descriptions about people, places, digital assets, things, ... DataManager - A capability that manages collections of data. Engine - A programmable engine for running automated processes. WorkflowEngine - An engine capable of running a mixture of human and automated tasks as part of a workflow process. ReportingEngine - An engine capable of creating reports by combining information from multiple data sets. AnalyticsEngine - An engine capable of running analytics models using data from one or more data sets. DataMovementEngine - An engine capable of copying data from one data store to another. DataVirtualizationEngine - An engine capable of creating new data sets by dynamically combining data from one or more data stores or data sets. EventBroker - A capability that supports event-based services, typically around topics. SoftwareService s - A capability that provides externally callable functions to other services. ApplicationService - A software service that supports a reusable business function. MetadataIntegrationService - A software service that exchanges metadata between servers. MetadataAccessService - A software service that provides access to stored metadata. EngineHostingService - A software service that provides services that delegate to a hosted engine. UserViewService - A software service that provides user interfaces access to digital resources. NetworkGateway - A connection point enabling network traffic to pass between two networks. DatabaseManager - A capability that manages data organized as relational schemas. EnterpriseAccessLayer - Repository services for the Open Metadata Access Services ( OMAS ) supporting federated queries and aggregated events from the connected cohorts. CohortMember - A capability enabling a server to access an open metadata repository cohort. GovernanceEngine - A collection of related governance services of the same type. GovernanceActionEngine - A collection of related governance services supporting the Governance Action Framework ( GAF ). OpenDiscoveryEngine - A collection of related governance services supporting the Open Discovery Framework ( ODF ). In addition, it is possible to augment software server capabilities with the following classifications: CloudService - A capability enabled for a tenant on a cloud platform. ContentCollectionManager - A manager of controlled documents and related media. FileSystem - A capability that supports a store of files organized into a hierarchy of file folders for general use. FileManager - A manager of a collection of files and folders. NotificationManager - A server capability that is distributing events from a topic to its subscriber list.","title":"Software Server Capabilities"},{"location":"types/0/0042-software-server-capabilities/#0042-software-server-capabilities","text":"","title":"0042 Software Server Capabilities"},{"location":"types/0/0042-software-server-capabilities/#softwareservercapability","text":"Within a software server are many capabilities. Different organizations and tools can choose the granularity in which the capabilities are captured in order to provide appropriate context to data assets and the decisions made around them. These are the software server capabilities defined in the open types: APIManager - A capability that manages callable APIs that typically delegate onto Software Services. Application - A capability supporting a specific business function. Catalog - A capability that manages collections of descriptions about people, places, digital assets, things, ... DataManager - A capability that manages collections of data. Engine - A programmable engine for running automated processes. WorkflowEngine - An engine capable of running a mixture of human and automated tasks as part of a workflow process. ReportingEngine - An engine capable of creating reports by combining information from multiple data sets. AnalyticsEngine - An engine capable of running analytics models using data from one or more data sets. DataMovementEngine - An engine capable of copying data from one data store to another. DataVirtualizationEngine - An engine capable of creating new data sets by dynamically combining data from one or more data stores or data sets. EventBroker - A capability that supports event-based services, typically around topics. SoftwareService s - A capability that provides externally callable functions to other services. ApplicationService - A software service that supports a reusable business function. MetadataIntegrationService - A software service that exchanges metadata between servers. MetadataAccessService - A software service that provides access to stored metadata. EngineHostingService - A software service that provides services that delegate to a hosted engine. UserViewService - A software service that provides user interfaces access to digital resources. NetworkGateway - A connection point enabling network traffic to pass between two networks. DatabaseManager - A capability that manages data organized as relational schemas. EnterpriseAccessLayer - Repository services for the Open Metadata Access Services ( OMAS ) supporting federated queries and aggregated events from the connected cohorts. CohortMember - A capability enabling a server to access an open metadata repository cohort. GovernanceEngine - A collection of related governance services of the same type. GovernanceActionEngine - A collection of related governance services supporting the Governance Action Framework ( GAF ). OpenDiscoveryEngine - A collection of related governance services supporting the Open Discovery Framework ( ODF ). In addition, it is possible to augment software server capabilities with the following classifications: CloudService - A capability enabled for a tenant on a cloud platform. ContentCollectionManager - A manager of controlled documents and related media. FileSystem - A capability that supports a store of files organized into a hierarchy of file folders for general use. FileManager - A manager of a collection of files and folders. NotificationManager - A server capability that is distributing events from a topic to its subscriber list.","title":"SoftwareServerCapability"},{"location":"types/0/0045-servers-and-assets/","text":"0045 Servers and Assets \u00b6 ServerAssetUse \u00b6 Asset s are managed or consumed by SoftwareServerCapabilities , which is captured by the ServerAssetUse relationship.","title":"Servers and Assets"},{"location":"types/0/0045-servers-and-assets/#0045-servers-and-assets","text":"","title":"0045 Servers and Assets"},{"location":"types/0/0045-servers-and-assets/#serverassetuse","text":"Asset s are managed or consumed by SoftwareServerCapabilities , which is captured by the ServerAssetUse relationship.","title":"ServerAssetUse"},{"location":"types/0/0050-applications-and-processes/","text":"0050 Applications and Processes \u00b6 Application \u00b6 Application s provide business or management logic. They are often custom-built but may also be brought as a package. They are deployed onto a server as a SoftwareServerCapability . Deprecated types The RuntimeForProcess relationship is superfluous: use ServerAssetUse since Application is a SoftwareServerCapability .","title":"Applications and Processes"},{"location":"types/0/0050-applications-and-processes/#0050-applications-and-processes","text":"","title":"0050 Applications and Processes"},{"location":"types/0/0050-applications-and-processes/#application","text":"Application s provide business or management logic. They are often custom-built but may also be brought as a package. They are deployed onto a server as a SoftwareServerCapability . Deprecated types The RuntimeForProcess relationship is superfluous: use ServerAssetUse since Application is a SoftwareServerCapability .","title":"Application"},{"location":"types/0/0055-data-processing-engines/","text":"0055 Data Processing Engines \u00b6 Engine \u00b6 The Engine entity represents a programmable engine for running automated processes. Classifications \u00b6 WorkflowEngine \u00b6 The WorkflowEngine classification designates an engine as capable of running a mixture of human and automated tasks as part of a workflow process. ReportingEngine \u00b6 The ReportingEngine classification designates an engine as capable of creating reports by combining information from multiple data sets. AnalyticsEngine \u00b6 The AnalyticsEngine classification designates an engine as capable of running analytics models and using data from one or more data sets. DataMovementEngine \u00b6 The DataMovementEngine classification designates an engine as capable of copying data from one data store to another. DataVirtualizationEngine \u00b6 The DataVirtualizationEngine classification designates an engine as capable of creating new data sets by dynamically combining data from one or more data stores or data sets.","title":"Data Processing Engines"},{"location":"types/0/0055-data-processing-engines/#0055-data-processing-engines","text":"","title":"0055 Data Processing Engines"},{"location":"types/0/0055-data-processing-engines/#engine","text":"The Engine entity represents a programmable engine for running automated processes.","title":"Engine"},{"location":"types/0/0055-data-processing-engines/#classifications","text":"","title":"Classifications"},{"location":"types/0/0055-data-processing-engines/#workflowengine","text":"The WorkflowEngine classification designates an engine as capable of running a mixture of human and automated tasks as part of a workflow process.","title":"WorkflowEngine"},{"location":"types/0/0055-data-processing-engines/#reportingengine","text":"The ReportingEngine classification designates an engine as capable of creating reports by combining information from multiple data sets.","title":"ReportingEngine"},{"location":"types/0/0055-data-processing-engines/#analyticsengine","text":"The AnalyticsEngine classification designates an engine as capable of running analytics models and using data from one or more data sets.","title":"AnalyticsEngine"},{"location":"types/0/0055-data-processing-engines/#datamovementengine","text":"The DataMovementEngine classification designates an engine as capable of copying data from one data store to another.","title":"DataMovementEngine"},{"location":"types/0/0055-data-processing-engines/#datavirtualizationengine","text":"The DataVirtualizationEngine classification designates an engine as capable of creating new data sets by dynamically combining data from one or more data stores or data sets.","title":"DataVirtualizationEngine"},{"location":"types/0/0056-asset-managers/","text":"0056 Asset Managers \u00b6 AssetManager \u00b6 The AssetManager classification represents a technology that manages metadata about assets and may also provide services to manage and/or govern the assets themselves (or at least track such actions). Data catalogs and other types of metadata catalogs are examples of asset managers. Examples of asset managers Amundsen , Marquez and Apache Atlas are examples of data catalogs, and therefore of asset managers. An Egeria deployment using a metadata server and one or more integration daemons can also be enabled as an asset manager. The AssetManager classification on a SoftwareServerCapability entity is used by the Asset Manager OMAS to represent the third party asset manager that it is exchanging metadata with. Identities from this third party asset manager are linked to the AssetManager entity using the ExternalIdScope relationship. UserProfileManager \u00b6 The UserProfileManager classification describes a system that manages user profile information - such as a company directory. UserAccessManager \u00b6 The UserAccessManager classification describes a user directory such as LDAP. MasterDataManager \u00b6 A MasterDataManager classification describes a server that manages the rationalization of master data stored in many systems.","title":"0056 asset managers"},{"location":"types/0/0056-asset-managers/#0056-asset-managers","text":"","title":"0056 Asset Managers"},{"location":"types/0/0056-asset-managers/#assetmanager","text":"The AssetManager classification represents a technology that manages metadata about assets and may also provide services to manage and/or govern the assets themselves (or at least track such actions). Data catalogs and other types of metadata catalogs are examples of asset managers. Examples of asset managers Amundsen , Marquez and Apache Atlas are examples of data catalogs, and therefore of asset managers. An Egeria deployment using a metadata server and one or more integration daemons can also be enabled as an asset manager. The AssetManager classification on a SoftwareServerCapability entity is used by the Asset Manager OMAS to represent the third party asset manager that it is exchanging metadata with. Identities from this third party asset manager are linked to the AssetManager entity using the ExternalIdScope relationship.","title":"AssetManager"},{"location":"types/0/0056-asset-managers/#userprofilemanager","text":"The UserProfileManager classification describes a system that manages user profile information - such as a company directory.","title":"UserProfileManager"},{"location":"types/0/0056-asset-managers/#useraccessmanager","text":"The UserAccessManager classification describes a user directory such as LDAP.","title":"UserAccessManager"},{"location":"types/0/0056-asset-managers/#masterdatamanager","text":"A MasterDataManager classification describes a server that manages the rationalization of master data stored in many systems.","title":"MasterDataManager"},{"location":"types/0/0057-integration-capabilities/","text":"0057 Integration Capabilities \u00b6 SoftwareService \u00b6 A SoftwareService provides a well-defined software component that can be called by remote clients across the network. They may offer a request-response or an event-driven interface or both. ApplicationService \u00b6 Typically, software services implement specific business functions such as on-boarding a new customer, taking an order or sending an invoice. These are called ApplicationService s Egeria offers specialized software services related to the capture and management of open metadata. These are shown as specialist types: MetadataIntegrationService \u00b6 A MetadataIntegrationService describes an Open Metadata Integration Service ( OMIS ) that runs in an integration daemon . MetadataAccessService \u00b6 A MetadataAccessService describes an Open Metadata Access Service ( OMAS ) that runs in a metadata access point . EngineHostingService \u00b6 An EngineHostingService describes an Open Metadata Engine Service ( OMES ) that runs in an engine host . UserViewService \u00b6 A UserViewService describes an Open Metadata View Service ( OMVS ) that runs in a view server .","title":"Integration Capabilities"},{"location":"types/0/0057-integration-capabilities/#0057-integration-capabilities","text":"","title":"0057 Integration Capabilities"},{"location":"types/0/0057-integration-capabilities/#softwareservice","text":"A SoftwareService provides a well-defined software component that can be called by remote clients across the network. They may offer a request-response or an event-driven interface or both.","title":"SoftwareService"},{"location":"types/0/0057-integration-capabilities/#applicationservice","text":"Typically, software services implement specific business functions such as on-boarding a new customer, taking an order or sending an invoice. These are called ApplicationService s Egeria offers specialized software services related to the capture and management of open metadata. These are shown as specialist types:","title":"ApplicationService"},{"location":"types/0/0057-integration-capabilities/#metadataintegrationservice","text":"A MetadataIntegrationService describes an Open Metadata Integration Service ( OMIS ) that runs in an integration daemon .","title":"MetadataIntegrationService"},{"location":"types/0/0057-integration-capabilities/#metadataaccessservice","text":"A MetadataAccessService describes an Open Metadata Access Service ( OMAS ) that runs in a metadata access point .","title":"MetadataAccessService"},{"location":"types/0/0057-integration-capabilities/#enginehostingservice","text":"An EngineHostingService describes an Open Metadata Engine Service ( OMES ) that runs in an engine host .","title":"EngineHostingService"},{"location":"types/0/0057-integration-capabilities/#userviewservice","text":"A UserViewService describes an Open Metadata View Service ( OMVS ) that runs in a view server .","title":"UserViewService"},{"location":"types/0/0070-networks-and-gateways/","text":"0070 Networks and Gateways \u00b6 The network model for open metadata is very simple, to allow hosts to be grouped into the networks they are connected to. This can show details such as where hosts are isolated in private networks, and where the gateways onto the Internet are.","title":"Networks and Gateways"},{"location":"types/0/0070-networks-and-gateways/#0070-networks-and-gateways","text":"The network model for open metadata is very simple, to allow hosts to be grouped into the networks they are connected to. This can show details such as where hosts are isolated in private networks, and where the gateways onto the Internet are.","title":"0070 Networks and Gateways"},{"location":"types/0/0090-cloud-platforms-and-services/","text":"0090 Cloud Platforms and Services \u00b6 The cloud platforms and services model shows that cloud computing is not so different from what we have been doing before. Cloud infrastructure and services are classified as such to show that the organization is not completely in control of the technology supporting their data and processes. CloudProvider \u00b6 The CloudProvider is the organization that provides and runs the infrastructure for a cloud service. Typically, the host it offers is actually a HostCluster . The cloud provider may offer infrastructure as a service (IaaS), in which case, an organization can deploy VirtualContainer s onto the cloud provider's HostCluster . CloudPlatform \u00b6 If the cloud provider is offering platform as a service (PaaS), an application may deploy server capability onto the CloudPlatform . CloudService \u00b6 If the cloud provider is offering Software as a Service (SaaS) then it has provided APIs backed by pre-deployed server capability that an organization can call as a CloudService .","title":"Cloud Platforms and Services"},{"location":"types/0/0090-cloud-platforms-and-services/#0090-cloud-platforms-and-services","text":"The cloud platforms and services model shows that cloud computing is not so different from what we have been doing before. Cloud infrastructure and services are classified as such to show that the organization is not completely in control of the technology supporting their data and processes.","title":"0090 Cloud Platforms and Services"},{"location":"types/0/0090-cloud-platforms-and-services/#cloudprovider","text":"The CloudProvider is the organization that provides and runs the infrastructure for a cloud service. Typically, the host it offers is actually a HostCluster . The cloud provider may offer infrastructure as a service (IaaS), in which case, an organization can deploy VirtualContainer s onto the cloud provider's HostCluster .","title":"CloudProvider"},{"location":"types/0/0090-cloud-platforms-and-services/#cloudplatform","text":"If the cloud provider is offering platform as a service (PaaS), an application may deploy server capability onto the CloudPlatform .","title":"CloudPlatform"},{"location":"types/0/0090-cloud-platforms-and-services/#cloudservice","text":"If the cloud provider is offering Software as a Service (SaaS) then it has provided APIs backed by pre-deployed server capability that an organization can call as a CloudService .","title":"CloudService"},{"location":"types/1/","text":"Area 1 Models - Collaboration \u00b6 Area 1 collects information from people using their organization's assets. It includes their use of the assets and their feedback. It also manages crowd-sourced recommended enhancements to the metadata from other areas before it is approved and incorporated into the governance program. 0110 Actors - generic description of a profile describing a person, team or engine. 0112 People - profiles and roles for people 0115 Teams - team profiles and structures 0117 IT Profiles - profiles for engines 0130 Projects - projects for organizing change to the IT landscape 0135 Meetings - meetings where ideas, situations and plans are discussed and reviewed 0137 Actions for People - descriptions of actions for people 0140 Communities - communities of like-minded people 0150 Feedback - comments, likes, reviews and ratings 0155 Crowd Sourcing - collecting ideas from subject matter experts 0160 Notes - maintaining notes and note logs","title":"Area 1 Collaboration"},{"location":"types/1/#area-1-models-collaboration","text":"Area 1 collects information from people using their organization's assets. It includes their use of the assets and their feedback. It also manages crowd-sourced recommended enhancements to the metadata from other areas before it is approved and incorporated into the governance program. 0110 Actors - generic description of a profile describing a person, team or engine. 0112 People - profiles and roles for people 0115 Teams - team profiles and structures 0117 IT Profiles - profiles for engines 0130 Projects - projects for organizing change to the IT landscape 0135 Meetings - meetings where ideas, situations and plans are discussed and reviewed 0137 Actions for People - descriptions of actions for people 0140 Communities - communities of like-minded people 0150 Feedback - comments, likes, reviews and ratings 0155 Crowd Sourcing - collecting ideas from subject matter experts 0160 Notes - maintaining notes and note logs","title":"Area 1 Models - Collaboration"},{"location":"types/1/0110-actors/","text":"0110 Actors \u00b6 Most metadata repositories are run in a secure mode requiring incoming requests to include the requester\u2019s security credentials. Therefore we have an identifier for each unique logged on security identity (aka userId). This identity is recorded with specific entities and relationships when they are created or updated. The userId for a server is also captured in the metadata model (see 0040 Servers in Area 0 ) so it is possible to correlate the actions of a data processing server with changes to the metadata. UserIdentity \u00b6 UserIdentity provides a structure for storing the security authentication information about a person. Initially we have a simple string for the userId - but this could be extended to include more sophisticated identification information. ActorProfile \u00b6 An ActorProfile describes the actual person, or possibly team if group userIds are being used, that is working either with the data assets or with the metadata directly. The profile is a record to add additional information about the person or engine that is making the requests. They may have more than one UserIdentity. Actors are associated with the new metadata that they create and comment on via their user identities. More information about the person behind the user identity is available through the ActorProfile. This separation is maintained because the user identity is the only information available on calls to the metadata repository. The ActorProfile is used to aggregate the activity of the individual or team (or IT infrastructure - see ITProfile ). This includes crowd-sourcing and project participation.","title":"Actors"},{"location":"types/1/0110-actors/#0110-actors","text":"Most metadata repositories are run in a secure mode requiring incoming requests to include the requester\u2019s security credentials. Therefore we have an identifier for each unique logged on security identity (aka userId). This identity is recorded with specific entities and relationships when they are created or updated. The userId for a server is also captured in the metadata model (see 0040 Servers in Area 0 ) so it is possible to correlate the actions of a data processing server with changes to the metadata.","title":"0110 Actors"},{"location":"types/1/0110-actors/#useridentity","text":"UserIdentity provides a structure for storing the security authentication information about a person. Initially we have a simple string for the userId - but this could be extended to include more sophisticated identification information.","title":"UserIdentity"},{"location":"types/1/0110-actors/#actorprofile","text":"An ActorProfile describes the actual person, or possibly team if group userIds are being used, that is working either with the data assets or with the metadata directly. The profile is a record to add additional information about the person or engine that is making the requests. They may have more than one UserIdentity. Actors are associated with the new metadata that they create and comment on via their user identities. More information about the person behind the user identity is available through the ActorProfile. This separation is maintained because the user identity is the only information available on calls to the metadata repository. The ActorProfile is used to aggregate the activity of the individual or team (or IT infrastructure - see ITProfile ). This includes crowd-sourcing and project participation.","title":"ActorProfile"}]}